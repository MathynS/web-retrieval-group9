query sentence: branch-and-bound technique for algorithms
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 2126-very-loopy-belief-propagation-for-unwrapping-phase-images.pdf

Very loopy belief propagation for
unwrapping phase images
Brendan Freyl Ralf Koetter2 Nemanja Petrovic
Probabilistic and Statistical Inference Group University of Toronto
http://www.psi.toronto.edu
Electrical and Computer Engineering University of Illinois at Urbana
Abstract
Since the discovery that the best error-correcting decoding algorithm can be viewed as belief propagation in a cycle-bound graph
researchers have been trying to determine under what circumstances loopy belief propagation is effective for probabilistic inference Despite several theoretical advances in our understanding of
loopy belief propagation to our knowledge the only problem that
has been solved using loopy belief propagation is error-correcting
decoding on Gaussian channels We propose a new representation
for the two-dimensional phase unwrapping problem and we show
that loopy belief propagation produces results that are superior to
existing techniques This is an important result since many imaging techniques including magnetic resonance imaging and interferometric synthetic aperture radar produce phase-wrapped images
Interestingly the graph that we use has a very large number of
very short cycles supporting evidence that a large minimum cycle
length is not needed for excellent results using belief propagation
Introduction
Phase unwrapping is an easily stated fundamental problem in image processing
Ghiglia and Pritt Each real-valued observation on a or 2-dimensional
grid is measured modulus a known wavelength which we take to be without loss
of generality Ib shows the wrapped I-dimensional waveform obtained from the
original waveform shown in la Every time the original waveform goes above
or below it is wrapped to or respectively The goal of phase unwrapping is to
infer the original unwrapped curve from the wrapped measurements using using
knowledge about which signals are more probable a priori
In two dimensions exact phase unwrapping is exponentially more difficult than 1dimensional phase unwrapping and has been shown to be NP-hard in general Chen
and Zebker lc shows the wrapped output of a magnetic resonance
imaging device courtesy of Liang Notice the fringe lines boundaries
across which wrappings have occurred Id shows the wrapped terrain height
measurements from an interferometric synthetic aperture radar courtesy of Sandia
National Laboratories New Mexico
Figure A waveform measured on a 1-dimensional grid The phase-wrapped version
ofthe waveform in where the wavelength is A wrapped intensity ma from a magnetic
resonance imaging device measured on a 2-dimensional grid courtesy of Liang
A wrapped topographic map measured on a 2-dimensional grid courtesy of Sandia National
Laboratories New Mexico
A sensible goal in phase unwrapping is to infer the gradient field of the original
surface The surface can then be reconstructed by integration Equivalently the
goal is to infer the number of relative wrappings or integer shifts between every
pair of neighboring measurements Positive shifts correspond to an increase in the
number of wrappings in the direction of the or coordinate whereas negative
shifts correspond to a decrease in the number of wrappings in the direction of the
or coordinate After arbitrarily assigning an absolute number of wrappings to
one point the absolute number of wrappings at any other point can be determined
by summing the shifts along a path connecting the two points To account for
direction when taking a step against the direction of the coordinate the shift
should be subtracted
When neighboring measurements are more likely closer together than farther apart
a priori I-dimensional waveforms can be unwrapped optimally in time that is linear
in the waveform length For every pair of neighboring measurements the shift that
makes the unwrapped values as close together as possible is chosen For example
the shift between and would be whereas the shift between and
would be
For 2-dimensional surfaces and images there are many possible I-dimensional paths
between any two points These paths should be examined in combination since the
sum of the shifts along every such path should be equal Viewing the shifts as state
variables the cut-set between any two points is exponential in the size of the grid
making exact inference for general priors NP-hard Chen and Zebker
The two leading fully-automated techniques for phase unwrapping are the least
squares method and the branch cut technique Ghiglia and Pritt Some other
techniques perform better in some circumstances but need additional information
or require hand-tweaking The least squares method begins by making a greedy
guess at the gradient between every pair of neighboring points The resulting vector
field is not the gradient field of a surface since in a valid gradient field the sum of
the gradients around every closed loop must be zero that is the curl must be
For example the loop of measurements will lead to gradients of
around the loop which do not sum to The least squares method
proceeds by projecting the vector field onto the linear subspace of gradient fields
The result is integrated to produce the surface The branch cut technique also
begins with greedy decisions for the gradients and then identifies untrustworthy
regions of the image whose gradients should not be used during integration As
shown in our results section both of these techniques are suboptimal
Previously we attempted to use a relaxed mean field technique to solve this problem
Achan Frey and Koetter Here we take a new approach that works better
and is motivated by the impressive results of belief propagation in cycle-bound
graphs for error-correcting decoding Wiberg Loeliger and Koetter MacKay
and Neal Frey and Kschischang Kschischang and Frey McEliece
MacKay and Cheng In contrast to other work Ghiglia and Pritt
Chen and Zebker Koetter we introduce a new framework for
quantitative evaluation which impressively places belief propagation much closer
to the theoretical limit than other leading methods
It is well-known that belief propagation the sum-product algorithm probability propagation is exact in graphs that are trees Pearl but it has been
discovered only recently that it can produce excellent results in graphs with many
cycles Impressive results have been obtained using loopy belief propagation for
super-resolution Freeman and Pasztor and for infering layered representations of scenes Frey However despite several theoretical advances in our
understanding of loopy belief propagation Weiss and Freeman and proposals for modifications to the algorithm Yedidia Freeman and Weiss
to our knowledge the only problem that has been solved by loopy belief propagation
is error-correcting decoding on Gaussian channels
We conjecture that although phase unwrapping is generally NP-hard there exists a
near-optimal phase unwrapping algorithm for Gaussian process priors Further we
believe that algorithm to be loopy belief propagation
Loopy Belief Propagation for Phase Unwrapping
As described above the goal is to infer the number of relative wrappings or integer
shifts between every pair of neighboring measurements Denote the x-direction
shift at by a(x and the y-direction shift at by b(x as shown in
Fig.2a If the sum of the shifts around every short loop of shifts
b(x a(x in is zero then perturbing a path will
not change the sum of the shifts along the path So a valid set of shifts
b(x in an image must
satisfy the constraint
b(x
a(x,y
for Since
is a measure of curl at we refer to as a zero-curl constraint reflecting
the fact that the curl of a gradient field is In this way phase unwrapping is
formulated as the problem of inferring the most probable set of shifts subject to
satisfying all zero-curl constraints
We assume that given the set of shifts the unwrapped surface is described by a loworder Gaussian process The joint distribution over the shifts
I and the wrapped measurements
x-direction shifts
t5
Vi
I
I
fl
fl
it2
it
Figure Positive x-direction shifts arrows labeled and positive y-direction shifts
arrows labeled between neighboring measurements in a patch of points marked by
A graphical model that describes the zero-curl constraints black discs between
neighboring shift variables white discs 3-element probability vectors on the relative
shifts between neighboring variables or are propagated across the network
Constraint-to-shift vectors are computed from incoming shift-to-constraint vectors Shiftto-constraint vectors are computed from incoming constraint-to-shift vectors Estimates of
the marginal probabilities of the shifts given the data are computed by combining incoming
constra int-to-sh ift vectors
can be expressed in he form
P(S ex
a(x,y
x=l y=l
N-l
y=l
2u
M-l
2u
y=l
The zero-curl constraints are enforced by which evaluates to if its argument is
oand evaluates to otherwise We assume he slope of he surface is limited so hat
he unknown shifts take on he values and a is he variance between two
neighboring measurements in he unwrapped image but we find hat in practice it
can be estimated directly from he wrapped image
Phase unwrapping consists of making inferences about he a's and b's in he above
probability model For example he marginal probability hat he x-direction shift
at is given an observed wrapped image is
kl<I ex
P(S
S:a(x
For an grid he above sum has roughly terms and so exact inference
is intractable
The factorization of he joint distribution in can be described by a graphical
model as shown in In his graph each white disc sits on he border between
two measurements marked by and corresponds to eit her an x-direction shift
or a y-direction shift Each black disc corresponds to a zero-curl constraint
in and is connected to he shifts hat it constrains to sum to
robability propagation computes messages 3-vectors denoted by which are
passed in both directions on every edge in he network The elements of each 3vector correspond to he allowed values of he neighboring shift and Each
of hese 3-vectors can be hought of as a probability distribut ion over he possible
values hat he shift can take on
Each element in a constraint-to-shift message summarizes the evidence from the
other shifts involved in the constraint and is computed by averaging the allowed
configurations of evidence from the other shifts in the constraint For example if
ILl IL2 and IL3 are 3-vectors entering a constraint as shown in the outgoing
3-vector is computed using
f.t4i
J(k
j)f.tljf.t2kf.t31
and then normalized for numerical stability The other messages produced at the
constraint are computed in a similar fashion
Shift-to-constraint messages are computed by weighting incoming constraint-to-shift
messages with the likelihood for the shift For example if ILl is a 3-vector entering
an x-direction shift as shown in the outgoing 3-vector IL2 is computed using
and then normalized Messages produced by y-direction shifts are computed in a
similar fashion
At any step in the message-passing process the messages on the edges connected
to a shift variable can be combined to produce an approximation to the marginal
probability for that shift given the observations For example if ILl and IL2 are the
3-vectors entering an x-direction shift as shown in the approximation is
P(a(x il<I f.tlif.t2i)/(Lf.tljf.t2j
Given a wrapped image the variance a is estimated directly from the wrapped
image the probability vectors are initialized to uniform distributions and then
probability vectors are propagated across the graph in an iterative fashion Different
message-passing schedules are possible ranging from fully parallel to a forwardbackward-up-down type schedule in which messages are passed across the network
to the right then to the left then up and then down For an grid each
iteration takes O(N scalar computations
After probability propagation converges after a fixed number of iterations
estimates of the marginal probabilities of the shifts given the data are computed and
the most probable value of each shift variable is selected The resulting configuration
of the shifts can then be integrated to obtain the unwrapped surface If some
zero-curl constraints remain violated a robust integration technique such as least
squares integration Ghiglia and Pritt should be used
Experimental results
Generally belief propagation in cycle-bound graphs is not guaranteed to converge
Even if it does converge the approximate marginals may not be close to the true
marginals So the algorithm must be verified by experiments
On surfaces drawn from Gaussian process priors we find that the belief propagation
algorithm produces significantly lower reconstruction errors than the least squares
method and the branch cut technique
Here we focus on the performances of the algorithms for real data recorded from a
synthetic aperture radar device Since our algorithm assumes the surface
is Gaussian given the shifts a valid concern is that it will not perform well when
the Gaussian process prior is incorrect
en
CO
Minimum wavelength
required for error-free
unwrapping using algs
that infer relative
shifts of and
18 22 24 26
Wavelength
Figure After iterations of belief propagation using the phase-wrapped surface from
Id hard decisions were made for the shift variables and the resulting shifts were integrated
to produce this unwrapped surface Reconstruction error versus wrapping wavelength for
our technique the least squares method and the branch cuts technique
3a shows the surface that is obtained by setting to the mean squared difference between neighboring wrapped values applying iterations of belief propagation making hard decisions for the integer shifts and integrating the resulting
gradients Since this is real data we do not know the ground truth However
compared to the least squares method our algorithm preserves more detail The
branch cut technique is not able to unwrap the entire surface
To obtain quantitative results on reconstruction error we use the surface produced
by the least squares method as the ground truth To determine the effect of wrapping wavelength on algorithm performance we rewrap this surface using different
wavelengths For each wavelength we compute the reconstruction error for belief
propagation least squares and branch cuts Note that by using least squares to
obtain the ground truth we may be biasing our results in favor of least squares
3b shows the logarithm of he mean squared error in the gradient field of the
reconstructed surface as a function of the wrapping wavelength on a log-scale
The plot for the mean squared error in he surface heights looks similar As
unwrapping becomes impossible and as unwrapping becomes trivial since
no wrappings occur so algorithms have waterfall-shaped curves
The belief propagation algorithm clearly obtains significantly lower reconstruction
errors Viewed another way belief propagation can tolerate much lower wrapping
wavelengths for a given reconstruction error Also it turns out that for this surface
it is impossible for an algorithm that infers relative shifts of and to obtain a
reconstruction error of unless A Belief propagation obtains a zero-error
wavelength that is significantly closer to this limit than the least squares method
and the branch cuts technique
Conclusions
Phase unwrapping is a fundamental problem in image processing and although it
has been shown to be NP-hard for general priors Chen and Zebker we
conjecture there exists a near-optimal phase unwrapping algorithm for Gaussian
process priors Further we believe that algorithm to be loopy belief propagation
Our experimental results show that loopy belief propagation obtains significantly
lower reconstruction errors compared to the least squares method and the branch
cuts technique Ghiglia and Pritt and performs close to the theoretical limit
for techniques that infer relative wrappings of and The belief propagation
algorithm runs in about the same time as the other techniques

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4338-rapid-deformable-object-detection-using-dual-tree-branch-and-bound.pdf

Rapid Deformable Object Detection using Dual-Tree
Branch-and-Bound
Iasonas Kokkinos
Center for Visual Computing
Ecole Centrale de Paris
iasonas.kokkinos@ecp.fr
Abstract
In this work we use Branch-and-Bound to efficiently detect objects with deformable part models Instead of evaluating the classifier score exhaustively over
image locations and scales we use BB to focus on promising image locations
The core problem is to compute bounds that accommodate part deformations for
this we adapt the Dual Trees data structure to our problem
We evaluate our approach using Mixture-of-Deformable Part Models We obtain exactly the same results but are times faster on average We also develop a multiple-object detection variation of the system where hypotheses for
categories are inserted in a common priority queue For the problem of finding the
strongest category in an image this results in a speedup
Introduction
Deformable Part Models DPMs deliver state-of-the-art object detection results on challenging
benchmarks when trained discriminatively and have become a standard in object recognition research At the heart of these models lies the optimization of a merit function the classifier scorewith respect to the part displacements and the global object pose In this work we take the classifier
for granted using the models of and focus on the optimization problem
The most common detection algorithm used in conjunction with DPMs relies on Generalized Distance Transforms GDTs whose complexity is linear in the image size Despite its amazing
efficiency this algorithm still needs to first evaluate the score everywhere before picking its maxima
In this work we use Branch-and-Bound in conjunction with part-based models For this we exploit
the Dual Tree data structure developed originally to accelerate operations related to Kernel
Density Estimation We use DTs to provide the bounds required by Branch-and-Bound
Our method is fairly generic it applies to any star-shape graphical model involving continuous
variables and pairwise potentials expressed as separable decreasing binary potential kernels We
evaluate our technique using the mixture-of-deformable part models of Our algorithm delivers
exactly the same results but is times faster We also develop a multiple-object detection
variation of the system where all object hypotheses are inserted in the same priority queue If our
task is to find the best k-best object hypotheses in an image this can result in a speedup
Previous Work on Efficient Detection
Cascaded object detection has led to a proliferation of vision applications but far less work
exists to deal with part-based models The combinatorics of matching have been extensively studied
for rigid objects while used A for detecting object instances For categories recent works
19 18 have focused on reducing the high-dimensional pose search space during
detection by initially simplifying the cost function being optimized mostly using ideas similar to
A and coarse-to-fine processing In the recent work of thresholds pre-computed on the training
set are used to prune computation and result in substantial speedups compared to GDTs
Branch-and-bound prioritizes the search of promising image areas as indicated by an upper
bound on the classifier?s score A most influential paper has been the Efficient Subwindow Search
ESS technique of where an upper bound of a bag-of-words classifier score delivers the bounds
required by BB. Later combined Graph-Cuts with BB for object segmentation while in a
general cascade system was devised for efficient detection with a nonlinear classifier
Our work is positioned with respect to these works as follows unlike existing BB works
we use the DPM cost and thereby accommodate parts in a rigorous energy minimization framework
And unlike the pruning-based works we do not make any approximations or assumptions about when it is legitimate to stop computation our method is exact
We obtain the bound required by BB from Dual Trees To the best of our knowledge Dual Trees
have been minimally been used in object detection we are only aware of the work in which used
DTs to efficiently generate particles for Nonparametric Belief Propagation Here we show that DTs
can be used for part-based detection which is related conceptually but entirely different technically
Preliminaries
We first describe the cost function used in DPMs then outline the limitations of GDT-based detection and finally present the concepts of Dual Trees relevant to our setting Due to lack of space we
refer to for further details on DPMs and to for Dual Trees
Merit function for DPMs
We consider a star-shaped graphical model consisting of a set of nodes nP n0 is
called the root and the part nodes n1 nP are connected to the root Each node has a unary
observation potential Up indicating the fidelity of the image at to the node in Up
is the inner product of a HOG feature at with a discriminant wp for
The location xp hp vp of part is constrained with respect to the root location v0
in terms of a quadratic binary potential Bp xp of the form
Bp xp xp Ip xp h0 Hp vp v0 Vp
where Ip diag(Hp Vp is a diagonal precision matrix and mp is the nominal difference
of root-part locations We will freely alternate between the vector and its horizontal/vertical h/v
coordinates Moreover we consider and H0 V0 large enough so that B0 xp will
be zero for xp and practically infinite elsewhere
If the root is at the merit for part being
at xp is given by mp xp Up xp
Bp xp summing over gives the score mp xp of a root-and-parts configuration
xP The detector score at point is obtained by maximizing over those with
this amounts to computing
max mp xp
max Up xp hp Hp vp Vp
xp
xp
A GDT can be used to maximize each summand in jointly for all values of in time O(N
where is the number of possible locations This is dramatically faster than the naive O(N
computation For a P-part model complexity decreases from O(N to O(N
Still the factor can make things slow for large images If we know that a certain threshold will
be used for detection for a classifier trained with SVMs the GDT-based approach turns out
to be wasteful as it treats equally all image locations even those where we can quickly realize that
the classifier score cannot exceed this threshold
This is illustrated in in we show the part-root configuration that gives the maximum
score and in the score of a bicycle model from over the whole image domain Our approach
Input Detection result Detector score BB for arg maxx BB for
Figure Motivation for Branch-and-Bound approach standard part-based models evaluate a classifier?s
score over the whole image domain Typically only a tiny portion of the image domain should be positivein we draw a black contour around for an SVM-based classifier BB ignores large
intervals with low by upper bounding their values and postponing their exploration in favor of more
promising ones In we show as heat maps the upper bounds of the intervals visited by BB until the strongest
location was explored and in of the intervals visited until all locations with were explored
speeds up detection by upper bounding the score of the detector within intervals of while using
low-cost operations This allows us to use a prioritized search strategy that can refine these bounds
on promising intervals while postponing the exploration of less promising intervals
This is demonstrated in where we show as heat maps the upper bounds of the intervals
visited by BB parts of the image where the heat maps are more fine grained correspond to image
locations that seemed promising If our goal is to maximize BB discards a huge amount of
computation as shown in even with a more conservative criterion finding all
a large part of the image domain is effectively ignored and the algorithm obtains refined bounds
only around interesting image locations
Dual Trees Data Structures for Set-Set interactions
The main technical challenge is to efficiently compute upper bounds for a model involving deformable parts our main contribution consists in realizing that this can be accomplished with the
Dual Tree data structure of We now give a high-level description of Dual Trees leaving concrete aspects for their adaptation to the detection problem we assume the reader is familiar with
KD-trees
Dual Trees were developed to efficiently evaluate expressions of the form
K(xj
XS
XD
where is a separable decreasing kernel a Gaussian with diagonal covariance We refer
to XS as source terms and to XD as domain terms the idea being that the source points XS
generate a field which we want evaluate at the domain locations XP
Naively performing the computation in considers all source-domain interactions and takes
operations The Dual Tree algorithm efficiently computes this sum by using two KD-trees
one for the source locations XS and another for the domain locations XD This allows for
substantial speedups when computing for all domain points as illustrated in if a chunk
of source points cannot affect a chunk of domain points we skip computing their domain-source
point interactions
DPM opitimization using Dual Tree Branch and Bound
Brand and Bound is a maximization algorithm for non-parametric non-convex or even nondifferentiable functions BB searches for the interval containing the function?s maximum using a
prioritized search strategy the priority of an interval is determined by the function?s upper bound
within it Starting from an interval containing the whole function domain BB increasingly narrows
down to the solution at each step an interval of solutions is popped from a priority queue split
into sub-intervals Branch and a new upper bound for those intervals is computed Bound These
intervals are then inserted in the priority queue and the process repeats until a singleton interval is
popped If the bound is tight for singletons the first singleton will be the function?s global maximum
Figure Left Dual Trees efficiently deal with the interaction of source red and domain points blue
using easily computable bounds For instance points lying in square cannot have a large effect on points in
square A therefore we do not need to go to a finer level of resolution to exactly estimate their interactions
Right illustration of the terms involved in the geometric bound computations of
Coming to our case the DPM criterion developed in Sec. is a sum of scores of the form
sp max mp xp max Up hp vp hp h0 Hp vp v0 Vp
xP
hp vp
Using Dual Tree terminology the source points correspond to part locations xp XSp xp
and the domain points to object locations XD Dual Trees allow us to efficiently
derive bounds for sp XD the scores that a set of object locations can have due to a
set
of part locations Once these are formed we add over parts to bound the score
sp XD This provides the bound needed by Branch-and Bound
We now present our approach through a series intermediate problems These may be amenable to
simpler solutions but the more complex solutions discussed finally lead to our algorithm
Maximization for One Domain Point
We first introduce notation we index the source/domain points in XS XD using i/j respectively We
denote by wip Up the unary potential of part at location We shift the unary scores by the
nominal offsets which gives new source locations hi vi hi vi
Finally we drop from mp Hp and Vp unless necessary We can now write as
v0 max H(hi h0 vi v0
i?Sp
To evaluate at v0 we use prioritized search over intervals of Sp starting from Sp
and gradually narrowing down to the best To prioritize intervals we use a KD-tree for the source
points XSp to quickly compute bounds of In specific if Sn is the set of children of the
n-th node of the KD-tree for Sp consider the subproblem
mn v0 max
i?Sn
H(hi h0 vi v0 max
i?Sn
Gi
where Gi H(hi h0 vi v0 stands for the geometric part of We know that for
all points hi vi within Sn we have hi rn and vi bn tn where are the left right
bottom top axes defining n?s bounding box Bn We can then bound Gi within Sn as follows
Gn
min(?l h0 min(?b v0
Gn
max h0 h0 max v0 v0
where and Gn Gi Gn Sn The upper bound is zero inside Bn and uses
the boundaries of Bn that lie closest to v0 when v0 is outside Bn The lower bound uses
the distance from v0 to the furthest point within Bn
Regarding the term in for both bounds we can use the value wj arg maxi?Sn
This is clearly suited for the upper bound For the lower bound since Gi Gn Sn we
have maxi?Sn Gi wj Gj wj Gn So wj Gn provides a proper lower bound for
maxi?Sn Gi Summing up we bound as wj Gn mn v0 wj Gn
l1
l2
m1
m2
n1
n2
o1
o2
Figure Supporter pruning source nodes are among the possible supporters of domain-node
Their upper and lower bounds shown as numbers to the right of each node are used to prune them Here the
upper bound for is smaller than the maximal lower bound among supporters from this implies the
upper bound of n?s children contributions to l?s children shown here for l1 will not surpass the lower bound
of o?s children We can thus safely remove from the supporters
We can use the upper bound in a prioritized search for the maximum of v0 as described in
Table Starting with the root of the KD-tree we expand its children nodes estimate their prioritiesupper bounds and insert them in a priority queue The search stops when the first leaf node is
popped this provides the maximizer as its upper and lower bounds coincide and all other elements
waiting in queue have smaller upper bounds The lower bound is useful in Sec.
Maximization for All Domain Points
Having described how KD-trees to provide bounds in the single domain point case we now describe
how Dual Trees can speedup this operation in when treating multiple domain points simultaneously
In specific we consider the following maximization problem
arg max arg max max H(hi hj vi vj
x?XD
j?D i?S
where XD is the set of domain points/indices and are the source indices The previous algorithm could deliver by computing repeatedly for each XD and picking the maximizer
But this will repeat similar checks for neighboring domain points which can instead be done jointly
For this as in the original Dual Tree work we build a second KD-tree for the domain points Domain tree as opposed to Source tree The nodes in the Domain tree domain-nodes correspond
to intervals of domain points that are processed jointly This saves repetitions of similar bounding
operations and quickly discards large domain areas with poor bounds
For the bounding operations as in Sec. we consider the effect of source points contained in a
node Sn of the Source tree The difference is that now we bound the maximum of this quantity over
domain points contained in a domain-node Dl In specific we consider the quantity
ml,n max max
j?Dl i?Sn
H(hi hj vi vj
Bounding Gi,j H(hi hj vi vj involves two 2D intervals one for the domain-node
and one for the domain-node If the interval for node is centered at hn vn and has dimensions
dh,n dv,n we use d?h dh,l dh,n d?v dv,l dv,n and write
Gl,n max(?hn hl d?h hl hn d?h max(?vn vl d?v vl vn d?v
Gl,n max hn hl d?h hl hn d?h max vn vl d?v vl vn d?v
We illustrate these bounds in The upper bound is zero if the boxes overlap or else equals the
scaled distance of their closest points The lower bound uses the furthest points of the two boxes
As in Sec. we use wn maxi?Sn for the first term in and bound ml,n as follows
Gl,n wn ml,n Gl,n wn
This expression bounds the maximal value that a point in domain-node can have using
contributions from points in source-node Our initial goal was to find the maximum using all
possible source point contributions We now describe a recursive approach to limit the set of sourcenodes considered in a manner inspired from the multi-recursion approach of
For this we associate every domain-node with a set Sl of supporter source-nodes that can yield
the maximal contribution to points in We start by associating the root node of the Domain tree
with the root node of the Source-tree which means that all domain-source point interactions are
originally considered
We then recursively increase the resolution of the Domain-tree in parallel with the resolution of
the Source-tree More specifically to determine the supporters for a child of domain-node we
consider only the children of the source-nodes in Sl formally denoting by pa and ch the parent and
child operations respectively we have Sm n?Spa(m
Our goal is to reduce computation by keeping Sm small This is achieved by pruning based on both
the lower and upper bounds derived above The main observation is that when we go from parents
to children we decrease the number of source/domain points this tightens the bounds makes
the upper bounds less optimistic and the lower bounds more optimistic Denoting the maximal
lower bound for contributions to parent node by Gl maxn?Sl Gl,n this means that Gk Gl if
pa(k On the flip side Gl,n Gk,q if pa(k pa(q This means that if for sourcenode at the parent level Gl,n Gl at the children level the children of will contribute something
worse than Gm the lower bound on l?s child score We therefore do not need to keep among Sl its
children?s contribution will be certainly worse than the best contribution from other node?s children
Based on this observation we can reduce the set of supporters while guaranteeing optimality
Pseudocode summarizing this algorithm is provided in Table The bounds in are used in a
prioritized search algorithm for the maximum of over The algorithm uses a priority queue
for Domain tree nodes initialized with the root of the Domain tree the whole range of possible
locations At each iteration we pop a Domain tree node from the queue compute upper bounds
and supporters for its children which are then pushed in the priority queue The first leaf node that
is popped contains the best domain location its upper bound equals its lower bound and all other
nodes in the priority queue have smaller upper bounds therefore cannot result in a better solution
Maximization over All Domain Points and Multiple Parts Branch and Bound for DPMs
The algorithm we described in the previous subsection is essentially a Branch-and-Bound
algorithm for the maximization of a merit function
arg max arg max max H(hi h0 vi v0
i?Sp
corresponding to a DPM with a single-part To see this recall that at each step BB pops a
domain of the function being maximized from the priority queue splits it into subdomains Branch
and computes a new upper bound for the subdomains Bound In our case Branching amounts
to considering the two descendants of the domain node being popped while Bounding amounts to
taking the maximum of the upper bounds of the domain node supporters
The single-part DPM optimization problem is rather trivial but adapting the technique to the multipart case is now easy For this we rewrite in a convenient form as
v0
max wp,i Hp hpi h0 Vp vip v0
i?S
using the conventions we used in Namely we only consider using points in for object parts
and subtract mp from hi vi to yield simple quadratic forms since mp is part-dependent we now
have a superscript for hi vi Further we have in general different variables for different
parts so we brought back the subscript for these Finally wp,i depends on since the same image
point will give different unary potentials for different object parts
From this form we realize that computing the upper bound of within a range of values of
as required by Branch-and-Bound is as easy as it was for the single terms in the previous secPP
tion In specific we have mp where mp are the individual part contributions
PP
PP
since maxx mp maxx mp we can separately upper bound the individual part
contributions and sum them up to get an overall upper bound
Pseudocode describing the maximization algorithm is provided in Table Note that each part has its
own KDtree SourcT[p we build a separate Source-tree per part using the part-specific coordinates
hp and weights wp,i Each part?s contribution to the score is computed using the supporters it
lends to the node the total bound is obtained by summing the individual part bounds
Single Domain Point
IN ST Source Tree Location
OUT arg maxxi ST
Push(S,ST.root
while do
Pop(S,popped
if popped.UB popped.LB then
return popped
end if
for side Left,Right do
child popped.side
child.UB BoundU(x,child
child.LB BoundL(x,child
Push(S,child
end for
end while
Multiple Domain Points Multiple Parts
IN DT Source
Trees/Domain Tree
OUT arg maxx?DT maxi?ST xp
Seed DT.root
for to do
Seed.supporters[p ST[p].Root
end for
Push(S,Seed
while do
Pop(S,popped
if popped.UB popped.LB then
return popped
end if
for side Left,Right do
child popped.side
UB
for part do
supp Descend(popped.supp[part
UP,s Bound(child,supp,DT,ST[p
child.supp[part
UB UB UP
end for
child.UB UB
Push(S,child
end for
end while
Multiple Domain Points
IN ST DT Source/Domain Tree
OUT arg maxx?DT maxi?ST
Seed DT.root
Seed.supporters ST.Root
Push(S,Seed
while do
Pop(S,popped
if popped.UB popped.LB then
return popped
end if
for side Left,Right do
child popped.side
supp Descend(popped.supp
UB,supc Bound(child,supp,DT,ST
child.UB UB
child.supc supc
Push(S,child
end for
end while
Bounding Routine
IN child,supporters,DT,ST
OUT supch LB Chosen supporters Max LB
UB LB
for supporters do
UB[n BoundU(DT.node[child],ST.node[n
LB[n BoundL(DT.node[child],ST.node[n
end for
MaxLB max(LB
supch supporters(find(UB>MaxLB
Return supch MaxLB
Table Pseudocode for the algorithms presented in Section
Results Application to Deformable Object Detection
To estimate the merit of BB we first compare with the mixtures-of-DPMs developed and distributed
by We directly extend the Branch-and-Bound technique that we developed for a single DPM to
deal with multiple scales and mixtures of DPMs by inserting all object hypotheses
into the same queue To detect multiple instances of objects at multiple scales we continue BB after
getting the best scoring object hypothesis As termination criterion we choose to stop when we pop
an interval whose upper bound is below a fixed threshold
Our technique delivers essentially the same results as One minuscule difference is that BB
uses floating point arithmetic for the part locations while in GDT they are necessarily processed at
integer resolution other than that the results are identical We therefore do not provide any detection
performance curves but only timing results
Coming to time efficiency in we compare the results of the original DPM mixture model
and our implementation We use images from the Pascal dataset and a mix of models for
different object clases gains vary per category We consider the standard detection scenario where
we want to detect all objects in an image having score above a certain threshold We show how
Speedup Single object
Speedup
Image rank
Speedup front?end
Speedup M?objects 1?best Speedup 20?objects k?best
Image rank
Image rank
Image rank
Figure Single-object speedup of Branch and Bound compared to GDTs on images from the Pascal
dataset Multi-object speedup Speedup due to the front-end computation of the unary potentials
Please see text for details
the threshold affects the speedup we obtain for a conservative threshold the speedup is typically
tenfold but as we become more aggressive it doubles
As a second application we consider the problem of identifying the dominant object present in
the image the category the gives the largest score Typically simpler models like bag-of-words
classifiers are applied to this problem based on the understanding that part-based models can be
time-consuming therefore applying a large set of models to an image would be impractical
Our claim is that Branch-and-Bound allows us to pursue a different approach where in fact having
more object categories can increase the speed of detection if we leave the unary potential computation aside In specific our approach can be directly extended to the multiple-object detection
setting as long as the scores computed by different object categories are commensurate they can all
be inserted in the same priority queue In our experiments we observed that we can get a response
faster by introducing more models The reason for this is that including into our object repertoire a
model giving a large score helps BB stop otherwise BB keeps searching for another object
In plots we show systematic results on the Pascal dataset We compare the time that
would be required by GDT to perform detection of all multiple objects considered in Pascal to that
of a model simultaneously exploring all models In we show how finding the first-best result is
accelerated as the number of objects increases while in we show how increasing the in
k-best affects the speedup For small values of the gains become more pronounced Of course if
we use a fixed threshold the speedup would not change when compared to plot since essentially
the objects do not interact in any way we do not use nonmaximum suppression But as we turn to
the best-first problem the speedup becomes dramatic ranging in the order of up to a hundred times
We note that the timings refer to the message passing part implemented with GDT and not the
computation of unary potentials which is common for both models and is currently the bottleneck
Even though it is tangential to our contribution in this paper we mention that as shown in plot
we compute unary potentials approximately five times faster than the single-threaded convolution
provided by by exploiting Matlab?s optimized matrix multiplication routines
Conclusions
In this work we have introduced Dual-Tree Branch-and-Bound for efficient part-based detection
We have used Dual Trees to compute upper bounds on the cost function of a part-based model and
thereby derived a Branch-and-Bound algorithm for detection Our algorithm is exact and makes no
approximations delivering identical results with the DPMs used in but in typically less
time Further we have shown that the flexibility of prioritized search allows us to consider new
tasks such as multiple-object detection which yielded further speedups The main challenge for
future work will be to reduce the unary term computation cost we intend to use BB for this task too
Acknowledgements
We are grateful to the authors of for making their code available and to the reviewers for
constructive feedback This work was funded by grant ANR-10-JCJC

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2780-spectral-bounds-for-sparse-pca-exact-and-greedy-algorithms.pdf

Spectral Bounds for Sparse PCA
Exact and Greedy Algorithms
Baback Moghaddam
MERL
Cambridge MA USA
baback@merl.com
Yair Weiss
Hebrew University
Jerusalem Israel
yweiss@cs.huji.ac.il
Shai Avidan
MERL
Cambridge MA USA
avidan@merl.com
Abstract
Sparse PCA seeks approximate sparse eigenvectors whose projections
capture the maximal variance of data As a cardinality-constrained and
non-convex optimization problem it is NP-hard and is encountered in
a wide range of applied fields from bio-informatics to finance Recent
progress has focused mainly on continuous approximation and convex
relaxation of the hard cardinality constraint In contrast we consider an
alternative discrete spectral formulation based on variational eigenvalue
bounds and provide an effective greedy strategy as well as provably
optimal solutions using branch-and-bound search Moreover the exact
methodology used reveals a simple renormalization step that improves
approximate solutions obtained by any continuous method The resulting
performance gain of discrete algorithms is demonstrated on real-world
benchmark data and in extensive Monte Carlo evaluation trials
Introduction
PCA is indispensable as a basic tool for factor analysis and modeling of data But despite
its power and popularity one key drawback is its lack of sparseness factor loadings
are linear combinations of all the input variables Yet sparse representations are generally
desirable since they aid human understanding with gene expression data reduce
computational costs and promote better generalization in learning algorithms In machine
learning input sparseness is closely related to feature selection and automatic relevance
determination problems of enduring interest to the learning community
The earliest attempts at sparsifying PCA in the statistics literature consisted of simple
axis rotations and component thresholding with the underlying goal being essentially
that of subset selection often based on the identification of principal variables The
first true computational technique called SCoTLASS by Jolliffe Uddin provided
a proper optimization framework using Lasso but it proved to be computationally
impractical Recently Zou proposed an elegant algorithm SPCA using their
Elastic Net framework for L1 penalized regression on regular PCs solved very efficiently
using least angle regression LARS Subsequently d?Aspremont relaxed the
hard cardinality constraint and solved for a convex approximation using semi-definite
programming Their direct formulation for sparse PCA called DSCPA has
yielded promising results that are comparable to if not better than Zou al.?s Lasso-based
method as demonstrated on the standard Pit Props benchmark dataset known in the
statistics community for its lack of sparseness and subsequent difficulty of interpretation
We pursued an alternative approach using a spectral formulation based on the variational
principle of the Courant-Fischer Min-Max theorem for solving maximal eigenvalue
problems in dimensionality-constrained subspaces By its very nature the discrete view
leads to a simple post-processing renormalization step that improves any approximate
solution those given in and also provides bounds on sub)optimality
More importantly it points the way towards exact and provably optimal solutions using
branch-and-bound search Our exact computational strategy parallels that of Ko
who solved a different optimization problem maximizing entropy with bounds
on determinants In the experiments we demonstrate the power of greedy and exact
algorithms by first solving for the optimal sparse factors of the real-world Pit Props data
a de facto benchmark used by and then present summary findings from a large
comparative study using extensive Monte Carlo evaluation of the leading algorithms
Sparse PCA Formulation
Sparse PCA can be cast as a cardinality-constrained quadratic program given a
maximize the quadratic form
symmetric positive-definite covariance matrix A
Ax variance with a sparse vector having no more than non-zero elements
A
card(x
where card(x denotes the L0 norm This optimization problem is non-convex NP-hard
and therefore intractable Assuming we can solve for the optimal vector
subsequent
sparse factors can be obtained using recursive deflation of A as in standard numerical
routines The sparseness is controlled by the value(s of different factors and can be
viewed as a design parameter or as an unknown quantity itself known only to the oracle
Alas there are currently no guidelines for setting especially with multiple factors
orthogonality is often relaxed and unlike ordinary PCA some decompositions may not be
unique.1 Indeed one of the contributions of this paper is in providing a sound theoretical
basis for selecting thus clarifying the art of crafting sparse PCA factors
max
subject to
Note that without the cardinality constraint the quadratic form in is a RayleighRitz quotient obeying the analytic bounds min Ax/x0 max with
corresponding unique eigenvector solutions Therefore the optimal objective value
variance is simply the maximum eigenvalue of the principal eigenvector
un
Note throughout the paper the rank of all ui is in increasing order of magnitude
hence min and max With the nonlinear cardinality constraint however
the optimal objective value is strictly less than max for and the principal
eigenvectors are no longer instrumental in the solution Nevertheless we will show that the
eigenvalues of A continue to play a key role in the analysis and design of exact algorithms
Optimality Conditions
First let us consider what conditions must be true if the oracle revealed the optimal solution
to us a unit-norm vector
with cardinality yielding the maximum objective value
This would necessarily imply that
A
Ak where Rk contains the same
non-zero elements in
and Ak is the principal submatrix of A obtained by deleting
the rows and columns corresponding to the zero indices of
equivalently by extracting
the rows and columns of non-zero indices Like
the k-vector will be unit norm and
Ak is then equivalent to a standard unconstrained Rayleigh-Ritz quotient Since this
subproblem?s maximum variance is max Ak then this must be the optimal objective
We will now summarize this important observation with the following proposition
We should note that the multi-factor version of is ill-posed without additional constraints
on basis orthogonality cardinality variable redundancy ordinal rank and allocation of variance
Proposition The optimal value of the sparse PCA optimization problem in
is equal to max where A?k is the principal submatrix of A with the largest
maximal eigenvalue In particular the non-zero elements of the optimal sparse factor
are
exactly equal to the elements of u?k the principal eigenvector of A?k
This underscores the inherent combinatorial nature of sparse PCA and the equivalent
class of cardinality-constrained optimization problems However despite providing an
exact formulation and revealing necessary conditions for optimality and in such simple
matrix terms this proposition does not suggest an efficient method for actually finding the
principal submatrix A?k short of an enumerative exhaustive search which is impractical
for due to the exponential growth of possible submatrices Still exhaustive search
is a viable method for small which guarantees optimality for toy problems and small
real-world datasets thus calibrating the quality of approximations via the optimality gap
Variational Renormalization
Proposition immediately suggests a rather simple but as it turns out quite effective
computational fix for improving candidate sparse PC factors obtained by any continuous
algorithm the various solutions found in
Proposition Let
be a unit-norm candidate factor with cardinality as found by any
approximation technique Let be the non-zero subvector of
and uk be the principal
maximum eigenvector of the submatrix Ak defined by the same non-zero indices of
If
uk Ak then
is not the optimal solution Nevertheless by replacing
nonzero
elements with those of uk we guarantee an increase in the variance from to Ak
This variational renormalization suggests somewhat ironically that given a continuous
approximate solution it is almost certainly better to discard the loadings and keep only
the sparsity pattern with which to solve the smaller unconstrained subproblem for the
indicated submatrix Ak This simple procedure fix as referred to herein can never
decrease the variance and will surely improve any continuous algorithm?s performance
In particular the rather expedient but ad-hoc technique of simple thresholding
setting the smallest absolute value loadings of un to zero and then
normalizing to unit-norm is therefore not recommended for sparse PCA. In Section
we illustrate how this straw-man algorithm can be enhanced with proper renormalization
Consequently past performance benchmarks using this simple technique may need revision
previous results on the Pit Props dataset Section Indeed most of the sparse
PCA factors published in the literature can be readily improved almost by inspection with
the proper renormalization and at the mere cost of a single k-by-k eigen-decomposition
Eigenvalue Bounds
Recall that the objective value in is bounded by the spectral radius max
by the Rayleigh-Ritz theorem Furthermore the spectrum of A?s principal submatrices
was shown to play a key role in defining the optimal solution Not surprisingly the two
eigenvalue spectra are related by an inequality known as the Inclusion Principle
Theorem Inclusion Principle Let A be a symmetric matrix with spectrum
and let Ak be any principal submatrix of A for with eigenvalues Ak
For each integer such that
Ak
Proof The proof which we omit is a rather straightforward consequence of imposing a
sparsity pattern of cardinality as an additional orthogonality constraint in the variational
inequality of the Courant-Fischer Min-Max theorem for example
In other words the eigenvalues of a symmetric matrix form upper and lower bounds for the
eigenvalues of all its principal submatrices A special case of with leads
to the well-known eigenvalue interlacing property of symmetric matrices
An An An An
Hence the spectra of An and interleave or interlace each other with the eigenvalues
of the larger matrix bracketing those of the smaller one Note that for positive-definite
symmetric matrices covariances augmenting Am to adding a new variable will
always expand the spectral range reducing min and increasing max Thus for eigenvalue
maximization the inequality constraint card(x in is a tight equality at the
optimum Therefore the maximum variance is achieved at the preset upper limit of
cardinality Moreover the function the optimal variance for a given cardinality is
monotone increasing with range max
max where max
is the largest diagonal
element variance in A. Hence a concise and informative way to quantify the performance
of an algorithm is to plot its variance curve and compare it with the optimal
Since we seek to maximize variance the relevant inclusion bound is obtained by setting
in which yields lower and upper bounds for Ak max Ak
max Ak max
This shows that the k-th smallest eigenvalue of A is a lower bound for the maximum
variance possible with cardinality The utility of this lower bound is in doing away with
the guesswork and the oracle in setting Interestingly we now see that the spectrum of
A which has traditionally guided the selection of eigenvectors for dimensionality reduction
in classical PCA can also be consulted in sparse PCA to help pick the cardinality
required to capture the desired minimum variance The lower bound is also useful
for speeding up branch-and-bound search next Section Note that if is close to
max then practically any principal submatrix Ak can yield a near-optimal solution
The right-hand inequality in is a fixed loose upper bound max for all
But in branch-and-bound search any intermediate subproblem Am with
yields a new and tighter bound max Am for the objective Therefore all bound
computations are efficient and relatively inexpensive using the power method
The inclusion principle also leads to some interesting constraints on nested submatrices
For example among all possible principal submatrices of A
obtained by deleting the j-th row and column there is at least one submatrix A A\j
whose maximal eigenvalue is a major fraction of its parent see in
Am
The implication of this inequality for search algorithms is that it is simply not possible for
the spectral radius of every submatrix A\j to be arbitrarily small especially for large
Hence with large matrices large cardinality nearly all the variance is captured
Combinatorial Optimization
Given Propositions and the inclusion principle the interlacing property and especially
the monotonic nature of the variance curves a general class of binary integer
programming optimization techniques seem ideally suited for sparse PCA. Indeed
a greedy technique like backward elimination is already suggested by the bound in
start with the full index set I and sequentially delete the variable
which yields the maximum max until only elements remain However for
small cardinalities the computational cost of backward search can grow to near
maximum complexity Hence its counterpart forward selection is preferred
start with the null index set I and sequentially add the variable which yields the
maximum max until elements are selected Forward greedy search has worstcase complexity The best overall strategy for this problem was empirically
found to be a bi-directional greedy search run a forward pass from to plus a second
independent backward pass from to and pick the better solution at each This
proved to be remarkably effective under extensive Monte Carlo evaluation and with realworld datasets We refer to this discrete algorithm as greedy sparse PCA or GSPCA
Despite the expediency of near-optimal greedy search it is nevertheless worthwhile to
invest in optimal solution strategies especially if the sparse PCA problem is in the
application domain of finance or engineering where even a small optimality gap can
accrue substantial losses over time As with Ko our branch-and-bound relies
on computationally efficient bounds in our case the upper bound in used on all
active subproblems in a FIFO queue for depth-first search The lower bound in
can be used to sort the queue for a more efficient best-first search This exact algorithm
referred to as ESPCA is guaranteed to terminate with the optimal solution Naturally
the search time depends on the quality variance of initial candidates The solutions found
by dual-pass greedy search GSPCA were found to be ideal for initializing ESPCA as
their quality was typically quite high Note however that even with good initializations
branch-and-bound search can take a long time hours for In
practice early termination with set thresholds based on eigenvalue bounds can be used
In general a cost-effective strategy that we can recommend is to first run GSPCA at
least the forward pass and then either settle for its near-optimal variance or else use it
to initialize ESPCA for finding the optimal solution A full GSPCA run has the added
benefit of giving near-optimal solutions for all cardinalities at once with run-times that are
typically faster than a single approximation with a continuous method
Experiments
We evaluated the performance of GSPCA and validated ESPCA on various synthetic
covariance matrices with as well as real-world datasets from the UCI ML
repository with excellent results We present few typical examples in order to illustrate the
advantages and power of discrete algorithms In particular we compared our performance
against continuous techniques simple thresholding SPCA using an Elastic
Net L1 regression and DSPCA using semidefinite programming
We first revisited the Pit Props dataset which has become a standard benchmark and
a classic example of the difficulty of interpreting fully loaded factors with standard PCA.
The first ordinary PCs capture of the total variance so following the methodology in
we compared the explanatory power of our exact method ESPCA using sparse PCs.
Table shows the first PCs and their loadings SPCA captures of the variance with
a cardinality pattern of the k?s for the PCs thus totaling 18 non-zero loadings
whereas DSPCA captures with a sparser cardinality pattern totaling
non-zero loadings We aimed for an even sparser pattern with only non-zero
loadings yet captured nearly the same variance more than SPCA with 18
loadings and slightly less than DSPCA with loadings
Using the evaluation protocol in we compared the cumulative variance and cumulative
cardinality with the published results of SPCA and DSPCA in Figure Our goal was to
match the explained variance but do so with a sparser representation The ESPCA loadings
in Table are optimal under the definition given in Section The run-time of ESPCA
including initialization with a bi-directional pass of GSPCA was negligible for this dataset
Computing each factor took less than msec in Matlab on a 3GHz P4.
SPCA PC1
PC2
PC3
DSPCA PC1
PC2
PC3
ESPCA PC1
PC2
PC3
x3
x4
x5
x6
x7
x8
x9
Table Loadings for first sparse PCs of the Pit Props data See Figure for plots of the
corresponding cumulative variances Original SPCA and DSPCA loadings taken from
18
SPCA
DSPCA
ESPCA
SPCA
DSPCA
ESPCA
Cumulative Cardinality
Cumulative Variance
of PCs
of PCs
Figure Pit Props cumulative variance and cumulative cardinality for first sparse PCs.
Sparsity patterns cardinality ki for PCi with are for SPCA magenta
for DSPCA green and an optimal for ESPCA red The factor loadings for the
first sparse PCs are shown in Table Original SPCA and DSPCA results taken from
To specifically demonstrate the benefits of the variational renormalization of Section
consider SPCA?s first sparse factor in Table the 1st row of SPCA block found by iterative
penalized optimization and unit-norm scaling It captures of the total data
variance but after the variational renormalization the variance increases to Similarily
the first sparse factor of DSPCA in Table row of DSPCA block captures of
the total variance whereas after variational renormalization it captures a gain of
for the mere additional cost of a eigen-decomposition Given that variational
renormalization results in the maximum variance possible for the indicated sparsity pattern
omitting such a simple post-processing step is counter-productive since otherwise the
approximations would be in a sense doubly sub-optimal both globally and locally in
the subspace subset of the sparsity pattern found
We now give a representative summary of our extensive Monte Carlo evaluation
of GSPCA and the continuous algorithms To show the most typical or average-case
performance we present results with random covariance matrices from synthetic stochastic
Brownian processes of various degrees of smoothness ranging from sub-Gaussian to superGaussian Every MC run consisted of covariance matrices and the normalized
variance curves For each matrix ESPCA was used to find the optimal solution as
ground truth for subsequent calibration analysis and performance evaluation
For SPCA we used the LARS-based Elastic Net SPCA Matlab toolbox of Sjo strand
which is equivalent to Zou al.?s SPCA source code which is also freely available in R.
For DSPCA we used the authors own Matlab source code which uses the SDP toolbox
SeDuMi1.0x The main DSPCA routine PrimalDec(A was called with instead
of for all as per the recommended calibration documentation in
In our MC evaluations all continuous methods SPCA and DSPCA had variational
renormalization post-processing applied to their the declared solution Note that
comparing GSPCA with the raw output of these algorithms would be rather pointless since
ST
SPCA
DSPCA
GSPCA
log frequency
variance
DSPCA original
DSPCA Fix
Optimal
cardinality
optimality ratio
Figure Typical variance curve for a continuous algorithm without post-processing
original dash green and with variational renormalization Fix solid green Optimal variance
black by ESPCA At optimality ratio increases from to gain Monte
Carlo study log-likelihood of optimality ratio at max-complexity for ST blue
DSPCA green SPCA magenta and GSPCA red Continuous methods were fixed in
frequency
mean optimality ratio
ST
SPCA
DSPCA
GSPCA
cardinality
ST
SPCA
DSPCA
GSPCA
cardinality
Figure Monte Carlo summary statistics means of the distributions of optimality ratio
Figure for all and estimated probability of finding the optimal solution for each cardinality
without the fix their variance curves are markedly diminished as in Figure
Figure shows the histogram of the optimality ratio ratio of the captured to
optimal variance shown here at half-sparsity from a typical MC
run of different covariances matrices In order to view the one-sided tails of
the distributions we have plotted the log of the histogram values Figure shows the
corresponding mean values of the optimality ratio for all Among continuous algorithms
the SDP-based DSPCA was generally more effective almost comparable to GSPCA For
the smaller matrices LARS-based SPCA matched DSPCA for all In terms of
complexity and speed however SPCA was about times faster than DSPCA But GSPCA
was times faster than SPCA Finally we note that even simple thresholding
once enhanced with the variational renormalization performs quite adequately despite its
simplicity as it captures at least of the optimal variance as seen in Figure
Figure shows an alternative but more revealing performance summary the fraction
of the trials in which the optimal solution was actually found essentially
the likelihood of success This all-or-nothing performance measure elicits important
differences between the algorithms In practical terms only GSPCA is capable of finding
the optimal factor more than of the time for DSPCA Naturally without the
variational fix not shown continuous algorithms rarely ever found the optimal solution
Discussion
The contributions of this paper can be summarized as an exact variational formulation
of sparse PCA requisite eigenvalue bounds a principled choice of a simple
renormalization fix for any continuous method fast and effective greedy search
GSPCA and a less efficient but optimal method ESPCA Surprisingly simple
thresholding of the principal eigenvector was shown to be rather effective especially
given the perceived straw-man it was considered to be Naturally its performance will
vary with the effective rank eigen-gap of the covariance matrix In fact it is not
hard to show that if A is exactly rank-1 then ST is indeed an optimal strategy for all
However beyond such special cases continuous methods can not ultimately be competitive
with discrete algorithms without the variational renormalization fix in Section
We should note that the somewhat remarkable effectiveness of GSPCA is not entirely
unexpected and is supported by empirical observations in the combinatorial optimization
literature that greedy search with sub)modular cost functions having the monotonicity
property the variance curves is known to produce good results In terms of
quality of solutions GSPCA consistently out-performed continuous algorithms with runtimes that were typically faster than LARS-based SPCA and roughly faster
than SDP-based DSPCA Matlab CPU times averaged over all
Nevertheless we view discrete algorithms as complementary tools especially since the
leading continuous algorithms have distinct advantages For example with very highdimensional datasets Zou al.?s LARS-based method is currently the
only viable option since it does not rely on computing or storing a huge covariance matrix
Although d?Aspremont mention the possibility of solving larger systems much
faster using Nesterov?s 1st-order method this would require a full matrix in memory
same as discrete algorithms Still their SDP formulation has an elegant robustness
interpretation and can also be applied to non-square matrices for a sparse SVD
Acknowledgments
The authors would like to thank Karl Sjo strand DTU for his customized code and helpful advice in
using the LARS-SPCA toolbox and Gert Lanckriet Berkeley for providing the Pit Props data

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5495-learning-to-search-in-branch-and-bound-algorithms.pdf

Learning to Search in Branch-and-Bound Algorithms
He He Hal Daum?e III
Department of Computer Science
University of Maryland
College Park MD
hhe,hal}@cs.umd.edu
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore MD
jason@cs.jhu.edu
Abstract
Branch-and-bound is a widely used method in combinatorial optimization including mixed integer programming structured prediction and MAP inference
While most work has been focused on developing problem-specific techniques
little is known about how to systematically design the node searching strategy
on a branch-and-bound tree We address the key challenge of learning an adaptive node searching order for any class of problem solvable by branch-and-bound
Our strategies are learned by imitation learning We apply our algorithm to linear
programming based branch-and-bound for solving mixed integer programs
We compare our method with one of the fastest open-source solvers SCIP and
a very efficient commercial solver Gurobi We demonstrate that our approach
achieves better solutions faster on four MIP libraries
Introduction
Branch-and-bound is a systematic enumerative method for global optimization of nonconvex and combinatorial problems In the machine learning community B&B has been used as an
inference tool in MAP estimation In applied domains it has been applied to the inference
stage of structured prediction problems dependency parsing scene understanding
ancestral sequence reconstruction B&B recursively divides the feasible set of a problem into
disjoint subsets organized in a tree structure where each node represents a subproblem that searches
only the subset at that node If computing bounds on a subproblem does not rule out the possibility
that its subset contains the optimal solution the subset can be further partitioned branched as
needed A crucial question in B&B is how to specify the order in which nodes are considered An
effective node ordering strategy guides the search to promising areas in the tree and improves the
chance of quickly finding a good incumbent solution which can be used to rule out other nodes
Unfortunately no theoretically guaranteed general solution for node ordering is currently known
Instead of designing node ordering heuristics manually for each problem type we propose to speed
up B&B search by automatically learning search heuristics that are adapted to a family of problems
Non-problem-dependent learning While our approach learns problem-specific policies
it can be applied to any family of problems solvable by the B&B framework We use
imitation learning to automatically learn the heuristics free of the trial-and-error tuning
and rule design by domain experts in most B&B algorithms
Dynamic decision-making Our decision-making process is adaptive on three scales First
it learns different strategies for different problem types Second within a problem type it
can evaluate the hardness of a problem instance based on features describing the solving
progress Third within a problem instance it adapts the searching strategy to different
levels of the B&B tree and makes decisions based on node-specific features
This material is based upon work supported by the National Science Foundation under Grant No.
training examples
prune
INF
ub
lb
global lower and
upper bound
ub
lb
node expansion
order
ub
lb
optimal node
fathomed node
ub
lb
INF
min
3x 5y
3x 5y
Figure Using branch-and-bound to solve an integer linear programming minimization
Easy incorporation of heuristics Most hand-designed strategies handle only a few heuristics and they set weights on different heuristics by domain knowledge or manual experimentation In our model multiple heuristics can be simply plugged in as state features for
the policy allowing a hybrid heuristic to be learned effectively
We assume that a small set of solved problems are given at training time and the problems to be
solved at test time are of the same type We learn a node selection policy and a node pruning policy
from solving the training problems The node selection policy repeatedly picks a node from the
queue of all unexplored nodes and the node pruning policy decides if the popped node is worth
expanding We formulate B&B search as a sequential decision-making process We design a simple
oracle that knows the optimal solution in advance and only expands nodes containing the optimal
solution We then use imitation learning to learn policies that mimic the oracle?s behavior without
perfect information these policies must even mimic how the oracle would act in states that the oracle would not itself reach as such states may be encountered at test time We apply our approach to
linear programming based B&B for solving mixed integer linear programming MILP problems and achieve better solutions faster on MILP problem libraries than Gurobi a recent fast
commercial solver competitive with Cplex and SCIP one of the fastest open-source solvers
The Branch-and-Bound Framework An Application in Mixed Integer
Linear Programming
Consider an optimization problem of minimizing over a feasible set where is usually discrete
B&B uses a divide
Sp and conquer strategy is recursively divided into its subsets F1 F2 Fp
such that Fi The recursion tree is an enumeration tree of all feasible solutions whose
nodes are subproblems and edges are the partition conditions Slightly abusing notation we will use
Fi to refer to both the subset and its corresponding B&B node from now on A convex relaxation
of each subproblem is solved to provide an upper/lower bound for that node and its descendants We
denote the upper and lower bound at node by ub Fi and lb Fi respectively where ub and lb
are bounding functions
A common setting where B&B is ubiquitously applied is MILP A MILP optimization problem has
linear objective and constraints and also requires specified variables to be integer We assume we
are minimizing the objective function in MILP from now on At each node we drop the integrality
constraints and solve its LP relaxation We present a concrete example in Figure The optimization
problem is shown in the lower right corner At node a local lower bound shown in lower half of
each circle is found by the LP solver A local upper bound shown in upper part of the circle is
available if a feasible solution is found at this node We automatically get an upper bound if the LP
solution happens to be integer feasible or we may obtain it by heuristics
B&B maintains a queue of active nodes starting with a single root node on it At each step
we pop a node Fi from using a node selection strategy and compute its bounds A node Fi
root
problem
solution
rank
nodes
push
children
Algorithm Policy Learning
Yes
No
pop
fathom
No
Yes
queue
empty
Yes
prune
No
DS DP
for to do
for in problem set do
DS DP
OLLECT XAMPLE(Q
DS
DS
DP
train classifiers using DS and DP
return Best on dev set
Figure Our method at runtime left and the policy learning algorithm right Left our
policy-guided branch-and-bound search Procedures in the rounded rectangles shown in blue are
executed by policies Right the DAgger learning algorithm We start by using oracle policies
and to solve problems in and collect examples along oracle trajectories In each iteration
we retrain our policies on all examples collected so far training sets DD and DS then collect
additional examples by running the newly learned policies The OLLECT XAMPLE procedure is
described in Algorithm
is fathomed no further exploration in its subtree if one of the following cases is true
lb Fi is larger than the current global upper bound which means all solutions in its subtree can
not possibly be better than the incumbent lb Fi ub Fi at this point B&B has found the
best solution in the current subtree The subproblem is infeasible In Figure fathomed nodes
are shown in double circles and infeasible nodes are labeled by
If a node is not fathomed it is branched into children of Fi that are pushed onto L. Branching
conditions are shown next to each edge in Figure The algorithm terminates when is empty or
the gap between the global upper bound and lower bound achieves a specified tolerance level In the
example in Figure we follow a DFS order Starting from the root node the blue arrows points to
the next node popped from to be branched Updated global lower and upper bounds after a node
expansion is shown on the board under each branched node
Learning Control Policies for Branch-and-Bound
A good search strategy should find a good incumbent solution early and identify non-promising
nodes before they are expanded However naively applying a single heuristic through the whole
process ignores the dynamic structure of the B&B tree For example DFS should only be used at
nodes that promise to lead to a good feasible solution that may replace the incumbent Best-boundfirst search can quickly discard unpromising nodes but should not be used frequently at the top
levels of the tree since the bound estimate is not accurate enough yet Therefore we propose to
learn policies adaptive to different problem types and different solving stages
There are two goals in a B&B search finding the optimal solution and proving its optimality There
is a trade-off between the two goals we may be able to return the optimal solution faster if we do
not invest the time to prove that all other solutions are worse Thus we will aim only to search for
a good possibly optimal solution without a rigorous proof of optimality This allows us to prune
unpromising portions of the search tree more aggressively In addition obtaining a certificate of
optimality is usually of secondary priority for practical purposes
We assume the branching strategy and the bounding functions are given We guide search on the
enumeration tree by two policies Recall that B&B maintains a priority queue of all nodes to be
expanded The node selection policy determines the priorities used Once the highest-priority node
is popped the node pruning policy decides whether to discard or expand it given the current progress
of the solver This process continues iteratively until the tree is empty or the gap reaches some
specified tolerance All other techniques used during usual branch-and-bound search can still be
applied with our method The process is shown in Figure
Oracle Imitation learning requires an oracle at training time to demonstrate the desired behavior
Our ideal oracle would expand nodes in an order that minimized the number of node expansions
subject to finding the optimal solution In real branch-and-bound systems however the optimal
sequence of expanded nodes cannot be obtained without substantial computation After all the effect
of expanding one node depends not only on local information such as the local bounds it obtains
but also on how many pruned nodes it may lead to and many other interacting strategies such as
branching variable selection Therefore given our single goal of finding a good solution quickly we
design an oracle that finds the optimal solution without a proof of optimality We assume optimal
solutions are given for training problems.1 Our node selection oracle will always expand the
node whose feasible set contains the optimal solution We call such a node an optimal node For
example in Figure the oracle knows beforehand that the optimal solution is thus it
will only search along edges and the optimal nodes are shown in red circles All other
non-optimal nodes are fathomed by the node pruning oracle if not already fathomed by standard
rules discussed in Section We denote the optimal node at depth by Fd where and
is the root node
Imitation Learning We formulate the above approach as a sequential decision-making process
defined by a state space an action space A and a policy space A trajectory consists of a
sequence of states s1 s2 sT and actions a1 a2 aT A policy maps a state to an
action at In our B&B setting is the whole tree of nodes visited so far with the
bounds computed at these nodes The node selection policy has an action space select node
Fi Fi queue of active nodes which depends on the current state st The node pruning policy
is a binary classifier that predicts a class in prune expand given st and the most recently
selected node the policy is only applied when this node was not fathomed At training time the
oracle provides an optimal action a for any possible state S. Our goal is to learn a policy that
mimics the oracle?s actions along the trajectory of states encountered by the policy Let Fi Rp
and Fi Rq be feature maps for and respectively The imitation problem can be reduced
to supervised learning the policy classifier/regressor takes a feature-vector description
of the state st and attempts to predict the oracle action a?t
A generic node selection policy assigns a score to each active node and pops the highest-scoring
one For example DFS uses a node?s depth as its score best-bound-first search uses a node?s
lower bound as its score Following this scheme we define the score of a node as wT Fi and
st select node arg maxFi 2L wT Fi where is a learned weight vector and is the
queue of active nodes We obtain by learning a linear ranking function that defines a total order
on the set of nodes on the priority queue wT Fi
if Fi Fi0 During training
we only specify the order between optimal nodes and non-optimal nodes However at test time
a total order is obtained by the classifier?s automatic generalization non-optimal nodes close to
optimal nodes in the feature space will be ranked higher
DAgger is an iterative imitation learning algorithm It repeatedly retrains the policy to make decisions that agree better with the oracle?s decisions in those situations that were encountered when
running past versions of the policy Thus it learns to deal well with a realistic distribution of situations that may actually arise at test time Our training algorithm is shown in Algorithm Algorithm illustrates how we collect examples during B&B. In words when pushing an optimal node
to the queue we want it ranked higher than all nodes currently on the queue when pushing a nonoptimal node we want it ranked lower than the optimal node on the queue if there is one note that
at any time there can be at most one optimal node on the queue when popping a node from the
queue we want it pruned if it is not optimal In the left part of Figure we show training examples
collected from the oracle policy
Analysis
We show that our method has the following upper bound on the expected number of branches
Theorem Given a node selection policy which ranks some non-optimal node higher than an
optimal node with probability a node pruning policy which expands a non-optimal node with
probability and prunes an optimal node with probablity assuming under the
For prediction tasks the optimal solutions usually come for free in the training set otherwise an off-theshelf solver can be used
Algorithm Running B&B policies and collect example for problem
procedure OLLECT XAMPLE(Q
training set DS DP
while
do
Fk
pops a node from
if Fk
is optimal then DP
DP
Fk expand
else DP
DP
Fk prune
if Fk is not fathomed and Fk expand then
expand Fk
if an optimal node Fn
DS
DS
return DS DP
then
Fd
Fi0 and Fi0 Fd
policy?s state distribution we have
expected number of branches
where
Let the optimal node at depth be Fd Note that at each push step there is at most one optimal
node on the queue Consider a queue having one optimal node Fd and non-optimal nodes ranked
before the optimal one The following lemma is useful in our proof
Lemma The average number of pops before we get to Fd is
among which the number
of branches is NB opt
and
the
number
of
non-optimal
nodes pushed after Fd is
Npush opt
where opt indicates the situation
where one optimal node is on the queue
Consider a queue having no optimal node and non-optimal nodes which means an optimal internal node has been pruned or the optimal leaf has been found We have
Lemma The average number of pops to empty the queue is among which the number of
branches is NB opt where opt indicates the situation where no optimal node is on
the queue
Proofs of the above two lemmas are given in Appendix A.
Let Md Fd denote the number of branches until the queue is empty after pushing Fd to a
queue with Md nodes The total number of branches during the B&B process is When
pushing Fd we compare it with all nodes on the queue and the number of non-optimal nodes
ranked before it follows a binomial distribution md Md We then have the following two
cases Fd will be pruned with probability the expected number of branches is NB md opt
Fd will not be pruned with probability we first pop all nodes before Fd resulting in
Npush md opt new nodes after it we then expand Fd get
and push it on a queue with
Npush md opt Md md nodes Thus the total expected number of branches is
NB md opt
The recursion equation is
Md Fd Emd Bin(?,Md NB md
NB Md opt
At termination we have
MD FD
EmD Bin(?,MD NB mD opt)+NB MD mD opt NB MD opt
Note that we ignore node fathoming in this recursion The path of optimal nodes may stop at Fd
where thus Md Fd is an upper bound of the actual expected number of branches The
expectation over md can be computed by replacing md by Md since all terms are linear in md
Solving for gives the upper bound in Theorem Details are given in Appendix B.
For the oracle and it branches at most times when solving a problem For nonoptimal policies as for all pruning-based methods our method bears the risk of missing the optimal
solution The depth at which the first optimal node is pruned follows a geometric distribution and
its mean is In practice we can put higher weight on the class prune to learn a high-precision
classifier smaller
Experiments
Datasets We apply our method to LP-based B&B for solving MILP problems We use four problem
libraries suggested in MIK2 is a set of MILP problems with knapsack constraints Regions
and Hybrid are sets of problems of determining the winner of a combinatorial auction generated
from different distributions by the Combinatorial Auction Test Suite CATS)3 CORLAT
is a real dataset used for the construction of a wildlife corridor for grizzly bears in the Northern
Rockies region The number of variables ranges from to over the number of constraints
ranges from to Each problem set is split into training test and development sets Details of
the datasets are presented in Appendix C. For each problem we run SCIP until optimality and take
the single returned solution to be the optimal one for purposes of training We exclude problems
which are solved at the root in our experiment
Policy learning For each problem set we split its training set into equal-sized subsets randomly and
run DAgger on one subset in each iteration until we have taken two passes over the entire set Too
many passes may result in overfitting for policies in later iterations We use LIBLINEAR in the
step of training classifiers in Algorithm Since mistakes during early stages of the search are more
serious our training places higher weight on examples from nodes closer to the root for both policies
More specifically the example weights at each level of the B&B tree decay exponentially at rate
where is the maximum depth4 corresponding to the fact that the subtree size increases
exponentially For pruning policy training we put a higher weight tuned from on the
class prune to counter data imbalance and to learn a high-precision classifier as discussed earlier
The class weight and SVM?s penalty parameter are tuned for each library on its development set
The features we used can be categorized into three groups node features computed from the
current node including lower bound5 estimated objective depth whether it is a child/sibling of
the last processed node branching features computed from the branching variable leading to
the current node including pseudocost difference between the variable?s value in the current LP
solution and the root LP solution difference between its value and its current bound tree features
computed from the B&B tree including global upper and lower bounds integrality gap number of
solutions found whether the gap is infinite The node selection policy includes primarily node
features and branching feature and the node pruning policy includes primarily branching features
and tree features To combine these features with depth of the node we partition the tree into
uniform levels and features at each level are stacked together Since the range of objective values
varies largely across problems we normalize features related to the bound by dividing its actual
value by the root node?s LP objective All of the above features are cheap to obtain Actually they
use information recorded by most solvers thus do not result in much overhead
Results We compare with SCIP Version using Cplex as the LP solver and Gurobi
Version SCIP?s default node selection strategy switches between depth-first search and
best-first search according a plunging depth computed online Gurobi applies different strategies
including pruning for subtrees rooted at different nodes Both solvers adopt the branch2
Downloaded from http://ieor.berkeley.edu/?atamturk/data
Available at http://www.cs.ubc.ca/?kevinlb/CATS
The rate is chosen such that examples at depth are weighted by and examples at by
If the node is a child of the most recent processed node its LP is not solved yet and its bounds will be the
same as its parent?s
Dataset
MIK
Regions
Hybrid
CORLAT
Ours
speed OGap
Ours prune only
IGap
speed OGap
IGap
SCIP time
Gurobi node
OGap
OGap
IGap
IGap
fail
fail
Table Performance on solving MILP problems from four libraries We compare two versions
of our algorithm one with both search and pruning policies and one with only the pruning policy
with SCIP with a node limit SCIP node and Gurobi with a time limit Gurobi We
report results on three measures speedup with respect to SCIP in default setting the optimality
gap OGap computed as the percentage difference between the best objective value found and the
optimal objective value the integrality gap IGap computed as the percentage difference between
the upper and lower bounds Here fail means the solver cannot find a feasible solution The
numbers are averaged over all instances in each dataset Bolded scores are statistically tied with the
best score according to a t-test with rejection threshold
and-cut framework combined with presolvers and primal heuristics Our solver is implemented
based on SCIP and also calls Cplex to solve LPs.
We compare runtime with SCIP in its default setting which does not terminate before a proved
status solved infeasible unbounded To compare the tradeoff between runtime and solution
quality we first run our dynamic B&B algorithm and obtain the average runtime we then run SCIP
with the same time limit Since runtime is rather implementation-dependent and Gurobi is about
four times faster than SCIP we use the number of nodes explored as time measure for Gurobi
As Gurobi and SCIP apply roughly the same techniques cutting-plane generation heuristics at
each node we believe fewer nodes explored implies runtime improvement had we implemented our
algorithm based on Gurobi Similarly we set Gurobi?s node limit to the average number of nodes
explored by our algorithm
The results are summarized in Table Our method speeds up SCIP up to a factor of with
less than loss in objectives of the found solutions on most datasets On CORLAT the loss is
larger within since these problems are generally harder both SCIP and Gurobi failed to find
even one feasible solution given a time/node limit on some problems Note that SCIP in its default
setting works better on Regions and Hybrid and Gurobi better on the other two while our adaptive
solver performs well consistently This shows that effectiveness of strategies are indeed problem
dependent
Ablation analysis To assess the effect of node selection and pruning separately we report details
of their classification performance in Tabel Both policies cost negligible time compared with the
total runtime We also show result of our method with the pruning policy only in Table We can
see that the major contribution comes from pruning We believe there are two main reasons there
may not be enough information in the features to differentiate an optimal node from non-optimal
ones the effect of node selection may be covered by other interacting techniques for instance a
non-optimal node could lead to better bounds due to the application of cutting planes
Informative features We rank features on each level of the tree according to the absolute values
of their weights for each library Although different problem sets have its own specific weights and
rankings of features a general pattern is that closer to the top of the tree the node selection policy
prefers nodes which are children of the most recently solved node resembles DFS and have better
bounds in lower levels it still prefers deeper nodes but also relies on pseudocosts of the branching
variable and estimates of the node?s objective since these features get more accurate as the search
goes deeper The node pruning policy tends to not pruning when there are few solutions found and
the gap is infinite it also relies much on differences between the branching variable?s value its value
in the root LP solution and its current bound
Cross generalization To testify that our method learns strategies specific to the problem type we
apply the learned policies across datasets using policies trained on dataset A to solve problems
in dataset B. We plot the result as a heatmap in Figure using a measure combining runtime and the
MIK
ions
CORLA Reg
MIK
Hybrid
Policy Dataset
Regions
Hybrid
time opt gap
CORLAT
Test Dataset
Figure Performance of policies cross
datasets The y-axis shows datasets on which
a policy is trained The x-axis shows datasets
on which a policy is tested Each block shows
runtime+optimality gap where runtime
and gap are scaled to for experiments on
the same test dataset Values in each row are
normalized by the diagonal element on that row
Dataset
prune prune err comp time
rate FP FN
err selectprune
MIK
Regions
Hybrid
CORLAT
Table Classification performance of the node
selection and pruning policy We report the percentage of nodes pruned prune rate false positive and false negative error rate of the
pruning policy comparison error of the selection
policy only for comparisons between one optimal
and one non-optimal node as well as the percentage of time used on decision making
optimality gap We invert the values so that hotter blocks in the figure indicate better performance
Note that there is a hot diagonal In addition MIK and CORLAT are relatively unique policies
trained on other datasets lose badly there On the other hand Hybrid is more friendly to other
policies This probably suggests that for this library most strategies works almost equally well
Related Work
There is a large amount of work on applying machine learning to make dynamic decisions inside
a long-running solver The idea of learning heuristic functions for combinatorial search algorithms
dates back to Recently aims to balance load in parallel B&B by predicting the
subtree size at each node Nodes of the largest predicted subtree size are further split into smaller
problems and sent to the distributed environment with other nodes in a batch In a SVM
classifier is used to decide if probing bound tightening technique should be used at a node in
B&B. However both prior methods handle a relatively simple setting where the model only predicts
information about the current state so that they can simply train by standard supervised learning
This is manifestly not the case for us Since actions have influence over future states standard
supervised learning does not work as well as DAgger an imitation learning technique that focuses
on situations most likely to be encountered at test time
Our work is also closely related to speedup learning where the learner observes a solver solving
problems and learns patterns from past experience to speed up future computation and
learned ranking functions to control beam search setting similar to ours in planning and structured
prediction respectively used supervised learning to imitate strong branching in B&B for solving
MIP. The primary distinction in our work is that we explicitly formulate the problem as a sequential
decision-making process thus take aciton?s effects on future into account We also add the pruning
step besides prioritization for further speedup
Conclusion
We have presented a novel approach to learn an adaptive node searching order for different classes of
problems in branch-and-bound algorithms Our dynamic solver learns when to leave an unpromising
area and when to stop for a good enough solution We have demonstrated on multiple datasets that
compared to a commercial solver our approach finds solutions with a better objective and establishes
a smaller gap using less time In the future we intend to include a time budget in our model so that
we can achieve a user-specified trade-off between solution quality and searching time We are also
interested in applying multi-task learning to transfer policies between different datasets

<<----------------------------------------------------------------------------------------------------------------------->>

title: 312-bumptrees-for-efficient-function-constraint-and-classification-learning.pdf

Bumptrees for Efficient Function Constraint and
Classification Learning
Stephen M. Omohundro
International Computer Science Institute
Center Street Suite
Berkeley California
Abstract
A new class of data structures called bumptrees is described These
structures are useful for efficiently implementing a number of neural
network related operations An empirical comparison with radial basis
functions is presented on a robot ann mapping learning task Applications to density estimation classification and constraint representation
and learning are also outlined
WHAT IS A BUMPTREE
A bumptree is a new geometric data structure which is useful for efficiently learning representing and evaluating geometric relationships in a variety of contexts They are a natural
generalization of several hierarchical geometric data structures including oct-trees k-d
trees balltrees and boxtrees They are useful for many geometric learning tasks including
approximating functions constraint surfaces classification regions and probability densities from samples In the function approximation case the approach is related to radial basis
function neural networks but supports faster construction faster access and more flexible
modification We provide empirical data comparing bumptrees with radial basis functions
in section
A bumptree is used to provide efficient access to a collection of functions on a Euclidean
space of interest It is a complete binary tree in which a leaf corresponds to each function
of interest There are also functions associated with each internal node and the defining
constraint is that each interior node's function must be everwhere larger than each of the
Omohundro
functions associated with the leaves beneath it In many cases the leaf functions will be
peaked in locali7..ed regions which is the origin of the name A simple kind of bump function is spherically symmetric about a center and vanishes outside of a specified ball Figure
shows the structure of a two-dimensional bumptree in this setting
Ball supported bump
leaf functions
abc
tree structure
tree functions
Figure A two-dimensional bumptree
A particularly important special case of bumptrees is used to access collections of Gaussian
functions on multi-dimensional spaces Such collections are used for example in representing smooth probability distribution functions as a Gaussian mixture and arises in many
adaptive kernel estimation schemes It is convenient to represent the quadratic exponents
of the Gaussians in the tree rather than the Gaussians themselves The simplest approach is
to use quadratic functions for the internal nodes as well as the leaves as shown in Figure
though other classes of internal node functions can sometimes provide faster access
A
abc
Figure A bumptree for holding Gaussians
Many of the other hierarchical geometric data structures may be seen as special cases of
bumptrees by choosing appropriate internal node functions as shown in Figure Regions
may be represented by functions which take the value inside the region and which vanish
outside of it The function shown in Figure 3D is aligned along a coordinate axis and is constant on one side of a specified value and decreases quadratically on the other side It is represented by specifying the coordinate which is cut the cut location the constant value in
some situations and the coefficient of quadratic decrease Such a function may be evaluated extremely efficiently on a data point and so is useful for fast pruning operations Such
evaluations are effectively what is used in Sproull to implement fast nearest neighbor computation The bumptree structure generalizes this kind of query to allow for different scales for different points and directions The empirical results presented in the next
section are based on bumptrees with this kind of internal node function
Bumptrees for Efficient Function Constraint and Classification Learning
A.
cy
B.
D.
Figure Internal bump functions for A oct-trees kd-trees boxtrees Omohundro
and for balItrees Omohundro and for Sproull's higher
performance kd-tree Sproull
There are several approaches to choosing a tree structure to build over given leaf data Each
of the algorithms studied for balltree construction in Omohundro may be applied to
the more general task of bumptree construction The fastest approach is analogous to the
basic k-d tree construction technique Friedman al and is top down and recursively splits the functions into two sets of almost the same size This is what is used in the
simulations described in the next section The slowest but most effective approach builds
the tree bottom up greedily deciding on the best pair of functions to join under a single parent node Intermediate in speed and quality are incremental approaches which allow one to
dynamically insert and delete leaf functions
Bumptrees may be used to efficiently support many important queries The simplest kind
of query presents a point in the space and asks for all leaf functions which have a value at
that point which is larger than a specified value The bumptree allows a search from the root
to prune any subtrees whose root function is smaller than the specified value at the point
More interesting queries are based on branch and bound and generalize the nearest neighbor queries that k-d trees support A typical example in the case of a collection of Gaussians
is to request all Gaussians in the set whose value at a specified point is within a specified
factor say of the Gaussian whose value is largest at that point The search proceeds
down the most promising branches rust continually maintains the largest value found at
any point and prunes away subtrees which are not within the given factor of the current
largest function value
THE ROBOT MAPPING LEARNING TASK
System
Kinematic space
R3
Visual space
R6
Figure Robot arm mapping task
Omohundro
Figure shows the setup which defines the mapping learning task we used to study the effectiveness of the balltree data structure This setup was investigated extensively by Mel
and involves a camera looking at a robot arm The kinematic state of the ann is defmed by three angle control coordinates and the visual state by six visual coordinates of
highlighted spots on the arm The mapping from kinematic to visual space is a nonlinear
map from three dimensions to six The system attempts to learn this mapping by flailing the
ann around and observing the visual state for a variety of randomly chosen kinematic
states From such a set of random input/output pairs the system must generalize the mapping to inputs it has not seen before This mapping task was chosen as fairly representative
of typical problems arising in vision and robotics
The radial basis function approach to mapping learning is to represent a function as a linear
combination of functions which are spherically symmetric around chosen centers
wjg
Xj In the simplest form which we use here the basis functions are
centered on the input points More recent variations have fewer basis functions than sample
points and choose centers by clustering The timing results given here would be in terms of
the number of basis functions rather than the number of sample points for a variation of this
type Many fonns for the basis functions themselves have been suggested In our study both
Gaussian and linearly increasing functions gave similar results The coefficients of the radial basis functions are chosen so that the sum forms a least squares best fit to the data Such
fits require a time proportional to the cube of the number of parameters in general The experiments reported here were done using the singular value decomposition to compute the
best fit coefficients
The approach to mapping learning based on bumptrees builds local models of the mapping
in each region of the space using data associated with only the training samples which are
nearest that region These local models are combined in a convex way according to influence functions which are associated with each model Each influence function is peaked
in the region for which it is most salient The bumptree structure organizes the local models
so that only the few models which have a great influence on a query sample need to be evaluated If the influence functions vanish outside of a compact region then the tree is used to
prune the branches which have no influence If a model's influence merely dies off with
distance then the branch and bound technique is used to determine contributions that are
greater than a specified error bound
If a set of bump functions sum to one at each point in a region of interest they are called a
partition of unity We fonn influence bumps by dividing a set of smooth bumps either
Gaussians or smooth bumps that vanish outside a sphere by their sum to form an easily
computed partiton of unity Our local models are affine functions determined by a least
squares fit to local samples When these are combined according to the partition of unity
the value at each point is a convex combination of the local model values The error of the
full model is therefore bounded by the errors of the local models and yet the full approximation is as smooth as the local bump functions These results may be used to give precise
bounds on the average number of samples needed to achieve a given approximation error
for functions with a bounded second derivative In this approach linear fits are only done
on a small set of local samples avoiding the computationally expensive fits over the whole
data set required by radial basis functions This locality also allows us to easily update the
model online as new data arrives
Bumptrees for Efficient Function Constraint and Classification Learning
If bi are bump functions such as Gaussians then ftj
bi(x
fonns a partition
Lbj(X
of unity If are the local affine models then the final smoothly interpolated approximating function is Lfti(x)mi(x The influence bumps are centered on the
sample points with a width determined by the sample density The affine model associated
with each influence bump is detennined by a weighted least squares fit of the sample points
nearest the bwnp center in which the weight decreases with distance
Because it performs a global fit for a given number of samples points the radial basis function approach achieves a smaller error than the approach based on bumptrees In terms of
construction time to achieve a given error however bwnptrees are the clear winner.Figure
shows how the mean square error for the robot arm mapping task decreases as a function
of the time to construct the mapping
Mean Square Error
Learning time sees
Figure Mean square error as a function of learning time
Perhaps even more important for applications than learning time is retrieval time Retrieval
using radial basis functions requires that the value of each basis function be computed on
each query input and that these results be combined according to the best fit weight matrix
This time increases linearly as a function of the number of basis functions in the representation In the bumptree approach only those influence bumps and affine models which are
not pruned away by the bumptree retrieval need perform any computation on an input Figure shows the retrieval time as a function of number of training samples for the robot mapping task The retrieval time for radial basis functions crosses that for balltrees at about
samples and increases linearly off the graph The balltree algorithm has a retrieval time
which empirically grows very slowly and doesn't require much more time even when
samples are represented
While not shown here the representation may be improved in both size and generalization
capacity by a best first merging technique The idea is to consider merging two local models
and their influence bumps into a single model The pair which increases the error the least
Omohundro
is merged frrst and the process is repeated until no pair is left whose meger wouldn't exceed
an error criterion This algorithm does a good job of discovering and representing linear
parts of a map with a single model and putting many higher resolution models in areas with
strong nonlinearities
Retrieval time sees
Gaussian RBF
Bumptree
2K 4K 6K 8K
Figure Retrieval time as a function of number of training samples
EXTENSIONS TO OTHER TASKS
The bumptree structure is useful for implementing efficient versions of a variety of other
geometric learning tasks Omohundro Perhaps the most fundamental such task is
density estimation which attempts to model a probability distribution on a space on the basis of samples drawn from that distribution One powerful technique is adaptive kernel estimation Devroye and Gyorfi The estimated distribution is represented as a
Gaussian mixture in which a spherically symmetric Gaussian is centered on each data point
and the widths are chosen according to the local density of samples A best-first merging
technique may often be used to produce mixtures consisting of many fewer non-symmetric
Gaussians A bumptree may be used to fmd and organize such Gaussians Possible internal
node functions include both quadratics and the faster to evaluate functions shown in Figure
It is possible to effICiently perform many operations on probability densities represented in
this way The most basic query is to return the density at a given location The bumptree
may be used with branch and bound to achieve retrieval in logarithmic expected time It is
also possible to quickly fmd marginal probabilities by integrating along certain dimensions
The tree is used to quickly identify the Gaussian which contribute Conditional distributions may also be represented in this form and bumptrees may be used to compose two such
distributions
Above we discussed mapping learning and evaluation In many situations there are not the
natural input and output variables required for a mapping If a probability distribution is
peaked on a lower dimensional surface it may be thought of as a constraint Networks of
Bumptrees for Efficient Function Constraint and Classification Learning
constraints which may be imposed in any order among variables are natural for describing
many problems Bumptrees open up several possibilities efficiently representing and
propagating smooth constraints on continuous variables The most basic query is to specify
known external constraints on certain variables and allow the network to further impose
whatever constraints it can Multi-dimensional product Ganssians can be used to represent
joint ranges in a set of variables The operation of imposing a constraint surface may be
thought of as multiplying an external constraint Gaussian by the function representing the
constraint distribution Because the product of two Gaussians is a Gaussian this operation
always produces Gaussian mixtures and bumptrees may be used to facilitate the operation
A representation of constraints which is more like that used above for mappings consbUcts
surfaces from local affine patches weighted by influence functions We have developed a
local analog of principle components analysis which builds up surfaces from random samples drawn from them As with the mapping structures a best-frrst merging operation may
be used to discover affine sbUcture in a constraint surface
Finally bumptrees may be used to enhance the performance of classifiers One approach is
to directly implement Bayes classifiers using the adaptive kernel density estimator described above for each class distribution function A separate bumptree may be used for
each class or with a more sophisticated branch and bound a single tree may be used for the
whole set of classes
In summary bumptrees are a natural generalization of several hierarchical geometric access structures and may be used to enhance the performance of many neural network like
algorithms While we compared radial basis functions against a different mapping learning
technique bumptrees may be used to boost the retrieval performance of radial basis functions directly when the basis functions decay away from their centers Many other neural
network approaches in which much of the network does not perform useful work for every
query are also susceptible to sometimes dramatic speedups through the use of this kind of
access SbUCture

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1625-unsupervised-and-supervised-clustering-the-mutual-information-between-parameters-and-observations.pdf

Unsupervised and supervised clustering
the mutual information between
parameters and observations
Didier Herschkowitz
Jean-Pierre Nadal
Laboratoire de Physique Statistique de l'E.N.S
Ecole Normale Superieure
24 rue Lhomond Paris cedex 05 France
herschko@lps.ens.fr
nadal@lps.ens.fr
http://www.lps ens.frrrisc/rescomp
Abstract
Recent works in parameter estimation and neural coding have
demonstrated that optimal performance are related to the mutual
information between parameters and data We consider the mutual
information in the case where the dependency in the parameter
vector of the conditional of each observation vector
is through the scalar product only We derive bounds and
asymptotic behaviour for the mutual information and compare with
results obtained on the same model with the replica technique
INTRODUCTION
In this contribution we consider an unsupervised clustering task Recent results
on neural coding and parameter estimation supervised and unsupervised learning
tasks show that the mutual information between data and parameters equivalently between neural activities and stimulus is a relevant tool for deriving optimal
performances Clarke and Barron Nadal and Parga Opper and Kinzel
Haussler and Opper Opper and Haussler Rissanen BruneI
and Nadal
Laboratory associated with ENS and Universities Paris VI
and Paris VII.
Mutual Information between Parameters and Observations
With this tool we analyze a particular case which has been studied extensively
with the replica echnique in the framework of statistical mechanics Watkin and
Nadal Reimann and Van den Broeck Buhot and Gordon After
introducing the model in the next section we consider the mutual information
between the patterns and the parameter We derive a bound on it which is of
interest for not too large We show how the free energy associated to Gibbs
learning is related to the mutual information We then compare the exact results
with replica calculations We show that the asymptotic behaviour of
the mutual information is in agreement with the exact result which is known to be
related to the Fish er information Clarke and Barron Rissanen Brunei
and Nadal However for moderate values of a pIN we can eliminate false
solutions of the replica calculation Finally we give bounds related to the mutual
information between the parameter and its estimators and discuss common features
of parameter estimation and neural coding
THE MODEL
We consider the problem where a direction unit vector of dimension has
to be found based on the observation of patterns The probability distribution
of the patterns is uniform except in the unknown symmetry-breaking direction
Various instances of this problem have been studied recently within the satistical
mechanics framework making use of the replica technique Watkin and Nadal
Reimann and Van den Broeck Buhot and Gordon More specifically
it is assumed that a set of patterns is generated by independant
samplings from a non-uniform probability distribution where Ol ON
represents the symmetry-breaking orientation The probability is written in the
form
o/S ex
where is the dimension of the space A is the overlap and characterizes
the structure of the data in the breaking direction As justified within the Bayesian
and Statistical Physics frameworks one has to consider a prior distribution on the
parameter space the uniform distribution on the sphere
The mutual information J(DIO between the data and is defined by
It can be rewritten
J(DIO
where
a
dOp(O exp
J.L=l
In the statistical physics literature In is a free energy The brackets
stand for the average over the pattern distribution and is the average over
the resulting overlap distribution We will consider properties valid for any Nand
any others for and the replica calculations are valid for Nand large
at any given value of a
D. Herschkowitz and Nadal
LINEAR BOUND
The mutual information a positive quantity cannot grow faster than linearly in the
amount of data We derive the simple linear bound
We proove the inequality for the case The extension to the case
is straightforward The mutual information can be written as H(DIB
The calculation of H(DIB is straightforward
H(DIB
Now the entropy of the data dDP(D)lnP(D is lower or equal to the
entropy of a Gaussian distribution with the same variance We thus calculate the
covariance matrix of the data
where
6ij
denotes the average over the parameter distribution
pN
We then have
i=l
where I are the eigen value of the matrix BiB Using
In(l
I Bt and the
property
i=l
we obtain
Putting and together we find the inequality l.From this and it
follows also
REPLICA CALCULATIONS
In the limit with a finite the free energy becomes self-averaging that
is equal to its average and its calculation can be performed by standard replica
technique This calculation is the same as calculations related to Gibbs learning
done in Reimann and van den Broeck Buhot and Gordon but the
interpretation of the order parameters is different Assuming replica symmetry we
reproduce in fig.2 results from Buhot and Gordon for the behaviour with a
of which is the typical overlap between two directions compatible with the data
The overlap distribution was chosen to get patterns distributed according to
two clusters along the symmetry-breaking direction
exp
In fig.2 and fig.1 we show the corresponding behaviour of the average free energy
and of the mutual information
Mutual Information between Parameters and Observations
Discussion
Up to aI and the mutual information is in a purely linear phase
a This correspond to a regime where the data have no correlations
For a aI the replica calculation admits up to three differents solutions In view of
the fact that the mutual information can never decrease with a and that the average
free energy can not be positive it follows that only two behaviours are acceptable
In the first leaves the solution at aI and follows the lower branch until a
where it jumps to the upper branch This is the stable way The second possibility
is that until a2 where it directly jumps to the upper branch In Buhot and
Gordon it has been suggested that One can reach the upper branch well
before a Here we have thus shown that it is only possible from It remains
also the possibility of a replica symetry breacking phase in this range of a
In the limit a
information
the replica calculus gives for the behaviour of the mutual
I(DIO
In(a
The r.h.s can be shown to be equal to half the logarithm of the determinant of the
Fish er information matrix which is the exact asymptotic behaviour Clarke and
Barron BruneI and Nadal It can be shown that this behaviour for
implies that the best possible estimator based on the data will saturate
the Cramer-Rao bound Blahut It has already been noted that the
asymptotics performance in estimating the direction as computed by the replica
technique saturate this bound Van den Broeck What we have check here
is that this manifests itself in the behaviour of the mutual information for large a
Bounds for specific estimators
Given the data one wants to find an estimate of the parameter The amount
of information I(DIO limits the performance of the estimator Indeed one has
I(JIO I(DIO This basic relationship allows to derive interesting bounds based
on the choice of particular estimators We consider first Gibbs learning which
consists in sampling a direction from the a posteriori probability P(JID
P(DIJ)p(J In this particular case the differential entropy ofthe estimator
and of the parameter are equal If Qg is the variance of the
Gibbs estimator one gets for a Gaussian prior on the relations
These relations together with the linear bound allows to bound the order parameter Qg for small a where this bound is of interest
The Bayes estimator consists in taking for the center of mass of the a posteriori
probability In the limit a this distribution becomes Gaussian centered at its
most probable value We can thus assume PBayes JIO to be Gaussian with mean
QbB and variance Qb then the first inequality in with Qg replaced by
Qb and Gibbs by Bayes is an equality Then using the Cramer-Rao bound on the
variance of the estimator that is one can
bound the mutual information for the Bayes estimator
IBay es(JIB
a
D. Herschkowitz and J-P. Nadal
These different quantities are shown on
CONCLUSION
We have studied the mutual information between data and parameter in a problem
of unsupervised clustering ve deriyed bounds asymptotic behaviour and compared these results with replica calculations Most of the results concerning the
behaviour of the mutual information observed for this particular clustering task
are universal in that they will be qualitatively the same for any problem which
can be formulated as either a parameter estimation task or a neural coding/signal
processing task In particular there is a linear regime for small enough amount of
data number of coding cells up to a maximal value related to the VC dimension
of the system For large data size the behaviour is logarithmic that is I lnp
Nadal and Parga Opper and Haussler or lnp Clarke and Barron Opper and Haussler BruneI and Nadal depending on the
smoothness of the model A mOre detailed review with mOre such universal features
exact bounds and relations between unsupervised and supervised learning will be
presented elsewhere Nadal Herschkowitz to appear in Phys rev E).
Acknowledgements
We thank Arnaud Buhot and Mirta Gordon for stimulating discussions This work
has been partly supported by the French contract DGA 96 A/DSP

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5159-global-map-optimality-by-shrinking-the-combinatorial-search-area-with-convex-relaxation.pdf

Global MAP-Optimality by Shrinking the
Combinatorial Search Area with Convex Relaxation
Bogdan Savchynskyy1
J?org Kappes2
Paul Swoboda2
Christoph Schn?orr1,2
Heidelberg Collaboratory for Image Processing Heidelberg University Germany
bogdan.savchynskyy@iwr.uni-heidelberg.de
Image and Pattern Analysis Group Heidelberg University Germany
kappes,swoboda,schnoerr}@math.uni-heidelberg.de
Abstract
We consider energy minimization for undirected graphical models also known as
the MAP-inference problem for Markov random fields Although combinatorial
methods which return a provably optimal integral solution of the problem made a
significant progress in the past decade they are still typically unable to cope with
large-scale datasets On the other hand large scale datasets are often defined on
sparse graphs and convex relaxation methods such as linear programming relaxations then provide good approximations to integral solutions
We propose a novel method of combining combinatorial and convex programming techniques to obtain a global solution of the initial combinatorial problem
Based on the information obtained from the solution of the convex relaxation our
method confines application of the combinatorial solver to a small fraction of the
initial graphical model which allows to optimally solve much larger problems
We demonstrate the efficacy of our approach on a computer vision energy minimization benchmark
Introduction
The focus of this paper is energy minimization for Markov random fields In the most common
pairwise case this problem reads
min min
xv
uv xu xv
x?XG
x?XG
v?VG
uv?EG
where VG EG denotes an undirected graph with the set of nodes VG and the set of
edges EG uv variables xv belong to the finite label sets Xv VG potentials Xv
uv Xu Xv VG uv EG are associated with the nodes and the edges of respectively
We denote by XG the Cartesian product v?VG Xv
Problem is known to be NP-hard in general hence existing methods either consider its convex
relaxations or/and apply combinatorial techniques such as branch-and-bound combinatorial search
cutting plane etc on top of convex relaxations The main contribution of this paper is a novel
method to combine convex and combinatorial approaches to compute a provably optimal solution
The method is very general in the sense that it is not restricted to a specific convex programming
or combinatorial algorithm although some algorithms are more preferable than others The main
restriction of the method is the neighborhood structure of the graph it has to be sparse Basic grid
graphs of image data provide examples satisfying this requirement The method is applicable also to
higher-order problems defined on so called factor graphs however we will concentrate mainly
on the pairwise case to keep our exposition simple
Underlying idea demonstrates the main idea of our method Let A and be two subgraphs
covering G. Select them so that the only common nodes of these subgraphs lie on their mutual border
A
label mismatch
Solve A and separately
Check consistency on A
Increase
Figure Underlying idea of the proposed method the initial graph is split into two subgraphs A
blue+yellow and red+yellow assigned to a convex and a combinatorial solver respectively If
the integral solutions provided by both solvers do not coincide on the common border A yellow
of the two subgraphs the subgraph is increased by appending mismatching nodes green and the
border is adjusted respectively
defined in terms of the master-graph G. Let x?A and x?B be optimal labelings computed
independently on A and B. If these labelings coincide on the border then under some additional
conditions the concatenation of x?A and x?B is an optimal labeling for the initial problem as we
show in Section Theorem
We select the subgraph A such that it contains a simple part of the problem for which the convex
relaxation is tight This part is assigned to the respective convex program solver The subgraph
contains in contrast the difficult combinatorial subproblem and is assigned to a combinatorial
solver If the labelings x?A and x?B do not coincide on some border node we increase the
subgraph by appending the node and edges from to correspondingly decrease A and
iii recompute x?A and x?B This process is repeated until either labelings x?A and x?B coincide on
the border or equals G. The sparsity of is required to avoid fast growth of the subgraph B.
We refer to Section for a detailed description of the algorithm where we in particular specify the
initial selection of the subgraphs A and and the methods for encouraging consistency of x?A
and x?B on the boundary A and providing equivalent results with just a single run of the convex
relaxation solver These techniques will be described for the local polytope relaxation known also
as a linear programming relaxation of
Related work The literature on problem is very broad both regarding convex programming and
combinatorial methods Here we will concentrate on the local polytope relaxation that is essential
to our approach
The local polytope relaxation of was proposed and analyzed in also the recent
review An alternative view on the same relaxation was proposed in This view appeared to
be very close to the idea of the Lagrangian or dual decomposition technique for applications
to This idea stimulated development of efficient solvers for convex relaxations of Scalable
solvers for the LP relaxation became a hot topic in recent years The algorithms however
which guarantee attainment of the optimum of the convex relaxation at least theoretically are quite
slow in practice see comparisons in Remarkably the fastest scalable algorithms
for convex relaxations are based on coordinate descent the diffusion algorithm known from
the seventies and especially its dual decomposition based variant TRW-S There are other
closely related methods based on the same principle Although these algorithms do not
guarantee attainment of the optimum they converge to points fulfilling a condition known as
arc consistency or weak tree agreement We show in Section that this condition plays a
significant role for our approach It is a common observation that in the case of sparse graphs and/or
strong evidence of the unary terms VG the approximate solutions delivered by such solvers
are quite good from the practical viewpoint The belief that these solutions are close to optimal
ones is evidenced by numerical bounds which these solvers provide as a byproduct
The techniques used in combinatorial solvers specialized to problem include most of the classical tools cutting plane combinatorial search and branch-and-bound methods were adapted to the
problem The ideas of the cutting plane method form the basis for tightening the LP relaxation
within the dual decomposition framework the recent review and

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3135-branch-and-bound-for-semi-supervised-support-vector-machines.pdf

Branch and Bound for
Semi-Supervised Support Vector Machines
Olivier Chapelle1
Max Planck Institute
ubingen Germany
chapelle@tuebingen.mpg.de
Vikas Sindhwani
University of Chicago
Chicago USA
vikass@cs.uchicago.edu
S. Sathiya Keerthi
Yahoo Research
Santa Clara USA
selvarak@yahoo-inc.com
Abstract
Semi-supervised SVMs VM attempt to learn low-density separators by
maximizing the margin over labeled and unlabeled examples The associated optimization problem is non-convex To examine the full potential
of S3 VMs modulo local minima problems in current implementations we
apply branch and bound techniques for obtaining exact globally optimal
solutions Empirical evidence suggests that the globally optimal solution
can return excellent generalization performance in situations where other
implementations fail completely While our current implementation is only
applicable to small datasets we discuss variants that can potentially lead
to practically useful algorithms
Introduction
A major line of research on extending SVMs to handle partially labeled datasets is based on
the following idea solve the standard SVM problem while treating the unknown labels as
additional optimization variables By maximizing the margin in the presence of unlabeled
data one learns a decision boundary that traverses through low data-density regions while
respecting labels in the input space In other words this approach implements the cluster
assumption for semi-supervised learning that points in a data cluster have similar labels
This idea was first introduced in under the name Transductive SVM but since it learns
an inductive rule defined over the entire input space we refer to this approach as Semisupervised SVM VM).
Since its first implementation in a wide spectrum of techniques have been applied to
solve the non-convex optimization problem associated with S3 VMs local combinatorial
search gradient descent continuation techniques convex-concave procedures
and deterministic annealing While non-convexity is partly responsible for this diversity of methods it is also a departure from one of the nicest features of SVMs Several
experimental studies have established that S3 VM implementations show varying degrees
of empirical success This is conjectured to be closely tied to their susceptibility to local
minima problems
The following questions motivate this paper How well do current S3 VM implementations
approximate the exact globally optimal solution of the non-convex problem associated with
S3 VMs Can one expect significant improvements in generalization performance by better
approaching the global solution We believe that these questions are of fundamental importance for S3 VM research and are largely unresolved This is partly due to the lack of simple
implementations that practitioners can use to benchmark new algorithms against the global
solution even on small-sized problems
Now part of Yahoo Research chap@yahoo-inc.com
Our contribution in this paper is to outline a class of Branch and Bound algorithms that
are guaranteed to provide the globally optimal solution for S3 VMs. Branch and bound
techniques have previously been noted in the context of S3 VM in but no details were
presented there We implement and evaluate a branch and bound strategy that can serve
as an upper baseline for S3 VM algorithms This strategy is not practical for typical semisupervised settings where large amounts of unlabeled data is available But we believe it
opens up new avenues of research that can potentially lead to more efficient variants
Empirical results on some semi-supervised tasks presented in section show that the exact
solution found by branch and bound has excellent generalization performance while other
S3 VM implementations perform poorly These results also show that S3 VM can compete
and even outperform graph-based techniques on problems where the latter
class of methods have typically excelled
Semi-Supervised Support Vector Machines
We consider the problem of binary classification The training set consists of labeled
examples and of the unlabeled examples with
In the linear case the following objective function is minimized on both the hyperplane
parameters and and on the label vector yu yn
min
w,b,yu
ip
ip
under constraints Non linear decision boundaries can
be constructed using the kernel trick While in general any convex loss function can be
used it is common to either penalize the training errors linearly or quadratically
In the rest of the paper we consider The first two terms in correspond
to a standard SVM. The last one takes into account the unlabeled points and can be seen
as an implementation of the cluster assumption or low density separation assumption
indeed it drives the outputs of the unlabeled points away from figure
Loss
Signed output
Figure With in the loss of a point with label and signed output is
For an unlabeled point this is miny
For simplicity we take C. But in practice it is important to set these two values
independently because reflects our confidence in the labels of the training points while
corresponds to our belief in the low density separation assumption In addition we add
the following balancing constraint to
max(yi
This constraint is necessary to avoid unbalanced solutions and has also been used in the
original implementation Ideally the parameter should be set to the ratio of positive
points in the unlabeled set Since it is unknown is usually estimated through the class
ratio on the labeled set In that case one may wish to soften this constraint as in
For the sake of simplicity in the rest of the paper we set to the true ratio of positive
points in the unlabeled set
Let us call I the objective function to be minimized
yu
There are two main strategies to minimize I
For a given fixed and the optimal yu is simply given by the signs of Then
a continuous optimization on and can be done But note that the constraint is
then not straightforward to enforce
For a given yu the optimization on and is a standard SVM training Let?s define
yu min yu
w,b
Now the goal is to minimize over a set of binary variables and each evaluation of is
a standard SVM training This was the approach followed in and the one that we take
in this paper The constraint is implemented by setting yu for all vectors yu
not satisfying it
Branch and bound
Branch and bound basics
Suppose we want to minimize a function over a space where is usually discrete A
branch and bound algorithm has two main ingredients
Branching the region is recursively split into smaller subregions This yields a tree
structure where each node corresponds to a subregion
Bounding consider two disjoint subregions nodes A and Suppose that
an upper bound say on the best value of over A is known and a lower bound
say on the best value of over is known and that a Then we know
there is an element in the subset A that is better than all elements of B. So when
searching for the global minimizer we can safely discard the elements of from the
search the subtree corresponding to is pruned
Branch and bound for S3 VM
The aim is to minimize over all 2u possible choices for the vector yu which constitute
the set introduced above The binary search tree has the following structure Any
node corresponds to a partial labeling of the data set and its two children correspond to
the labeling of some unlabeled point One can thus associate with any node a labeled
set containing both the original labeled examples and a subset of unlabeled examples
yj to which the labels yj have been assigned One can also associate an
unlabeled set corresponding to the subset of unlabeled points which
have not been assigned a label yet The size of the subtree rooted at this node is thus
The root of the tree has only the original set of labeled examples associated with it i.e
is empty The leaves in the tree correspond to a complete labeling of the dataset is
empty All other nodes correspond to partial labelings
As for any branch and bound algorithm we have to decide about the following choices
Branching For a given node in the tree a partial labeling of the unlabeled set what
should be its two children which unlabeled point should be labeled next
Bounding Which upper and lower bounds should be used
Exploration In which order will the search tree be examined In other words which
subtree should be explored next Note that the tree is not built explicitly but on
the fly as we explore it
There are actually only
ur
effective choices because of the constraint
Concerning the upper bound we decided to have the following simple strategy for a leaf
node the upper bound is simply the value of the function for a non leaf node there is no
upper bound In other words the upper bound is the best objective function found so far
Coming back to the notations of section the set A is the leaf corresponding to the best
solution found so far and the set is the subtree that we are considering to explore
Because of this choice for the upper bound a natural way to explore the tree is a depth first
search Indeed it is important to go to the leaves as often as possible in order to have a
tight upper bound and thus perform aggressive pruning
The choice of the lower bound and the branching strategy are presented next
Lower bound
We consider a simple lower bound based on the following observation The minimum of the
objective function is smaller when than when But corresponds
to a standard SVM ignoring the unlabeled data We can therefore compute a lower bound
at a given node by optimizing a standard SVM on the labeled set associated with this node
We now present a more general framework for computing lower bounds It is based on the
dual objective function of SVMs Let yU be the dual objective function where yU
corresponds to the labels of the unlabeled points which have not been assigned a label yet
yU
yj K(xi
2C
The dual feasibility is
and
Now suppose that we have a strategy that given yU finds a vector satisfying
Since the dual is maximized
yU max yU yU
where has been defined in
Let Q(yU yU and lb a lower bound on the value of min Q(yU where
the minimum is taken over all yU satisfying the balancing constraint Then lb is also a
lower bound for the value of the objective function corresponding to that node
The goal is thus to find a choice for such that a lower bound on can be computed
efficiently The choice corresponding to the lower bound presented above is the following
Train an SVM on the labeled points obtain the vector and complete it with zeros for the
unlabeled points Then Q(yU is the same for all the possible labelings of the unlabeled
points and the lower bound is the SVM objective function on the labeled points
Here is a sketch of another possibility for that one can explore instead of completing
the vector by zeros we complete it by a constant
would typically be of the
which
same order of magnitude as Then Q(yU
Hy where Hij Kij
To lower bound one can use results from the quadratic zero-one programming
literature
or solve
a
constrained
eigenvalue
problem
Finally
note
that
unless
the
constraint
be
satisfied
One
remedy
is
to
train
the
supervised
SVM with
will not
the constraint
2ru because of In the primal
this amounts to penalizing the bias term
Branching
At a given node some unlabeled points have already been assigned a label Which unlabeled
point should be labeled next Since our strategy is to reach a good solution as soon as
possible last paragraph of section it seems natural to assign the label that we are
the most confident about A simple possibility would be to branch on the unlabeled point
which is the nearest from another labeled point using a reliable distance metric
But we now present a more principled approach based on the analysis of the objective value
We say that we are confident about a particular label of an unlabeled point when assigning
the opposite label results in a big increase of the objective value this partial solution would
then be unlikely to lead to the optimal one
Let us formalize this strategy Remember from section that a node is associated with
a set of currently labeled examples and a set of unlabeled examples Let be the
SVM objective function trained on the labeled set
min
w,b
As discussed in the previous section the lower bound is Now our branching strategy
consists in selecting the following point in
arg max s(L
In other words we want to find the unlabeled point and its label which would make the
objective function increase as much as possible Then we branch on but start exploring
the branch with the most likely label This strategy has an intuitive link with the label
propagation idea an unlabeled point which is near from a labeled point is likely to be
of the same label otherwise the objective function would be large
A main disadvantage of this approach is that to solve a lot of SVM trainings are
necessary It is however possible to approximately compute s(L The idea is
similar to the fast approximation of the leave-one-out solution Here the situation is
add-one-in If an SVM has been trained on the set it is possible to efficiently compute
the solution when one point is added in the training set This is under the assumption that
the set of support vectors does not change when adding this point In practice the set is
likely to change and the solution will only be approximate
Proposition Consider training an SVM on a labeled set with quadratic penalization of
the errors cf or Let be the learned function and sv be the set of support vectors
Then if sv does not change while adding a point in the training set
yf
s(L
where Sx2
sv
K(xi 2C
x)i?sv
Ksv
and
i,j?sv
Ksv
ysv
The proof is omitted because of lack of space It is based on the fact that ysv
and relies on the block matrix inverse formula
Algorithm
The algorithm is implemented recursively algorithm At the beginning the upper
bound can either be set to or to a solution found by another algorithm
Note that the SVM trainings are incremental whenever we go down the tree one point is
added in the labeled set For this reason the retraining can be done efficiently also see
since effectively we just need to update the inverse of a matrix
Experiments
We consider here two datasets where other S3 VM implementations are unable to achieve
satisfying test error rates This naturally raises the following questions Is this weak per
Algorithm Branch and bound for S3 VM(BB
Function S3 VM(Y ub
Recursive implementation
Input
a partly labeled vector for unlabeled
ub an upper bound on the optimal objective value
Output
optimal fully labeled vector
corresponding
objective function
if
Yi ur OR
Yi ur then
return
Constraint can not be satisfied
end if
Do not explore this subtree
SVM(Y
Compute the SVM objective function on the labeled points
if ub then
return
The lower bound is higher than the upper bound
end if
Do not explore this subtree
if is fully labeled then
return
We are at a leaf
end if
Find index and label as in
Find next unlabeled point to label
Yi
Start first by the most likely label
S3 VM(Y ub
Find recursively the best solution
Yi Yi
Switch the label
v2 S3 VM(Y min(ub
Explore other branch with updated upper-bound
if v2 then
and v2
Keep the best solution
end if
formance due to the unsuitability of the S3 VM objective function for these problems or do
these methods get stuck at highly sub-optimal local minima
Two moons
The two moons dataset is now a standard benchmark for semi-supervised learning algorithms Most graph-based methods such as easily solve this problem but so far all
S3 VM algorithms find it difficult to construct the right boundary an exception is using
an L1 loss We drew random realizations of this dataset fixed the bandwidth of an
RBF kernel to and set Each moon contained unlabeled points
We compared cS3 CCCP SVMlight and DA For the first
methods there is no direct way to enforce the constraint However these methods have
a constraint that the mean output on the unlabeled point should be equal to some constant
This constant is normally fixed to the mean of the labels but for the sake of consistency we
did a dichotomy search on this constant in order to have satisfied
Results are presented in table Note that the test errors for other S3 VM implementations
are likely to be improved by hyperparameter tuning but they will still stay very high For
comparison we have also included the results of a state-of-the-art graph based method
LapSVM whose hyperparameters were optimized for the test error and the threshold
adjusted to satisfy the constraint
Matlab source code and a demo of our algorithm on the two moons dataset is accessible
as supplementary material with this paper
COIL
Extensive benchmark results reported in benchmark chapter show that on problems
where classes are expected to reside on low-dimensional non-linear manifolds handwritten digits graph-based algorithms significantly outperform S3 VM implementations
Table Results on the two moons dataset averaged over random realizations
VM
cS3 VM
CCCP
SVMlight
DA
BB
LapSVM
Test error
64
Objective function
N/A
We consider here such a dataset by selecting three confusible classes from the COIL20
dataset figure There are 72 images per class corresponding to rotations of
degrees and thus yielding a one dimensional manifold We randomly selected images per
class to be in the labeled set and the rest being unlabeled Results are reported in table
The hyperparameters were chosen to be and
Figure The cars from the COIL dataset subsampled to
Table Results on the Coil dataset averaged over random realizations
VM
cS3 VM
CCCP
SVMlight
DA
BB
LapSVM
Test error
Objective function
N/A
From tables and it appears clearly that the S3 VM objective function leads to excellent
test errors other S3 VM implementations fail completely in finding a good minimum of
the objective function2 and the global S3 VM solution can actually outperform graphbased alternatives even if other S3 VM implementations are not found to be competitive
Concerning the running time it is of the order of a minute for both datasets We do not
expect this algorithm to be able to handle datasets much larger than couple of hundred
points
Discussion and Conclusion
We implemented and evaluated one strategy amongst many in the class of branch and
bound methods to find the globally optimal solution of S3 VMs. The work of is the most
closely related to our methods However that paper presents an algorithm for linear S3 VMs
and relies on generic mixed integer programming which does not make use of the problem
structure as our methods can
This basic implementation can perhaps be made more efficient by choosing better bounding
and branching schemes Also by fixing the upper bound as the currently best objective
The reported test errors are somehow irrelevant and should not be used for ranking the different
algorithms They should just be interpreted as failure
value we restricted our implementation to follow depth-first search It is conceivable that
breadth-first search is equally or more effective in conjunction with alternative upper bounding schemes Pruning can be done more aggressively to speed-up termination at the expense
of obtaining a solution that is suboptimal within some tolerance prune if a
Finally we note that a large family of well-tested branch and bound procedures from zeroone quadratic programming literature can be immediately applied to the S3 VM problem for
the special case of squared loss An interesting open question is whether one can provide
a guarantee for polynomial time convergence under some assumptions on the data and the
kernel
Concerning the running time of our current implementation we have observed that it is
most efficient whenever the global minimum is significantly smaller than most local minima
in that case the tree can be pruned efficiently This happens when the clusters are well
separated and and are not too small
For these reasons we believe that this implementation does not scale to large datasets but
should instead be considered as a proof of concept the S3 VM objective function is very well
suited for semi-supervised learning and more effort should be made on trying to efficiently
find good local minima

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5189-despot-online-pomdp-planning-with-regularization.pdf

DESPOT Online POMDP Planning with Regularization
Adhiraj Somani
Nan Ye
David Hsu
Wee Sun Lee
Department of Computer Science
National University of Singapore
adhirajsomani@gmail.com yenan,dyhsu,leews}@comp.nus.edu.sg
Abstract
POMDPs provide a principled framework for planning under uncertainty but are
computationally intractable due to the curse of dimensionality and the curse
of history This paper presents an online POMDP algorithm that alleviates these
difficulties by focusing the search on a set of randomly sampled scenarios A
Determinized Sparse Partially Observable Tree DESPOT compactly captures the
execution of all policies on these scenarios Our Regularized DESPOT R-DESPOT
algorithm searches the DESPOT for a policy while optimally balancing the size of
the policy and its estimated value obtained under the sampled scenarios We give
an output-sensitive performance bound for all policies derived from a DESPOT
and show that R-DESPOT works well if a small optimal policy exists We also give
an anytime algorithm that approximates R-DESPOT Experiments show strong
results compared with two of the fastest online POMDP algorithms Source code
along with experimental settings are available at http://bigbird.comp
nus.edu.sg/pmwiki/farm/appl
Introduction
Partially observable Markov decision processes POMDPs provide a principled general framework
for planning in partially observable stochastic environments However POMDP planning is computationally intractable in the worst case The challenges arise from three main sources First
a POMDP may have a large number of states Second as the state is not fully observable the
agent must reason with beliefs which are probability distributions over the states Roughly the size
of the belief space grows exponentially with the number of states Finally the number of actionobservation histories that must be considered for POMDP planning grows exponentially with the
planning horizon The first two difficulties are usually referred to as the curse of dimensionality
and the last one the curse of history To address these difficulties online POMDP planning
for a survey chooses one action at a time and interleaves planning and plan execution At each
time step the agent performs a D-step lookahead search It plans the immediate next action for the
current belief only and reasons in the neighborhood of the current belief rather than over the entire
belief space Our work adopts this online planning approach
Recently an online POMDP planning algorithm called POMCP has successfully scaled up to very
large POMDPs POMCP which is based on Monte Carlo tree search tries to break the two
curses by sampling states from the current belief and sampling histories with a black-box simulator It uses the UCT algorithm to control the exploration-exploitation trade-off during the online
lookahead search However UCT is sometimes overly greedy and suffers the worst-case performance of exp(exp samples to find a sufficiently good action
This paper presents a new algorithm for online POMDP planning It enjoys the same strengths
as POMCP?breaking the two curses through sampling?but avoids POMCP?s extremely poor
worst-case behavior by evaluating policies on a small number of sampled scenarios In each
planning step the algorithm searches for a good policy derived from a Determinized Sparse Partially Observable Tree DESPOT for the current belief and executes the policy for one step A
DESPOT summarizes the execution of all policies under sampled scenarios It is structurally
similar to a standard belief tree but contains only belief nodes reachable under the scenarios
Composition of exponential functions
a1
o2
o1
o1
a2
a1
a2
o2
o1
o2
o1
o1
o2
a1
a2
o2
o1
o2
Figure We can view a DESPOT as a sparsely
sampled belief tree While a belief tree of height
contains nodes where and
are the sizes of the action set and the observation set respectively a corresponding DESPOT
contains only nodes leading to dramatic improvement in computational efficiency
when is small
One main result of this work is an output-sensitive
bound showing that a small number of sampled
Figure A belief tree of height gray scenarios is sufficient to give a good estimate
and a corresponding DESPOT black obtained with
sampled scenarios Every tree nodes represents a of the true value of any policy provided that
the size of is small Section Our Regubelief Every colored dot represents a scenario
larized DESPOT R-DESPOT algorithm interprets
this lower bound as a regularized utility function which it uses to optimally balance the size of a
policy and its estimated performance under the sampled scenarios We show that R-DESPOT computes a near-optimal policy whenever a small optimal policy exists Section For anytime online
planning we give a heuristic approximation Anytime Regularized DESPOT AR-DESPOT to the
R-DESPOT algorithm Section Experiments show strong results of AR-DESPOT compared with
two of the fastest online POMDP algorithms Section
Related Work
There are two main approaches to POMDP planning offline policy computation and online search
In offline planning the agent computes beforehand a policy contingent upon all possible future
scenarios and executes the computed policy based on the observations received Although offline
planning algorithms have achieved dramatic progress in computing near-optimal policies
they are difficult to scale up to very large POMDPs because of the exponential number
of future scenarios that must be considered
In contrast online planning interleaves planning and plan execution The agent searches for a single
best action for the current belief only executes the action and updates the belief The process
then repeats at the new belief A recent survey lists three main categories of online planning
algorithms heuristic search branch-and-bound pruning and Monte Carlo sampling AR-DESPOT
contains elements of all three and the idea of constructing DESPOTs through deterministic sampling
is related to those in However AR-DESPOT balances the size of a policy and its estimated
performance during the online search resulting in improved performance for suitable planning tasks
During the online search most algorithms including those based on Monte Carlo sampling
explicitly represents the belief as a probability distribution over the state space This
however limits their scalability for large state spaces because a single belief update can take time
quadratic in the number of states In contrast DESPOT algorithms represent the belief as a set of
particles just as POMCP does and do not perform belief update during the online search
Online search and offline policy computation are complementary and can be combined by
using approximate or partial policies computed offline as the default policies at the bottom of the
search tree for online planning or as macro-actions to shorten the search horizon
Determinized Sparse Partially Observable Trees
POMDP Preliminaries
A POMDP is formally a tuple A where is a set of states A is a set of actions
is a set of observations a s0 is the probability of transitioning to state s0 when the
agent takes action a in state a is the probability of observing if the agent
takes action a and ends in state and is the immediate reward for taking action a in state
A POMDP agent does not know the true state but receives observations that provide partial information on the state The agent maintains a belief often represented as a probability distribution
over S. It starts with an initial belief b0 At time it updates the belief bt according to Bayes
rule by incorporating information from the action taken at time and the resulting observation
bt zt A policy A specifies the action a A at belief B. The value of
a policy at a beliefP
is the expected total
discounted
reward obtained by following with initial
b0 for some discount factor
belief
One way of online POMDP planning is to construct a belief tree Figure with the current belief
b0 as the initial belief at the root of the tree and perform lookahead search on the tree for a policy
that maximizes Each node of the tree represents a belief A node branches into action
edges and each action edge branches further into observation edges If a node and its child
represent beliefs and b0 respectively then b0 a for some a A and Z. To search
a belief tree we typically truncate it at a maximum depth and perform a post-order traversal At
each leaf node we simulate a default policy to obtain a lower bound on its value At each internal
node we apply Bellman?s principle of optimality to choose a best action
nX
max
a)V a
a?A
s?S
z?Z
which recursively computes the maximum value of action branches and the average value of observation branches The results are an approximately optimal policy
represented as a policy tree
and the corresponding value A policy tree retains only the chosen action branches but all
observation branches from the belief tree2 The size of such a policy is the number of tree nodes
Our algorithms represent a belief as a set of particles sampled states We start with an initial
belief At each time step we search for a policy
as described above The agent executes the
first action a of
and receives a new observation We then apply particle filtering to incorporate
information from a and into an updated new belief The process then repeats
DESPOT
While a standard belief tree captures the execution of all policies under all possible scenarios a
DESPOT captures the execution of all policies under a set of sampled scenarios Figure It contains
all the action branches but only the observation branches under the sampled scenarios
We define DESPOT constructively by applying a deterministic simulative model to all possible action
sequences under scenarios sampled from an initial belief b0 A scenario is an abstract simulation
trajectory starting with some state s0 Formally a scenario for a belief is a random sequence
in which the start state s0 is sampled according to and each is a real number
sampled independently and uniformly from the range The deterministic simulative model is a
function A such that if a random number is distributed uniformly over
then a is distributed according to a s0 a When
we simulate this model for an action sequence a2 a3 under a scenario the
simulation generates a trajectory a1 s1 z1 a2 s2 z2 where st zt at for
The simulation trajectory traces out a path z1 a2 z2 from the root of the
standard belief tree We add all the nodes and edges on this path to the DESPOT Each DESPOT node
contains a set consisting of all scenarios that it encounters The start states of the scenarios in
form a particle set that represents approximately We insert the scenario into
the set and insert st into the set bt for the belief node bt reached at the end
of the subpath z1 a2 z2 at zt for Repeating this process for every action
sequence under every sampled scenario completes the construction of the DESPOT
A DESPOT is determined completely by the scenarios which are sampled randomly a priori
Intuitively a DESPOT is a standard belief tree with some observation branches removed While
a belief tree of height has nodes a corresponding DESPOT has only
nodes because of reduced observation branching under the sampled scenarios Hence the name
Determinized Sparse Partially Observable Tree DESPOT
To evaluate a policy under sampled scenarios define as the total discounted reward of the
trajectory obtained by simulating under a scenario Then is an
estimate of the value of at under a set of scenarios We then apply the usual belief
tree search from the previous subsection to a DESPOT to find a policy having good performance
under the sampled scenarios We call this algorithm Basic DESPOT B-DESPOT
The idea of using sampled scenarios for planning is exploited in hindsight optimization as
well HO plans for each scenario independently and builds separate trees each with
nodes In contrast DESPOT captures all scenarios in a single tree with
nodes and allows us to reason with all scenarios simultaneously For this reason DESPOT can
provide stronger performance guarantees than HO.
A policy tree can be represented more compactly by labeling each node by the action edge that follows and
then removing the action edge We do not use this representation here
Regularized DESPOT
To search a DESPOT for a near-optimal policy B-DESPOT chooses a best action at every internal
node of the DESPOT according to the scenarios it encounters This however may cause overfitting
the chosen policy optimizes for the sampled scenarios but does not perform well in general as
many scenarios are not sampled To reduce overfitting our R-DESPOT algorithm leverages the idea
of regularization which balances the estimated performance of a policy under the sampled scenarios
and the policy size If the subtree at a DESPOT node is too large then the performance of a policy
for this subtree may not be estimated reliably with scenarios Instead of searching the subtree for
a policy R-DESPOT terminates the search and uses a simple default policy from this node onwards
To derive R-DESPOT we start with two theoretical results The first one provides an output-sensitive
lower bound on the performance of any arbitrary policy derived from a DESPOT It implies that
despite its sparsity a DESPOT contains sufficient information for approximate policy evaluation
and the accuracy depends on the size of the policy The second result shows that by optimizing
this bound we can find a policy with small size and high value For convenience we assume that
Rmax for all and a A but the results can be easily extended to accommodate
negative rewards The proofs of both results are available in the supplementary material
Formally a policy tree derived from a DESPOT contains the same root as the DESPOT but only one
action branch at each internal node Let denote the class of all policy trees derived from
DESPOTs that have height and are constructed from sampled scenarios for belief b0 Like a
DESPOT a policy tree may not contain all observation branches If the execution of
encounters an observation branch not present in we simply follow the default policy from then
on Similarly we follow the default policy when reaching a leaf node We now bound the error on
the estimated value of a policy derived from a DESPOT
Theorem For any every policy tree satisfies
Rmax
with probability at least where is the estimated value of under any set of randomly
sampled scenarios for belief b0
The second term on the right hand side RHS of captures the additive error in estimating the
value of policy tree and depends on the size of We can make this error arbitrarily small
by choosing a suitably large the number of sampled scenarios Furthermore this error grows
logarithmically with and indicating that the approximation scales well with large action and
observation sets The constant can be tuned to tighten the bound A smaller value allows the first
term on the RHS of to approximate better but increases the additive error in the second term
We have specifically constructed the bound in this multiplicative-additive form due to Haussler
in order to apply efficient dynamic programming techniques in R-DESPOT
Now a natural idea is to search for a near-optimal policy by maximizing the RHS of which
guarantees the performance of by accounting for both the estimated performance and the size of
Theorem Let be an optimal policy at a belief b0 Let be a policy derived from a DESPOT
that has height and is constructed from randomly sampled scenarios for belief b0 For any
if maximizes
Rmax
among all policies derived from the DESPOT then
Rmax
with probability at least
Theorem implies that if a small optimal policy tree exists then we can find a near-optimal
policy with high probability by maximizing Note that is a globally optimal policy at b0 It
may or may not lie in The expression in can be rewritten in the form
similar to that of regularized utility functions in many machine learning algorithms
We now describe R-DESPOT which consists of two main steps First R-DESPOT constructs a
DESPOT of height using scenarios just as B-DESPOT does To improve online planning
performance it may use offline learning to optimize the values for and K. Second R-DESPOT
performs bottom-up dynamic programming on and derive a policy tree that maximizes
For a given policy tree derived the DESPOT we define the regularized weighted discounted
utility RWDU for a node of
V?b
where is the number of scenarios passing through node is the discount factor is
the depth of in the tree is the subtree of rooted at and is a fixed constant Then the
regularized utility is simply We can compute recursively
ab
and
b0
ab
ab
where ab is the chosen action of at the node CH is the set of child nodes of in and
is the start state associated with the scenario
We now describe the dynamic programming procedure that searches for an optimal policy in For
any belief node in let be the maximum RWDU of under any policy tree derived from
for some
We compute recursively If is a leaf node of
default policy Otherwise
max
max
a
where CH(b is the set of child nodes of under the action branch a The first maximization
in chooses between executing the default policy or expanding the subtree at The second
maximization chooses among the different actions available The value of an optimal policy for
the DESPOT rooted at the belief b0 is then and can be computed with bottom-up dynamic
programming in time linear in the size of
Anytime Regularized DESPOT
To further improve online planning performance for large-scale POMDPs we introduce ARDESPOT an anytime approximation of R-DESPOT AR-DESPOT applies heuristic search and branchand-bound pruning to uncover the more promising parts of a DESPOT and then searches the partially
constructed DESPOT for a policy that maximizes the regularized utility in Theorem A brief summary of AR-DESPOT is given in Algorithm Below we provides some details on how AR-DESPOT
performs the heuristic search Section and constructs the upper and lower bounds for branchand-bound pruning Sections and
DESPOT Construction by Forward Search
AR-DESPOT incrementally constructs a DESPOT using heuristic forward search Initially
contains only the root node with associated belief b0 and a set of scenarios sampled according
b0 We then make a series of trials each of which augments by tracing a path from the root to a
leaf of and adding new nodes to at the end of the path For every belief node in we maintain
an upper bound and a lower bound on which is the value of the optimal policy
for under the set of scenarios Similarly we maintain bounds and on the
b0
Q-value b0
A trial starts the root of
In each step it chooses the action branch a that maximizes for the current node and
then chooses the observation branch that maximizes the weighted excess uncertainty at the child
node b0 a
WEU(b0
excess(b0
where excess(b0 and is a constant specifying the desired gap
between the upper and lower bounds at the root b0 If the chosen node a has negative
Algorithm AR-DESPOT
Set b0 to the initial belief
loop
UILD ESPOT
Compute an optimal policy for
RUN RIAL(b
if then
return
if is a leaf node then
Expand one level deeper and insert
all new nodes into as children of
a arg maxa?A
arg maxz?Zb,a WEU a
a
if WEU(b then
return RUN RIAL(b
else
return
ing
Execute the first action of a of
Receive observation
Update the belief b0 a
UILD ESPOT(b0
Sample a set of random scenarios
for b0
Insert b0 into as the root node
while time permitting do
RUN RIAL
Back up upper and lower bounds for every node on the path from to b0
return
excess uncertainty the trial ends Otherwise it continues until reaching a leaf node of We then
expand the leaf node one level deeper by adding new belief nodes for every action and every
observation as children of Finally we trace the path backward to the root and perform backup on
both the upper and lower bounds at each node along the way For the lower-bound backup
max
a
a?A
z?Zb,a
where Zb,a is the set of observations encountered when action a is taken at under all scenarios in
The upper bound backup is the same We repeat the trials as long as time permits thus making
the algorithm anytime
Initial Upper Bounds
There are several approaches for constructing the initial upper bound at a node of a DESPOT A
simple one is the uninformative bound of Rmax To obtain a tighter bound we may exploit
domain-specific knowledge Here we give a domain-independent construction which is the average
upper bound over all scenarios in The upper bound for a particular scenario is the maximum value achieved by any arbitrary policy under Given we have a deterministic planning
problem and solve it by dynamic programming on a trellis of time slices Trellis nodes represent
states and edges represent actions at each time step The path with highest value in the trellis gives
the upper bound under Repeating this procedure for every and taking the average gives
an upper bound on the value of under the set It can be computed in time
Initial Lower Bounds and Default Policies
To construct the lower bound at a node we may simulate any policy for steps under the scenarios
in and compute the average total discounted reward all in time One possibility is
to use a fixed-action policy for this purpose A better one is to handcraft a policy that chooses an
action based on the history of actions and observations a technique used in However it is
often difficult to handcraft effective history-based policies We thus construct a policy using the
belief where is the mode of the probability distribution and A
is a mapping that specifies the action at the state S. It is much more intuitive to construct
and we can approximate easily by determining the most frequent state using Note that
although history-based policies satisfy the requirements of Theorem belief-based policies do not
The difference is however unlikely to be significant to affect performance in practice
Experiments
To evaluate AB-DESPOT experimentally we compared it with four other algorithms Anytime Basic DESPOT AB-DESPOT is AR-DESPOT without the dynamic programming step that computes
RWDU It helps to understand the benefit of regularization AEMS2 is an early successful online
POMDP algorithm POMCP has scaled up to very large POMDPs SARSOP is a
state-of-the-art offline POMDP algorithm It helps to calibrate the best performance achievable
for POMDPs of moderate size In our online planning tests each algorithm was given exactly
second per step to choose an action For AR-DESPOT and AB-DESPOT and 90
The regularization parameter for AR-DESPOT was selected offline by running the algorithm with a
training set distinct from the online test set The discount factor is For POMCP we used
the implementation from the original authors3 but modified it in order to support very large number
of observations and strictly follow the 1-second time limit for online planning
We evaluated the algorithms on four domains including a very large one with about states
Table In summary compared with AEMS2 AR-DESPOT is competitive on smaller POMDPs
but scales up much better on large POMDPs Compared with POMCP AR-DESPOT performs better
than POMCP on the smaller POMDPs and scales up just as well
We first tested the algorithms on Tag a standard benchmark problem In Tag the agent?s goal
is to find and tag a target that intentionally moves away Both the agent and target operate in a grid
with 29 possible positions The agent knows its own position but can observe the target?s position
only if they are in the same location The agent can either stay in the same position or move to
the four adjacent positions paying a cost for each move It can also perform the tag action and
is rewarded if it successfully tags the target but is penalized if it fails For POMCP we used the
Tag implementation that comes with the package but modified it slightly to improve its default
rollout policy The modified policy always tags when the agent is in the same position as the robot
providing better performance For AR-DESPOT we use a simple particle set default policy which
moves the agent towards the mode of the target in the particle set For the upper bound we average
the upper bound for each particle as described in Section The results Table show that ARDESPOT gives comparable performance to AEMS2
Theorem suggests that AR-DESPOT may still perform well when the observation space is large
if a good small policy exists To examine the performance of AR-DESPOT on large observation
spaces we experimented with an augmented version of Tag called LaserTag In LaserTag the
agent moves in a rectangular grid with obstacles placed in random cells The behavior
of the agent and opponent are identical to that in Tag except that in LaserTag the agent knows it
location before the game starts whereas in Tag this happens only after the first observation is seen
The agent is equipped with a laser that gives distance
estimates in directions The distance between adjacent cells is considered one unit and the laser reading
in each direction is generated from a normal distribution centered at the true distance of the agent from the
nearest obstacle in that direction with a standard deviation of units The readings are discretized into
whole units so an observation comprises a set of integers For a map of size is of the order
of The environment for LaserTag is shown in Figure As can be seen from Table AR-DESPOT outperforms POMCP on this problem We can also see the Figure Laser Tag. The agent moves in a
grid with obstacles placed randomly in
effect of regularization by comparing AR-DESPOT with 87
cells It is equipped with a noisy laser that
AB-DESPOT It is not feasible to run AEMS2 or SAR gives distance estimates in directions
SOP on this problem in reasonable time because of the
very large observation space
To demonstrate the performance of AR-DESPOT on large state spaces we experimented with the
RockSample problem The RockSample(n problem mimics a robot moving in an grid
containing rocks each of which may be good or bad At each step the robot either moves to an
adjacent cell samples a rock or senses a rock Sampling gives a reward of if the rock is good
and otherwise Both moving and sampling produce a null observation Sensing produces an
observation in good bad with the probability of producing the correct observation decreasing
http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Applications.html
Table Performance comparison according to the average total discounted reward achieved The results
for SARSOP and AEMS2 are replicated from and respectively SARSOP and AEMS2 failed to run
on some domains because their state space or observation space is too large For POMCP both results from
our own tests and those from parentheses are reported We could not reproduce the earlier published
results possibly because of the code modification and machine differences
Tag
LaserTag
No. States
No. Actions
No. Observations
SARSOP
AEMS2
POMCP
AB-DESPOT
AR-DESPOT
Pocman
13
exponentially with the agent?s distance from the rock A terminal state is reached when the agent
moves past the east edge of the map For AR-DESPOT we use a default policy derived from the
particle set as follows a new state is created with the positions of the robot and the rocks unchanged
and each rock is labeled as good or bad depending on whichever condition is more prevalent in the
particle set The optimal policy for the resulting state is used as the default policy The optimal
policy for all states is computed before the algorithm begins using dynamic programming with the
same horizon length as the maximum depth of the search tree For the initial upper bound we use the
method described in Section As in we use a particle filter to represent the belief to examine
the behavior of the algorithms in very large state spaces For POMCP we used the implementation
in but ran it on the same platform as AR-DESPOT As the results for our runs of POMCP are
poorer than those reported in we also reproduce their reported results in Table The results
in Table indicate that AR-DESPOT is able to scale up to very large state spaces Regularization
does not appear beneficial to this problem possibly because it is mostly deterministic except for the
sensing action
Finally we implemented Pocman the partially observable version of the video game Pacman as
described in Pocman has an extremely large state space of approximately We compute
an approximate upper bound for a belief by summing the following quantities for each particle in
it and taking the average over all particles reward for eating each pellet discounted by its distance
from pocman reward for clearing the level discounted by the maximum distance to a pellet default
per-step reward of for a number of steps equal to the maximum distance to a pellet penalty for
eating a ghost discounted by the distance to the closest ghost being chased if any penalty for dying
discounted by the average distance to the ghosts and half the penalty for hitting a wall if pocman
tries to double back along its direction of movement This need not always be an upper bound
but AR-DESPOT can be modified to run even when this is the case For the lower bound we use
a history-based policy that chases a random ghost if visible when pocman is under the effect of a
powerpill and avoids ghosts and doubling-back when it is not This example shows that AR-DESPOT
can be used successfully even in cases of extremely large state space
Conclusion
This paper presents DESPOT a new approach to online POMDP planning Our R-DESPOT algorithm
and its anytime approximation AR-DESPOT search a DESPOT for an approximately optimal policy
while balancing the size of the policy and the accuracy on its value estimate Theoretical analysis
and experiments show that the new approach outperforms two of the fastest online POMDP planning
algorithms It scales up better than AEMS2 and it does not suffer the extremely poor worst-case
behavior of POMCP The performance of AR-DESPOT depends on the upper and lower bounds
supplied Effective methods for automatic construction of such bounds will be an interesting topic
for further investigation
Acknowledgments This work is supported in part by MoE AcRF grant National
Research Foundation Singapore through the SMART IRG program and US Air Force Research
Laboratory under agreement

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4973-aggregating-optimistic-planning-trees-for-solving-markov-decision-processes.pdf

Aggregating Optimistic Planning Trees for Solving
Markov Decision Processes
Gunnar Kedenburg
INRIA Lille Nord Europe idalab GmbH
gunnar.kedenburg@inria.fr
Rapha?l Fonteneau
University of Li?ge INRIA Lille Nord Europe
raphael.fonteneau@ulg.ac.be
R?mi Munos
INRIA Lille Nord Europe Microsoft Research New England
remi.munos@inria.fr
Abstract
This paper addresses the problem of online planning in Markov decision processes
using a randomized simulator under a budget constraint We propose a new
algorithm which is based on the construction of a forest of planning trees where
each tree corresponds to a random realization of the stochastic environment The
trees are constructed using a safe optimistic planning strategy combining the
optimistic principle order to explore the most promising part of the search
space first with a safety principle which guarantees a certain amount of uniform
exploration In the decision-making step of the algorithm the individual trees are
aggregated and an immediate action is recommended We provide a finite-sample
analysis and discuss the trade-off between the principles of optimism and safety
We also report numerical results on a benchmark problem Our algorithm performs
as well as state-of-the-art optimistic planning algorithms and better than a related
algorithm which additionally assumes the knowledge of all transition distributions
Introduction
Adaptive decision making algorithms have been used increasingly in the past years and have attracted
researchers from many application areas like artificial intelligence financial engineering
medicine and robotics These algorithms realize an adaptive control strategy through
interaction with their environment so as to maximize an a priori performance criterion
A new generation of algorithms based on look-ahead tree search techniques have brought a breakthrough in practical performance on planning problems with large state spaces Techniques based on
planning trees such as Monte Carlo tree search and in particular the UCT algorithm UCB
applied to Trees see have allowed to tackle large scale problems such as the game of Go
These methods exploit that in order to decide on an action at a given state it is not necessary to build
an estimate of the value function everywhere Instead they search locally in the space of policies
around the current state
We propose a new algorithm for planning in Markov Decision Problems MDPs We assume that
a limited budget of calls to a randomized simulator for the MDP the generative model in is
available for exploring the consequences of actions before making a decision The intuition behind
our algorithm is to achieve a high exploration depth in the look-ahead trees by planning in fixed
realizations of the MDP and to achieve the necessary exploration width by aggregating a forest of
planning trees forming an approximation of the MDP from many realizations Each of the trees
is developed around the state for which a decision has to be made according to the principle of
optimism in the face of uncertainty combined with a safety principle
We provide a finite-sample analysis depending on the budget split into the number of trees and
the number of node expansions in each tree We show that our algorithm is consistent and that it
identifies the optimal action when given a sufficiently large budget We also give numerical results
which demonstrate good performance on a benchmark problem In particular we show that our
algorithm achieves much better performance on this problem than OP-MDP when both algorithms
generate the same number of successor states despite the fact that OP-MDP assumes knowledge
of all successor state probabilities in the MDP whereas our algorithm only samples states from a
simulator
The paper is organized as follows first we discuss some related work in section In section the
problem addressed in this paper is formalized before we describe our algorithm in section Its
finite-sample analysis is given in section We provide numerical results on the inverted pendulum
benchmark in section In section we discuss and conclude this work
Related work
The optimism in the face of uncertainty paradigm has already lead to several successful results
for solving decision making problems Specifically it has been applied in the following contexts
multi-armed bandit problems which can be seen as single state MDPs planning algorithms
for deterministic systems and stochastic systems and global optimization of stochastic
functions that are only accessible through sampling See for a detailed review of the optimistic
principle applied to planning and optimization
The algorithm presented in this paper is particularly closely related to two recently developed online
planning algorithms for solving MDPs namely the OPD algorithm for MDPs with deterministic
transitions and the OP-MDP algorithm which addresses stochastic MDPs where all transition
probabilities are known A Bayesian adaptation of OP-MDP has also been proposed for planning
in the context where the MDP is unknown
Our contribution is also related to where random ensembles of state-action independent disturbance scenarios are built the planning problem is solved for each scenario and a decision is made
based on majority voting Finally since our algorithm proceeds by sequentially applying the first
decision of a longer plan over a receding horizon it can also be seen as a Model Predictive Control
technique
Formalization
Let A be a Markov decision process where the set and A respectively denote
the state space and the finite action space with of the MDP. When an action a A is
selected in state of the MDP it transitions to a successor state s0 with probability
We further assume that every successor state set is finite and their cardinality
is bounded by N. Associated with the transition is an deterministic instantaneous reward
a s0
While the transition probabilities may be unknown it is assumed that a randomized simulator is
available which given a state-action pair outputs a successor state s0 The ability
to sample is a weaker assumption than the knowledge of all transition probabilities In this paper we
consider the problem of planning under a budget constraint only a limited number of samples may
be drawn using the simulator Afterwards a single decision has to be made
Let A denote a deterministic policy Define the value function of the policy in a state as
the discounted sum of expected rewards
r(st s0
where the constant is called the discount factor Let be an optimal policy a policy
that maximizes in all states It is well known that the optimal value function is the
solution to the Bellman equation
max
a?A
a s0
s0
Given the action-value function
s0 p(s a an
optimal policy can be derived as argmaxa?A
Algorithm
We name our algorithm ASOP for Aggregated Safe Optimistic Planning The main idea behind it
is to use a simulator to obtain a series of deterministic realizations of the stochastic MDP to plan
in each of them individually and to then aggregate all the information gathered in the deterministic
MDPs into an empirical approximation to the original MDP on the basis of which a decision is made
We refer to the planning trees used here as single successor state trees S3-trees in order to distinguish
them from other planning trees used for the same problem the OP-MDP tree where all possible
successor states are considered Every node of a S3-tree represents a state and has at most
one child node per state-action a representing a successor state s0 S. The successor state is drawn
using the simulator during the construction of the S3-tree
The planning tree construction using the SOP algorithm for Safe Optimistic Planning is described
in section The ASOP algorithm which integrates building the forest and deciding on an action
by aggregating the information in the forest is described in section
Safe optimistic planning in S3-trees the SOP algorithm
SOP is an algorithm for sequentially constructing a S3-tree It can be seen as a variant of the OPD
algorithm for planning in deterministic MDPs SOP expands up to two leaves of the planning tree
per iteration The first leaf the optimistic one is a maximizer of an upper bound called b-value on
the value function of the deterministic realization of the MDP explored in the S3-tree The b-value
of a node is defined as
ri
where ri is the sequence of rewards obtained along the path to and is the depth of the node
the length of the path from the root to Only expanding the optimistic leaf would not be enough
to make ASOP consistent this is shown in the appendix Therefore a second leaf the safe one
defined as the shallowest leaf in the current tree is also expanded in each iteration A pseudo-code is
given as algorithm
Algorithm SOP
Data The initial state s0 and a budget
Result A planning tree
Let denote a tree consisting only of a leaf representing s0
Initialize the cost counter
while do
Form a subset of leaves of containing a leaf of minimal depth and a leaf of maximal b-value
computed according to the two leaves can be identical
foreach do
Let denote the state represented by
foreach a A do
if then
Use the simulator to draw a successor state s0
Create an edge in from to a new leaf representing s0
Let
return
Aggregation of S3-trees the ASOP algorithm
ASOP consists of three steps In the first step it runs independent instances of SOP to collect
information about the MDP in the form of a forest of S3-trees It then computes action-values
of a single empirical MDP based on the collected information in which states are
represented by forests on a transition the forest is partitioned into groups by successor states and
the corresponding frequencies are taken as the transition probabilities Leaves are interpreted as
absorbing states with zero reward on every action yielding a trivial lower bound A pseudo-code for
this computation is given as algorithm ASOP then outputs the action
argmax
a?A
The optimal policy of the empirical MDP has the property that the empirical lower bound of its value
computed from the information collected by planning in the individual realizations is maximal over
the set of all policies We give a pseudo-code for the ASOP algorithm as algorithm
Algorithm ActionValue
Data A forest and an action a with each tree in representing the same state
Result An empirical lower bound for the value of a in
Let denote the edges representing action a at any of the root nodes of
if then
return
else
Let be the set of trees pointed to by the edges in E.
Enumerate the states represented by any tree in by I for some finite I.
foreach I do
Denote the set of trees in which represent si by Fi
Let maxa0 A ActionValue(Fi a0
Let p?i Fi
return i?I p?i a s0i
Algorithm ASOP
Data The initial state s0 a per-tree budget and the forest size
Result An action to take
for do
Let Ti SOP(s0
return argmaxa?A ActionValue({T1 Tm
Finite-sample analysis
In this section we provide a finite-sample analysis of ASOP in terms of the number of planning trees
and per-tree budget An immediate consequence of this analysis is that ASOP is consistent the
action returned by ASOP converges to the optimal action when both and tend to infinity
Our loss measure is the simple regret corresponding to the expected value of first playing the action
returned by the algorithm at the initial state s0 and acting optimally from then on compared to
acting optimally from the beginning
Rn,m
First let us use the safe part of SOP to show that each S3-tree is fully explored up to a certain depth
when given a sufficiently large per-tree budget
Lemma For any once a budget of has been spent by SOP on an S3-tree
the state-actions of all nodes up and including those at depth have all been sampled exactly once
Pd
Proof A complete A|-ary tree contains nodes in level so it contains
nodes up to and including level In each of these nodes actions need to be explored We
complete the proof by noticing that SOP spends at least half of its budget on shallowest leaves
Let and
denote the value functions for a policy in the infinite completely explored S3-tree
defined by a random realization and the finite S3-tree constructed by SOP for a budget of in the
same realization respectively From Lemma we deduce that if the per-tree budget is at least
log
we obtain
ri for any policy
ASOP aggregates the trees and computes the optimal policy
of the resulting empirical MDP whose
transition probabilities are defined by the frequencies over the S3-trees of transitions from
state-action to successor states Therefore
is actually a policy maximizing the function
If the number of S3-trees and the per-tree budget are large we therefore expect the optimal
policy
of the empirical MDP to be close to the optimal policy of the true MDP. This is the result
stated in the following theorem
Theorem For any and if the number of S3-trees is at least
log
log
and the per-tree budget is at least
log
then Rm,n
Proof Let and and fix realizations of the stochastic MDP for
some satisfying Each realization corresponds to an infinite completely explored S3-tree
Let denote some per-tree budget satisfying
Analogously to we know from Lemma that given our choice of SOP constructs trees which
are completely explored up to depth
fulfilling
log
Consider the following truncated value functions let denote the sum of expected discounted
rewards obtained in the original MDP when following policy for steps and then receiving reward
zero from there on and let denote the analogous quantity in the MDP corresponding to
realization
Pm
Pm
Define for all policies the quantities v?m,n
v?i and
Since the trees are complete up to level and the rewards are non-negative we deduce that we have
for each and each policy thus the same will be true for the averages
v?m,n
Notice that
From the Chernoff-Hoeffding inequality we have that for any
fixed policy since the truncated values lie in
Now we need a uniform bound over the set of all possible policies The number of distinct policies is
at each level there are at most states that can be reached by following a
policy at previous levels
so there are different
choices that policies can make at level Thus
since log log we have
max
The action returned by ASOP is
where
argmax v?m,n
Finally it follows that with probability at least
Rn,m
v?m,n
v?m,n
v?m,n
by definition of
by truncation
v?m,n
by
by
v?m,n
by
by
by truncation
Remark The total budget required to return an optimal action with high probability is
log(K|A
thus of order Notice that this rate is poorer by a factor than the rate obtained
for uniform planning in this is a direct consequence of the fact that we are only drawing samples
whereas a full model of the transition probabilities is assumed in
Remark Since there is a finite number of actions by denoting the optimality gap between
the best and the second-best optimal action values we have that the optimal arm is identified high
log(K|A
probability the simple regret is after a total budget of order
Remark The optimistic part of the algorithm allows a deep exploration of the MDP. At the same
time it biases the expression maximized by
in towards near-optimal actions of the deterministic
realizations Under the assumptions of theorem the bias becomes insignificant
Remark Notice that we do not use the optimistic properties of the algorithm in the analysis The
analysis only uses the safe part of the SOP planning the fact that one sample out of two are
devoted to expanding the shallowest nodes An analysis of the benefit of the optimistic part of the
algorithm similar to the analyses carried out in would be much more involved and is deferred
to a future work However the impact of the optimistic part of the algorithm is essential in practice
as shown in the numerical results
Numerical results
In this section we compare the performance of ASOP to OP-MDP UCT and FSSS We
use the noisy inverted pendulum benchmark problem from which consists of swinging up and
stabilizing a weight attached to an actuated link that rotates in a vertical plane Since the available
power is too low to push the pendulum up in a single rotation from the initial state the pendulum has
to be swung back and forth to gather energy prior to being pushed up and stabilized
The inverted pendulum is described by the state variables
and the
differential equation
mgl K(K where kg m2
kg Nm s/rad Nm/A and
The state variable is constrained to by saturation The discrete time problem
is obtained by mapping actions from A to segments of a piecewise control signal
each in duration and then numerically integrating the differential equation on the constant
segments using RK4. The actions are applied stochastically with probability the intended
voltage is applied in the control signal whereas with probability the smaller voltage is
applied The goal is to stabilize the pendulum in the unstable equilibrium pointing up at
rest when starting from state pointing down at rest This goal is expressed by the penalty
function a s0 02 a2 where s0 The reward function is obtained
by scaling and translating the values of the penalty function so that it maps to the interval with
The discount factor is set to
OP-MDP
UCT
FSSS
FSSS
FSSS
ASOP
ASOP
Sum of discounted rewards
17
Calls to the simulator per step
Figure Comparison of ASOP to OP-MDP UCT and FSSS on the inverted pendulum benchmark
problem showing the sum of discounted rewards for simulations of time steps
The algorithms are compared for several budgets In the cases of ASOP UCT and FSSS the budget
is in terms of calls to the simulator OP-MDP does not use a simulator Instead every possible
successor state is incorporated into the planning tree together with its precise probability mass and
each of these states is counted against the budget As the benchmark problem is stochastic and
internal randomization for the simulator is used in all algorithms except OP-MDP the performance
is averaged over repetitions The algorithm parameters have been selected manually to achieve
good performance For ASOP we show results for forest sizes of two and three For UCT the
Chernoff-Hoeffding term multiplier is set to the results are not very sensitive in the value
therefore only one result is shown For FSSS we use one to three samples per state-action For
both UCT and FSSS a rollout depth of seven is used OP-MDP does not have any parameters The
results are shown in figure We observe that on this problem ASOP performs much better than
OP-MDP for every value of the budget and also performs well in comparison to the other sampling
based methods UCT and FSSS
Figure shows the impact of optimistic planning on the performance of our aggregation method For
forest sizes of both one and three optimistic planning leads to considerably increased performance
This is due to the greater planning depth in the lookahead tree when using optimistic exploration
For the case of a single tree performance decreases presumably due to overfitting on the stochastic
problem for increasing budget The effect disappears when more than one tree is used
Conclusion
We introduced ASOP a novel algorithm for solving online planning problems using a randomized
simulator for the MDP under a budget constraint The algorithm works by constructing a forest
of single successor state trees each corresponding to a random realization of the MDP transitions
Each tree is constructed using a combination of safe and optimistic planning An empirical MDP
is defined based on the forest and the first action of the optimal policy of this empirical MDP is
returned In short our algorithm targets structured problems where the value function possesses
some smoothness property around the optimal policies of the deterministic realizations of the MDP in
a sense defined in by using the optimistic principle to focus rapidly on the most promising
area(s of the search space It can also find a reasonable solution in unstructured problems since some
of the budget is allocated for uniform exploration ASOP shows good performance on the inverted
pendulum benchmark Finally our algorithm is also appealing in that the numerically heavy part of
constructing the planning trees in which the simulator is used can be performed in a distributed way
Sum of discounted rewards
17
Safe+Optimistic
Safe+Optimistic
Safe
Safe
Optimistic
Optimistic
Calls to the simulator per step
Figure Comparison of different planning strategies on the same problem as in figure The
Safe strategy is to use uniform planning in the individual trees the Optimistic strategy is to use
OPD. ASOP corresponds to the Safe+Optimistic strategy
Acknowledgements
We acknowledge the support of the BMBF project ALICE the European Community?s
Seventh Framework Programme under grant no CompLACS and the Belgian
PAI DYSCO Rapha?l Fonteneau is a post-doctoral fellow of the FNRS We also thank Lucian
Busoniu for sharing his implementation of OP-MDP
Appendix Counterexample to consistency when using purely optimistic planning in S3-trees
Consider the MDP in figure with zero reward transitions in the middle branch where
and are chosen such that 13 and The trees are constructed
iteratively and every iteration consists of exploring a leaf of maximal b-value where exploring a
leaf means introducing a single successor state per action at the selected leaf The state-action values
are 31
32
31
23 13
95
and
There are two
possible outcomes when sampling the action a which occur with probabilities and 23 respectively
Outcome I The upper branch of action a is sampled In this case the contribution to the forest is an
arbitrarily long reward path for action a and a finite reward path for action
Outcome The lower branch of action a is sampled Because
the lower branch will
be explored only up to times as its b-value is then lower than the value and therefore any b-value
of action The contribution of this case to the forest is a finite reward path for action a and an
arbitrary long depending on the budget reward path for action
For an increasing exploration budget per tree and an increasing number of trees the approximate
action values of action a and obtained by aggregation converge to 13
and
respectively
Therefore the decision rule will select action for a sufficiently large budget even though a is the
optimal action This leads to simple regret of 18
s0
a
Figure The middle branch of this MDP is never explored deep enough if only the node with
the largest b-value is sampled in each iteration Transition probabilities are given in gray where not
equal to one

<<----------------------------------------------------------------------------------------------------------------------->>

