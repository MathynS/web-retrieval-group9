query sentence: where to buy power-efficient batteries for mini reactor
---------------------------------------------------------------------
title: 1740-low-power-wireless-communication-via-reinforcement-learning.pdf

Low Power Wireless Communication via
Reinforcement Learning
Timothy Brown
Electrical and Computer Engineering
University of Colorado
Boulder CO
tirnxb@colorado.edu
Abstract
This paper examines the application of reinforcement learning to a wireless communication problem The problem requires that channel utility be maximized while simultaneously minimizing battery usage We
present a solution to this multi-criteria problem that is able to significantly reduce power consumption The solution uses a variable discount
factor to capture the effects of battery usage
Introduction
Reinforcement learning has been applied to resource allocation problems in telecommunications channel allocation in wireless systems network routing and admission
control in telecommunication networks These have demonstrated reinforcement learning can find good policies that significantly increase the application reward
within the dynamics of the telecommunication problems However a key issue is how
to treat the commonly occurring multiple reward and constraint criteria in a consistent way
This paper will focus on power management for wireless packet communication channels
These channels are unlike wireline channels in that channel quality is poor and varies over
time and often one side of the wireless link is a battery operated device such as a laptop
computer In this environment power management decides when to transmit and receive
so as to simultaneously maximize channel utility and battery life
A number of power management strategies have been developed for different aspects of
battery operated computer systems such as the hard disk and CPU Managing the
channel is different in that some control actions such as shutting off the wireless transmitter
make the state of the channel and the other side of the communication unobservable
In this paper we consider the problem of finding a power management policy that simultaneously maximizes the radio communication's earned revenue while minimizing battery
usage The problem is recast as a stochastic shortest path problem which in turn is mapped
to a discounted infinite horizon with a variable discount factor Results show significant
reductions in power usage
T. Brown
Figure The five components of the radio communication system
Problem Description
The problem is comprised of five components as shown in Figure mobile application
mobile radio wireless channel base station radio and base station application The applications on each end generate packets that are sent via a radio across the channel to the
radio and then application on the other side The application also defines the utility of a
given end-to-end performance The radios implement a simple acknowledgment/retransmit
protocol for reliable transmission The base station is fixed and has a reliable power supply
and therefore is not power constrained The mobile power is limited by a battery and it
can choose to turn its radio off for periods of time to reduce power usage Note that even
with the radio off the mobile system continues to draw power for other uses The channel
adds errors to the packets The rate of errors depends on many factors such as location
of mobile and base station intervening distance and levels of interference The problem
requires models for each of these components To be concrete the specific models used in
this paper are described in the following sections It should be emphasized that in order to
focus on the machine learning issues simple models have been chosen More sophisticated
models can readily be included
The Channel
The channel carries fixed-size packets in synchronous time slots All packet rates are normalized by the channel rate so that the channel carries one packet per unit time in each
direction The forward and reverse channels are orthogonal and do not interfere
Wireless data channels typically have low error rates Occasionally due to interference or
signal fading the channel introduces many errors This variation is possible even when the
mobile and base station are stationary The channel is modeled by a two state Gilbert-Elliot
model In this model the channel is in either a good or a bad state with a packet
error probabilities Pg and Pb where Pg Pb The channel is symmetric with the same loss
rate in both directions The channel stays in each state with a geometrically distributed
holding time with mean holding times hg and hb time slots
Mobile and Base Station Application
The traffic generated by the source is a bursty ON/OFF model that alternates between generating no packets and generating packets at rate TON. The holding times are geometrically
distributed with mean holding times hON and hOFF The traffic in each direction is independent and identically distributed
The Radios
The radios can transmit data from the application and send it on the channel and simultaneously receive data from the other radio and pass it on to its application The radios
implement a simple packet protocol to ensure reliability Packets from the sources are
queued in the radio and sent one by one Packets consist of a header and data The header
carries acknowledgements ACK's with the most recent packet received without error The
header contains a checksum so that errors in the payload can be detected Errored packets
Low Power Wireless Communication via Reinforcement Learning
Parameter Name
Channel Error Rate Good
Channel Error Rate Bad
Channel Holding Time Good
Channel Holding Time Bad
Source On Rate
Source Holding Time On
Source Holding Time Off
Power Radio Off
Power Radio On
Power Radio Transmitting
Real Time Max Delay
Web Browsing Time Scale
Symbol
pg
Pb
hg
hb
TON
hON
hOFF
POFF
PON
PTX
dmax
do
Value
7W
lOW
Table Application parameters
cause the receiving radio to send a packet with a negative acknowledgment NACK to the
other radio instructing it to retransmit the packet sequence starting from the errored packet
The NACK is sent immediately even if no data is waiting and the radio must send an empty
packet Only unerrored packets are sent on to the application The header is assumed to
always be received without errorl
Since the mobile is constrained by power the mobile is considered the master and the base
station the slave The base station is always on and ready to transmit or receive The mobile
can turn its radio off to conserve power Every ON-OFF and OFF-ON transition generates
a packet with a message in the header indicating the change of state to the base station
These message packets carry no data The mobile expends power at three levels-PoFF
Po and Ptx--corresponding to the radio off receiver on but no packet transmitted and
receiver on packet transmitted
Reward Criteria
Reward is earned for packets passed in each direction The amount depends on the application In this paper we consider three types of applications an e-mail application a
real-time application and a web browsing application In the e-mail application a unit
reward is given for every packet received by the application In the real time application a
unit reward is given for every packet received by the application with delay less than dmax
The reward is zero otherwise In the web browsing application time is important but not
critical The value of a packet with delay is l/do)d where do is the desired time
scale of the arrivals
The specific parameters used in this experiment are given in Table These were gathered
as typical values from It should be emphasized that this model is the simplest
model that captures the essential characteristics of the problem More realistic channels
protocols applications and rewards can readily be incorporated but for this paper are left
out for clarity
A packet error rate of implies a bit error rate of less than Error correcting codes in the
header can easily reduce this error rate to a low value The main intent is to simplify the protocol for
this paper so that time-outs and other mechanisms do not need to be considered
T. Brown
Component
Channel
Application
Mobile
Mobile
Base Station
States
good,ba
ON,OFF
ON,OFF
List of waiting and unacknowledged packets and their current delay
List of waiting and unacknowledged packets and their current delay
Table Components to System State
Markov Decision Processes
At any given time slot the system is in a particular configuration defined by the state
of each of the components in Table The system state is where we include
the time in order to facilitate accounting for the battery The mobile can choose to toggle
its radio between the ON and OFF state and rewards are generated by successfully received
packets The task of the learner is to determine a radio ON/OFF policy that maximizes the
total reward for packets received before batteries run out
The battery life is not a fixed time First it depends on usage Second for a given drain
the capacity depends on how long the battery was charged how long it has sat since being
charged the age of the battery etc In short the battery runs out at a random time The
system can be modeled as a stochastic shortest path problem whereby there exists a terminal
state So that corresponds to the battery empty in which no more reward is possible and the
system remains permanently at no cost
Multi-criteria Objective
Formally the goal is to learn a policy for each possible system state so as to maximize
where is the expectation over possible trajectories starting from state using
policy is the reward for packets received at time and is the last time step before
the batteries run out
Typically is very large and this inhibits fast learning So in order to promote faster
learning we convert this problem to a discounted problem that removes the variance caused
by the random stopping times At time given action while in state the terminal
state is reached with probability Ps(t Setting the value of the terminal state to we
can convert our new criterion to maximize
where the product is the probability of reaching time In words future rewards are discounted by Ps and the discounting is larger for actions that drain the batteries
faster Thus a more power efficient strategy will have a discount factor closer to one which
correctly extends the effective horizon over which reward is captured
Q-Iearning
RL methods solve MDP problems by learning good approximations to the optimal value
function given by the solution to the Bellman optimality equation which takes the
Low Power Wireless Communication via Reinforcement Learning
following form
max
aEA(s
where is the set of actions available in the current state a is the effective
immediate payoff and Esf is the expectation over possible next states
We learn an appr<;>ximation to using Watkin's Q-learning algorithm Bellman's equation
can be rewritten in Q-factor as
max
aEA(s
In every time step the following decision is made The Q-value of turning on in the next
state is compared to the Q-value of turning off in the next state If turning on has higher
value the mobile turns on Else the mobile turns off
Whatever our decision we update our value function as follows on a transition from state
to on action a
a
max
bEA(Sf
where is the learning rate In order for Q-Iearning to perform well all potentially important state-action pairs must be explored At each state with probability we apply
a random action instead of the action recommended by the Q-value However we still use
to update Q-values using the action recommended by the Q-values
Structural Limits to the State Space
For theoretical reasons it is desirable to use a table lookup representation In practice
since the mobile radio decides using information available to it this is impossible for the
following reasons The state of the channel is never known directly The receiver only
observes errored packets It is possible to infer the state but only when packets are actually
received and channel state changes introduce inference errors
Traditional packet applications rarely communicate state information to the transport layer
This state information could also be inferred But given the quickly changing application
dynamics the application state is often ignored For the particular parameters in Table
rON the application is on if and only if it generates a packet so its state is
completely specified by the packet arrivals and does not need to be inferred
The most serious deficiency to a complete state space representation is that when the mobile
radio turns OFF it has no knowledge of state changes in the base station Even when it is
ON the protocol does not have provisions for transferring directly the state information
Again this implies that state information must be inferred
One approach to these structural limits is to use a POMDP approach which we leave to
future work In this paper we simply learn deterministic policies on features that estimate
the state
Simplifying Assumptions
Beyond the structural problems of the previous section we must treat the usual problem that
the state space is huge For instance assuming even moderate maximum queue sizes and
maximum wait times yields states If one considers e-mail like applications where
TX Brown
Component
Mobile Radio
Mobile Radio
Mobile Radio
Channel
Base Radio
Feature
is radio ON or OFF
number of packets waiting at the mobile
wait time of first packet waiting at the mobile
number of errors received in last time slots
number of time slots since mobile was last ON
Table Decision Features Measured by Mobile Radio
wait times of minutes of time slot wait times with many packets waiting possible
the state space exceeds states Thus we seek a representation to reduce the size and
complexity of the state space This reduction is taken in two parts The first is a feature
representation that is possible given the structural limits of the previous section the second
is a function approximation based on these feature vectors
The feature vectors are listed in Table These are chosen since they are measurable at
the mobile radio For function approximation we use state aggregation since it provably
converges
Simulation Results
This section describes simulation-based experiments on the mobile radio control problem
For this initial study we simplified the problem by setting Pg Pb no channel
errors
State aggregation was used with aggregate states The battery termination probability
ps(a was simply PIlOOO where is the power appropriate for the state and action chosen
from Table This was chosen to have an expected battery life much longer than the time
scale of the traffic and channel processes
Three policies were learned one for each application reward criteria The resulting policies
are tested by simulating for time slots
In each test run an upper and lower bound on the energy usage is computed The upper
bound is the case of the mobile radio always on The lower bound is a policy that ignores
the reward criteria but still delivers all the packets In this policy the radio is off and packets
are accumulated until the latter portion of the test run when they are sent in one large group
Policies are compared using the normalized power savings This is a measure of how close
the policy is to the lower bound with and being the upper and lower bound
The results are given in Table The table also lists the average reward per packet received
by the application For the e-mail application which has no constraints on the packets the
average reward is identically one
Conclusion
This paper showed that reinforcement learning was able to learn a policy that significantly
reduced the power consumption of a mobile radio while maintaining a high application
utility It used a novel variable discount factor that captured the impact of different actions
on battery life This was able to gain to of the possible power savings
2There exist policies that exceed this power if they toggle oNand oFFoften and generate many
notification packets But the always on policy is the baseline that we are trying to improve upon
Low Power Wireless Communication via Reinforcement Learning
Application
E-mail
Real Time
Web Browsing
Normalized
Power Savings
Average
Reward
Table Simulation Results
In the application the paper used a simple model of the radio channel battery etc It also
used simple state aggregation and ignored the partially observable aspects of the problem
Future work will address more accurate models function approximation and POMDP approaches
Acknowledgment
This work was supported by CAREER Award and NSF Grant

----------------------------------------------------------------

title: 6024-regret-lower-bound-and-optimal-algorithm-in-finite-stochastic-partial-monitoring.pdf

Regret Lower Bound and Optimal Algorithm in
Finite Stochastic Partial Monitoring
Junpei Komiyama
The University of Tokyo
junpei@komiyama.info
Junya Honda
The University of Tokyo
honda@stat.t.u-tokyo.ac.jp
Hiroshi Nakagawa
The University of Tokyo
nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
Partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players In this game the learner chooses
an action and at the same time the opponent chooses an outcome then the learner
suffers a loss and receives a feedback signal The goal of the learner is to minimize the total loss In this paper we study partial monitoring with finite actions
and stochastic outcomes We derive a logarithmic distribution-dependent regret
lower bound that defines the hardness of the problem Inspired by the DMED
algorithm Honda and Takemura for the multi-armed bandit problem we
propose PM-DMED an algorithm that minimizes the distribution-dependent regret PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments To show the optimality of PM-DMED with respect to the regret
bound we slightly modify the algorithm by introducing a hinge function PMDMED-Hinge Then we derive an asymptotically optimal regret upper bound of
PM-DMED-Hinge that matches the lower bound
Introduction
Partial monitoring is a general framework for sequential decision making problems with imperfect
feedback Many classes of problems including prediction with expert advice the multi-armed
bandit problem dynamic pricing the dark pool problem label efficient prediction
and linear and convex optimization with full or bandit feedback can be modeled as an instance
of partial monitoring
Partial monitoring is formalized as a repeated game played by two players called a learner and an
opponent At each round the learner chooses an action and at the same time the opponent chooses
an outcome Then the learner observes a feedback signal from a given set of symbols and suffers
some loss both of which are deterministic functions of the selected action and outcome
The goal of the learner is to find the optimal action that minimizes his/her cumulative loss Alternatively we can define the regret as the difference between the cumulative losses of the learner and
the single optimal action and minimization of the loss is equivalent to minimization of the regret
A learner with a small regret balances exploration acquisition of information about the strategy of
the opponent and exploitation utilization of information The rate of regret indicates how fast the
learner adapts to the problem a linear regret indicates the inability of the learner to find the optimal
action whereas a sublinear regret indicates that the learner can approach the optimal action given
sufficiently large time steps
The study of partial monitoring is classified into two settings with respect to the assumption on the
outcomes On one hand in the stochastic setting the opponent chooses an outcome distribution
before the game starts and an outcome at each round is an sample from the distribution On
the other hand in the adversarial setting the opponent chooses the outcomes to maximize the regret
of the learner In this paper we study the former setting
Related work
The paper by Piccolboni and Schindelhauer is one of the first to study the regret of the finite partial monitoring problem They proposed the FeedExp3 algorithm which attains O(T minimax
regret on some problems This bound was later improved by Cesa-Bianchi to O(T
who also showed an instance in which the bound is optimal Since then most literature on partial
monitoring has dealt with the minimax regret which is the worst-case regret over all possible opponent?s strategies Bart?ok classified the partial monitoring problems into four categories
in terms of the minimax regret a trivial problem with zero regret an easy problem with
regret1 a hard problem with regret and a hopeless problem with regret This shows
that the class of the partial monitoring problems is not limited to the bandit sort but also includes
larger classes of problems such as dynamic pricing Since then several algorithms with a
regret bound for easy problems have been proposed Among them the Bayes-update
Partial Monitoring BPM algorithm is state-of-the-art in the sense of empirical performance
Distribution-dependent and minimax regret we focus on the distribution-dependent regret that
depends on the strategy of the opponent While the minimax regret in partial monitoring has been extensively studied little has been known on distribution-dependent regret in partial monitoring To the
authors knowledge the only paper focusing on the distribution-dependent regret in finite discrete
partial monitoring is the one by Bart?ok which derived O(log distribution-dependent regret for easy problems In contrast to this situation much more interest in the distribution-dependent
regret has been shown in the field of multi-armed bandit problems Upper confidence bound
the most well-known algorithm for the multi-armed bandits has a distribution-dependent regret
bound and algorithms that minimize the distribution-dependent regret KL-UCB has
been shown to perform better than ones that minimize the minimax regret MOSS even in
instances in which the distributions are hard to distinguish Scenario in Garivier
Therefore in the field of partial monitoring we can expect that an algorithm that minimizes the
distribution-dependent regret would perform better than the existing ones
Contribution the contributions of this paper lie in the following three aspects First we derive
the regret lower bound in some special classes of partial monitoring multi-armed bandits an
O(log regret lower bound is known to be achievable In this paper we further extend this lower
bound to obtain a regret lower bound for general partial monitoring problems Second we propose
an algorithm called Partial Monitoring DMED PM-DMED We also introduce a slightly modified
version of this algorithm PM-DMED-Hinge and derive its regret bound PM-DMED-Hinge is the
first algorithm with a logarithmic regret bound for hard problems Moreover for both easy and hard
problems it is the first algorithm with the optimal constant factor on the leading logarithmic term
Third performances of PM-DMED and existing algorithms are compared in numerical experiments
Here the partial monitoring problems consisted of three specific instances of varying difficulty In
all instances PM-DMED significantly outperformed the existing methods when a number of rounds
is large The regret of PM-DMED on these problems quickly approached the theoretical lower
bound
Problem Setup
This paper studies the finite stochastic partial monitoring problem with actions outcomes
and A symbols An instance of the partial monitoring game is defined by a loss matrix li,j
RN and a feedback matrix hi,j where A}. At the beginning the learner is informed of and H. At each round a learner selects an
action and at the same time an opponent selects an outcome The learner
ignores a polylog factor
Note that
suffers loss which he/she cannot observe the only information the learner receives is the
signal We consider a stochastic opponent whose strategy for selecting outcomes is
governed by the opponent?s strategy PM where PM is a set of probability distributions over
an ary outcome The outcome of each round is an sample from
The goal of the learner is to minimize the cumulative loss over
rounds Let the optimal action be the one that minimizes the loss in
expectation that is arg mini?[N
where Li is the i-th
row of L. Assume that is unique Without loss of generality we
can assume that Let Li L1 and Ni
be the number of rounds before the t-th in which action is selected
The performance of the algorithm is measured by the pseudo regret
Regret(T
C2
C4
C1
C5
C3
Ni
Figure Cell decomposition of a partial monitoring
which is the difference between the expected loss of the learner and instance with
the optimal action It is easy to see that minimizing the loss is equivalent to minimizing the regret The expectation of the regret measures the performance of an algorithm that the learner uses
For each action let Ci be the set of opponent strategies for which action is optimal
Ci PM Li Lj
We call Ci the optimality cell of action Each optimality cell is a convex closed polytope Furthermore we call the set of optimality cells CN the cell decomposition as shown in Figure
Let Cic PM Ci be the set of strategies with which action is not optimal
The signal matrix Si of action is defined as Si where
if is true and otherwise The signal matrix defined here is slightly different from the one
in the previous papers Bart?ok in which the number of rows of Si is the number
of the different symbols in the i-th row of H. The advantage in using the definition here is that
Si RA is a probability distribution over symbols that the algorithm observes when it selects
an action Examples of signal matrices are shown in Section An instance of partial monitoring
is globally observable if for all pairs of actions Li Lj ImSk In this paper we
exclusively deal with globally observable instances in view of the minimax regret this includes
trivial easy and hard problems
Regret Lower Bound
A good algorithm should work well against any opponent?s strategy We extend this idea by introducing the notion of strong consistency a partial monitoring algorithm is strongly consistent if it
satisfies E[Regret(T o(T a for any a and PM given and H.
In the context of the multi-armed bandit problem Lai and Robbins derived the regret lower
bound of a strongly consistent algorithm an algorithm must select each arm until its number of
draws Ni satisfies log Ni where is the KL divergence between the two
one-parameter distributions from which the rewards of action and the optimal action are generated
Analogously in the partial monitoring problem we can define the minimum number of observations
Lemma For sufficiently large a strongly consistent algorithm satisfies
E[Ni Si log o(log
where
Si and log is the KL divergence between two discrete
distributions in which we define log
p?i
Lemma can be interpreted as follows for each round consistency requires the algorithm to
make sure that the possible risk that action is optimal is smaller than Large deviation principle states that the probability that an opponent with strategy behaves like is
roughly
exp i?Ni t)D(pi Si Therefore wec need to continue exploration of the actions
until Ni t)D(pi Si log holds for any C1 to reduce the risk to exp log
The proof of Lemma is in Appendix in the supplementary material Based on the technique
used in Lai and Robbins the proof considers a modified game in which another action is
optimal The difficulty in proving the lower bound in partial monitoring lies in that the feedback
structure can be quite complex for example to confirm the superiority of action over one might
need to use the feedback from action
Still we can derive the lower bound by utilizing
the consistency of the algorithm in the original and modified games
We next derive a lower bound on the regret
based on Lemma Note that the expectation of the
regret can be expressed as E[Regret(T E[Ni L1 Let
Rj ri
ri D(pi Si
inf
q?cl(Cj Sj
where denotes a closure Moreover let
Cj pi
inf
ri Rj
ri Li Lj
the optimal solution of which is
Rj pi ri Rj
ri Li Lj Cj pi
The value log is the possible minimum regret for observations such that the minimum divergence of from any C1c is larger than log Using Lemma yields the following
regret lower bound
Theorem The regret of a strongly consistent algorithm is lower bounded as
E[Regret(T log o(log
From this theorem we can naturally measure the harshness of the instance by whereas
the past studies Vanchinathan ambiguously define the harshness as the closeness to
the boundary of the cells Furthermore we show in Lemma in the Appendix that
C1c the regret bound has at most quadratic dependence on C1c which is
defined in Appendix as the closeness of to the boundary of the optimal cell
PM-DMED Algorithm
In this section we describe the partial monitoring deterministic minimum empirical divergence PMDMED algorithm which is inspired by DMED for solving the multi-armed bandit problem
Let p?i be the empirical distribution of the symbols under the selection of action
Namely the k-th element of p?i is hi(t We
sometimes
omit from p?i when it is clear from the context Let the empirical divergence of PM
be Ni
pi the exponential of which can be considered as a likelihood that is
the opponent?s strategy
The main routine of PM-DMED is in Algorithm At each loop the actions in the current list ZC
are selected once The list for the actions in the next loop ZN is determined by the subroutine in
Algorithm The subroutine checks whether the empirical divergence of each point C1c is larger
than log or not If it is large enough it exploits the current information by selecting
the optimal action based on the estimation that minimizes the empirical divergence Otherwise
it selects the actions with the number of observations below the minimum requirement for making
the empirical divergence of each suboptimal point C1c larger than log
Unlike the armed bandit problem in which a reward is associated with an action in the partial
monitoring problem actions outcomes and feedback signals can be intricately related Therefore
we need to solve a non-trivial optimization to run PM-DMED Later in Section we discuss a
practical implementation of the optimization
Algorithm Main routine of PM-DMED and Algorithm PM-DMED subroutine for adding
PM-DMED-Hinge
actions to ZN without duplication
Parameter
Initialization select each action once
Compute an arbitrary such that
ZC ZR ZN
while do
arg min
Ni
pi
for ZC in an arbitrarily fixed order
do
Select and receive feedback
and let arg mini
ZR ZR
If
ZR then put into ZN
Add actions to ZN in accordance with If there are actions
ZR such that
Algorithm PM-DMED
Ni log
Algorithm PM-DMED-Hinge
then put them into ZN
end for
If
ZC ZR ZN ZN
Ni log
pi
end while
then compute some
pi
and put all actions such that
ZR and
ri Ni log into ZN
Necessity of log exploration PM-DMED tries to observe each action to some extent
which is necessary for the following reason consider a four-state game characterized by
and
The optimal action here is action which does not yield any useful information By using action
one receives three kinds of symbols from which one can estimate and
where is the j-th component of From this an algorithm can find that is not very
small and thus the expected loss of actions and is larger than that of action Since the feedback
of actions and are the same one may also use action in the same manner However the loss per
observation is and for actions and respectively and thus it is better to use action This
difference comes from the fact that Since an algorithm does not know
beforehand it needs to observe action the only source for distinguishing from
Yet an optimal algorithm cannot select it more than log times because it affects the O(log
factor in the regret In fact O((log a observations of action with some a are sufficient to
poly(a
be convinced that
For this reason PM-DMED
with probability
selects each action log times
Experiment
Following Bart?ok we compared the performances of algorithms in three different games
the four-state game Section a three-state game and dynamic pricing Experiments on the armed bandit game was also done and the result is shown in Appendix
The three-state game which is classified as easy in terms of the minimax regret is characterized by
and
The signal matrices of this game are
S1
S2
and S3
Random
FeedExp3
CBP
BPM-LEAST
BPM-TS
PM-DMED
LB
regret
round
Random
FeedExp3
CBP
BPM-LEAST
BPM-TS
PM-DMED
LB
round
round
dynamic pricing benign
regret
three-states harsh
Random
FeedExp3
CBP
BPM-LEAST
BPM-TS
PM-DMED
LB
Random
FeedExp3
CBP
BPM-LEAST
BPM-TS
PM-DMED
LB
round
round
dynamic pricing intermediate
round
three-states intermediate
regret
regret
three-states benign
Random
FeedExp3
CBP
BPM-LEAST
BPM-TS
PM-DMED
LB
Random
FeedExp3
CBP
BPM-LEAST
BPM-TS
PM-DMED
LB
regret
regret
regret
dynamic pricing harsh
Random
CBP
BPM-LEAST
BPM-TS
PM-DMED
LB
round
four-states
Figure Regret-round semilog plots of algorithms The regrets are averaged over runs LB is
the asymptotic regret lower bound of Theorem
Dynamic pricing which is classified as hard in terms of the minimax regret is a game that models
a repeated auction between a seller learner and a buyer opponent At each round the seller sets
a price for a product and at the same time the buyer secretly sets a maximum price he is willing to
pay The signal is buy or no-buy and the seller?s loss is either a given constant no-buy or the
difference between the buyer?s and the seller?s prices The loss and feedback matrices are
and
where signals and correspond to no-buy and buy The signal matrix of action is
Si
Following Bart?ok we set and
In our experiments with the three-state game and dynamic pricing we tested three settings regarding
the harshness of the opponent at the beginning of a simulation we sampled points uniformly
at random from PM then sorted them by We chose the top and
harshest ones as the opponent?s strategy in the harsh intermediate and benign settings respectively
We compared Random FeedExp3 CBP with BPM-LEAST BPM-TS and
PM-DMED with Random is a naive algorithm that selects an action uniformly random
FeedExp3 requires a matrix such that and thus one cannot apply it to the four-state
game CBP is an algorithm of logarithmic regret for easy games The parameters and of
CBP were
set in accordance with Theorem in their paper BPM-LEAST is a Bayesian algorithm
regret for easy games and BPM-TS is a heuristic of state-of-the-art performance The
with
priors of two BPMs were set to be uninformative to avoid a misspecification as recommended in
their paper
Algorithm PM-DMED-Hinge subroutine for adding actions to ZN without duplication
Parameters for a/(log log for a
Compute arbitrary which satisfies
arg min
and let arg mini
If
ZR then put into ZN
If
Ni
pi Ni
or there exists an action such that
pi Ni
then put all actions
ZR into ZN
If there are actions such that
Ni log
then put the actions not in ZR into ZN
If
Ni log
pi Ni
then compute some
pi Ni
and put all actions such that
ZR and ri Ni log into ZN If such ri is infeasible then
put all action
ZR into ZN
The computation of in and the evaluation of the condition in involve convex optimizations which were done with Ipopt Moreover obtaining in is classified as a linear
semi-infinite programming LSIP problem a linear programming with finitely many variables
and infinitely many constraints Following the optimization of BPM-LEAST we resorted to a
finite sample approximation and used the Gurobi LP solver in computing at each round
we sampled points from PM and relaxed the constraints on the samples To speed up the
computation we skipped these optimizations in most rounds with large and used the result of
the last computation The computation of the coefficient of the regret lower bound
Theorem is also an LSIP which was approximated by sample points from C1c
The experimental results are shown in Figure In the four-state game and the other two games with
an easy or intermediate opponent PM-DMED outperforms the other algorithms when the number of
rounds is large In particular in the dynamic pricing game with an intermediate opponent the regret
of PM-DMED at is ten times smaller than those of the other algorithms Even in the harsh
setting in which the minimax regret matters PM-DMED has some advantage over all algorithms
except for BPM-TS With sufficiently large the slope of an optimal algorithm should converge to
LB. In all games and settings the slope of PM-DMED converges to LB which is empirical evidence
of the optimality of PM-DMED
Theoretical Analysis
Section shows that the empirical performance of PM-DMED is very close to the regret lower
bound in Theorem Although the authors conjecture that PM-DMED is optimal it is hard to
analyze PM-DMED The technically hardest part arises from the case in which the divergence of
each action is small but not yet fully converged To circumvent this difficulty we can introduce a
discount factor Let
Rj ri
inf
ri D(pi Si
q?cl(Cj Sj
where max(X Note that Rj in is a natural generalization of Rj
in Section in the sense that Rj Rj Event Ni log R1
pi
means that the number of observations Ni is enough to ensure that the discounted empirical divergence of each C1c is larger than log Analogous to Rj we define
Cj pi
inf
ri Rj
ri Lj Li
and its optimal solution by
Rj pi ri Rj
ri Lj Li Cj pi
We also define PM
minj?=i
the optimal region of action
with margin PM-DMED-Hinge shares the main routine of Algorithm with PM-DMED and lists
the next actions by Algorithm Unlike PM-DMED it discounts Ni from the empirical
divergence
pi Moreover when is close to the cell boundary it encourages more
exploration to identify the cell it belongs to by
Theorem Assume that the following regularity conditions hold for pi is
unique at pi Si Moreover for it holds that
cl(int(C1c cl(cl(C1c for all in some neighborhood of where and
denote the closure and the interior respectively Then
E[Regret(T log o(log
We prove this theorem in Appendix Recall that
pi is the set of optimal solutions
of an LSIP In this problem KKT conditions and the duality theorem apply as in the case of finite
constraints thus we can check whether Condition holds or not for each see Ito
and

----------------------------------------------------------------

title: 1788-neural-network-based-model-predictive-control.pdf

Neural Network Based Model Predictive
Control
Stephen Piche
Pavilion Technologies
Austin TX
spiche@pav.com
Jim Keeler
Pavilion Technologies
Austin TX
jkeeler@pav.com
Greg Martin
Pavilion Technologies
Austin TX
gmartin@pav.com
Gene Boe
Pavilion Technologies
Austin TX
gboe@pav.com
Doug Johnson
Pavilion Technologies
Austin TX
djohnson@pav.com
Mark Gerules
Pavilion Technologies
Austin TX
mgerules@pav.com
Abstract
Model Predictive Control a control algorithm which uses
an optimizer to solve for the optimal control moves over a future
time horizon based upon a model of the process has become a standard control technique in the process industries over the past two
decades In most industrial applications a linear dynamic model
developed using empirical data is used even though the process itself is often nonlinear Linear models have been used because of the
difficulty in developing a generic nonlinear model from empirical
data and the computational expense often involved in using nonlinear models In this paper we present a generic neural network
based technique for developing nonlinear dynamic models from empirical data and show that these models can be efficiently used in
a model predictive control framework This nonlinear MPC based
approach has been successfully implemented in a number of industrial applications in the refining petrochemical paper and food
industries Performance of the controller on a nonlinear industrial
process a polyethylene reactor is presented
Introduction
Model predictive control has become the standard technique for supervisory control
in the process industries with over applications in the refining petrochemicals
chemicals pulp and paper and food processing industries Model Predictive
Control was developed in the late and came into wide-spread use particularly
in the refining industry in the The economic benefit of this approach to control
has been documented
Piche J. Keeler G. Martin G. Boe D. Johnson and M. Gerules
Several factors have contributed to the wide-spread use of MPC in the process
industries
Multivariate Control Industrial processes are typically coupled multipleinput multiple-output MIMO systems MIMO control can be implemented using MPC.
Constraints Constraints on the inputs and outputs of a process due to
safety considerations are common in the process industries These constraints can be integrated into the control calculation using MPC.
Sampling Period Unlike systems in other industries such as automotive or
aerospace the open-loop settling times for many processes is on the order
of hours rather than milliseconds This slow settling time translates to
sampling periods on the order of minutes Because the sampling period is
sufficiently long the complex optimization calculations that are required to
implement MPC can be solved at each sampling period
Commercial Tools Commercial tools that facilitate model development and
controller implementation have allowed proliferation of MPC in the process
industries
Unti recently industrial applications of MPC have relied upon linear dynamic
models even though most processes are nonlinear MPC based upon linear models
is acceptable when the process operates at a single setpoint and the primary use of
the controller is the rejection of disturbances However many chemical processes
including polymer reactors do not operate at a single setpoint These processes
are often required to operate at different set points depending upon the grade of the
product that is to be produced Because these processes operate over the nonlinear
range of the system linear MPC often results in poor performance To properly
control these processes a nonlinear model is needed in the MPC algorithm
This need for nonlinear models in MPC is well recognized A number of researchers
and commercial companies have developed both simulation and industrial applications using a variety of different technologies including both first principles and
empirical approaches such as neural networks Although a variety of different
models have been developed they have not been practical for wide scale industrial
application On one hand nonlinear models built using first principle techniques
are expensive to develop and are specific to a process Conversely many empirically
based nonlinear models are not appropriate for wide scale use because they require
costly plant tests in multiple operating regions or because they are too computationally expensive to use in a real-time environment
This paper presents a nonlinear model that has been developed for wide scale industrial use It is an empirical model based upon a neural network which is developed
using plant test data from a single operating region and historical data from all
regions This is in contrast to the usual approach of using plant test data from
multiple regions This model has been used on over industrial applications and
was recognized in a recent survey paper on nonlinear MPC as the most widely used
nonlinear MPC controller in the process industries[l
Neural Network Based Model Predictive Control
After providing a brief overview of model predictive control in the next section
we present details on the formulation of the nonlinear model After describing the
model an industrial application is presented that validates the usefulness of the
nonlinear model in an MPC algorithm
Model Predictive Control
Model predictive control is based upon solving an optimization problem for the
control actions at each sampling interval Using MPC an optimizer computes
future control actions that minimize the difference between a model of the process
and desired performance over a time horizon typically the time horizon is greater
than the open-loop settling time of the process For example given a linear model
of process
where represents the input to the process the optimizer may be used to minimize an objective function at time
Ut+i
i=l
where Yt is the desired set point for the output and is the length of the time
horizon In addition to minimizing an objective function the optimizer is used to
observe a set of constraints For example it is common to place upper and lower
bounds on the inputs as well as bounds on the rate of change of the input
Ut+i Ul ower
Ut+i Ut+i-l AUlower
upper
AUupper
where
and Ulower are the upper and lower input bounds while AUupper and
are the upper and lower rate of change bounds After the trajectory of
future control actions is computed only the first value in the trajectory is sent as a
setpoint to the actuators The optimization calculation is re-run at each sampling
interval using a model which has been updated using feedback
Uupper
AUlower
The form of the model the objective function the constraints and the type of
optimizer have been active areas of research over the past two decades A number
of excellent survey papers on MPC cover these topics As discussed above
we have selected a MIMO nonlinear model which is presented in the next section
Although the objective function given above contains two terms desired output
and input move suppression the objective function used in our implementation
contains thirteen separate terms The details of the objective function are beyond
the scope of this paper Our implementation uses the constraints given above in
and Because we use nonlinear models a nonlinear programming technique
must be used to solve the optimization problem We use LS-GRG which is a reduced
gradient solver
S. Piche J. Keeler G. Martin G. Roe D. Johnson and Gerules
A Generic and Parsimonious Nonlinear Model
For a nonlinear model to achieve wide-spread industrial use the model must be
parsimonious so that it can be efficiently used in an optimization problem Furthermore it must be developed from limited process data As discussed below the
nonlinear model we use is composed of a combination of a nonlinear steady state
model and a linear dynamic model which can be derived from available data The
method of combining the models results in a parsimonious nonlinear model
Process data and component models
The quantity and quality of available data ultimately determines the structure of
an empirical model In developing our models the available data dictated the type
of model that could be created In the process industries two types of data are
available
Historical data The values of the inputs and outputs of most processes
are saved at regular intervals to a data base Furthermore most processing companies retain historical data associated with their plant for several
years
Plant tests Open-loop testing is a well accepted practice for determining
the process dynamics for implementation ofMPC However open-loop testing in multiple operating regions is not well accepted and is impractical in
most cases even if it were accepted
Most practitioners of MPC models have used plant test data and ignored historical
data Practitioners have ignored the historical data in the past because it was
difficult to extract and preprocess the data and build models Historical data
was also viewed as not useful because it was collected in closed-loop and therefore
process dynamics could not be extracted in many cases Using only the plant test
data the practitioner is limited to linear dynamic models
We chose to use the historical data because it can be used to create nonlinear
steady state models of processes that operate at multiple setpoints Combining the
nonlinear steady state model with linear dynamic models from the plant test data
provides a generic approach to developing nonlinear models
To easily facilitate the development of nonlinear models a suite of tools has been
developed for data extraction and preprocessing as well as model training The
nonlinear steady state models
Yss NNss(u
are implemented by a feedforward neural network and trained using variants of
the backpropagation algorithm The developer has a great deal of flexibility in
determining the architecture of the network including the ability to select which
inputs affect which outputs Finally an algorithm for specifying bounds on the
gain Jacobian of the model has recently been implemented
Because of limited plant test data the dynamic models are restricted to second
order models with input time delay
Yt
alYt-l
b1 Ut-d-l b2
Neural Network Based Model Predictive Control
The parameters of are identified by minimizing the squared error between the
model and the plant test data To prevent a biased estimate of the parameters
the identification problem is solved using an optimizer because of the correlation in
the model inputs Tools for selecting the identification regions and viewing the
results are provided
Combining the nonlinear steady state and dynamic models
A variety of techniques exist for combining nonlinear steady state and linear dynamic models The dynamic models can be used to either preprocess the inputs
or postprocess the outputs of the steady state model These models referred to as
Hammerstein and Weiner models respectively contain a large number of parameters and are computationally expensive in an optimization problem when the model
has many inputs and outputs These models when based upon neural networks
also extrapolate poorly
Gain scheduling is often used to combine nonlinear steady state models and linear
dynamic models Using a neural network steady state model the gain at the current
operating point Ui
gi
ayss
au I
U=Ui
is used to update the gain of the linear dynamic model of
where
2gi
al a2
b1 b2
al a2
b1 b2
The difference equation is linearized about the point Ui and Yi thus
Yi and Ui To simplify the equations above a single-input singleoutput system is used Gain scheduling results in a parsimonious model that
is efficient to use in the MPC optimization problem however because this model
does not incorporate information about the gain over the entire trajectory its use
leads to suboptimal performance in the MPC algorithm
Our nonlinear model approach remedies this problem By solving a steady state
optimization problem whenever a setpoint change is made it is possible to compute
the final steady state values of the inputs Given the final steady state input
values the gain associated with the final steady state can be computed For a
system this gain is given by
Using the initial and final gain associated with a setpoint change the gain structure
over the entire trajectory can be approximated This two point gain scheduling
overcomes the limitations of regular gain scheduling in MPC algorithms
Piche Keeler G. Martin G. Boe. D. Johnson and Gerules
Combining the initial and final gain with the linear dynamic model a quadratic
difference equation is derived for the overall nonlinear model
where
bi
b2
al
ud
al 9d
ud
and VI and V2 are given by and Use of the gain at the final steady state
introduces the last two terms of This model allows the incorporation of gain
information over the entire trajectory in the MPC algorithm The gain at of at
Ui is 9i while at uf it is Between the two points the gain is a linear combination
of 9i and For processes with large gain changes such as polymer reactors this
can lead to dramatic improvements in MPC controller performance
An additional benefit of using the model of is that we allow the user to bound
the initial and final gain and thus control the amount of nonlinearity used in the
model For practitioners who are use to implementing MPC with linear models
using gain bounds allows them to transition from linear to nonlinear models This
ability to control the amount of nonlinearity used in the model has been important
for acceptance of this new model in many applications Finally bounding the gains
can be used to guarantee extrapolation performance of the model
The nonlinear model of fits the criteria needed in order to allow wide spread
use of nonlinear models for MPC. The model is based upon readily available data
and has a parsimonious representation allowing models with many inputs and outputs to be efficiently used in the optimizer Furthermore it addresses the primary
nonlinearity found in processes that being the significant change in gain over the
operating region
Polymer Application
The nonlinear model described above has been used in a wide-variety of industrial
applications including Kamyr digesters pUlp and paper milk evaporators and
dryers food processing toluene diamine purification chemicals polyethylene and
polypropylene reactors polymers and a fluid catalytic cracking unit refining
Highlights of one such application are given below
A MPC controller that uses the model described above has been applied to a Gas
Phase High Density Polyethylene reactor at Chevron Chemical Co. in Cedar Bayou
Texas The process produces homopolymer and copolymer grades over a wide
range of melt indices It's average production rate per year is tons
Optimal control of the process is difficult to achieve because the reactor is a highly
coupled nonlinear MIMO system inputs and outputs For example a number
of input-output pairs exhibit gains that varying by a factor of or more over the
operating region In addition grade changes are made every few days During these
transitions nonprime polymer is produced Prior to commissioning these controllers
Neural Network Based Model Predictive Control
these transitions took several hours to complete Linear and gain scheduling based
controller have been tried on similar reactors and have delivered limited success
The nonlinear model was constructed using only historical data The nonlinear
steady state model was trained upon historical data from a two year period This
data contained examples of all the products produced by the reactor Accurate dynamic models were derived both from historical data and knowledge of the process
thus no step tests were conducted on the process
Excellent performance of this controller has been reported A two-fold decrease
in the variance of the primary quality variable melt index has been achieved In
addition the average transition time has been decreased by Unscheduled
shutdowns which occurred previously have been eliminated Finally the controller
which has been on-line for two years has gained high operator acceptance
Conclusion
A generic and parsimonious nonlinear model which can be used in an MPC algorithm has been presented The model is created by combining a nonlinear steady
state model with a linear dynamic models They are combined using a two-point
gain scheduling technique This nonlinear model has been used for control of a
nonlinear MIMO polyethylene reactor at Chevron Chemical Co. The controller has
also been used in other applications in the refining chemicals food processing
and pulp and paper industries

----------------------------------------------------------------

title: 1004-iceg-morphology-classification-using-an-analogue-vlsi-neural-network.pdf

ICEG Morphology Classification using an
Analogue VLSI Neural Network
Richard Coggins Marwan Jabri Barry Flower and Stephen Pickard
Systems Engineering and Design Automation Laboratory
Department of Electrical Engineering
University of Sydney Australia
Email richardc@sedal.su.oz.au
Abstract
An analogue VLSI neural network has been designed and tested
to perform cardiac morphology classification tasks Analogue techniques were chosen to meet the strict power and area requirements
of an Implantable Cardioverter Defibrillator ICD system The robustness of the neural network architecture reduces the impact of
noise drift and offsets inherent in analogue approaches The network is a multi-layer percept ron with on chip digital weight
storage a bucket brigade input to feed the Intracardiac Electrogram ICEG to the network and has a winner take all circuit
at the output The network was trained in loop and included a
commercial ICD in the signal processing path The system has successfully distinguished arrhythmia for different patients with better
than true positive and true negative detections for dangerous
rhythms which cannot be detected by present ICDs The chip was
implemented in CMOS and consumes less than maximum average power in an area of
INTRODUCTION
To the present time most ICDs have used timing information from ventricular
leads only to classify rhythms which has meant some dangerous rhythms can not
be distinguished from safe ones limiting the use of the device Even two lead
Richard Coggins Marwan Jabri Barry Flower Stephen Pickard
HO
I.SO
O.SO
Figure The Morphology of ST and VT retrograde
atrial/ventricular systems fail to distinguish some rhythms when timing information alone is used Leong and Jabri A case in point is the separation of Sinus Tachycardia from Ventricular Tachycardia with retrograde conduction
ST is a safe arrhythmia which may occur during vigorous exercise and is characterised by a heart rate of approximately beats/minute VT retrograde also
occurs at the same low rate but can be a potentially fatal condition False negative
detections can cause serious heart muscle injury while false positive detections deplete the batteries cause patient suffering and may lead to costly transplantation
of the device Figure shows however the way in which the morphology changes
on the ventricular lead for these rhythms Note that the morphology change is
predominantly in the QRS complex where the letters QRS are the conventional
labels for the different points in the conduction cycle during which the heart is
actually pumping blood
For a number of years researchers have studied template matching schemes in order
to try and detect such morphology changes However techniques such as correlation
waveform analysis Lin though quite successful are too computationally intensive to meet power requirements In this paper we demonstrate that
an analogue VLSI neural network can detect such morphology changes while still
meeting the strict power and area requirements of an implantable system The
advantages of an analogue approach are born out when one considers that an energy efficient analogue to digital converter such as Kusumoto uses
per conversion implying power consumption for analogue to digital
conversion of the ICEG alone Hence the integration of a bucket brigade device and
analogue neural network provides a very efficient way of interfacing to the analogue
domain Further since the network is trained in loop with the ICD in real time
the effects of device offsets noise QRS detection jitter and signal distortion in the
analogue circuits are largely alleviated
The next section discusses the chip circuit designs Section describes the method
ICEG Morphology Classification Using an Analogue VLSI Neural Network
AowAcId
1axl AIRy
Column
AoIcIr
I
o.ta Reglsl
IClkcMmux
I
I WTAI
DOD DO
Figure Floor Plan and Photomicrograph of the chip
used to train the network for the morphology classification task Section describes
the classifier performance on seven patients with arrhythmia which can not be
distinguished using the heart rate only Section summarises the results remaining
problems and future directions for the work
ARCHITECTURE
The neural network chip consists of a multilayer perceptron an input bucket
brigade device BBD and a winner take all WTA circuit at the output A floor
plan and photomicrograph of the chip appears in figure The BBD samples the
incoming ICEG at a rate of For three class problems the winner take all
circuit converts the winning class to a digital signal For the two class problem
considered in this paper a simple thresholding function suffices The following
subsections briefly describe the functional elements of the chip The circuit diagrams
for the chip building blocks appear in figure
BUCKET BRIGADE DEVICE
One stage of the bucket brigade circuit is shown in figure The BBD uses a
two phase clock to shift charge from cell to cell and is based on a design by
Leong Leong The BBD operates by transferring charge deficits from
to in each of the cells PHIl and PHI2 are two phase non-overlapping clocks
The cell is buffered from the synapse array to maintain high charge transfer efficiency A sample and hold facility is provided to store the input on the gates of the
synapses The BBD clocks are generated off chip and are controlled by the QRS
complex detector in the lCD
SYNAPSE
This synapse has been used on a number of neural network chips previously
e.g Coggins The synapse has five bits plus sign weight storage which
Richard Coggins Marwan Jabri Barry Flower Stephen Pickard
NEURON
BUJIOIII
BUCKET BRIGADE ClLL
Figure Neuron Bucket Brigade and Synapse Circuit Diagrams
sets the bias to a differential pair which performs the multiplication The bias

----------------------------------------------------------------

title: 1642-robust-learning-of-chaotic-attractors.pdf

Robust Learning of Chaotic Attractors
Rembrandt Bakker
Chemical Reactor Engineering
Delft Univ of Technology
r.bakker@stm.tudelft?nl
Jaap C. Schouten
Marc-Olivier Coppens
Chemical Reactor Engineering Chemical Reactor Engineering
Eindhoven Univ of Technology
Delft Univ of Technology
J.C.Schouten@tue.nl
coppens@stm.tudelft?nl
Floris Takens
C. Lee Giles
Cor M. van den Bleek
Dept Mathematics
University of Groningen
NEC Research Institute
Princeton Nl
Chemical Reactor Engineering
Delft Univ of Technology
F. Takens@math.rug.nl
giles@research.nj.nec.com
vdbleek@stm.tudelft?nl
Abstract
A fundamental problem with the modeling of chaotic time series data is that
minimizing short-term prediction errors does not guarantee a match
between the reconstructed attractors of model and experiments We
introduce a modeling paradigm that simultaneously learns to short-tenn
predict and to locate the outlines of the attractor by a new way of nonlinear
principal component analysis Closed-loop predictions are constrained to
stay within these outlines to prevent divergence from the attractor Learning
is exceptionally fast parameter estimation for the sample laser data
from the Santa Fe time series competition took less than a minute on
a MHz Pentium PC.
Introduction
We focus on the following objective given a set of experimental data and the assumption that
it was produced by a deterministic chaotic system find a set of model equations that will
produce a time-series with identical chaotic characteristics having the same chaotic attractor
The common approach consists oftwo steps identify a model that makes accurate shorttenn predictions and generate a long time-series with the model and compare the
nonlinear-dynamic characteristics of this time-series with the original measured time-series
Principe found that in many cases the model can make good short-tenn predictions
but does not learn the chaotic attractor The method would be greatly improved if we could
minimize directly the difference between the reconstructed attractors of the model-generated
and measured data instead of minimizing prediction errors However we cannot reconstruct
the attractor without first having a prediction model Until now research has focused on how
to optimize both step and step For example it is important to optimize the prediction
horizon of the model and to reduce complexity as much as possible This way it was
possible to learn the attractor of the benchmark laser time series data from the Santa Fe
DelftChemTech Chemical Reactor Engineering Lab lulianalaan BL Delft The
Netherlands http://www.cpt.stm.tudelft.nllcptlcre!researchlbakker
R. Bakker J. C. Schouten Coppens F. Takens C. L. Giles and C. M. Bleek
time series competition While training a neural network for this problem we noticed that
the attractor of the model fluctuated from a good match to a complete mismatch from one
iteration to another We were able to circumvent this problem by selecting exactly that model
that matches the attractor However after carrying out more simulations we found that what
we neglected as an unfortunate phenomenon is really a fundamental limitation of current
approaches
An important development is the work of Principe who use Kohonen Self Organizing
Maps SOMs to create a discrete representation of the state space of the system This creates
a partitioning of the input space that becomes an infrastructure for local linear model
construction This partitioning enables to verify if the model input is near the original data
detect if the model is not extrapolating without keeping the training data set with the model
We propose a different partitioning of the input space that can be used to learn the outlines
of the chaotic attractor by means of a new way of nonlinear Principal Component Analysis
and enforce the model never to predict outside these outlines The nonlinear PCA
algorithm is inspired by the work of Kambhatla and Leen on local PCA they partition the
input space and perform local PCA in each region Unfortunately this introduces
discontinuities between neighboring regions We resolve them by introducing a hierarchical
partitioning algorithm that uses fuzzy boundaries between the regions This partitioning closely
resembles the hierarchical mixtures of experts of Jordan and Jacobs
In Sec. we put forward the fundamental problem that arises when trying to learn a chaotic
attractor by creating a short-term prediction model In Sec. we describe the proposed
partitioning algorithm In Sec. it is outlined how this partitioning can be used to learn the
outline of the attractor by defining a potential that measures the distance to the attractor In Sec.
we show modeling results on a toy example the logistic map and on a more serious
problem the laser data from the Santa Fe time series competition Section concludes
The attractor learning dilemma
Imagine an experimental system with a chaotic attractor and a time-series of noise-free
measurements taken from this system The data is used to fit the parameters of the model
FwC whereF is a nonlinear function wcontains its adjustable parameters
and is a positive constant What happens if we fit the parameters by nonlinear least
squares regression Will the model be stable will the closed-loop long term prediction
converge to the same attractor as the one represented by the measurements
Figure shows the result of a test by Diks that compares the difference between the
model and measured attractor The figure shows that while the neural network is trained to
predict chaotic data the model quickly
converges to the measured attractor
but once in a while from one
iteration to another the match between
the attractors is lost
I
To understand what causes this
instability imagine that we try to fit the
I
Lo..
parameters of a model
ii Zt
while the real system has a point
attractor a where is the state of
training progress leg iterations
the system and a its attracting value Figure Diks test monitoring curve for a neural
Clearly measurements taken from this
network model trained on data from an
system contain no information to
experimental chaotic pendulum
Robust Learning of Chaotic Attractors
estimate both ii and B. If we fit the model parameters with non-robust linear least squares
may be assigned any value and if its largest eigenvalue happens to be greater than zero the
model will be unstable
For the linear model this problem has been solved a long time ago with the introduction of
singular value decomposition There still is a need for a nonlinear counterpart of this technique
in particular since we have to work with very flexible models that are designed to fit a wide
variety of nonlinear shapes see for example the early work of Lapedes and Farber It is
already common practice to control the complexity of nonlinear models by pruning or
regularization Unfortunately these methods do not always solve the attractor learning
problem since there is a good chance that a nonlinear term explains a lot of variance in one
part of the state space while it causes instability of the attractor without affecting the one-stepahead prediction accuracy elsewhere In Secs and we will introduce a new method for
nonlinear principal component analysis that will detect and prevent unstable behavior
The split and fit algorithm
The nonlinear regression procedure of this section will form the basis of the nonlinear principal
component algorithm in Sec. It consists of a partitioning of the input space a local
linear model for each region and iii fuzzy boundaries between regions to ensure global
smoothness The partitioning scheme is outlined in Procedure
Procedure Partitioning the input space
Start with the entire set of input data
Determine the direction of largest variance of perform a singular value
decomposition of into the product ULVT and take the eigenvector column
of with the largest singular value on the diagonal of EJ.
Split the data in two subsets to be called clusters by creating a plane
perpendicular to the direction of largest variance through the center of
gravity of Z.
Next select the cluster with the largest sum squared error to be split next
and recursively apply until a stopping criteria is met
Figures and show examples of the partitioning The disadvantage of dividing regression
problems into localized subproblems was pointed out by Jordan and Jacobs the spread of
the data in each region will be much smaller than the spread of the data as a whole and this
will increase the variance of the model parameters Since we always split perpendicular to the
direction of maximum variance this problem is minimized
The partitioning can be written as a binary tree with each non-terminal node being a split and
each terminal node a cluster Procedure creates fuzzy boundaries between the clusters
Procedure Creating fuzzy boundaries
An input enters the tree at the top of the partitioning tree
The Euclidean distance to the splitting hyperplane is divided by the
bandwidth f3 of the split and passed through a sigmoidal function with range
This results in i's share in the subset on z's side of the splitting
plane The share in the other subset is I-a.
The previous step is carried out for all non-terminal nodes of the tree
R. Bakker J. C. Schouten Coppens F. Takens C. L. Giles and C. M. Bleek
The membership Pc of to subset terminal node is computed by
taking the product of all previously computed shares along the path from
the terminal node to the top of the tree
If we would make all parameters adjustable that is the orientation of the splitting
hyperplanes the bandwidths and iii the local linear model parameters the above model
structure would be identical to the hierarchical mixtures of experts of Jordan and Jacobs
However we already fixed the hyperplanes and use Procedure to compute the bandwidths
Procedure Computing the Bandwidths
The bandwidths of the terminal nodes are taken to be a constant we use
the confidence limit of a normal distribution times the variance of the
subset before it was last split in the direction of the eigenvector of that last split
The other bandwidths do depend on the input They are computed by
climbing upward in the tree The bandwidth of node is computed as a
weighted sum between the fJs of its right and left child by the implicit formula
Pn=OL PL uRPR in which uLand OR depend on Pn Starting from initial guess
Pn=PL if or else Pn=PR the formula is solved in a few iterations
This procedure is designed to create large overlap between neighboring regions and almost no
overlap between non-neighboring regions What remains to be fitted is the set of the local
linear models The j-th output of the split&fit model for a given input zp is computed
Yj,p
fl ii;zp where iicand contain the linear model parameters of subset
c=J
and is the number of clusters We can determine the parameters of all local linear models in
one global fit that is linear in the parameters However we prefer to locally optimize the
parameters for two reasons it makes it possible to locally control the stability of the
attractor and do the principal component analysis of Sec. and the computing time for a
linear regression problem with regressors scales If we would adopt global fitting
would scale linearly with and while growing the model the regression problem would
quickly become intractable We use the following iterative local fitting procedure instead
Procedure Iterative Local Fitting
Initialize a by matrix of residuals to zero being the number of
outputs and the number of data
For cluster if an estimate for its linear model parameters already exists
forc each input vector add flcJYv
l,p to the matrix of
residuals otherwise add
flpYj,p to Yj.P being the j-th element of the deSIred output vector for sample
Least squares fit the linear model parameters of cluster to predict the
current residuals and subtract the new estimate from R.
Do for each cluster and repeat the fitting several times default
From simulations we found that the above fast optimization method converges to the global
minimum if it is repeated many times Just as with neural network training it is often better to
use early stopping when the prediction error on an independent test set starts to increase
Robust Learning o/Chaotic Attractors
Nonlinear Principal Component Analysis
To learn a chaotic attractor from a single experimental time-series we use the method ofdelays
the state consists of delays taken from the time series The embedding dimension must
be chosen large enough to ensure that it contains sufficient infonnation for faithful
reconstruction of the chaotic attractor see Takens Typically this results in an mdimensional state space with all the measurents covering only a much lower dimensional but
non-linearly shaped subspace This creates the danger pointed out in Sec. the stability of the
model in directions perpendicular to this low dimensional subspace cannot be guaranteed
With the split fit algorithm from Sec. we can learn the non-linear shape of the low
dimensional subspace and if the state of the system escapes from this subspace we use the
algorithm to redirect the state to the nearest point on the subspace See Malthouse for
limitations of existing nonlinear peA approaches To obtain the low dimensional subspace we
proceed according to Procedure
Procedure Learning the Low-dimensional Subspace
Augment the output of the model with the m-dimensional statel the
model will learn to predict its own input
In each cluster perfonn a singular value decomposition to create a set of
principal directions sorted in order of decreasing explained variance The
result of this decomposition is also used in step of Procedure
Allow the local linear model of each cluster to use no more than mred of
these principal directions
Define a potential to be the squared Euclidian distance between the
state and its prediction by the model
The potential implicitly defines the
lower dimensional subspace if a state
is on the subspace will be zero
will increase with the distance of
from the subspace The model has
learned to predict its own input with
small error meaning that it has tried
to reduce as much as possible at
exactly those points in state space
where the training data was sampled
In other words will be low if the
input is close to one of the original
points in the training data set From
the split&fit algorithm we can
analytically compute the gradient
dPldl Since the evaluation of the
split&fit model involves a backward
computing the bandwidths and
forward pass computing
memberships the gradient algorithm
involves a forward and backward
pass through the tree The gradient is
used to project states that are off the
nonlinear subspace onto the subspace
Figure Projecting two-dimensional data on a onedimensional self-intersecting subspace The
colorscale represents the potential white indicates
R. Bakker C. Schouten Coppens F. Takens C. L. Giles and C. M. Bleek
in one or a few Newton-Rhapson iterations
Figure illustrates the algorithm for the
problem of creating a one-dimensional
representation of the number The
training set consists of clean samples Xl
and shows how a set of noisy
inputs is projected by a 48 subset split&fit
model onto the one-dimensional subspace
Note that the center of the cannot be well
represented by a one-dimensional space We
leave development of an algorithm that
X1
automatically detects the optimum local
subspace dimension for future research
Figure Learning the attractor of the twoinput logistic map The order of creation of the
Application Examples
splits is indicated The colorscale represents the
potential white indicates
First we show the nonlinear principal
component analysis result for a toy
example the logistic map Zt+l If we use a model Zt+l Fw(zt where the
prediction only depends on one previous output there is no lower dimensional space to which
the attractor is confined However if we allow the output to depend on more than a single
delay we create a possibility for unstable behavior Figure shows how well the split&fit
algorithm learns the one-dimensional shape of the attractor after creating only five regions The
parabola is slightly deformed seen from the white lines perpendicular to the attractor but this
may be solved by increasing the number of splits
Next we look at the laser data The complex behavior of chaotic systems is caused by an
interplay of destabilizing and stabilizing forces the destabilizing forces make nearby points in
state space diverge while the stabilizing forces keep the state of the system bounded This
process known as stretching and folding results in the attractor of the system the set of
points that the state of the system will visit after all transients have died out In the case of the
laser data this behavior is clear cut destabilizing forces make the signal grow exponentially
until the increasing amplitude triggers a collapse that reinitiates the sequence We have seen
in neural network based models and in this study that it is very hard for the models to cope
with the sudden collapses Without the nonlinear subspace correction of Sec. most of the
train data
time
Figure Laser data from the Santa Fe time series competition The sample train
data set is followed by iterated prediction of the model After every prediction a
correction is made to keep Sec. small Plot shows before this correction
Robust Learning of Chaotic Attractors
models we tested grow without bounds after one or more rise and collapse sequences That is
not very surprising the training data set contains only three examples of a collapse Figure
shows how this is solved with the subspace correction every time the model is about to grow
to infinity a high potential is detected depicted in and the state of the system is
directed to the nearest point on the subspace as learned from the nonlinear principal component
analysis After some trial and error we selected an embedding dimension of and a
reduced dimension mred of The split&fit model starts with a single dataset and was grown
until 48 subsets At that point the error on the sample train set was still decreasing
rapidly but the error on an independent sample test set increased We compared the
reconstructed attractors of the model and measurements using samples of closed-loop
generated and samples of measured data No significant difference between the two
could be detected by the Diles test
Conclusions
We present an algorithm that robustly models chaotic attractors It simultaneously learns
to make accurate short term predictions and the outlines of the attractor In closed-loop
prediction mode the state of the system is corrected after every prediction to stay within these
outlines The algorithm is very fast since the main computation is to least squares fit a set of
local linear models In our implementation the largest matrix to be stored is by being
the number of data and the number of clusters We see many applications other than attractor
learning the split&fit algorithm can be used as a fast learning alternative to neural networks
and the new form of nonlinear peA will be useful for data reduction and object recognition
We envisage to apply the technique to a wide range of applications from the control and
modeling of chaos in fluid dynamics to problems in finance and biology to fluid dynamics
Acknowledgements
This work is supported by the Netherlands Foundation for Chemical Research SON with financial
aid from the Netherlands Organization for Scientific Research

----------------------------------------------------------------

title: 1600-non-linear-pi-control-inspired-by-biological-control-systems.pdf

Non-linear PI Control Inspired by
Biological Control Systems
Lyndon J. Brown
Gregory E. Gonye
James S. Schwaber
Experimental Station E.!. DuPont deNemours Co. Wilmington DE
Abstract
A non-linear modification to PI control is motivated by a model
of a signal transduction pathway active in mammalian blood pressure regulation This control algorithm labeled PII proportional
with intermittent integral is appropriate for plants requiring exact set-point matching and disturbance attenuation in the presence
of infrequent step changes in load disturbances or set-point The
proportional aspect of the controller is independently designed to
be a disturbance attenuator and set-point matching is achieved
by intermittently invoking an integral controller The mechanisms
observed in the Angiotensin AT1 signaling pathway are used to
control the switching of the integral control Improved performance
over PI control is shown on a model of cyc1opentenol production
A sign change in plant gain at the desirable operating point causes
traditional PI control to result in an unstable system Application of this new approach to this problem results in stable exact
set-point matching for achievable set-points
Biological processes have evolved sophisticated mechanisms for solving difficult control problems By analyzing and understanding these natural systems it is possible
that principles can be derived which are applicable to general control systems This
approach has already been the basis for the field of artificial neural networks which
are loosely based on a model of the electrical signaling of neurons A suitable candidate system for analysis is blood pressure control Tight control of blood pressure
is critical for survival of an animal Chronically high levels can lead to premature
death Low blood pressure can lead to oxygen and nutrient deprivation and sudden
load changes must be quickly responded to or loss of consciousness can result The
baroreflex reflexive change of heart rate in response to blood pressure challenge
has been previously studied in order to develop some insights into biological control
systems
Jyndon.j brown@usa.dupont com
Address correspondence to this author
Gregory.E.Gonye-PHD@usa.dupont com James.S.Scwhaber@usa.dupont.com
L. J. Brown G. E. Gonye and J. S. Schwaber
Neurons exhibit complex dynamic behavior that is not directly revealed by their
electrical behavior but is incorporated in biochemical signal transduction pathways This is an important basis for plasticity of neural networks The area of the
brain to which the baroreceptor afferents project is the nucleus of tractus solitarus
The neurons in the NTS are rich with diverse receptors for signaling pathways It is logical that this richness and diversity playa crucial role in the signal
processing that occurs here Hormonal and neurotransmitter signals can activate
signal transduction pathways in the cell which result in physical modification of
some components of a cell or altered gene regulation Fuxe al have shown the
presence of the angiotensin AT receptor pathway in NTS neurons and Herbert
has demonstrated its ability to affect the baroreflex
To develop understanding of the effects of biochemical pathways a detailed kinetic
model of the angiotensin/AT pathway was developed Certain features of this
model and the baroreflex have interesting characteristics from a control engineering
perspective These features have been used to develop a novel control strategy
The resulting control algorithm utilizes a proportional controller that intermittently
invokes integral action to achieve set-point matching Thus the controller will be
labeled PII.
The use of integral control is popular as it guarantees cancellation of offsets and
ensures exact set-point matching However the use of integral control does have
drawbacks It introduces significant lag in the feedback system which limits the
bandwidth of the system Increasing the integral gain in order to improve response
time can lead to systems with excessive overshoot excessive settling times and
less robustness to plant changes or uncertainty Many processes in the chemical
industry have a steady-state response curve with a maximum and frequently the
optimal operating condition is at this peak Unfortunately any controller with true
integral action will be unstable at this operating point
In a crude sense the integrator learns the constant control action required to achieve
set-point matching If the integral control is viewed as a simple learning device than
a logical step is to remove it from the feedback loop once the necessary offset has
been learned If the offset is being successfully compensated for only noise remains
as a source for learning It has been well established that learning based on nothing
but noise leads to undesirable results The maxim garbage in garbage out will
apply Without integral control the proportional controller can be made more aggressive while maintaining stability margins and/or control actions at similar levels
This control strategy will be appropriate for plants with infrequent step changes in
set-points or loads The challenge becomes deciding when and how to perform this
switching so that the resulting controller provides significant improvements
Angiotensin III ATI receptor Signal Transduction Model
Regulation of blood pressure is a vital control problem in mammals Blood pressure
is sensed by stretch sensitive cells in the aortic arch and carotid sinus These cells
transmit signals to neurons in the NTS which are combined with other signals from
the central nervous system CNS resulting in changes to the cardiac output and
vascular tone This control is implemented by two parallel systems in the CNS
the sympathetic and parasympathetic nervous systems The sympathetic system
primarily affects the vascular tone and the parasympathetic system affects cardiac
output Cardiac control can have a larger and faster effect but long term
application of this control is injurious to the overall health of the animal Pottman
al have suggested that these two systems separately control for long term
set-point control and fast disturbance rejection
Non-Linear PI Control Inspired by Biological Control Systems
One receptor in NTS neuronal cells is the AT1 receptor which binds Angiotensin
II. The NTS is located in the brain stem where much of the processing of the autonomic regulatory systems reside Angiotensin infusion in this region of the brain
has been shown to significantly affect blood pressure control In order to understand this aspect of neuronal behavior a detailed kinetic model of this signaling
pathway was developed The pathway is presented in Figure The outputs can
be considered to be the concentrations of Gq?GTP GO-y activated protein kinase
and/or calmodulin dependent protein kinase
Several reactions in the cascade are of interest The binding of phospholipase is
significantly slower than the other steps in the reaction This can be modeled as
a first order transfer function with a long time constant or as a pure integrator
The IP receptor is a ligand gated channel on the membrane of the endoplasmic
reticulum As Figure shows when IP binds to this receptor calcium is
released from the ER into the cells cytoplasm However the IP3 receptor also
has binding sites on its cytoplasmic domain for binding calcium The first has
relatively fast dynamics and causes a substantial increase in the channel opening
The second calcium binding site has slower dynamics and inactivates the channel
The effect of this first binding site is to introduce positive feedback into the model
In traditional control literature positive feedback is generally undesirable Thus it
is very interesting to see positive feedback in neuronal control systems
A typical surface response for the model comparing the time response of activated
calmodulin versus the peak concentration of a pulse of angiotensin is shown in
Figure The results are consistent with behavior of cells measured by Li and
Guyenet The output level is seen to abruptly rise after a delay which is a
decreasing function of the magnitude of the input Unlike a linear system both the
magnitude and speed of the response of the system are functions of the magnitude
of the input Further the relaxing of the system to its equilibrium is a very slow
response as compared to its activation This behavior can be attributed to the
positive feedback response inherent to the IP3 receptor The effect of the slow
dynamics of the phospholipase binding and the IP3 receptor dynamics results in
an activation behavior similar to a threshold detector on the integrated input signal
However removal of the input results in a slow recovery back to zero The activation
of the calcium calmodulin dependent protein kinase can lead to phosphorilation of
channels that result in synaptic conductance changes that are functionally related
to the amount of activated kinase The activation of calcium calmodulin can also
lead to changes in gene regulation that could potentially result in long term changes
in the neurons synaptic conductances
Proportional with Intermittent Integral Control
Key features from the model that are incorporated in the control law are
separate controllers for set-point control and disturbance attenuation
activation of set-point controller when integrated error exceeds threshold
strength of integral action when activated will be a function of the speed
with which activation was achieved
smooth removal of integral action without disruption of control action
The PII controller begins initially as a proportional controller with a nominal offset
added to its output The integrated error is monitored The integral controller
is turned on when the integrated error exceeds a threshold Once the integral
control action is activated it remains active as long as the error is excessive Once
the error is not significant then the integral control action can be removed in a
L. J. Brown G. E. Gonye and J. S. Schwaber
r:ti*rsuoorut
hra
adb
ra ld
p=PIP2
Cell Membrane
Cytoplasm
dar
atl
metabollt
ER
CaM
Di:uslon
I Receptor CaM
Ca
Figure Schematic and Surface Responses of Angiotensin
I
ATI Model
smooth manner This has been achieved by allowing the value of the integral gain
Ki to decay exponentially It is important that this is done in such a manner
as not to affect the actual control signal This can be achieved by adjusting the
offset appropriately Since Kpe Kiels and Ki ex then can be made
constant for constant by adding offset Ko where Ko ex Kiel The integral
action is completely removed once Ki has decayed to the point where it is no longer
significant In order to make the effect of activation of the integrator correspond
to the behavior of the angiotensin model the integrated error is scaled by the time
spent reaching the threshold when the integrator is turned on This corresponds to
point above
If the error undergoes significant change when the integrator is already fully active
the system will behave similarly to a system with a PI controller whose gains have
been set too high This may result in significant overshoot and possibly instability
There is a small chance that even with infrequent step changes the residual error or
random disturbance could trigger the integrator immediately before a step change
In a biological control system control does not rest in one neuron or necessarily in
one signal transduction pathway but in multiple pathways Furthermore study of
individual cells shows a great deal of variability in the details of their behavior By
implementing the intermittent integral control as a sum of many equivalent controllers as in left side of Figure with variability in their threshold parameters
a controller can be developed that is not subject to the chance of being fully activated by random disturbance or residual error During steady-state operation these
integrators will quickly deactivate when noise or small disturbances trigger them
as the error will be less than the threshold However an actual step change in the
error signal will result in all or most of the integrators activating and remaining
active until the error is compensated for
The block diagram on right side of Figure and the time dependent definitions in
Table precisely define the control algorithm for the single integrator case
Non-Linear PI Control Inspired by Biological Control Systems
5uml
kis=ki
xu=xu1
eu=eu1<eu2<eu3<eu4<eu5
Figure Block Diagrams for Control Algorithm Implementations
If
then
to
Ki(t
X(tO
and
Xu
IKi(t)1 Ki and eu
IKi(t)1 Ki
Otherwise
Ki(tO Ko(to
tt(tO
to
it
K*i'X
Ki(t KdecayKi(t Ko(t KdecayKi(t)X(t
Ko(t
Ki(t
Ko(t
it(t
Table Definition of Gains for PII Control
Control of CSTR Reactor for Cyclopentenol Production
The model of the CSTR reactor is taken from The basic process converts
cyclopentadiene to cyclopentenol Cyclopentenol can undergo a further undesirable
reaction to form cyclopentadiol and cyclopentadiene can undergo an alternative
reaction to form dicyclopentadiene The rates of the reactions are temperature
dependent Inputs to the model are flow rate and the jacket temperature The first
input is the control input and the jacket temperature is an unmeasured disturbance
with a root mean square deviation of about a nominal value of C. The
regulated output will be the cyclopentenol concentration in the outflow
The steady-state response of this process is shown in Figure Operation in the
region labeled up to the peak of the curve labeled VIII has been considered At
the point labeled VIII the steady-state gain of the plant goes to Plants with
steady-state gains which change sign can not be stably controlled with PI control
An additional complicating factor is that the plant has significant inverse response
in this region
Criteria for this control design problem in order of importance are
operate between 45 and ljhour with reasonable high frequency gain
minimize the overshoot
minimize rise time
L. J. Brown G. E. Gonye and J. S. Schwaber
minimize the inverse response
Satisfying the first and last criteria should ensure a robust controller Precise numerical performance criteria for the rise time have not been specified as no fixed
values are reasonable for the entire region
A PI controller as well as a PH controller have been designed and the results are
displayed in Figures The controller parameters were Kp 75 Ki for
the standard PI controller The PH controller used equally weighted parallel
integrators with Kp total and Kdecay The threshold
parameters were chosen as eu
I
I
1l
01
Foedtalo lib
03
Tme(t'lOUnl
PI
R. I'M'ICe
Figure Steady-State Response of Cyclopentenol CSTR Reactor and Output Concentration from CSTR Reactor
The set-point was chosen to be a series of smoothed steps Smoothing was performed with a first-order low-pass filter with unity DC gain and a time constant of
hours-i While operating in the region of design from to hours and to
hours the PH controlled system as compared to the PI controlled system had reduced inverse response less worst-case overshoot similar response times and greater
disturbance attenuation A closer examination of the PI controlled system during
the interval hours showed that at this extreme operating point oscillations
of a fixed period begin to appear This indicates the existence of poorly damped
poles The PH controlled system did not show this degradation of performance
The set-point was raised to nearly the maximum achievable concentration This
allows examination of the behavior of the controller when operating near regions
of uncertainty in the sign of the plant gain This operating point achieves the
maximum possible conversion to cyclopentenol and thus has significant economic
advantages In the region from to lOs there is a reduction in the disturbance response with the PH controller At this operating point the PI controlled
system can be shown to be locally st able However the effects of integrated noise
easily allow the system trajectory to escape the region of attraction As expected
the PI controlled system went unstable The PH controlled system remains well
behaved The simulation was run for a total simulated time of 43 hours at this operating point and repeated many times without seeing any loss of stability with PH
controller With PI control the system went unst able within hours for each trial
Thus PH control allows operation at set-points closer to maximums or minimums
Conclusion
The mechanisms that biological control systems employ to successfully control nonlinear time varying multivariable physiological systems under very demanding per
Non-Linear PI Control Inspired by Biological Control Systems
formance requirements are likely to have application in process control problems In
addition to neural networks already incorporated in advanced controllers cells process information through biochemical signal transduction networks that may also
contain useful non-linear mechanisms A model of one such pathway has been developed and features have been identified which can be used to develop an improved
control system
The fundamental idea is to design two separate control laws one intermittently
used for cancelling infrequently changing but mostly predictable disturbances and
another for attenuating white disturbances The first controller learns the simple
characteristics of the predictable disturbance When the predictable disturbance is
learned it can be canceled with an open loop controller and no further learning
takes place However if it appears that the open loop controller is not cancelling the
disturbance further learning takes place until the disturbance is again successfully
cancelled The second controller is designed strictly for fast disturbance attenuation
Without the lag inherent in integration the controller can be made more aggressive
resulting in better performance The two controllers can be integrated by applying
the threshold and switching mechanisms identified in the signal transduction model

----------------------------------------------------------------

title: 379-feedback-synapse-to-cone-and-light-adaptation.pdf

FEEDBACK SYNAPSE TO CONE AND LIGHT ADAPTATION
Josef Skrzypek
Machine Perception Laboratory
UCLA Los Angeles California
INTERNET SKRZYPEK@CS.UCLA.EDU
Abstract
Light adaptation allows cone vIslOn to remain functional between
twilight and the brightest time of day even though at anyone time their
intensity-response characteristic is limited to log units of the stimulating light One mechanism underlying LA was localized in the outer segment of an isolated cone We found that by adding annular illhmination
an I-R characteristic of a cone can be shifted along the intensity domain
Neural network involving feedback synapse from horizontal cells to cones is
involved to be in register with ambient light level of the periphery An
equivalent electrical circuit with three different transmembrane channels
leakage photocurrent and feedback was used to model static behavior of a
cone SPICE simulation showed that interactions between feedback synapse
and the light sensitive conductance in the outer segment can shift the I-R
curves along the intensity domain provided that phototransduction mechanism is not saturated during maximally hyperpolarized light response
INTRODUCTION
Light response in cones
In the vertebrate retina cones respond to a small spot of light with sustained hyperpolarization which is graded with the stimulus over three log units of intensity Mechanisms underlying this I-R relation was suggested to result from statistical superposition of
invariant single-photon hyperpolarizing responses involvnig sodium conductance
changes that are gated by cyclic nuclcotides The shape of the response measured
in cones depends on the size of the stimulating spot of light presumably because of peripheral signals mediated by a negative feedback synapse from horizontal cells the
hyperpolarizing response to the spot illumination in the central portion of the cone receptive field is antagonized by light in the surrounding periphery Thus the cone
Skrzypek
membrane is influenced by two antagonistic effects feedback driven by peripheral illumination and the light sensitive conductance in the cone outer segment Although it
has been shown that key aspects of adaptation can be observed in isolated cones
the effects of peripheral illumination on adaptation as related to feedback input from horizontal cells have not been examined It was reported that under appropriate stimulus
conditions the resting membrane potential for a cone can be reached at two drastically
different intensities for a spot/annulus combinations
We present here experimental data and modeling results which suggests that results of
feedback from horizontal cells to cones resemble the effect of the neural component of
light adaptation in cones Specifically peripheral signals mediated via feedback synapse
reset the cone sensitivity by instantaneously shifting the I-R curves to a new intensity
domain The full range of light response potentials is preserved without noticeable
compression
RESULTS
Identification of cones
Preparation and the general experimental procedure as well as criteria for identification
of cones has been detailed in Several criteria were used to distinguish cones from
other cells in the OPL such as the depth of recording in the retina the sequence of penetrations concomitant with characteristic light responses spectral
response curves receptive field diameter the fastest time from dark potential to the peak of the light response 6)domain of I-R curves and staining with
Lucipher Yellow These values represent averages derived from all intracellular recordings in 37 cones 84 bipolar cells more than horizontal cells and more
than rods
Experimental procedure
After identifying a cone its I-R curve was recorded Then in a presence of center illumination diameter urn which elicited maximal hyperpolarization from a cone the
periphery of the receptive field was stimulated with an annulus of inner diameter
urn and the outer diameter urn The annular intensity was adjusted to elicit depolarization of the membrane back to the dark potential level Finally the center
intensity was increased again in a stepwise manner to antagonize the effect of peripheral
illumination and this new I-R curve was recorded
Peripheral illumination shifts the I-R curve in cones
Sustained illumination of a cone with a small spot of light evokes a hyperpolarizing
response which after transient peak gradually repolarizes to some steady level
When the periphery of the relina is illuminated with a ring of light in the presence of
center spot the antagonistic component of response can be recorded in a form of sustained depolarization It has been argued previously that in the tiger salamander cones
this type of response in cones is mediated via synaptic input from horizontal cells
Feedback Synapse to Cone and Light Adaptation
The significance of this result is that the resting membrane potential for this cone can be
reached at two drastically different intensities for a spot/annulus combinations The action of an annular illumination is a fast depolarization of the membrane the whole process is completed in a fraction of a second unlike the previous reports where the course
of light-adaptation lasted for seconds or even minutes
Response due to spot of light measured at the peak of hyperpolarization increased in
magnitude with increasing intensity over three log units fig The same data is plotted as open circles in fig Initially annulus presented during the central illumination
did not produce a noticeable response Its amplitude reached maximum when the center
spot intensity was increased to log units Further increase of center intensity resulted in
disappearance of the annulus elicited depolarization Feedback action is graded with annular intensity and it depends on the balance between amount of light falling on the
center and the surround of the cone receptive field The change in cone's membrane potential due to combined effects of central and annular illumination is plotted as filled circles in fig lb This new intensity-response curve is shifted along the intensity axis by
approximately two log units Both I-R curves span approximately three log units of intensity The I-R curve due to combined center and surround illumination can be described
by the function VNm where Vm is a peak hyperpolarization and is a
constant intensity generating half-maximal response This relationship was suggested to be an indication of the light adaptation The I-R curve plotted using peak
response values open circles fits a continuous line drawn according to equation This has been argued previously to indicate absence of light adaptation
There is little if any compression or change in gain after the shift of the cone operating
point to some new domain of intensity The results suggest that peripheral illumination
can shift the center-spot elicited I-R curve of the cone thus resetting the responsegenerating mechanism in cones
Simulation of a cone model
The results presented in the previous sections imply that maximal hyperpolarization for
the cone membrane is not limited by the saturation in the phototransduction process
alone It seems reasonable to assume that such a limit may be in part detennined by the
batteries of involved ions Furthennore it appears that shifting I-R curves along the intensity domain is not dependent solely on the light adaptation mechanism localized to the
outer segment of a cone To test these propositions we developed a simplified compartmental model of a cone and we exercised it using SPICE Vladimirescu
All interactions can be modeled using Kirchoffs current law membrane current is
cm(dv/dt)+l ionic The leakage current is lleak Gleak(Vm-EleaJ light sensitive current is
Ilight Glight*(Vm-Elight and the feedback current is lib Gfb*(Vm-Efb The left branch
represents ohmic leakage channels leak which are associated with a constant battery
Eleak 70 The middle branch represents the light sensitive conductance Glight in
series with ionic battery Elight Light adaptation effects could be incorporated here by making Glight time varying and dependent on internal concentration of Calcium ions In our preliminary studies we were only interested in examining whether the
shift of I-R is possible and if it would explain the disappearance of depolarizing FB reponse with hyperpolarization by the center light This can be done with passive measurements of membrane potential amplitude The right-most branch represents ionic channels
that are controlled by the feedback synapse With Efb 65 mV Gfb is a time and
voltage independent feedback conductance
Skrzypek
The input resistance of an isolated cone is taken to be near Mohm Mohm
Attwell Assuming specific membrane resistance of Ohm*cm*cm and
that a cone is microns long and has a micron diameter at the base we get the leakage
conductance G1eak In our studies we assume G1eak to be linear altghouth
there is evidence that cone membrane rectifies Skrzypek The Glight and Gfb are assumed to be equal and add up to l/(lGohm The Glight varies with light intensity in proportion of two to three log units of intensity for a tenfold change in conductance This relation was derived empirically by comparing intensity response data obtained from a
cone Vm=f(LogI to Vm=f(LogGli$hJ generated by the model The changes in Glb
have not been calibrated to changes In light intensity of the annulus However we assume that lb can not undergo variation larger that Glight
Figure shows the membrane potential changes generated by the model plotted as a
function of Rlighh at different settings of the feedback resistance Rib. With increasing
Rfb there is a parallel shift along the abscissa without any changes in the shape of the
curve Increase in Rlight corresponds to increase in light intensity and the increasing magnitude of the light response from Om Eligh0 all the way down to 65 mV The increase in Rfb is associated with increasing intensity of the annular illumination which
causes additional hyperpolarization of the horizontal cell and consequently a decrease in
feedback transmitter released from HC to cones Since we assume the Etb a
more negative level than the nonnal resting membrane potential a decrease in Gfb would
cause a depolarizing response in the cone This can be observed here as a shift of the
curve along the abscissa In our model a hundred fold change in feedback resistance
from O.OlGohm to IGohm resulted in shift of the response-intensity curve by approximately two log units along the abscissa The relationship between changes in Rfb and the
shift of the response-intensity curve is nonlinear and additional increases in Rfb from
1Gohm to lOOGohm results in decreasing shifts
Membrane current undergoes similar parallel shift with changes in feedback conductance However the photocurrent lligh0 and the feedback current show only saturation with increasing Glight not shown The limits of either light or Ifb currents are defined
by the batteries of the model Since these currents are associated with batteries of opposite polarities the difference between them at various settings of the feedback conductance Gfb determines the amount of shift for Ileak along the abscissa The compression in
shift of response intensity curves at smaller values of Glb results from smaller and
smaller current flowing through the feedback branch of the circuit Consequently a
smaller Gib changes are required to get response in the dark than in the light
The shifting of the response-intensity curves generated by our model is not due to light
adaptation as described by although it is possible that feedback effects could be involved in modulating light-sensitive channels Our model suggests that in order to generate additional light response after the membrane of a cone was fully hyperpolarized by
light it is insufficient to have a feedback effect alone that would depolarize the cone
membrane Light sensitive channels that were not previously closed must also be
available
Feedback Synapse to Cone and Light Adaptation
DISCUSSION
The results presented here suggest that synaptic feedback from horizontal cells to cones
could contribute to the process of light adaptation at the photoreceptor level A complete
explanation of the underlying mechanism requires further studies but the results seem to
suggest that depolarization of the cone membrane by a peripheral illumination resets the
response-generating process in the cone This result can be explained withing the framework of the current hypothesis of the light adaptation recently summarized by
It is conceivable that feedback transmitter released from horizontal cells in the dark
opens channels to ions with reversal potential near 65 mV Hence hyperpolarizing
cone membrane by increasing center spot intensity would reduce the depolarizing feedback response as cone nears the battery of involved ions Additional increase in annular
illumination further reduces the feedback transmitter and the associated feedback conductance thus pushing cone's membrane potential away from the feedback battery
Eventually at some values of the center intensity cone membrane is so close to 65 mV
that no change in feedback conductance can produce a depolarizing response
ACKNOWLEDGEMENTS
Special gratitude to Prof Werblin for providing a superb research environment and generous support during early part of this project We acknowledge partial support by NSF
grant ARCO-UCLA Grant UCLA-SEASNET Grant MICROHughes grant ONR grant ARO grant

----------------------------------------------------------------

title: 1849-kernel-based-reinforcement-learning-in-average-cost-problems-an-application-to-optimal-portfolio-choice.pdf

Kernel-Based Reinforcement Learning in
Average-Cost Problems An Application
to Optimal Portfolio Choice
Dirk Ormoneit
Department of Computer Science
Stanford University
Stanford CA
ormoneit@cs.stanford.edu
Peter Glynn
EESOR
Stanford University
Stanford CA
Abstract
Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of
temporal-difference learning to estimate the value function of a
Markov Decision Process A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable In this work we present a new kernel-based approach to
reinforcement learning which overcomes this difficulty and provably
converges to a unique solution By contrast to existing algorithms
our method can also be shown to be consistent in the sense that
its costs converge to the optimal costs asymptotically Our focus
is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem
Introduction
Temporal-difference learning has been applied successfully to many real-world
applications that can be formulated as discrete state Markov Decision Processes
MDPs with unknown transition probabilities If the state variables are continuous
or high-dimensional the TD learning rule is typically combined with some sort of
function approximator a linear combination of feature vectors or a neural
network which may well lead to numerical instabilities see for example
Specifically the algorithm may fail to converge under several circumstances
which in the authors opinion is one of the main obstacles to a more wide-spread
use of reinforcement learning in industrial applications As a remedy we
adopt a non-parametric perspective on reinforcement learning in this work and we
suggest a new algorithm that always converges to a unique solution in a finite
number of steps In detail we assign value function estimates to the states in a
sample trajectory and we update these estimates in an iterative procedure The
updates are based on local averaging using a so-called weighting kernel Besides
numerical stability a second crucial advantage of this algorithm is that additional
training data always improve the quality of the approximation and eventually lead
to optimal performance that is our algorithm is consistent in a statistical sense
To the authors best knowledge this is the first reinforcement learning algorithm
for which consistency has been demonstrated in a continuous space framework
Specifically the recently advocated direct policy search or perturbation methods
can by construction at most be optimal in a local sense SMSMOO VRKOOj
Relevant earlier work on local averaging in the context of reinforcement learning
includes Rus97j and While these papers pursue related ideas their approaches differ fundamentally from ours in the assumption that the transition probabilities of the MDP are known and can be used for learning By contrast kernelbased reinforcement learning only relies on sample trajectories of the MDP and it
is therefore much more widely applicable in practice While our method addresses
both discounted and average-cost problems we focus on average-costs here and
refer the reader interested in discounted-costs to OSOOj For brevity we also defer
technical details and proofs to an accompanying paper OGOOj Note that averagecost reinforcement learning has been discussed by several authors
The remainder of this work is organized as follows In Section be provide basic
definitions and we describe the kernel-based reinforcement learning algorithm Section focuses on the practical implementation of the algorithm and on theoretical
issues Sections and present our experimental results and conclusions
Kernel-Based Reinforcement Learning
Consider a MDP defined by a sequence of states taking values in IR a sequence
of actions at taking values in A and a family of transition kernels
B)la A characterizing the conditional probability of the event
given and at-l a The cost function represents an immediate
penalty for applying action a in state Strategies policies or controls are understood as mappings of the form J1. IRd A and we let PX,/A denote the probability
distribution governing the Markov chain starting from Xo associated with the
policy J1.. Several regularity conditions are listed in detail in OGOOj
Our goal is to identify policies that are optimal in that they minimize the long-run
average-cost TJ/A liIllT-t oo Ex,/A
c(Xt An optimal policy can
be characterized as a solution to the Average-Cost Optimality Equation ACOE
TJ
argmin{c(x
a
min{c(x
a
where TJ is the minimum average-cost and has an interpretation as the differential value of starting in as opposed to drawing a random starting position from
the stationary distribution under a denotes the conditional expectation operator Ex,a which is assumed to be unknown so that cannot
be solved explicitly Instead we simulate the MDP using a fixed proposal strategy jl in reinforcement learning to generate a sample trajectory as training data
Formally let Zm denote such an m-step sample trajectory and let
A am-llas and c(zs as)IO be the sequences
of actions and costs associated with S. Then our objective can be reformulated as
the approximation of fJ based on information in A and C. In detail we will
construct an approximate expectation operator based on the training data
and use this approximation in place of the true operator rain this work Formally substituting a for rain and gives the Approximate Avemge-Cost
Optimality Equation AACOE
i)m
hm(x
flm(x
argmjn
Note that ifthe solutions i)m and hm to are well-defined they can be interpreted
as statistical estimates of TJ and in equation However i)m and need not
exist unless a is defined appropriately We therefore employ local averaging in
this work to construct in a way that guarantees the existence of a unique
fixed point of For the derivation of the local averaging operator note that
the task of approximating Ex,a[h(Xdl can be interpreted alternatively
as a regression of the target variable h(Xd onto the input Xo So-called
kernel-smoothers address regression tasks of this sort by locally averaging the target
values in a small neighborhood of This gives the following approximation
m-l
km
s=o
In detail we employ the weighting function or weighting kernel km in to
determine the weights that are used for averaging in equation Here km,a(zs is
a multivariate Gaussian normalized so as to satisfy the constraints km
if as a km,a(zs if as a and km Intuitively
assesses the future differential cost of applying action a in state by looking at all
times in the training data where a has been applied previously in a state similar
to and by averaging the current differential value estimates at the outcomes of
these previous transitions Because the weights zs are related inversely
to the distance Ilzs xii transitions originating in the neighborhood of are most
influential in this averaging procedure A more statistical interpretation of would
suggest that ideally we could simply generate a large number of independent samples
from the conditional distribution Px a and estimate Ex using Monte-Carlo
approximation Practically speaking this approach is clearly infeasible because in
order to assess the value of the simulated successor states we would need to sample
recursively thereby incurring exponentially increasing computational complexity A
more realistic alternative is to estimate m,ah as a local average of the rewards
that were generated in previous transitions originating in the neighborhood of
where the membership of an observation in the neighborhood of is quantified
using km zs Here the regularization parameter determines the width of the
Gaussian kernel and thereby also the size of the neighborhood used for averaging
Depending on the application it may be advisable to choose either fixed or as a
location-dependent function of the training data
Self-Approximating Property
As we illustrated above kernel-based reinforcement learning formally amounts to
substituting the approximate expectation operator m,a for a and then applying
dynamic programming to derive solutions to the approximate optimality equation
In this section we outline a practical implementation of this approach and
we present some of our theoretical results In particular we consider the relative
value iteration algorithm for average-cost MDPs that is described for example in
This procedure iterates a variant of equation to generate a sequence of
value function estimates that eventually converge to a solution of
respectively An important practical problem in continuous state MDPs is that the
intermediate functions need to be represented explicitly on a computer This requires some form of function approximation which may be numerically undesirable
and computationally burdensome in practice In the case of kernel-based reinforcement learning the so-called self-approximating property allows for a much more
efficient implementation in vector format also Specifically because
our definition of m,ah in only depends on the values of at the states in
the AACOE can be solved in two steps
In other words we first determine the values of hm at the points in using
and then compute the values at new locations in a second step using Note
that is a finite equation system by contrast to By introducing the vectors
and matrices zi km Zj,Zi for
and the relative value iteration algorithm can thus be written
conveniently as for details see OGOO
ew I new
Hence we end up with an algorithm that is analogous to value iteration except that
we use the weighting matrix q>a in place ofthe usual transition probabilities and
and a are vectors of points in the training set as opposed to vectors of states
Intuitively assigns value estimates to the states in the sample trajectory and
updates these estimates in an iterative fashion Here the update of each state is
based on a local average over the costs and values of the samples in its neighborhood
Since q>a and we can further exploit the analogy between
and the usual value iteration in an artificial MDP with transition probabilities
q>a to prove the following theorem
Theorem The relative value iteration converges to a unique fixed point
For details the reader is referred to OSOO OGOO Note that Theorem illustrates
a rather unique property of kernel-based reinforcement learning by comparison to
alternative approaches In addition we can show that under suitable regularity
conditions kernel-based reinforcement learning is consistent in the following sense
Theorem The approximate optimal cost
TI in the sense that
xo ji 1Tim
A
Tfm converges to the true optimal cost
TI
Also the true cost of the approximate strategy
Pm
converges to the optimal cost
Hence Pm performs as well as fJ asymptotically and we can also predict the optimal cost TJ using From a practical standpoint Theorem asserts that the
performance of approximate dynamic programming can be improved by increasing
the amount of training data Note however that the computational complexity
of approximate dynamic programming depends on the sample size In detail
the complexity of a single application of is in a naive implementation
and O(mlog in a more elaborate nearest neighbor approach This complexity
issue prevents the use of very large data sets using the exact algorithm described
above As in the case of parametric reinforcement learning we can of course restrict
ourselves to a fixed amount of computational resources simply by discarding observations from the training data or by summarizing clusters of data using sufficient
statistics Note that the convergence property in Theorem remains unaffected
by such an approximation
Optimal Portfolio Choice
In this section we describe the practical application of kernel-based reinforcement
learning to an investment problem where an agent in a financial market decides
whether to buy or sell stocks depending on the market situation In the finance
and economics literature this task is known as optimal portfolio choice and has
created an enormous literature over the past decades Formally let St symbolize
the value of the stock at time and let the investor choose her portfolio at from the
set A corresponding to the relative amount of wealth invested
in stocks as opposed to an alternative riskless asset At time the stock price
changes from St to and the portfolio of the investor participates in the price
movement depending on her investment choice Formally if her wealth at time is
it becomes
at St at time To render this simulation
as realistic as possible our investor is assumed to be risk-averse in that her fear of
losses dominates her appreciation of gains of equal magnitude A standard way to
express these preferences formally is to aim at maximizing the expectation of a concave utility function ofthe final wealth WT. Using the choice log
the investor's utility can be written as U(WT
at Hence
utilities are additive over time and the objective of maximizing can be
stated in an average-cost framework where Ex,a log
a
We present results using simulated and real stock prices With regard to the simulated data we adopt the common assumption in finance literature that stock prices
are driven by an Ito process with stochastic mean-reverting volatility
dSt
fJStdt
dVt
ylv;StdBt
vt)dt pylv;dBt
Here Vt is the time-varying volatility and and are independent Brownian motions The parameters of the model are fJ fJ and We
simulated daily data for the period of 13 years using the usual Euler approximation
of these equations The resulting stock prices volatilities and returns are shown in
Figure Next we grouped the simulated time series into sets of training and
Figure The simulated time-series of stock prices left volatility middle and
daily returns right Tt log(St/St-d over a period of one year
test data such that the last years are used as test sets and the three years
preceding each test year are used as training data Table reports the training and
test performances on each of these experiments using kernel-based reinforcement
learning and a enchmark buy hold strategy Performance is measured using
Year
13
Reinforcement Learning
Training
Test
Buy Hold
Test
I Training
Table Investment erformance on the simulated data initial wealth Wa
the Sharpe-ratio which is a standard measure of risk-adjusted investment performance In detail the Sharpe-ratio is defined as SR log(WT/Wo)/iT where iT is
the standard deviation of log(Wt!Wt over time Note that large values indicate
good risk-adjusted performance in years of positive growth whereas negative values cannot readily interpreted We used the root of the volatility standardized
to zero mean and unit variance as input information and determined a suitable
choice for the bandwidth parameter experimentally Our results in Table
demonstrate that reinforcement learning dominates buy hold in eight out of ten
years on the training set and in all seven years with positive growth on the test set
Table shows the results of an experiment where we replaced the artificial time
series with eight years of daily German stock index data DAX index
We used the years as est data and the three years preceding each test
year for training As the model input we computed an approximation of the root volatility using a geometric average of historical returns Note that the training
performance of reinforcement learning always dominates the buy hold strategy
and the test results are also superior to the enchmark except in the year
Year
Reinforcement Learning
Training
Test
Buy
Training
Hold
Test
Table Investment performance on the DAX data
Conclusions
We presented a new kernel-based reinforcement learning method that overcomes
several important shortcomings of temporal-difference learning in continuous-state
domains In particular we demonstrated that the new approach always converges
to a unique approximation of the optimal policy and that the quality of this approximation improves with the amount of training data Also we described a financial
application where our method consistently outperformed a benchmark model in an
artificial and a real market scenario While the optimal portfolio choice problem is
relatively simple it provides an impressive proof of concept by demonstrating the
practical feasibility of our method Efficient implementations of local averaging for
large-scale problems have been discussed in the data mining community Our work
makes these methods applicable to reinforcement learning which should be valuable
to meet the real-time and dimensionality constraints of real-world problems
Acknowledgements The work of Dirk Ormoneit was partly supported by the Deutsche
Forschungsgemeinschaft Saunak Sen helped with valuable discussions and suggestions

----------------------------------------------------------------

title: 1221-multi-task-learning-for-stock-selection.pdf

Multi-Task Learning for Stock Selection
Joumana Ghosn
Dept Informatique
Recherche Operationnelle
Universite de Montreal
Montreal Qc
Yoshua Bengio
Dept Informatique
Recherche Operationnelle
Universite de Montreal
Montreal Qc
ghosn~iro.umontreal.ca
bengioy~iro.umontreal ca
Abstract
Artificial Neural Networks can be used to predict future returns
of stocks in order to take financial decisions Should one build a
separate network for each stock or share the same network for all
the stocks In this paper we also explore other alternatives in
which some layers are shared and others are not shared When
the prediction of future returns for different stocks are viewed as
different tasks sharing some parameters across stocks is a form
of multi-task learning In a series of experiments with Canadian
stocks we obtain yearly returns that are more than above
various benchmarks
Introd uction
Previous applications of ANNs to financial time-series suggest that several of these
prediction and decision-taking tasks present sufficient non-linearities to justify the
use of ANNs Refenes Moody Levin and Rehfuss These models can
incorporate various types of explanatory variables so-called technical variables depending on the past price sequence micro-economic stock-specific variables such
as measures of company profitability and macro-economic variables which give
information about the business cycle
One question addressed in this paper is whether the way to treat these different variables should be different for different stocks should one use the same network
for all the stocks or a different network for each stock To explore this question
also AT&T Labs Holmdel NJ
Multi-Task Learning/or Stock Selection
we performed a series of experiments in which different subsets of parameters are
shared across the different stock models When the prediction of future returns for
different stocks are viewed as different tasks which may nonetheless have something in common sharing some parameters across stocks is a form of multi-task
learning
These experiments were performed on years of data concerning 35 large capitalization companies of the Toronto Stock Exchange Following the results of
previous experiments Bengio the networks were not trained to predict the
future return of stocks but instead to directly optimize a financial criterion This
has been found to yield returns that are significantly superior to training the ANNs
to minimize the mean squared prediction error
In section we review previous work on multi-task In section we describe the
financial task that we have considered and the experimental setup In section
we present the results of these experiments In section we propose an extension
of this work in which the models are re-parameterized so as to automatically learn
what must be shared and what need not be shared
Parameter Sharing and Multi-Task Learning
Most research on ANNs has been concerned with tabula rasa learning The learner
is given a set of examples XN YN chosen according to some
unknown probability distribution Each pair represents an input and a
desired value One defines a training criterion to be minimized in function
of the desired outputs and of the outputs of the learner The function is
parameterized by the parameters of the network and belongs to a set of hypotheses
that is the set of all functions that can be realized for different values of the
parameters The part of generalization error due to variance due to the specific
choice of training examples can be controlled by making strong assumptions on
the model by choosing a small hypotheses space But using an incorrect
model also worsens performance
Over the last few years methods for automatically choosing based on similar
tasks have been studied They consider that a learner is embedded in a world
where it faces many related tasks and that the knowledge acquired when learning a task can be used to learn better and/or faster a new task Some methods
consider that the related tasks are not always all available at the same time Pratt
Silver and Mercer knowledge acquired when learning a previous task
is transferred to a new task Instead all tasks may be learned in parallel Baxter
Caruana and this is the approach followed here Our objective is not
to use multi-task learning to improve the speed of learning the training data Pratt
Silver and Mercer but instead to improve generalization performance
For example in Baxter several neural networks one for each task are
trained simultaneouly The networks share their first hidden layers while all the
remaining layers are specific to each network The shared layers use the knowledge
provided from the training examples of all the tasks to build an internal representation suitable for all these tasks The remaining layers of each network use the
internal representation to learn a specific task
In the multitask learning method used by Caruana Caruana all the hidden
J. Ghosn and Y. Bengio
layers are shared They serve as mutual sources of inductive bias It was also
suggested that besides the relevant tasks that are used for learning it may be
possible to use other related tasks that we do not want to learn but that may
help to further bias the learner Caruana Baluja and Mitchell Intrator and
Edelman
In the family discovery method Omohundro a parameterized family of
models is built Several learners are trained separately on different but related
tasks and their parameters are used to construct a manifold of parameters When
a new task has to be learned the parameters are chosen so as to maximize the data
likelihood on the one hand and to maximize a family prior on the other hand
which restricts the chosen parameters to lie on the manifold
In all these methods the values of some or all the parameters are constrained
Such models restrict the size of the hypotheses space sufficiently to ensure good
generalization performance from a small number of examples
Application to Stock Selection
We apply the ideas of multi-task learning to a problem of stock selection and portfolio management We consider a universe of 36 assets including 35 risky assets
and one risk-free asset The risky assets are 35 Canadian large-capitalization stocks
from the Toronto Stock Exchange The risk-free asset is represented by 90-days
Canadian treasury bills The data is monthly and spans years from February
to January months Each month one can buy or sell some of these
assets in such a way as to distribute the current worth between these assets We do
not allow borrowing or short selling so the weights of the resulting portfolio are all
non-negative and they sum to
We have selected explanatory variables of which represent macro-economic
variables which are known to influence the business cycle and of which are microeconomic variables representing the profitability of the company and previous price
changes of the stock The macro-economic variables were derived from yields of
long-term bonds and from the Consumer Price Index The micro-economic variables
were derived from the series of dividend yields and from the series of ratios of stock
price to book value of the company Spline extrapolation not interpolation was
used to obtain monthly data from the quarterly or annual company statements or
macro-economic variables For these variables we used the dates at which their
value was made public not the dates to which they theoretically refer
To take into account the non-stationarity of the financial and economic time-series
and estimate performance over a variety of economic situations multiple training
experiments were performed on different training windows each time testing on
the following months For each architecture such trainings took place with
training sets of size and years respectively Furthermore multiple such
experiments with different initial weights were performed to verify that we did not
obtain lucky results due to particular initial weights The concatenated test
periods make an overall 5-year test period from February to January
The training algorithm is described in Bengio and is based on the optimization of the neural network parameters with respect to a financial criterion here
maximizing the overall profit The outputs of the neural network feed a trading
Multi-Task Learning/or Stock Selection
module The trading module has as input at each time step the output of the network as well as the weights giving the current distribution of worth between the
assets These weights depend on the previous portfolio weights and on the relative
change in value of each asset due to different price changes The outputs of the
trading module are the current portfolio weights for each of the assets Based on
the difference between these desired weights and the current distribution of worth
transactions are performed Transaction costs of of the absolute value of each
buy or sell transaction are taken into account Because of transaction costs the actions of the trading module at time influence the profitability of its future actions
The financial criterion depends in a non-additive way on the performance of the
network over the whole sequence To obtain gradients of this criterion with respect
to the network output we have to backpropagate gradients backward through time
through the trading module which computes a differentiable function of its inputs
Therefore a gradient step is performed only after presenting the whole training
sequence order of course In Bengio we have found this procedure
to yield significantly larger profits around better annual return at comparable risks in comparison to training the neural network to predict expected future
returns with the mean squared error criterion In the experiments the ANN was
trained for epochs
Experimental Results
Four sets of experiments with different types of parameter sharing were performed
with two different architectures for the neural network inputs a hidden
layer of units and output inputs units in the first hidden layer
units in the second hidden layer and output The output represents the belief
that the value of the stock is going to increase the expected future return over
three months when training with the MSE criterion
Four types of parameter sharing between the different models for each stock are
compared in these experiments sharing everything the same parameters for all the
stocks sharing only the parameters weights and biases of the first hidden layers
sharing only the output layer parameters and not sharing anything independent
models for each stock
The main results for the test period using the architecture are summarized
in Table and graphically depicted in Figure with the worth curves for the four
types of sharing The results for the test period using the architecture are
summarized in Table The ANNs were compared to two benchmarks a buy-andhold benchmark with uniform initial weights over all 35 stocks and the
Index Since the buy-and-hold benchmark performed better yearly return
than the Index yearly return during the test period
Tables and give comparisons with the buy-and-hold benchmark Variations
of average yearly return on the test period due to different initial weights were
computed by performing each of the experiments 18 times with different random
seeds The resulting standard deviations are less than when no parameters or
all the parameters are shared less than when the parameters of the first hidden
layers are shared and less than when the output layer is shared
The values of beta and alpha are computed by fitting the monthly return of the
portfolio to the return of the benchmark both adjusted for the risk-free return
J. Ghosn and Y. Bengio
Table Comparative results for the architecture four types
compared with the buy-and-hold benchmark text
buy share
share
share
hold
hidden output
all
Average yearly return
Standard deviation monthly
Beta
Alpha yearly
NA
t-statistic for alpha
Reward to variability
Excess return above benchmark
Maximum drawdown
of sharing are
no
sharing
Table Comparative results for the architecture three types of sharing
are compared with the buy-and-hold benchmark text
buy share share first share all
no
hold
all
hidden
hidden
sharing
Average yearly return
Standard deviation monthly
Beta
Alpha yearly
NA
t-statistic for alpha
Reward to variability
Excess return above benchmark
Maximum drawdown
ri interest rates according to the linear regression E(rp rd alpha beta(rMr Beta is simply the ratio of the covariance between the portfolio return and the
market return with the variance of the market According to the Capital Asset
Pricing Model Sharpe beta gives a measure of systematic risk as it
relates to the risk of the market whereas the variance of the return gives a measure
of total risk The value of alpha in the tables is annualized as a compound return
it represents a measure of excess return over the market benchmark adjusted for
market risk beta The hypothesis that alpha is clearly rejected in all cases
with t-statistics above and corresponding p-values very close to The reward
to variability Sharpe ratio as defined in Sharpe is another riskadjusted measure of performance
where O"p is the standard deviation of
Up
the portfolio return monthly returns were used here The excess return above
benchmark is the simple difference not risk-adjusted between the return of the
portfolio and that of the benchmark The maximum drawdown is another measure
of risk and it can be defined in terms of the worth curve worth[t is the ratio
between the value of the portfolio at time and its value at time The maximum
drawdown is then defined as max worth[s])-worth[t
max.:St worth[s
Three conclusions clearly come out of the tables and figure The main improvement is obtained by allowing some parameters to be not shared for the
a~chitecture although the best results are obtained with a shared hidden and a free
output layer there are no significant differences between the different types of partial
sharing or no sharing at all Sharing some parameters yielded more consistent
results across architectures than when not sharing at all The performance
obtained in this way is very much better than that obtained by the benchmarks
buy-and-hold or the yearly return is more than above the best
benchmark while the risks are comparable as measured by standard deviation of
Multi-Task Learning/or Stock Selection
Share WO
No Share
Share WI
Share A1l
MSE
Buy Hold
88
02
91
93 02
Figure Evolution of total worth in the 5-year test period for the
architecture and different types of sharing From top to bottom sharing
the hidden layer no sharing across stocks sharing the output layer sharing everything sharing everything with MSE training Buy and Hold benchmark
benchmark
return or by maximum drawdown
Future Work
We will extend the results presented here in two directions Firstly given the
impressive results obtained with the described approach we would like to repeat
the experiment on different data sets for different markets Secondly we would like
to generalize the type of multi-task learning by allowing for more freedom in the
way the different tasks influence each other
Following Omohundro the basic idea is to re-parameterize the parameters
Rnl of the ith model for all models in the following way where
Pi Rn3 and nl n2 n3 For example if is an affine
function this forces the parameters of each the different networks to lie on the
same linear manifold The position of a point on the manifold is given by a 2dimensional vector Pi and the manifold itself is specified by the n3 parameters of
The expected advantage of this approach with respect to the one used in this paper
is that different models corresponding to different stocks may share more
or less depending on how far their Pi is from the Pj'S for other models One does
not have to specify which parameters are free and which are shared but one has to
specify how many are really free per model and the shape of the manifold
Conclusion
The results presented of this paper show an interesting application of the ideas of
multi-task learning to stock selection In this paper we have addressed the question of whether ANNs trained for stock selection or portfolio management should
be different for each stock or shared across all the stocks We have found significantly better results when some or sometimes all of the parameters of the stock
models are free not shared Since a parcimonuous model is always preferable we
conclude that partially sharing the parameters is even preferable since it does not
J. Ghosn and Y. Bengio
yield a deterioration in performance and it yields more consistent results Another
interesting conclusion of this paper is that very large returns can be obtained at
risks comparable to the market using a combination of partial parameter sharing
and training with respect to a financial training criterion with a small number
of explanatory input features that include technical micro-economic and macroeconomic information

----------------------------------------------------------------

title: 1551-reinforcement-learning-for-trading.pdf

Reinforcement Learning for Trading
John Moody and Matthew Saffell
Oregon Graduate Institute CSE Dept
P.O Box Portland OR
moody saffell @cse.ogi.edu
Abstract
We propose to train trading systems by optimizing financial objective functions via reinforcement learning The performance functions that we consider are profit or wealth the Sharpe ratio and
our recently proposed differential Sharpe ratio for online learning In Moody Wu we presented empirical results that
demonstrate the advantages of reinforcement learning relative to
supervised learning Here we extend our previous work to compare Q-Learning to our Recurrent Reinforcement Learning RRL
algorithm We provide new simulation results that demonstrate
the presence of predictability in the monthly S&P Stock Index
for the year period through as well as a sensitivity
analysis that provides economic insight into the trader's structure
Introduction Reinforcement Learning for Thading
The investor's or trader's ultimate goal is to optimize some relevant measure of
trading system performance such as profit economic utility or risk-adjusted return In this paper we propose to use recurrent reinforcement learning to directly
optimize such trading system performance functions and we compare two different reinforcement learning methods The first Recurrent Reinforcement Learning
uses immediate rewards to train the trading systems while the second Q-Learning
Watkins approximates discounted future rewards These methodologies can
be applied to optimizing systems designed to trade a single security or to trade portfolios In addition we propose a novel value function for risk-adjusted return that
enables learning to be done online the differential Sharpe ratio
Trading system profits depend upon sequences of interdependent decisions and are
thus path-dependent Optimal trading decisions when the effects of transactions
costs market impact and taxes are included require knowledge of the current system
state In Moody Wu Liao Saffell we demonstrate that reinforcement
learning provides a more elegant and effective means for training trading systems
when transaction costs are included than do more standard supervised approaches
The authors are also with Nonlinear Prediction Systems
J. Moody and Saffell
Though much theoretical progress has been made in recent years in the area of reinforcement learning there have been relatively few successful practical applications
of the techniques Notable examples include Neuro-gammon Tesauro the
asset trader of Neuneier an elevator scheduler Crites Barto and a
space-shuttle payload scheduler Zhang Dietterich
In this paper we present results for reinforcement learning trading systems that
outperform the S&P Stock Index over a 25-year test period thus demonstrating
the presence of predictable structure in US stock prices The reinforcement learning
algorithms compared here include our new recurrent reinforcement learning RRL
method Moody Wu Moody and Q-Learning Watkins
Trading Systems and Financial Performance Functions
Structure Profit and Wealth for Trading Systems
We consider performance functions for systems that trade a single security with
price series Zt. The trader is assumed to take only long neutral or short positions
I of constant magnitude The constant magnitude assumption can
be easily relaxed to enable better risk control The position Ft is established or
maintained at the end of each time interval and is re-assessed at the end of
period A trade is thus possible at the end of each time period although
nonzero trading costs will discourage excessive trading A trading system return
is realized at the end of the time interval and includes the profit or loss
resulting from the position held during that interval and any transaction cost
incurred at time due to a difference in the positions Ft and Ft.
In order to properly incorporate the effects of transactions costs market impact and
taxes in a trader's decision making the trader must have internal state information
and must therefore be recurrent An example of a single asset trading system
that takes into account transactions costs and market impact has following decision
function Ft Ft-l It with It Yt where
denotes the learned system parameters at time and It denotes the information
set at time which includes present and past values of the price series Zt and an
arbitrary number of other external variables denoted Yt.
Trading systems can be optimized by maximizing performance functions such
as profit wealth utility functions of wealth or performance ratios like the Sharpe
ratio The simplest and most natural performance function for a risk-insensitive
trader is profit The transactions cost rate is denoted
Additive profits are appropriate to consider if each trade is for a fixed number
of shares or contracts of security Zt. This is often the case for example when
trading small futures accounts or when trading standard US FX contracts in dollardenominated foreign currencies With the definitions rt Zt and
for the price returns of a risky traded asset and a risk-free asset like TBills respectively the additive profit accumulated over time periods with trading
position size Jl is then defined as
PT
See
t=l
t=l
LRt Jl Ft
Moody for a detailed discussion of multiple asset portfolios
Reinforcement Learning for Trading
with Po
and typically FT
Fa Equation holds for continuous quantities also The wealth is defined as WT
Wo PT.
Multiplicative profits are appropriate when a fixed fraction of accumulated
wealth is invested in each long or short trade Here rt Zt-l I
and If no short sales are allowed and the leverage factor is set
fixed at the wealth at time Tis
WT Wo
I Rd Wo I Ft_t}r Ft-1rt
Ft
The Differential Sharpe Ratio for On-line Learning
Rather than maximizing profits most modern fund managers attempt to maximize
risk-adjusted return as advocated by Modern Portfolio Theory The Sharpe ratio is
the most widely-used measure of risk-adjusted return Sharpe Denoting as
before the trading system returns for period including transactions costs as
the Sharpe ratio is defined to be
Average(Re
Standard Deviation(Rt
where the average and standard deviation are estimated for periods T}.
Proper on-line learning requires that we compute the influence on the Sharpe ratio
of the return at time To accomplish this we have derived a new objective function called the differential Sharpe ratio for on-line optimization of trading system
performance Moody It is obtained by considering exponential moving
averages of the returns and standard deviation of returns in and expanding to
first order in the decay rate St Noting that only the
first order term in this expansion depends upon the return at time we define
the differential Sharpe ratio as
where the quantities At and are exponential moving estimates of the first and
second moments of
A Bt Bt TJ(R
A
A
Bt-d
Treating A and as numerical constants note that in the update equations
controls the magnitude of the influence of the return on the Sharpe ratio St
Hence the differential Sharpe ratio represents the influence of the trading return
realized at time on St.
Reinforcement Learning for Trading Systems
The goal in using reinforcement learning to adjust the parameters of a system is
to maximize the expected payoff or reward that is generated due to the actions
of the system This is accomplished through trial and error exploration of the
environment The system receives a reinforcement signal from its environment
J. Moody and M. Saffell
reward that provides information on whether its actions are good or bad The
performance function at time can be expressed as a function of the sequence of
trading returns UT
RT).
Given a trading system model the goal is to adjust the parameters in
order to maximize UT. This maximization for a complete sequence of trades
can be done off-line using dynamic programming or batch versions of recurrent
reinforcement learning algorithms Here we do the optimization on-line using a
reinforcement learning technique This reinforcement learning algorithm is based
on stochastic gradient ascent The gradient of UT with respect to the parameters
of the system after a sequence of trades is
dUT dRt dFt
dR dFt
dR
dFt
A simple on-line stochastic optimization can be obtained by considering only the
term in that depends on the most recently realized return during a forward
pass through the data
dU_t dF_t
dRt dFt
dFt
The parameters are then updated on-line using Because of the
recurrent structure of the problem necessary when transaction costs are included
we use a reinforcement learning algorithm based on real-time recurrent learning
Williams Zipser This approach which we call recurrent reinforcement
learning is described in Moody Wu Moody along with
extensive simulation results
Empirical Results S&P
I
TBill Asset Allocation
A long/short trading system is trained on monthly S&P stock index and 3month TBill data to maximize the differential Sharpe ratio The S&P target
series is the total return index computed by reinvesting dividends The 84 input
series used in the trading systems include both financial and macroeconomic data
All data are obtained from Citibase and the macroeconomic series are lagged by
one month to reflect reporting delays
A total of 45 years of monthly data are used from January through December
The first years of data are used only for the initial training of the system
The test period is the year period from January through December
The experimental results for the year test period are true ex ante simulated
trading results
For each year during through the system is trained on a moving window
of the previous years of data For the system is initialized with random
parameters For the 24 subsequent years the previously learned parameters are
used to initialize the training In this way the system is able to adapt to changing
market and economic conditions Within the moving training window the RRL
systems use the first years for stochastic optimization of system parameters and
the subsequent years for validating early stopping of training The networks
are linear and are regularized using quadratic weight decay during training with a
Reinforcement Learningfor Trading
regularization parameter of The Qtrader systems use a bootstrap sample
of the year training window for training and the final years of the training
window are used for validating early stopping of training The networks are twolayer feedforward networks with tanh units in the hidden layer
Experimental Results
The left panel in Figure shows box plots summarizing the test performance for
the full year test period of the trading systems with various realizations of the
initial system parameters over trials for the RRL system and trials for
the Qtrader system The transaction cost is set at Profits are reinvested
during trading and multiplicative profits are used when calculating the wealth The
notches in the box plots indicate robust estimates of the confidence intervals
on the hypothesis that the median is equal to the performance of the buy and hold
strategy The horizontal lines show the performance of the RRL voting Qtrader
voting and buy and hold strategies for the same test period The annualized monthly
Sharpe ratios of the buy and hold strategy the Qtrader voting strategy and the
RRL voting strategy are and respectively The Sharpe ratios
calculated here are for the returns in excess of the 3-month treasury bill rate
The right panel of Figure shows results for following the strategy of taking positions based on a majority vote of the ensembles of trading systems compared with
the buy and hold strategy We can see that the trading systems go short the S&P
during critical periods such as the oil price shock of the tight money
periods of the early the market correction of and the crash This
ability to take advantage of high treasury bill rates or to avoid periods of substantial
stock market loss is the major factor in the long term success of these trading models One exception is that the RRL trading system remains long during the
stock market correction associated with the Persian Gulf war though the Qtrader
system does identify the correction On the whole though the Qtrader system
trades much more frequently than the RRL system and in the end does not
perform as well on this data set
From these results we find that both trading systems outperform the buy and hold
strategy as measured by both accumulated wealth and Sharpe ratio These differences are statistically significant and support the proposition that there is predictability in the U.S. stock and treasury bill markets during the year period
through A more detailed presentation of the RRL results appears in
Moody
Gaining Economic Insight Through Sensitivity Analysis
A sensitivity analysis of the RRL systems was performed in an attempt to determine on which economic factors the traders are basing their decisions Figure
shows the absolute normalized sensitivities for of the more salient input series as
a function of time averaged over the members of the RRL committee The
sensitivity of input is defined as
IdXi I
IdXj I
Si dF max dF
where is the unthresholded trading output and
denotes input
2Ten trials were done for the Qtrader system due to the amount of computation
required in training the systems
J. Moody and M. Saffell
F,nal Eqully OIrador VI RRl
70
vcMI
so
I
Ml
I
RRL
Figure Test results for ensembles of simulations using the S&P stock index and 3-month Treasury Bill data over the time period The solid
curves correspond to the RRL voting system performance dashed curves to the
Qtrader voting system and the dashed and dotted curves indicate the buy and
hold performance The boxplots in show the performance for the ensembles
of RRL and Qtrader trading systems The horizontal lines indicate the performance of the voting systems and the buy and hold strategy Both systems
significantly outperform the buy and hold strategy shows the equity curves
associated with the voting systems and the buy and hold strategy as well as the
voting trading signals produced by the systems In both cases the traders avoid
the dramatic losses that the buy and hold strategy incurred during and
The time-varying sensitivities in Figure emphasize the nonstationarity of economic
relationships For example the yield curve slope which measures inflation expectations is found to be a very important factor in the while trends in long term
interest rates measured by the month difference in the AAA bond yield becomes
more important in the and trends in short term interest rates measured by
the month difference in the treasury bill yield dominate in the early
Conclusions and Extensions
In this paper we have trained trading systems via reinforcement learning to optimize
financial objective functions including our differential Sharpe ratio for online learning We have also provided results that demonstrate the presence of predictability
in the monthly S&P Stock Index for the year period through
We have previously shown with extensive simulation results Moody Wu
Moody that the RRL trading system significantly outperforms
systems trained using supervised methods for traders of both single securities and
portfolios The superiority of reinforcement learning over supervised learning is
most striking when state-dependent transaction costs are taken into account Here
we present results for asset allocation systems trained using two different reinforcement learning algorithms on a real economic dataset We find that the Qtrader
system does not perform as well as the RRL system on the S&P TBill asset
allocation problem possibly due to its more frequent trading This effect deserves
further exploration In general we find that Q-Iearning can suffer from the curse of
dimensionality and is more difficult to use than our RRL approach
Finally we apply sensitivity analysis to the trading systems and find that certain
interest rate variables have an influential role in making asset allocation decisions
Reinforcement Learningfor Trading
S",sltivity Analysis on RRL Commill
I
I
I
I
I
I
jos
I
I
I
03
I
VI.1d Curv Slop
Month Dill In AM Bond yield
Month Dill In TBIU Vieid
Figure Sensitivity traces for three of the inputs to the RRL trading system
averaged over the ensemble of traders The nonstationary relationships typical
among economic variables is evident from the time-varying sensitivities
We also find that these influences exhibit nonstationarity over time
Acknowledgements
We gratefully acknowledge support for this work from Nonlinear Prediction Systems and
from DARPA under contract and AASERT grant

----------------------------------------------------------------

