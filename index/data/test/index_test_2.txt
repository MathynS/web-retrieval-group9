query sentence: image processing algorithms
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 2409-a-mixed-signal-vlsi-for-real-time-generation-of-edge-based-image-vectors.pdf

A Mixed-Signal VLSI for Real-Time
Generation of Edge-Based Image Vectors
Masakazu Yagi Hideo Yamasaki and Tadashi Shibata
Department of Electronic Engineering
Department of Frontier Informatics
The University of Tokyo
Hongo Bunkyo-ku Tokyo Japan
mgoat@dent.osaka-u.ac.jp hideo@if.t.u-tokyo.ac.jp shibata@ee.t.u-tokyo.ac.jp
Abstract
A mixed-signal image filtering VLSI has been developed aiming at
real-time generation of edge-based image vectors for robust image
recognition A four-stage asynchronous median detection architecture based on analog digital mixed-signal circuits has been introduced to determine the threshold value of edge detection the key
processing parameter in vector generation As a result a fully
seamless pipeline processing from threshold detection to edge feature map generation has been established A prototype chip was
designed in a double-polysilicon three-metal-layer CMOS
technology and the concept was verified by the fabricated chip The
chip generates a 64-dimension feature vector from a 64x64-pixel
gray scale image every This is about times faster than the
software computation making a real-time image recognition system
feasible
In tro du ti
The development of human-like image recognition systems is a key issue in information technology However a number of algorithms developed for robust image
recognition so far are mostly implemented as software systems running on
general-purpose computers Since the algorithms are generally complex and include a
lot of floating point operations they are computationally too expensive to build
real-time systems Development of hardware-friendly algorithms and their direct
VLSI implementation would be a promising solution for real-time response systems
Being inspired by the biological principle that edge information is firstly detected in
the visual cortex we have developed an edge-based image representation algorithm
compatible to hardware processing In this algorithm multiple-direction edges extracted from an original gray scale image is utilized to form a feature vector Since the
spatial distribution of principal edges is represented by a vector it was named Projected Principal-Edge Distribution PPED or formerly called Principal Axis
Projection PAP The algorithm is explained later Since the PPED vectors
very well represent the human perception of similarity among images robust image
recognition systems have been developed using PPED vectors in conjunction with the
analog soft pattern classifier the digital VQ Vector Quantization processor
and support vector machines
The robust nature of PPED representation is demonstrated in where the system
was applied to cephalometric landmark identification identifying specific anatomical
landmarks on medical radiographs as an example one of the most important clinical
practices of expert dentists in orthodontics Typical X-ray images to be experienced by apprentice doctors were converted to PPED vectors and utilized as
templates for vector matching The system performance has been proven for head
film samples regarding the fundamental 26 landmarks Important to note is the
successful detection of the landmark on the soft tissue boundary the tip of the lower
lip shown in Landmarks on soft tissues are very difficult to detect as
compared to landmarks on hard tissues solid bones because only faint images are
captured on radiographs The successful detection is due to the median algorithm that
determines the threshold value for edge detection
Sella
Nasion
Orbitale
By our system
By expert dentists
Landmark on soft tissue
Image recognition using PPED vectors cephalometric landmark identification successful landmark detection on soft tissue
We have adopted the median value of spatial variance of luminance within the filtering kernel pixels which allows us to extract all essential features in a delicate
gray scale image However the problem is the high computational cost in determining
the median value It takes about sec to generate one PPED vector from a
64x64-pixel image standard image size for recognition in our system on a SUN
workstation making real time processing unrealistic About of the computation
time is for edge detection from an input image in which most of the time is spent for
median detection
Then the purpose of this work is to develop a new architecture median-filter VLSI
subsystem for real-time PPED-vector generation Special attention has been paid to
realize a fully seamless pipeline processing from threshold detection to edge feature
map generation by employing the four-stage asynchronous median detection architecture
je pa dg Dis tribution PPED
Projected Principal Edge Distribution PPED algorithm is briefly explained
using A 5x5-pixel block taken from a 64x64-pixel target image is subjected
to edge detection filtering in four principal directions horizontal vertical and
45-degree directions In the figure horizontal edge filtering is shown as an example
The filtering kernels used for edge detection are given in In order to determine the threshold value for edge detection all the absolute-value differences
between two neighboring pixels are calculated in both vertical and horizontal directions and the median value is taken as the threshold By scanning the 5x5-pixel filtering kernels in the target image four edge-flag maps are generated which are
called feature maps In the horizontal feature map for example edge flags in every
four rows are accumulated and spatial distribution of edge flags are represented by a
histogram having elements Similar procedures are applied to other three directions
to form respective histograms each having elements Finally a 64-dimension
vector is formed by series-connecting the four histograms in the order of horizontal
45-degree vertical and 45-degree
Feature Map
Horizontal
Horizontal
Threshold
Median
Scan
elements
Edge Detection
Edge Filter
PPED Vector
Horizontal Section
Horizontal
45-degree
Threshold Detection
Absolute value
difference between
neiboring pels
Vertical
45-degree
PPED algorithm and filtering kernels for edge detection
Sy stem Orga ni za ti
The system organization of the feature map generation VLSI is illustrated in
The system receives one column of data pixels at each clock and stores the
data in the last column of the image buffer The image buffer shifts all the stored
data to the right at every clock Before the edge filtering circuit EFC starts detecting
four direction edges with respect to the center pixel in the block the threshold
value calculated from all the pixel data in the block must be ready in time for the
processing In order to keep the coherence of the threshold detection and the edge
filtering processing the two last-in data locating at column and are given to median filter circuit MFC in advance via absolute value circuit AVC calculates
all luminance differences between two neighboring pixels in columns and
In this manner a fully seamless pipeline processing from threshold detection to edge
feature map generation has been established The key requirement here is that MFC
must determine the median value of the luminance difference data from the
5x5-pixel block fast enough to carry out the seamless pipeline processing For this
purpose a four-stage asynchronous median detection architecture has been developed
which is explained in the following
Edge Filtering Circuit EFC
Edge flags
45
Image buffer
pixels
One column
Absolute Value
Circuit AVC
Threshold
value
Median Filter
Circuit MFC
45
Feature maps
System organization of feature map generation VLSI
The well-known binary search algorithm was adopted for fast execution of median
detection The median search processing for five data is illustrated in for the
purpose of explanation In the beginning majority voting is carried out for the MSB?s
of all data Namely the number of is compared with the number of and the
majority group wins The majority group flag in this example is stored as the
MSB of the median value In addition the loser group is withdrawn in the following
voting by changing all remaining bits to the loser MSB in this example By
repeating the processing the median value is finally stored in the median value register
Elapse of time
Median Register
MVC0
MVC1
MVC2
MVC3
MVC0
MVC1
MVC2
MVC3
MVC0
MVC1
MVC2
MVC3
MVC0
MVC1
MVC2
MVC3
Majority Flag
Majority Voting Circuit MVC
Hardware algorithm for median detection by binary search
How the median value is detected from all the data horizontal luminance
difference data and vertical luminance difference data is illustrated in All
the data are stored in the array of median detection units At each clock the
array receives four vertical luminance difference data and five horizontal luminance
difference data calculated from the data in column and in The entire data are
shifted downward at each clock The median search is carried out for the upper four
bits and the lower four bits separately in order to enhance the throughput by pipelining
For this purpose the chip is equipped with eight majority voting circuits MVC
The upper four bits from all the data are processed by MVC in a single clock cycle
to yield the median value In the next clock cycle the loser information is transferred
to the lower four bits within each MDU and carry out the median search for
the lower four bits from all the data in the array
Vertical Luminance Difference
AVC AVC AVC AVC
Horizontal Luminance Difference
AVC AVC AVC AVC AVC
Shift
Shift
Median Detection Unit MDU
Units
Lower 4bit
Upper 4bit
MVC0
MVC2
MVC1
MVC3
MVC4
MVC5
MVC6
MVC7
MVCs for upper 4bit
MVCs for lower 4bit
Median detection architecture for all luminance difference data
The majority voting circuit MVC is shown in Output connected CMOS inverters are employed as preamplifiers for majority detection which was first proposed
in Ref. In the present implementation however two preamps receiving input
data and inverted input data are connected to a 2-stage differential amplifier Although this doubles the area penalty the instability in the threshold for majority
detection due to process and temperature variations has been remarkably improved as
compared to the single inverter thresholding in Ref. The MVC in has 41
input terminals although bits of data are inputted to the circuit at one time Bit
is always given to the terminal to yield as the majority when there is a tie in
the majority voting
PREAMP
IN0
PREAMP
IN0
OUT
W/L
ENBL
W/L
W/L
IN1
IN1
W/L
ENBL
W/L
W/L
Majority voting circuit
The edge filtering circuit EFC in is composed as a four-stage pipeline of
regular CMOS digital logic In the first two stages four-direction edge gradients are
computed and in the succeeding two stages the detection of the largest gradient and
the thresholding is carried out to generate four edge flags
a es
The feature map generation VLSI was fabricated in a double-poly
three-metal-layer CMOS technology A photomicrograph of the proof-of-concept
chip is shown in The measured waveforms of the MVC at operating frequencies of 10MHz and 90MHz are demonstrated in The input condition is in the
worst case Namely bits and bits were fed to the inputs The observed
computation time is about nsec which is larger than the simulation result of
nsec This was caused by the capacitance loading due to the probing of the test circuit
In the real circuit without external probing we confirmed the average computation
time of nsec
Edge-detection
Filtering Circuit
Processing Technology CMOS 2-Poly 3-Metal
Median Filter Control Unit
Chip Size
MVC
Majority Voting Circuit X8
Supply Voltage
Operation Frequengy 50MHz
Vector
Generator
Photomicrograph and specification of the fabricated proof-of-concept chip
1V/div 5ns/div
MVC_Output
1V/div 8ns/div
MVC_OUT
IN
IN
Majority Voting operation
Majority Voting operation
Measured waveforms of majority voting circuit MVC at operation frequencies of 10MHz and 90 MHz for the worst-case input data
The feature maps generated by the chip at the operation frequency of MHz are
demonstrated in The power dissipation was mW The difference between
the flag bits detected by the chip and those obtained by computer simulation are also
shown in the figure The number of error flags was from to out of flags
only a of the total The occurrence of such error bits is anticipated since we
employed analog circuits for median detection However such error does not cause
any serious problems in the PPED algorithm as demonstrated in Figs and
The template matching results with the top five PPED vector candidates in Sella
identification are demonstrated in where Manhattan distance was adopted as
the dissimilarity measure The error in the feature map generation processing yields a
constant bias to the dissimilarity and does not affect the result of the maximum likelihood search
Generated Feature maps
Difference as compared
to computer simulation
Sella
Horizontal
Plus 45-degrees
Vertical
Minus 45-degrees
Feature maps for Sella pattern generated by the chip
Generated PPED vector by the chip
Sella
Difference as compared
to computer simulation
Dissimilarity by Manhattan Distance
PPED vector for Sella pattern generated by the chip The difference in the
vector components between the PPED vector generated by the chip and that obtained
by computer simulation is also shown
Measured Data
Computer Simulation
1st Correct
2nd
3rd
4th
5th
Candidates in Sella recognition
Comparison of template matching results
Conclusion
A mixed-signal median filter VLSI circuit for PPED vector generation is presented A
four-stage asynchronous median detection architecture based on analog digital
mixed-signal circuits has been introduced As a result a fully seamless pipeline
processing from threshold detection to edge feature map generation has been established A prototype chip was designed in a CMOS technology and the fab
ricated chip generates an edge based image vector every sec which is about
times faster than the software computation
Acknowledgments
The VLSI chip in this study was fabricated in the chip fabrication program of VLSI
Design and Education Center VDEC the University of Tokyo with the collaboration
by Rohm Corporation and Toppan Printing Corporation The work is partially supported by the Ministry of Education Science Sports and Culture under Grant-in-Aid
for Scientific Research and by JST in the program of CREST

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1352-a-simple-and-fast-neural-network-approach-to-stereovision.pdf

A Simple and Fast Neural Network
Approach to Stereovision
Rolf D. Henkel
Institute of Theoretical Physics
University of Bremen
P.O. Box Bremen
http://axon.physik.uni-bremen.de/-rdh
Abstract
A neural network approach to stereovision is presented based on
aliasing effects of simple disparity estimators and a fast coherencedetection scheme Within a single network structure a dense disparity map with an associated validation map and additionally
the fused cyclopean view of the scene are available The network
operations are based on simple biological plausible circuitry the
algorithm is fully parallel and non-iterative
Introduction
Humans experience the three-dimensional world not as it is seen by either their left
or right eye but from a position of a virtual cyclopean eye located in the middle
between the two real eye positions The different perspectives between the left and
right eyes cause slight relative displacements of objects in the two retinal images
disparities which make a simple superposition of both images without diplopia
impossible Proper fusion of the retinal images into the cyclopean view requires the
registration of both images to a common coordinate system which in turn requires
calculation of disparities for all image areas which are to be fused
The Problems with Classical Approaches
The estimation of disparities turns out to be a difficult task since various random
and systematic image variations complicate this task Several different techniques
have been proposed over time which can be loosely grouped into feature area
A Simple and Fast Neural Network Approach to Stereovision
and phase-based approaches All these algorithms have a number of computational
problems directly linked to the very assumptions inherent in these approaches
In feature-based stereo intensity data is first converted to a set of features assumed
to be a more stable image property than the raw image intensities Matching
primitives used include zerocrossings edges and corner points Frisby or
higher order primitives like topological fingerprints for example Fleck
Generally the set of feature-classes is discrete causing the two primary problems
of feature-based stereo algorithms the famous false-matches"-problem and the
problem of missing disparity estimates
False matches are caused by the fact that a single feature in the left image can
potentially be matched with every feature of the same class in the right image
This problem is basic to all feature-based stereo algorithms and can only be solved
by the introduction of additional constraints to the solution In conjunction with
the extracted features these constraints define a complicated error measure which
can be minimized by cooperative processes Marr or by direct Ohta
or stochastic search techniques Yuille While cooperative processes and
stochastic search techniques can be realized easily on a neural basis it is not immediately clear how to implement the more complicated algorithmic structures of
direct search techniques neuronally Cooperative processes and stochastic search
techniques turn out to be slow needing many iterations to converge to a local
minimum of the error measure
The requirement of features to be a stable image property causes the second problem
of feature-based stereo stable features can only be detected in a fraction of the
whole image area leading to missing disparity estimates for most of the image area
For those image parts disparity estimates can only be guessed
Dense disparity maps can be obtained with area-based approaches where a suitable
chosen correlation measure is maximized between small image patches of the left and
right view However a neuronally plausible implementation of this seems to be not
readily available Furthermore the maximization turns out to be a computationally
expensive process since extensive search is required in configuration space
Hierarchical processing schemes can be utilized for speed-up by using information
obtained at coarse spatial scales to restrict searching at finer scales But for general
image data it is not guaranteed that the disparity information obtained at some
coarse scale is valid The disparity data might be wrong might have a different value
than at finer scales or might not be present at all Furthermore by processing data
from coarse to fine spatial scales hierarchical processing schemes are intrinsically
sequential This creates additional algorithmic overhead which is again difficult to
realize with neuronal structures
The same comments apply to phase-based approaches where a locally extracted
Fourier-phase value is used for matching Phase values are only defined modulo
and this wrap-around makes the use of hierarchical processing essential for
these types of algorithms Moreover since data is analyzed in different spatial
frequency channels it is nearly certain that some phase values will be undefined
at intermediate scales due to missing signal energy in this frequency band Fleet
Thus in addition to hierarchical processing some kind of exception handling
is needed with these approaches
R. D. Henkel
Stereovision by Coherence Detection
In summary classical approaches to stereovision seem to have difficulties with the
fast calculation of dense disparity-maps at least with plausible neural circuitry
In the following a neural network implementation will be described which solves
this task by using simple disparity estimators based on motion-energy mechanisms
Adelson Qian closely resembling responses of complex cells in visual
cortex DeAngelis Disparity units of these type belong to a class of disparity
estimators which can be derived from optical flow methods Barron Clearly
disparity calculations and optical flow estimation share many similarities The two
stereo views of a static scene can be considered as two time-slices cut out of
the space-time intensity pattern which would be recorded by an imaginary camera
moving from the position of the left to the position of the right eye However
compared to optical flow disparity estimation is complicated by the fact that only
two discrete time"-samples are available namely the images of the left and right
view positions
disparity calculations
to
Left
Right
Right
correct
wrong
correct
Figure The velocity of an image patch manifests itself as principal texture direction in the space-time flow field traced out by the intensity pattern in time left
Sampling such flow patterns at discrete times can create aliasing-effects which lead
to wrong estimates If one is using optical flow estimation techniques for disparity
calculations this problem is always present
For an explanation consider A surface patch shifting over time traces out
a certain flow pattern The principal texture direction of this flow indicates the
relative velocity of the image patch left Sampling the flow pattern only
at discrete time points the shift between two time-samples can be estimated
without ambiguity provided the shift is not too large middle However if a
certain limit is exceeded it becomes impossible to estimate the shift correctly given
the data right This is a simple aliasing-effect in the time"-direction an
everyday example can be seen as motion reversal in movies
In the case of stereovision aliasing-effects of this type are always present and they
limit the range of disparities a simple disparity unit can estimate Sampling theory
gives a relation between the maximal spatial wavevector k~ax equivalently the
minimum spatial wavelength present in the data and the largest disparity
which can be estimated reliably Henkel
7r
k~ax
A Simple and Fast Neural Network Approach to Stereovision
A well-known example of the size-disparity scaling expressed in equation is
found in the context of the spatial frequency channels assumed to exist in the
visual cortex Cortical cells respond to spatial wavelengths down to about half
their peak wavelength Aopt therefore they can estimate reliable only disparities
less than Aopt This is known as Marr's quarter-cycle limit Blake
Equation immediately suggests a way to extend the limited working range of
disparity estimators a spatial smoothing of the image data before or during disparity calculation reduces k'f:tax and in turn increases the disparity range However
spatial smoothing reduces also the spatial resolution of the resulting disparity map
Another way of modifying the usable range of disparity estimators is the application of a fixed preshift to the input data before disparity calculation This would
require prior knowledge of the correct preshift to be applied which is a nontrivial
problem One could resort to hierarchical coarse-to-fine schemes but the difficulties
with hierarchical schemes have already been elal rated
The aliasing effects discussed are a general feature of sampling visual space with
only two eyes instead of counteracting one can exploit them in a simple coherencedetection scheme where the multi-unit activity in stacks of disparity detectors tuned
to a common view direction is analyzed
Assuming that all disparity units in a stack have random preshifts or presmoothing
applied to their input data these units will have different but slightly overlapping
working ranges Di di in diax for valid disparity estimates An object with true
disparity seen in the common view direction of such a stack will therefore split
the stack into two disjunct classes the class of estimators with dEDi for all
and the rest of the stack with All disparity estimators will
code more or less the true disparity di but the estimates of units belonging to
will be subject to the random aliasing effects discussed depending in a complicated
way on image content and disparity range Di of the unit
We will thus have whenever units and belong to and random relationships otherwise A simple coherence detection within each stack searching
for all units with di and extracting the largest cluster found will be sufficient
to single out C. The true disparity in the view direction of the stack can be simply
estimated as an average over all coherently coding units
Neural Network Implementation
Repeating this coherence detection scheme in every view direction results in a fully
parallel network structure for disparity calculation Neighboring disparity stacks
responding to different view directions estimate disparity values independently from
each other and within each stack disparity units operate independently from each
other Since coherence detection is an opportunistic scheme extensions of the basic
algorithm to mUltiple spatial scales and combinations of different types of disparity
estimators are trivial Additional units are simply included in the appropriate
coherence stacks The coherence scheme will combine only the information from
the coherently coding units and ignore the rest of the data For this reason the
scheme also turns out to be extremely robust against single-unit failures
R. D. Henkel
disparity data
Left eye
Right eye
Cyclopean eye
Figure The network structure for a single horizontal scan-line left The view
directions of the disparity stacks split the angle between the left and right lines
of sight in the network and 3D-space in half therefore analyzing space along the
cyclopean view directions right
In the current implementation disparity units at a single spatial scale
are arranged into horizontal disparity layers Left and right image data is fed
into this network along diagonally running data lines This causes every disparity
layer to receive the stereo data with a certain fixed preshift applied leading to the
required slightly different working-ranges of neighboring layers Disparity units
stacked vertically above each other are collected into a single disparity stack which
is then analyzed for coherent activity
Results
The new stereo network performs comparable on several standard test image sets
The calculated disparity maps are similar to maps obtained by classical
area-based approaches but they display subpixel-precision Since no smoothing or
regularization is performed by the coherence-based stereo algorithm sharp disparity
edges can be observed at object borders
Within the network a simple validation map is available locally A measure of local
Figure Disparity maps for some standard test images small insets calculated
by the coherence-based stereo algorithm
A Simple and Fast Neural Network Approach to Stereovision
Figure The performance of coherence-based stereo on a difficult scene with specular highlights transparency and repetitive structures left The disparity map
middle is dense and correct except for a few structure-less image regions These
regions as well as most object borders are indicated in the validation map right
with a low dark validation count
coherence can be obtained by calculating the relative number of coherently acting
disparity units in each stack by calculating the ratio N(CUC where
is the number of units in class C. In most cases this validation map clearly marks
image areas where the disparity calculations failed for various reasons notably at
occlusions caused by object borders or in large structure-less image regions where
no reliable matching can be obtained compare Fig
Close inspection of disparity and validation maps reveals that these image maps
are not aligned with the left or the right view of the scene Instead both maps are
registered with the cyclopean view This is caused by the structural arrangement of
data lines and disparity stacks in the network Reprojecting data lines and stacks
back into 3D-space shows that the stacks analyze three-dimensional space along
lines splitting the angle between the left and right view directions in half This is
the cyclopean view direction as defined by Hering
It is easy to obtain the cyclopean view of the scene itself With If and If denoting
the left and right input data at the position of disparity-unit a summation over
all coherently coding disparity units in a stack
Figure A simple superposition of the left and right stereo images results in
diplopia left By using a vergence system the two stereo images can be aligned
better middle but diplopia is still prominent in most areas of the visual field
The fused cyclopean view of the scene left was calculated by the coherence-based
stereo network
R. D. Henkel
gives the image intensity I in the cyclopean view-direction of this stack Collecting
IC from all disparity stacks gives the complete cyclopean view as the third coregistered map of the network Fig
Acknowledgements
Thanks to Helmut Schwegler and Robert P. O'Shea for interesting discussions Image data courtesy of G. Medoni UCS Institute for Robotics Intelligent Systems B. Bolles AIC SRI International and G. Sommer Kiel
Cognitive Systems Group Christian-Albrechts-Universitat Kiel An internetbased implementation of the algorithm presented in this paper is available at
http://axon.physik.uni-bremen.de/-rdh/online~alc/stereo

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2689-modeling-nonlinear-dependencies-in-natural-images-using-mixture-of-laplacian-distribution.pdf

Modeling Nonlinear Dependencies in
Natural Images using Mixture of
Laplacian Distribution
Hyun Jin Park and Te Won Lee
Institute for Neural Computation UCSD
Gilman Drive La Jolla CA
hjinpark tewon}@ucsd.edu
Abstract
Capturing dependencies in images in an unsupervised manner is
important for many image processing applications We propose a
new method for capturing nonlinear dependencies in images of
natural scenes This method is an extension of the linear Independent
Component Analysis ICA method by building a hierarchical model
based on ICA and mixture of Laplacian distribution The model
parameters are learned via an EM algorithm and it can accurately
capture variance correlation and other high order structures in a
simple manner We visualize the learned variance structure and
demonstrate applications to image segmentation and denoising
In trod ction
Unsupervised learning has become an important tool for understanding biological
information processing and building intelligent signal processing methods Real
biological systems however are much more robust and flexible than current artificial
intelligence mostly due to a much more efficient representations used in biological
systems Therefore unsupervised learning algorithms that capture more sophisticated
representations can provide a better understanding of neural information processing
and also provide improved algorithm for signal processing applications For example
independent component analysis ICA can learn representations similar to simple cell
receptive fields in visual cortex and is also applied for feature extraction image
segmentation and denoising ICA can approximate statistics of natural image
patches by where is the data and is a source signal whose distribution is
a product of sparse distributions like a generalized Laplacian distribution
Au
But the representation learned by the ICA algorithm is relatively low-level In
biological systems there are more high-level representations such as contours
textures and objects which are not well represented by the linear ICA model ICA
learns only linear dependency between pixels by finding strongly correlated linear
axis Therefore the modeling capability of ICA is quite limited Previous approaches
showed that one can learn more sophisticated high-level representations by capturing
nonlinear dependencies in a post-processing step after the ICA step
The focus of these efforts has centered on variance correlation in natural images After
ICA a source signal is not linearly predictable from others However given variance
dependencies a source signal is still predictable in a nonlinear manner It is not
possible to de-correlate this variance dependency using a linear transformation
Several researchers have proposed extensions to capture the nonlinear dependencies
Portilla used Gaussian Scale Mixture GSM to model variance dependency in
wavelet domain This model can learn variance correlation in source prior and showed
improvement in image denoising But in this model dependency is defined only
between a subset of wavelet coefficients Hyvarinen and Hoyer suggested using a
special variance related distribution to model the variance correlated source prior
This model can learn grouping of dependent sources Subspace ICA or topographic
arrangements of correlated sources Topographic ICA Similarly Welling
suggested a product of expert model where each expert represents a variance
correlated group The product form of the model enables applications to image
denoising But these models don?t reveal higher-order structures explicitly
Our model is motivated by Lewicki and Karklin who proposed a 2-stage model where
the 1st stage is an ICA model and the nd-stage is a linear generative model
where another source generates logarithmic variance for the 1st stage
This model captures variance dependency structure explicitly but treating variance as
an additional random variable introduces another level of complexity and requires
several approximations Thus it is difficult to obtain a simple analytic PDF of source
signal and to apply the model for image processing problems
exp
log Bv
We propose a hierarchical model based on ICA and a mixture of Laplacian
distribution Our model can be considered as a simplification of model in by
constraining to be random vector where only one element can be Our model is
computationally simpler but still can capture variance dependency Experiments show
that our model can reveal higher order structures similar to In addition our model
provides a simple parametric PDF of variance correlated priors which is an important
advantage for adaptive signal processing Utilizing this we demonstrate simple
applications on image segmentation and image denoising Our model provides an
improved statistic model for natural images and can be used for other applications
including feature extraction image coding or learning even higher order structures
Modeling nonlinear dependencies
We propose a hierarchical or 2-stage model where the st stage is an ICA source signal
model and the 2nd stage is modeled by a mixture model with different variances figure
In natural images the correlation of variance reflects different types of regularities
in the real world Such specialized regularities can be summarized as context
information To model the context dependent variance correlation we use mixture
models where Laplacian distributions with different variance represent different
contexts For each image patch a context variable selects which Laplacian
distribution will represent ICA source signal Laplacian distributions have 0-mean
but different variances The advantage of Laplacian distribution for modeling context
is that we can model a sparse distribution using only one Laplacian distribution But
we need more than two Gaussian distributions to do the same thing Also conventional
ICA is a special case of our model with one Laplacian We define the mixture model
and its learning algorithm in the next sections
Figure Proposed hierarchical model stage is ICA generative model 2nd stage is
mixture of context dependent Laplacian distributions which model U. is a random
variable that selects a Laplacian distribution that generates the given image patch
Mixture of Laplacian Distribution
We define a PDF for mixture of M-dimensional Laplacian Distribution as
where is the number of data samples and is the number of mixtures
P(U P(u P(u
n,m
exp
un un un,M n-th data sample ui
Variance of k-th Laplacian distribution
probability of Laplacian distribution and
It is not easy to maximize directly and we use EM expectation maximization
algorithm for parameter estimation Here we introduce a new hidden context variable
that represents which Laplacian is responsible for a given data point Assuming
we know the hidden variable we can write the likelihood of data and as
zkn
kn
P(U P(u
exp kn
kn Hidden binary random variable if n-th data sample is generated from k-th
Laplacian other wise kn and kn for all
EM algorithm for learning the mixture model
The EM algorithm maximizes the log likelihood of data averaged over hidden variable
Z. The log likelihood and its expectation can be computed as in
log P(U kn log kn log
log kn log
The expectation in can be evaluated if we are given the data and estimated
parameters and For and EM algorithm uses current estimation and
kn kn
zkn
kn
kn kn
P(u
exp
cn
exp
Where the normalization constant can be computed as
cn kn kn
exp
The EM algorithm works by maximizing given the expectation computed from
can be computed using and estimated in the previous
iteration of EM algorithm This is E-step of EM algorithm Then in M-step of EM
algorithm we need to maximize over parameter and
First we can maximize with respect to by setting the derivative as
n,m
E{log
kn
E{z
E{z
Second for maximization of with respect to we can rewrite as below
log kn
As we see the derivative of with respect to cannot be Instead we need to
use Lagrange multiplier method for maximization A Lagrange function can be
defined as where is a Lagrange multiplier
kn log
n,k
By setting the derivative of to be with respect to and we can simply get
the maximization solution with respect to We just show the solution in
kn kn
Then the EM algorithm can be summarized as figure For the convergence criteria
we can use the expectation of log likelihood which can be calculated from
um is small random noise
Calculate the Expectation by
Initialize
exp
Maximize the log likelihood given the Expectation
kn kn
kn n,m kn
kn kn
If converged stop otherwise repeat from step
Figure Outline of EM algorithm for Learning the Mixture Model
Experimental Results
Here we provide examples of image data and show how the learning procedure is
performed for the mixture model We also provide visualization of learned variances
that reveal the structure of variance correlation and an application to image denoising
Learning Nonlinear Dependencies in Natural images
As shown in figure the st stage of the proposed model is simply the linear ICA. The
ICA matrix A and are learned by the FastICA algorithm We sampled
data from patches dim of natural images and use them for both
first and second stage learning ICA input dimension is and source dimension is
set to be The learned ICA basis is partially shown in figure The 2nd stage
mixture model is learned given the ICA source signals In the nd stage the number of
mixtures is set to 64 or Training by the EM algorithm is fast and several
hundred iterations are sufficient for convergence hour on a Pentium PC).
For the visualization of learned variance we adapted the visualization method from
Each dimension of ICA source signal corresponds to an ICA basis columns of A
and each ICA basis is localized in both image and frequency space Then for each
Laplacian distribution we can display its variance vector as a set of points in image
and frequency space Each point can be color coded by variance value as figure
Figure Visualization of learned variances and a2 visualize variance of
Laplacian and b1 and show that of Laplacian High variance value is mapped
to red color and low variance is mapped to blue In Laplacian variances for
diagonally oriented edges are high But in Laplacian variances for edges at
spatially right position are high Variance structures are related to contexts in the
image For example Laplacian explains image patches that have oriented textures
or edges Laplacian captures patches where left side of the patch is clean but right
side is filled with randomly oriented edges
A key idea of our model is that we can mix up independent distributions to get nonlinearly dependent distribution This modeling power can be shown by figure
Figure Joint distribution of nonlinearly dependent sources is a joint histogram
of ICA sources is computed from learned mixture model and is from learned
Laplacian model In variance of u2 is smaller than u1 at center area arrow but
almost equal to u1 at outside arrow B). So the variance of u2 is dependent on This
nonlinear dependency is closely approximated by mixture model in but not in
Unsupervised Image Segmentation
The idea behind our model is that the image can be modeled as mixture of different
variance correlated contexts We show how the learned model can be used to
classify different context by an unsupervised image segmentation task Given learned
model and data we can compute the expectation of a hidden variable from
Then for an image patch we can select a Laplacian distribution with highest
probability which is the most explaining Laplacian or context For segmentation
we use the model with Laplacians This enables abstract partitioning of images and
we can visualize organization of images more clearly figure
Figure Unsupervised image segmentation left is original image middle is color
labeled image right image shows color coded Laplacians with variance structure
Each color corresponds to a Laplacian distribution which represents surface or
textural organization of underlying contexts Laplacian captures smooth surface
and Laplacian captures contrast between clear sky and textured ground scenes
Application to Image Restoration
The proposed mixture model provides a better parametric model of the ICA source
distribution and hence an improved model of the image structure An advantage is in
the MAP maximum a posterior estimation of a noisy image If we assume Gaussian
noise the image generation model can be written as Then we can compute
MAP estimation of ICA source signal by and reconstruct the original image
Au
argmax log A argmax log A log
Since we assumed Gaussian noise in is Gaussian in
can be modeled as a Laplacian or a mixture of Laplacian distribution The mixture
distribution can be approximated by a maximum explaining Laplacian We evaluated
different methods for image restoration including ICA MAP estimation with simple
Laplacian prior same with Laplacian mixture prior and the Wiener filter Figure
shows an example and figure summarizes the results obtained with different noise
levels As shown MAP estimation with the mixture prior performs better than the
others in terms of SNR and SSIM Structural Similarity Measure
Figure Image restoration results signal variance noise variance
ICA MAP Mixture prior
ICA MAP Laplacian prior
iener
SSIM Index
SNR
ICA MAP(Mixture prior
ICA MAP(Laplacian prior
iener
Noisy Image
Noise variance
Noise variance
Figure SNR and SSIM for different algorithms signal variance
on
We proposed a mixture model to learn nonlinear dependencies of ICA source signals
for natural images The proposed mixture of Laplacian distribution model is a
generalization of the conventional independent source priors and can model variance
dependency given natural image signals Experiments show that the proposed model
can learn the variance correlated signals grouped as different mixtures and learn highlevel structures which are highly correlated with the underlying physical properties
captured in the image Our model provides an analytic prior of nearly independent and
variance-correlated signals which was not viable in previous models
The learned variances of the mixture model show structured localization in image and
frequency space which are similar to the result in Since the model is given no
information about the spatial location or frequency of the source signals we can
assume that the dependency captured by the mixture model reveals regularity in the
natural images As shown in image labeling experiments such regularities correspond
to specific surface types textures or boundaries between surfaces
The learned mixture model can be used to discover hidden contexts that generated
such regularity or correlated signal groups Experiments also show that the labeling of
image patches is highly correlated with the object surface types shown in the image
The segmentation results show regularity across image space and strong correlation
with high-level concepts
Finally we showed applications of the model for image restoration We compare the
performance with the conventional ICA MAP estimation and Wiener filter Our
results suggest that the proposed model outperforms other traditional methods It is
due to the estimation of the correlated variance structure which provides an improved
prior that has not been considered in other methods
In our future work we plan to exploit the regularity of the image segmentation result
to lean more high-level structures by building additional hierarchies on the current
model Furthermore the application to image coding seems promising

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2426-bayesian-color-constancy-with-non-gaussian-models.pdf

Bayesian Color Constancy
with Non-Gaussian Models
Charles Rosenberg
Thomas Minka
Alok Ladsariya
Computer Science Department
Carnegie Mellon University
Pittsburgh PA
Statistics Department
Carnegie Mellon University
Pittsburgh PA
Computer Science Department
Carnegie Mellon University
Pittsburgh PA
chuck@cs.cmu.edu
minka@stat.cmu.edu
alokl@cs.cmu.edu
Abstract
We present a Bayesian approach to color constancy which utilizes a nonGaussian probabilistic model of the image formation process The parameters of this model are estimated directly from an uncalibrated image
set and a small number of additional algorithmic parameters are chosen
using cross validation The algorithm is empirically shown to exhibit
RMS error lower than other color constancy algorithms based on the
Lambertian surface reflectance model when estimating the illuminants
of a set of test images This is demonstrated via a direct performance
comparison utilizing a publicly available set of real world test images
and code base
Introduction
Color correction is an important preprocessing step for robust color-based computer vision
algorithms Because the illuminants in the world have varying colors the measured color
of an object will change under different light sources We propose an algorithm for color
constancy which given an image will automatically estimate the color of the illuminant
assumed constant over the image allowing the image to be color corrected
This color constancy problem is ill-posed because object color and illuminant color are
not uniquely separable Historically algorithms for color constancy have fallen into two
groups The first group imposes constraints on the scene and/or the illuminant in order to
remove the ambiguities The second group uses a statistical model to quantify the probability of each illuminant and then makes an estimate from these probabilities The statistical
approach is attractive since it is more general and more automatic?hard constraints are a
special case of statistical models and they can be learned from data instead of being specified in advance But as shown by currently the best performance on real images
is achieved by gamut mapping a constraint-based algorithm And in the words of some
leading researchers even gamut mapping is not good enough for object recognition
In this paper we show that it is possible to outperform gamut mapping with a statistical
approach by using appropriate probability models with the appropriate statistical framework We use the principled Bayesian color constancy framework of but combine it
with rich nonparametric image models such as used by Color by Correlation The
result is a Bayesian algorithm that works well in practice and addresses many of the issues
with Color by Correlation the leading statistical algorithm
At the same time we suggest that statistical methods still have much to learn from
constraint-based methods Even though our algorithm outperforms gamut mapping on
average there are cases in which gamut mapping provides better estimates and in fact
the errors of the two methods are surprisingly uncorrelated This is an interesting result
because it suggests that gamut mapping exploits image properties which are different from
what is learned by our algorithm and probably other statistical algorithms If this is true
and if our statistical model could be extended in a way that captures these additional properties better algorithms should be possible in the future
The imaging model
Our approach is to model the observed image pixels with a probabilistic generative model
decomposing them as the product of unknown surface reflectances with an unknown illuminant Using Bayes rule we obtain a posterior for the illuminant and from this we
extract the estimate with minimum risk the minimum expected chromaticity error
Let be an image pixel with three color channels yr yg yb The pixel is assumed to be
the result of light reflecting off of a surface under the Lambertian reflectance model Denote
the power of the light in each channel by with each channel ranging from
zero to infinity For each channel a surface can reflect none of the light all of the light
or somewhere in between Denote this reflectance by xr xg xb with each channel
ranging from zero to one The model for the pixel is the well-known diagonal lighting
model
yr xr
yg xg
yb xb
To simplify the equations below we write this in matrix form as
diag
Lx
This specifies the conditional distribution In reality there are sensor noise and
other factors which affect the observed color but we will consider these to be negligible
Next we make the common assumption that the light and the surface have been chosen
independently so that The prior distribution for the illuminant
will be uniform over a constraint set described later in section
The most difficult step is to construct a model for the surface reflectances in an image
containing many pixels
We need a distribution for all reflectances One approach is to assume that the
reflectances are independent and Gaussian as in which gives reasonable results but can
be improved upon Our approach is to quantize the reflectance vectors into bins and
consider the reflectances to be exchangeable?a weaker assumption than independence
Exchangeability implies that the probability only depends on the number of reflectances
in
each bin Thus if we denote the reflectance histogram by nK where nk
then
nK
where is a function to be specified Independence is a special case of exchangeability
If
mk is the probability of a surface having a reflectance value in bin so that mk
then independence says
nK
mnk
As an alternative to this we have experimented with the Dirichlet-multinomial model
which employs a parameter to control the amount of correlation Under this model
smk
nK
smk
For large correlation is weak and the model reduces to For small correlation is
strong and the model expects a few reflectances to be repeated many times which is what
we see in real images When is very small the expression can be reduced to a simple
form
clip(nk
nK
smk
if nk
clip(nk
if nk
This resembles a multinomial distribution on clipped counts Unfortunately this distribution strongly prefers that the image contains a small number of different reflectances
which biases the light source estimate Empirically we have achieved our best results using
a normalized count modification of the model which removes this bias
m?kk
nK
clip(nk
nP
clip(nk
The modified counts sum to just like the original counts nk but are distributed equally
over all reflectances present in the image
The color constancy algorithm
The algorithm for estimating the illuminant has two parts discretize the set of all
illuminants on a fine grid and compute their likelihood and pick the illuminant which
minimizes the risk
The likelihood of the observed image data for a given illuminant is
p(X)dX
p(X
The quantity can be understood as the color-corrected image The determinant term
makes this a valid distribution over and has the effect of introducing a
preference for dimmer illuminants independently of the prior on reflectances Also implicit
in this likelihood are the bounds on which require reflectances to be in the range of zero
and one and thus we restrict our search to illuminants that satisfy
max yr
max yg
max yb
The posterior probability for then follows
p(X
The next step is to find the estimate of with minimum risk An answer that the illuminant
is when it is really incurs some cost denoted Let this function be quadratic
in some transformation of the illuminant vector
This occurs for example when the cost function is squared error in chromaticity Then the
minimum-risk estimate satisfies
The right-hand side the posterior mean of and the normalizing constant of the posterior
can be computed in a single loop over the grid of illuminants
Relation to other algorithms
In this section we describe related color constancy algorithms using the framework of the
imaging model introduced in section This is helpful because it allows us to compare all
of these algorithms in a single framework and understand the assumptions made by each
Independent Gaussian reflectances The previous work most similar to our own is by
and however these methods are not tested on real images They use a similar
imaging model and maximum-likelihood and minimum-risk estimation respectively The
difference is that they use a Gaussian prior for the reflectance vectors and assume the
reflectances for different pixels are independent The Gaussian assumption leads to a simple likelihood formula whose maximum can be found by gradient methods However as
mentioned by this is a constraining assumption and more appropriate priors would be
preferable
Scale by max The scale by max algorithm as tested in estimates the illuminant
by the simple formula
max yr
max yg
max yb
which is the dimmest illuminant in the valid set In the Bayesian algorithm this
solution can be achieved by letting the reflectances be independent and uniform over the
range to Then is constant and the maximum-likelihood illuminant is This
connection was also noticed by
Gray-world The gray-world algorithm chooses the illuminant such that the average
value in each channel of the corrected image is a constant This is equivalent to the
Bayesian algorithm with a particular reflectance prior Let the reflectances be independent
for each pixel and each channel with distribution p(xc exp(?2xc in each channel
The log-likelihood for is then
log p(Yc log
yc
whose maximum is as desired
2X
yc
const
Figure Plots of slices of the three dimensional color surface reflectance distribution
along a single dimension Row one plots green versus blue with at the upper left of
each subplot and slices in red whose magnitude increases from left to right Row two plots
red versus blue with slices in green Row three plots red versus green with slices in blue
Color by Correlation Color by Correlation also uses a likelihood approach but
with a different imaging model that is not based on reflectance Instead observed pixels
are quantized into color bins and the frequency of each bin is counted for each illuminant
in a finite set of illuminants Note that this is different from quantizing reflectances as
done in our approach Let mk be the frequency of color bin for illuminant and let
n1 nK be the color histogram of the image then the likelihood of is computed as
mk clip(nk
While theoretically this is very general there are practical limitations First there are training issues One must learn the color frequencies for every possible illuminant Since
collecting real-world data whose illuminant is known is difficult mk is typically trained
synthetically with random surfaces which may not represent the statistics of natural scenes
The second issue is that colors and illuminants live in an unbounded 3D space unlike
reflectances which are bounded In order to store a color distribution for each illuminant
brightness variation needs to be artificially bounded The third issue is storage To reduce
the storage of the mk Barnard al store the color distribution only for illuminants
of a fixed brightness However as they describe this introduces a bias in the estimation
they refer to as the discretization problem and try to solve it by penalizing bright illuminants The other part of the bias is due to using clipped counts in the likelihood As
explained in section a multinomial likelihood with clipped counts is a special case of the
Dirichlet-multinomial and prefers images with a small number of different colors This
bias can be removed using a different likelihood function such as
Parameter estimation
Reflectance Distribution
To implement the Bayesian algorithm we need to learn the real-world frequencies mk of
quantized reflectance vectors The direct approach to this would require a set of images
with ground truth information regarding the associated illumination parameters or alternately a set of images captured under a canonical illuminant and camera
Unfortunately it is quite difficult to collect a large number of images under controlled
conditions To avoid this issue we use bootstrapping as described in to approximate
the ground truth The estimates from some base color constancy algorithm are used as
a proxy for the ground truth This might seem to be problematic in that it would limit any
algorithm based on these estimates to perform only as well as the base algorithm However
this need not be the case if the errors made by the base algorithm are relatively unbiased
We used approximately randomly selected JPEG images from news sites on the web
for bootstrapping consisting mostly of outdoor scenes indoor news conferences and sporting event scenes The scale by max algorithm was used as our base algorithm Figure
is a plot of the probability distribution collected where lighter regions represent higher
probability values The distribution is highly structured and varies with the magnitude of
the channel response This structure is important because it allows our algorithm to disambiguate between potential solutions to the ill-posed illumination estimation problem
Pre-processing and quantization
To increase robustness pre-processing is performed on the image similar to that performed
in The first pre-processing step scales down the image to reduce noise and speed up
the algorithm A new image is formed in which each pixel is the mean of an by
block of the original image The second pre-processing step removes dark pixels from the
computation which because of noise and quantization effects do not contain reliable color
information Pixels whose yr yg yb channel sum is less than a given threshold are
excluded from the computation
In addition to the reflectance prior the parameters of our algorithm are the number of
reflectance histogram bins the scale down factor and the dark pixel threshold value To set
these parameters values the algorithm was run over a large grid of parameter variations and
performance on the tuning set was computed The tuning set was a subset of the model
data set described in and disjoint from the test set A total of images were used
objects imaged under illuminants The ball2 object was removed so that there was no
overlap between the tuning and test sets For the purpose of speed only images captured
with the Philips Ultralume and the Macbeth Judge fluorescent illuminants were included
The best set of parameters was found to be 32 32 32 reflectance bins scale down by
and omit pixels with a channel sum less than
Illuminant prior
To facilitate a direct comparison we adopt the two illuminant priors from Each is
uniform over a subset of illuminants The first prior full set discretizes the illuminants
uniformly in polar coordinates The second prior hull set is a subset of full set restricted
to be within the convex hull of the test set illuminants and other real world illuminants
Overall brightness is discretized in the range of to in steps
Experiments
Evaluation Specifics
To test the algorithms we use the publicly available real world image data set used
by Barnard Martin Coath and Funt in a comprehensive evaluation of color constancy
algorithms in The data set consists of images of scenes captured under light
sources for a total of images after the authors removed images which had collection
problems with ground truth illuminant information provided in the form of an RGB value
As in the rg error measure of illuminant error is measured in chromaticity space
The Bayesian algorithm is adapted to minimize this risk by computing the posterior mean
in chromaticity space The performance of an algorithm on the test set is reported as the
square root of the average across all images referred to as the RMS error
Table The average error of several color constancy algorithms on the test set The value
in parentheses is times the standard error of the average so that if two error intervals
do not overlap the difference is significant at the level
Algorithm
Scale by Max
Gamut Mapping without Segmentation
Gamut Mapping with Segmentation
Bayes with Bootstrap Set Model
Bayes with Tuning Set Model
RMS Error for Full Set
RMS Error for Hull Set
Scale by Max
Gamut Mapping without Segmentation
Gamut Mapping with Segmentation
Bayes with Bootstrap Set Model
Bayes with Tuning Set Model
Full Set
Hull Set
RMS error
Figure A graphical rendition of table The standard errors are scaled by so that if
two error bars do not overlap the difference is significant at the level
Results
The results1 are summarized in Table and Figure We compare two versions of our
Bayesian method to the gamut mapping and scale by max algorithms The appropriate
preprocessing for each algorithm was applied to the images to achieve the best possible
performance Note that we do not include results for color by correlation since the gamut
mapping results were found to be significantly better in In all configurations our
algorithm exhibits the lowest RMS error except in a single case where it is not statistically different than that of gamut mapping The differences for the hull set are especially
large The hull set is clearly a useful constraint that improves the performance of all of the
algorithms evaluated
The two versions of our Bayesian algorithm differ only in the data set used to build the
reflectance prior The tuning set while composed of separate images than the test set is
very similar and has known illuminants and accordingly gives the best results Yet the
performance when trained on a very different set of images the uncalibrated bootstrap set
of section is not that different particularly when the illuminant search is constrained
The gamut mapping algorithm called CRULE and ECRULE in is also presented in two
versions with and without segmenting the images as a preprocessing step as described in
These results were computed using software provided by Barnard and used to generate
the results in In the evaluation of color constancy algorithms in gamut mapping was
found on average to outperform all other algorithms when evaluated on real world images
It is interesting to note that the gamut mapping algorithm is sensitive to segmentation Since
fundamentally it should not be sensitive to the number of pixels of a particular color in the
image we must assume that this is because the segmentation is implementing some form of
noise filtering The Bayesian algorithm currently does not use segmentation
Scale by max is also included as a

<<----------------------------------------------------------------------------------------------------------------------->>

title: 779-address-block-location-with-a-neural-net-system.pdf

Address Block Location with a Neural Net System
Eric Cosatto
Hans Peter Graf
AT&T Bell Laboratories
Crawfords Corner Road
Holmdel NJ USA
Abstract
We developed a system for finding address blocks on mail pieces that can
process four images per second Besides locating the address block our
system also determines the writing style handwritten or machine printed and
moreover it measures the skew angle of the text lines and cleans noisy
images A layout analysis of all the elements present in the image is
performed in order to distinguish drawings and dirt from text and to separate
text of advertisement from that of the destination address
A speed of more than four images per second is obtained on a modular
hardware platform containing a board with two of the NET32K neural net
chips a SPARC2 processor board and a board with digital signal
processors The system has been tested with more than images Its
performance depends on the quality of the images and lies between
correct location in very noisy images to over in cleaner images
INTRODUCTION
The system described here has been integrated into an address reading machine developed for
the Remote Computer Reader project of the United States Postal Service While the actual
reading of the text is done by other modules this system solves one of the major problems
namely finding reliably the location of the destination address There are only a few constraints
on how and where an address has to be written hence they may appear in a wide variety of
styles and layouts Often an envelope contains advertising that includes images as well as text
Graf and Cosatto
Sometimes dirt covers part of the envelope image including the destination address Moreover
the image captured by the camera is thresholded and the reader is given a binary image This
binarization process introduces additional distortions in particular often the destination address
is surrounded by a heavy texture The high complexity of the images and their poor quality make
it difficult to find the location of the destination address requiring an analysis of all the elements
present in the image Such an analysis is compute-intensive and in our system it turned out to
be the major bottleneck for a fast throughput In fact finding the address requires much more
computation than reading it Special-purpose hardware in the form of the NET32K neural net
chips Graf Henderson is used to solve the address location problem
Finding address blocks has been the focus of intensive research recently as several companies
are developing address reading machines United States Postal Service The wide variety
of images that have to be handled has led other researchers to apply several different analysis
techniques to each image and then try to combine the results at the end see palumbo
In order to achieve the throughput required in an industrial application special purpose
processors for finding connected components and/or for executing Hough transforms have been
applied
In our system we use the NET32K processor to extract geometrical features from an image The
high compute power of this chip allows the extraction of a large number of features
simultaneously From this feature representation an interpretation of the image's content can
then be achieved with a standard processor Compared to an analysis of the original image the
analysis of the feature maps requires several orders of magnitude less computation Moreover
the feature representation introduces a high level of robustness against noise This paper gives
a brief overview of the hardware platfOlm in section and then describes the algorithms to find
the address blocks in section
THE HARDWARE
The NET32K system has been designed to serve as a high-speed image processing platform
where neural nets as well as conventional algorithms can be executed Three boards form the
whole system Two NET32K neural net chips are integrated with a sequencer and data
formatting circuits on one board The second board contains two digital signal processors
DSPs together with Mbytes of memOly Control of the whole system is provided by a board
containing a SPARC2 processor plus 64 Mbytes of memory A schematic of this system is
shown in Figure
Image buffering and communication with other modules in the address reader are handled by
the board with the SPARC2 processor When an image is received it is sent to the DSP board
and from there over to the NET32K processor The feature maps produced by the NET32K
processor are stored on the DSP board while the SPARC2 starts with the analysis of the feature
maps The DSP's main task is formatting of the data while the NET32K processor extracts all
the features Its speed of computation is more than billion multiply-accumulates per second
with operands that have one or two bits of resolution Images with a size of Sl2xS pixels are
processed at a rate of more than frames per second and 64 convolution kernels each with
a size of pixels can be scanned simultaneously over the image Each such kernel IS
tuned to detect the presence of a feature such as a line an edge or a comer
Address Block Location with a Neural Net System
IN~K IN
NET32K MODULE
I
I
It
DSP32C
SRAM
MEG
lt
Afr
DSP32C
DRAM
MEG
SRAM
MEG
L..
SPARC
VME BUS
Figure Schematic of the whole NET32K system Each of the dashed
boxes represents one 6U VME board The
conununication paths
aITOWS
show the
SEQUENCE OF ALGORITHMS
The final result of the address block location system is a box describing a tight bmmd around
the destination address if the address is machine printed Of handwritten addresses only the
zip code is read and hence one has to find a tight boundary around the zip code This
information is then passed along to reader modules of the address reading machine There is no
a priori knowledge about the writing style Therefore the system first has to discriminate
between handwritten and machine Plinted text At the end of the address block location process
additional algorithms are executed to improve the accuracy of the reader An overview of the
sequence of algorithms used to solve these tasks is shown in Figure The whole process is
divided into three major steps Preprocessing feature extraction and high-level analysis based
on the feature information
Preprocessing
To quickly get an idea about the complexity of the image a coarse evaluation of its layout is
done By sampling the density of the black pixels in various places of the image one can see
already whether the image is clean or noisy and whether the text is lightly printed or is dark
Oraf and Cosatto
The images are divided into four categories depending on their darkness and the level of noise
This infonnation is used in the subsequent processing to guide the choice of the features Only
about one percent of the pixels are taken into account for this analysis therefore it can be
executed quickly on the SPARC2 processor
clean light
Preprocessing
clean dark
IF.
Feature
maps
Extract features
NET32K
Feature
maps
Extract text lines
Cluster lines into groups
Classify groups of lines
MACHINE PRINT
Analyse group of lines
Determine level of noise
Clean with NET32K
HANDWRITIEN
Cluster text segments into lines
Analyse group of lines
Segment lines to find ZIP
Determine slanVskew angle
Figure Schematic of the sequence of algorithms for finding the
position of the address blocks
Feature Extraction
After the preprocessing the image is sent to the NET32K board where simple geometrical
features such as edges corners and lines are extracted Up to different feature maps are
generated where a pixel in one of the maps indicates the presence of a feature in this location
Some of these feature maps are used by the host processor for example to decide whether text
is handwritten or machine printed Other feature maps are combined and sent once more
through the NET32K processor in order to search for combinations of features representing
more complex features Typically the feature maps are thresholded so that only one bit per
pixel is kept More resolution of the computation results is available from the neural net chips
but in this way the amount of data that has to be analyzed is minimal and one bit of resolution
turned out to be sufficient
Examples of kernels used for the detection of strokes and text lines are shown in Figure In
the chip usually four line detectors of increasing height plus eight stroke detectors of different
orientations are stored Other detectors are tuned to edges and strokes of machine printed text
The line detectors respond to any black line of the proper height Due to the large width of
Address Block Location with a Neural Net System
pixels a kernel stretches over one or even several characters Hence a text line gives a response
similar to that produced by a continuous black line When the threshold is set properly a text
line in the original image produces a continuous line in the feature map even across the gaps
between characters and across small empty spaces between words For an interpretation of a
line feature map only the left and right end points of each connected component are stored In
this way one obtains a compact representation of the lines positions that are well suited for the
high-level analysis of the layout
Kernel Line detector
Image
the NET32K syste
IC::GUla
Feature
Kernel Stroke detector
Feature map
Figure 3:Examples of convolution kernels and their results The kernels sizes
are pixels and their pixels values are The upper part illustrates
the response of a line detector on a machine printed text line The lower kernel
extracts strokes of a celtain orientation from handwritten text
Handwritten lines are detected by a second technique because they are more irregular in height
and the characters may be spaced apm1 widely Detectors for strokes of the type shown in the
lower half ofFigw-e are well suited for sensing the presence of handwritten text The feature
maps resulting form handwritten text tend to exhibit blobs of pixels along the text line By
smearing such feature maps in horizontal direction the responses of individual strokes are
merged into lines that can then be used in the same way as described for the machine printed
lines
Horizontal smearing of text lines combined with connected component analysis is a well-known
Graf and Cosatto
technique often applied in layout analysis to find words and whole lines of text But when
applied to the pixels of an image such an approach works well only in clean images As soon
as there is noise present this technique produces ilTegular responses The key to success in a
real world environment is robustness against noise By extracting features first and then
analyzing the feature maps we drastically reduce the influence of noise Each of the convolution
kernels covers a range of pixels and its response depends on several dozens of pixels inside
this area If pixels in the image are corrupted by noise this has only a minor effect on the result
of the convolution and hence the appearance of the feature map
When the analysis is started it is unknown whether the address is machine printed or hand
written In order to distinguish between the two writing styles a simple one-layer classifier
looks at the results of four stroke detectors and of four line detectors It can determine reliably
whether text is handwritten or machine printed Additional useful information that can be
extracted easily from the feature maps is the skew angle of handwritten text People tend to
write with a skew anywhere from 45 degrees to almost 90 degrees In order to improve the
accuracy of a reader the text is first deskewed The most time consuming part of this operation
is to determine the skew angle of the writing The stroke detector with the maximum response
over a line is a good indicator of the skew angle of the text We compared this simple technique
with several alternatives and found it to be as reliable as the best other algorithm and much
faster to compute
High-level Analysis
The results of the feature extraction process are line segments each one marked as handwritten
or machine printed Only the left and right end points of such lines are stored At this point
there may still be line segments in this group that do not correspond to text but rather to solid
black lines or to line drawings Therefore each line segment is checked to determine whether
the ratio of black and white pixels is that found typically in text
Blocks of lines are identified by clustering the line segments into groups Then each block is
analyzed to see whether it can represent the destination address For this purpose such features
as the number of lines in the block its size position etc are used These features are entered
into a classifier that ranks each of the blocks Certain conditions such as a size that is too large
or if there are too many text lines in the block will lead to an attempt to split blocks If no good
result is obtained clustering is tried again with a changed distance metric where the horizontal
and the vertical distances between lines are weighted differently
If an address is machine printed the whole address block is passed on to the reader since not
only the zip code but the whole address including the city name the street name and the name
of the recipient have to be read A big problem for the reader present images of poor quality
particularly those with background noise and texture State-of-the-art readers handle machine
printed text reliably if the image quality is good but they may fail totally if the text is buried in
noise For that reason an address block is cleaned before sending it to the reader Feature
extraction with the NET32K board is used once more for this task this time with detectors tuned
to find all the strokes of the machine printed text Applying stroke detectors with the proper
width allows a good discrimination between the text and any noise Even texture that consists
of lines can be rejected reliably if the line thickness of the texture is not the same as that of the
text
Address Block Location with a Neural Net System
ksiQ Cal
t1r
Figure Example of an envelope image at various stages of the processing Top The
result of the clustering process to find the bounding box of the address Bottom right The
text lines within the address block are marked Bottom left Cuts in the text line with the
zip code and below that the result of the reader The zip code is actually the second
segment sent to the reader the first one is the string
If the address is handwritten only the zip code is sent to the reader In order to find the zip code
an analysis of the internal stmcture of the address block has to be done which starts with finding
the true text lines Handwritten lines are often not straight may be heavily skewed and may
contain large gaps Hence simple techniques such as connected component analysis do not
provide proper results ClusteJing of the line segments obtained from the feature maps provides
a reliable solution of this problem Once the lines are found each one is segmented into words
and some of them are selected as candidates for the zip code and are sent to the reader Figure
shows an example of an envelope image as it progresses through the various processing steps
The system has been tested extensively on overall more than images Most of these
tests were done in the assembled address reader but during development of the system large
Graf and Cosatto
tests were also done with the address location module alone One of the problems for evaluating
the peIformance is the lack of an objective quality measure When has an address been located
correctly Cutting off a small part of the address may not be detrimental to the final
interpretation while a bounding box that includes some additional text may slow the reader
down too much or it may throw off the interpretation Therefore it is not always clear when a
bounding box describing the address location is tight enough Another important factor
affecting the accw-acy numbers is how many candidate blocks one actually considers For all
these reasons accw-acy numbers given for address block location have to be taken with some
caution The results mentioned here were obtained by judging the images by eye If images are
clean and the address is surrounded by a white space larger than two line heights the location
is found correctly in more than of the cases Often more than one text block is found and
of these the destination address is the first choice in of the images for a typical layout If
the image is very noisy which actually happens surprisingly often a tight bound around the
address is found in of the cases These results were obtained with images chosen
from more than images to represent as much variety as possible Of these images
more than have a texture around the address and often this texture is so dark that a
human has difficulties to make out each character
CONCLUSION
Most of our algorithms described here consist of two parts feature extraction implemented with
a convolution and interpretation typically implemented with a small classifier Surprisingly
many algorithms can be cast into such a fOimat This common framework for algorithms has
the advantage of facilitating the implementation in particular when algorithms are mapped into
hardware Moreover the feature extraction with large convolution kernels makes the system
robust against noise This robustness is probably the biggest advantage of our approach Most
existing automatic reading systems are very good as long as the images are clean but they
deteriorate rapidly with decreasing image quality
The biggest drawback of convolutions is that they require a lot of computation In fact without
special purpose hardware convolutions are often too slow Our system relies on the NET32K
new-al net chips to obtain the necessary throughput The NET32K system is we believe at the
moment the fastest board system for this type of computation This speed is obtained by
systematically exploiting the fact that only a low resolution of the computation is required This
allows to use analog computation inside the chip and hence much smaller circuits than would
be the case in an all-digital circuit

<<----------------------------------------------------------------------------------------------------------------------->>

title: 50-an-adaptive-and-heterodyne-filtering-procedure-for-the-imaging-of-moving-objects.pdf

AN ADAPTIVE AND HETERODYNE FILTERING PROCEDURE
FOR THE IMAGING OF MOVING OBJECTS
F. H. Schuling H. A. K. Mastebroek and W. H. Zaagman
Biophysics Department Laboratory for General Physics
Westersingel 34 eM Groningen The Netherlands
ABSTRACT
Recent experimental work on the stimulus velocity dependent time resolving
power of the neural units situated in the highest order optic ganglion of the
blowfly revealed the at first sight amazing phenomenon that at this high level of
the fly visual system the time constants of these units which are involved in the
processing of neural activity evoked by moving objects are roughly spokeninverse proportional to the velocity of those objects over an extremely wide range
In this paper we will discuss the implementation of a two dimensional heterodyne
adaptive filter construction into a computer simulation model The features of this
simulation model include the ability to account for the experimentally observed
stimulus-tuned adaptive temporal behaviour of time constants in the fly visual
system The simulation results obtained clearly show that the application of such
an adaptive processing procedure delivers an improved imaging technique of
moving patterns in the high velocity range
A FEW REMARKS ON THE FLY VISUAL SYSTEM
The visual system of the diptera including the blowfly Calliphora
erythrocephala is very regularly organized and allows therefore very precise
optical stimulation techniques Also long term electrophysiological recordings can
be made relatively easy in this visual system For these reasons the blowfly which
is well-known as a very rapid and clever pilot turns out to be an extremely
suitable animal for a systematic study of basic principles that may underlie the
detection and further processing of movement information at the neural level
In the fly visual system the input retinal mosaic structure is precisely
mapped onto the higher order optic ganglia lamina medulla lobula This means
that each neural column in each ganglion in this visual system corresponds to a
certain optical axis in the visual field of the compound eye In the lobula complex
a set of wide-field movement sensitive neurons is found each of which integrates
the input signals over the whole visual field of the entire eye One of these wide
field neurons that has been classified as I by Hausen has been extensively
studied both anatomically2 as well as electrophysiologically5 The
obtained results generally agree very well with those found in behavioral
optomotor experiments on movement detection and can be understood in terms of
Reichardts correlation model
The I neuron is sensitive to horizontal movement and directionally
selective very high rates of action potentials spikes up to per second can be
recorded from this element in the case of visual stimuli which move horizontally
inward from back to front in the visual field pre/erred direction whereas
movement horizontally outward from front to back null direction suppresses
its activity
American Institute of Physics
EXPERIMENTAL RESULTS AS A MODELLING BASE
When the I neuron is stimulated in its preferred direction with a step wise
pattern displacement it will respond with an increase of neural activity By
repeating this stimulus step over and over one can obtain the averaged response
after a ms latency period the response manifests itself as a sharp increase in
average firing rate followed by a much slower decay to the spontaneous activity
level Two examples of such averaged responses are shown in the Post Stimulus
Time Histograms PSTH's of figure Time to peak and peak height are related
and depend on modulation depth stimulus step size and spatial extent of the
stimulus The tail of the responses can be described adequately by an exponential
decay toward a constant spontaneous firing rate
For each setting of the stimulus parameters the response parameters
defined by equation can be estimated by a least-squares fit to the tail of the
PSTH The smooth lines in figure are the results of two such fits
tlmsl
OJ
I'JO
tf
MoO IO
Mdl05
Fig.l
I
lsI
A veraged responses PSTH's obtained from the I neuron being
adapted to smooth stimulus motion with velocities top and
bottom respectively The smooth lines represent least-squares
fits to the PSTH's of the form Values of for the
two PSTH's are and 24 ms respectively de Ruyter van Steveninck
Fitted values of as a function of adaptation velocity for three
modulation depths M. The straight line is a least-squares fit to represent
the data for in the region It has the form
f=Q with ms and de Ruyter van Steveninck
Fig.2
Figure shows fitted values of the response time constant as a function of
the angular velocity of a moving stimulus square wave grating in most
experiments which was presented to the animal during a period long enough to let
its visual system adapt to this moving pattern and before the step wise pattern
displacement which reveals was given The straight line described by
with in Is and in ms represents a least-squares fit to the data over the
velocity range from to Is. For this range varies from to roughly
ms with ms and Defining the adaptation range of as
that interval of velocities for which decreases with increasing velocity we may
conclude from figure that within the adaptation range is not very sensitive to
the modulation depth
The outcome of similar experiments with a constant modulation depth of the
pattern and a constant pattern velocity but with four different values of
the contrast frequency fc the number of spatial periods per second that
traverse an individual visual axis as determined by the spatial wavelength As of the
pattern and the pattern velocity according to fc=v lAs reveal also an almost
complete independency of the behaviour of on contrast frequency Other
experiments in which the stimulus field was subdivided into regions with different
adaptation velocities made clear that the time constants of the input channels of
the I neuron were set locally by the values of the stimulus velocity in each
stimulus sub-region Finally it was found that the adaptation of is driven by
the stimulus velocity independent of its direction
These findings can be summarized qualitatively as follows in steady state
the response time constants of the neural units at the highest level in the fly
visual system are found to be tuned locally within a large velocity range
exclusively by the magnitude of the velocity of the moving pattern and not by its
direction despite the directional selectivity of the neuron itself We will not go
into the question of how this amazing adaptive mechanism may be hard-wired in
the fly visual system Instead we will make advantage of the results derived thus
far and attempt to fit the experimental observations into an image processing
approach A large number of theories and several distinct classes of algorithms to
encode velocity and direction of movement in visual systems have been suggested
by for example Marr and Ullman I I and van Santen and Sperling12
We hypothesize that the adaptive mechanism for the setting of the time
constants leads to an optimization for the overall performance of the visual system
by realizing a velocity independent representation of the moving object In other
words within the range of velocities for which the time constants are found to be
tuned by the velocity the representation of that stimulus at a certain level within
the visual circuitry should remain independent of any variation in stimulus
velocity
OBJECT MOTION DEGRADATION MODELLING
Given the physical description of motion and a linear space invariant model
the motion degradation process can be represented by the following convolution
integral
co co
JJ
flu dudv
where is the object intensity at position in the object coordinate
frame is the Point Spread Function PSF of the imaging system
which is the response at to a unit pulse at and is the image
intensity at the spatial position as blurred by the imaging system Any
possible additive white noise degradation of the already motion blurred image is
neglected in the present considerations
For a review of principles and techniques in the field of digital image
degradation and restoration the reader is referred to Harris 13 Sawchuk
Sondhi Nahi A boutalib 17 18 Hildebrand 19 Rajala de Figueiredo20
It has been demonstrated first by Aboutalib that for situations in which
the motion blur occurs in a straight line along one spatial coordinate say along the
horizontal axis it is correct to look at the blurred image as a collection of
degraded line scans through the entire image The dependence on the vertical
coordinate may then be dropped and eq reduces to
f(u)du
Given the mathematical description of the relative movement
corresponding PSF can be derived exactly and equation becomes
b(x f(u)du
the
where is the extent of the motion blur Typically a discrete version of
applicable for digital image processing purposes is described by
I
where and I take on integer values and is related to the motion blur extent
According to Aboutalib 18 a scalar difference equation model
can then be derived to model the motion degradation process
cmA(i-m
where is the m-dimensional state vector at position along a scan line is
the input intensity at position is the output intensity is the blur extent
is the number of elements in a line is a scalar a and are constant
matrices of order mxl and lxm respectively containing the discrete
values Cj of the blurring PSF for and is the Kronecker delta
function
INFLUENCE OF BOTH TIME CONSTANT AND VELOCITY
ON THE AMOUNT OF MOTION BLUR IN AN ARTIFICIAL
RECEPTOR ARRAY
To start with we incorporate in our simulation model a PSF derived from
equation to model the performance of all neural columnar arranged filters in
the lobula complex with the restriction that the time constants remain fixed
throughout the whole range of stimulus velocities Realization of this PSF can
easily be achieved via the just mentioned state space model
I.
I.
Fig.3
POSITION IN
ARTIFICIAL RECEPTOR ARRAY
upper part Demonstration of the effect that an increase in magnitude of
the time constants of an one-dimensional array of filters will result in
increase in motion blur while the pattern velocity remains constant
Original pattern shown in solid lines is a square-wave grating with a
spatial wavelength equal to artificial receptor distances The three
other wave forms drawn show that for a gradual increase increase in
magnitude of the time constants the representation of the original
square-wave will consequently degrade lower part A gradual increase in
velocity of the moving square-wave while the filter time constants are
kept fixed results also in a clear increase of degradation
First we demonstrate the effect that an increase in time constant while the
pattern velocity remains the same will result in an increase in blur Therefore we
introduce an one dimensional array of filters all being equipped with the same
time constant in their impulse response The original pattern shown in square and
solid lines in the upper part of figure consists of a square wave grating with a
spatial period overlapping artificial receptive filters The other patterns drawn
there show that for the same constant velocity of the moving grating an increase
in the magnitude of the time constants of the filters results in an increased blur in
the representation of that grating On the other hand an increase in velocity
while the time constants of the artificial receptive units remain the same also
results in a clear increase in motion blur as demonstrated in the lower part of
figure
Inspection of the two wave forms drawn by means of the dashed lines in
both upper and lower half of the figure yields the conclusion that apart from
rounding errors introduced by the rather small number of artificial filters
available equal amounts of smear will be produced when the product of time
constant and pattern velocity is equal For the upper dashed wave form the
velocity was four times smaller but the time constant four times larger than for its
equivalent in the lower part of the figure
ADAPTIVE SCHEME
In designing a proper image processing procedure our next step is to
incorporate the experimentally observed flexibility property of the time constants
in the imaging elements of our device In figure 4a a scheme is shown which
filters the information with fixed time constants not influenced by the pattern
velocity In figure 4b a network is shown where the time constants also remain
fixed no matter what pattern movement is presented but now at the next level of
information processing a spatially differential network is incorporated in order to
enhance blurred contrasts
In the filtering network in figure 4c first a measurement of the magnitude
of the velocity of the moving objects is done by thus far hypothetically introduced
movement processing algorithms modelled here as a set of receptive elements
sampling the environment in such a manner that proper estimation of local pattern
velocities can be done Then the time constants of the artificial receptive elements
will be tuned according to the estimated velocities and finally the same
differential network as in scheme 4b is used
The actual tuning mechanism used for our simulations is outlined in figure
once given the range of velocities for which the model is supposed to be
operational and given a lower limit for the time constant min min can be the
smallest value which physically can be realized the time constant will be tuned to
a new value according to the experimentally observed reciprocal relationship and
will for all velocities within the adaptive range be larger than the fixed minimum
value As demonstrated in the previous section the corresponding blur in the
representation of the moving stimulus will thus always be larger than for the
situation in which the filtering is done with fixed and smallest time constants
min More important however is the fact that due to this tuning mechanism the
blur will be constant since the product of velocity and time constant is kept
constant So once the information has been processed by such a system a velocity
independent representation of the image will be the result which can serve as the
input for the spatially differentiating network as outlined in figure 4c
The most elementary form for this differential filtering procedure is the one
in which the gradient of two filters K-I and K+l which are the nearest neighbors
of filter is taken and then added with a constant weighing factor to the central
output as drawn in figure and where the sign of the gradient depends on
the direction of the estimated movement Essential for our model is that we claim
that this weighing factor should be constant throughout the whole set of filters
and for the whole high velocity range in which the heterodyne imaging has to be
performed Important to notice is the existence of a so-called settling time the
minimal time needed for our movement processing device to be able to accurately
measure the object velocity Note this time can be set equal to zero in the case
that the relative stimulus velocity is known a priori as demonstrated in figure
Since without doubt within this settling period estimated velocity values will
come out erroneously and thus no optimal performance of our imaging device can
be expected in all further examples results after this initial settling procedure
will be shown
A
yV
rYO
i~J
t"if
Pattern movement in this figure is to the right
A Network consisting of a set of filters with a fixed pattern velocity
independent time constant in their impulse response
Identical network as in figure 4A now followed by a spatially
differentiating circuitry which adds the weighed gradients of two
neighboring filter outputs K-l and K+I to the central filter output
K.
The time constants of the filtering network are tuned by a
hypothetical movement estimating mechanism visualized here as a
number of receptive elements of which the combined output tunes
the filters A detailed description of this mechanism is shown in
figure This tuned network is followed by an identical spatially
differentiating circuit as described in figure
increasing velocity
decreasing time constant
min
Detailed description of the mechanism used to tune the time constants
The time constant of a specific neural channel is set by the pattern
velocity according to the relationship shown in the insert which is
derived from eq with I and I.
4r
I
I
I
I
I
I
I
I
4V
I
I
a
2V
Wi
8V
I
POSITION IN ARTIFICIAL RECEPTOR ARRAY
Fig.6
Thick lines square-wave stimulus pattern with a spatial wavelength
overlapping 32 artificial receptive elements Thick lines responses for
different pattern velocities in a system consisting of paralleling neural
filters equipped with time constants tuned by this velocity and followed
by a spatially differentiating network as described
Dashed lines responses to the different pattern velocities in a filtering
system with fixed time constants followed by the same spatial
differentiating circuitry as before Note the sharp over and under
shoots for this case
Results obtained with an imaging procedure as drawn in figure and 4c
are shown in figure The pattern consists of a square wave overlapping 32
picture elements The pattern moves to the left with different velocities
At each velocity only one wavelength is shown Thick lines
square wave pattern Dashed lines the outputs of an imaging device as depicted in
figure constant time constants and a constant weighing factor in the spatial
processing stage Note the large differences between the several outputs Thin
continuous lines the outputs of an imaging device as drawn in figure tuned
time constants according to the reciprocal relationship between pattern velocity
and time constant and a constant weighing factor in the spatial processing stage
For further simulation details the reader is referred to Zaagman Now the
outputs are almost completely the same and in good agreement with the original
stimulus throughout the whole velocity range
Figure shows the effect of the gradient weighing factor on the overall
filter performance estimated as the improvement of the deblurred images as
compared with the blurred image measured in dB This quantitative measure has
been determined for the case of a moving square wave pattern with motion blur
IX
ItI
a
weighing factor
Effect of the weighing factor on the overall filter performance Curve
measured for the case of a moving square-wave grating Filter
performance is estimated as the improvement in signal to noise ratio
I:iI
where is the original intensity at position in the image
is the intensity at the same position in the motion blurred image and
is the intensity at in the image generated with the adaptive
tuning procedure
extents comparable to those used for the simulations to be discussed in section IV.
From this curve it is apparent that for this situation there is an optimum value for
this weighing factor Keeping the weight close to this optimum value will result in
a constant output of our adaptive scheme thus enabling an optimal deblurring of
the smeared image of the moving object
On the other hand starting from the point of view that the time constants
should remain fixed throughout the filtering process we should had have to tune
the gradient weights to the velocity in order to produce a constant output as
demonstrated in figure where the dashed lines show strongly differing outputs of
a fixed time constant system with spatial processing with constant weight figure
4b In other words tuning of the time constants as proposed in this section results
in I the realization of the blur-constancy criterion as formulated previously and
as a consequence the possibility to deblur the obtained image oPtimally with
one and the same weighing factor of the gradient in the final spatial processing
layer over the whole heterodyne velocity range
COMPUTER SIMULATION RESULTS AND
CONCLUSIONS
The image quality improvement algorithm developed in the present
contribution has been implemented on a general purpose DG Eclipse Sjl40 minicomputer for our two dimensional simulations Figure Sa shows an undisturbed
image consisting of lines of each pixels with bit intensity resolution
Figure Sb shows what happens with the original image if the PSF is modelled
according to the exponential decay In this case the time constants of all
spatial information processing channels have been kept fixed Again information
content in the higher spatial frequencies has been reduced largely The
implementation of the heterodyne filtering procedure was now done as follows
first the adaptation range was defined by setting the range of velocities This
means that our adaptive heterodyne algorithm is supposed to operate adequately
only within the thus defined velocity range and that in that range the time
constants are tuned according to relationship and will always come out larger
than the minimum value min For demonstration purposes we set Q=I and in
eq thus introducing the phenomenon that for any velocity the two
dimensional set of spatial filters with time constants tuned by that velocity will
always produce a constant output independent of this velocity which introduces
the motion blur Figure Sc shows this representation It is important to note here
that this constant output has far more worse quality than any set of filters with
smallest and fixed time constants min would produce for velocities within the
operational range The advantage of a velocity independent output at this level in
our simulation model is that in the next stage a differential scheme can be
implemented as discussed in detail in the preceding paragraph Constancy of the
weighing factor which is used in this differential processing scheme is guaranteed
by the velocity independency of the obtained image representation
Figure Sd shows the result of the differential operation with an optimized
gradient weighing factor This weighing factor has been optimized based on an
almost identical performance curve as described previously in figure A clear
and good restoration is apparent from this figure though close inspection reveals
fine structure especially for areas with high intensities which is unrelated with
the original intensity distribution These artifacts are caused by the phenomenon
that for these high intensity areas possible tuning errors will show up much more
pronounced than for low intensities
Fig.8a
Fig.8b
8c
Fig.8d
a
Original bit picture
Motion degraded image with a PSF derived from
where is kept fixed to pixels and the motion blur extent is 32
pixels
Worst case the result of motion degradation of the original image
with a PSF as in figure 8b but with tuning of the time constants based
on the velocity
Restored version of the degraded image using the heterodyne adaptive
processing scheme
In conclusion a heterodyne adaptive image processing technique inspired by
the fly visual system has been presented as an imaging device for moving objects
A scalar difference equation model has been used to represent the motion blur
degradation process Based on the experimental results described and on this state
space model we developed an adaptive filtering scheme which produces at a
certain level within the system a constant output permitting further differential
operations in order to produce an optimally deblurred representation of the
moving object
ACKNOWLEDGEMENTS
The authors wish to thank mT Eric Bosman for his expert programming
assistance mr Franco Tommasi for many inspiring discussions and advises during
the implementation of the simulation model and dr Rob de Ruyter van Steveninck
for experimental help This research was partly supported by the Netherlands
Organization lor the Advancement 01 Pure Research through the
foundation Stichting voor Biolysica

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4346-select-and-sample-a-model-of-efficient-neural-inference-and-learning.pdf

Select and Sample A Model of Efficient
Neural Inference and Learning
Jacquelyn A. Shelton J?org Bornschein Abdul-Saboor Sheikh
Frankfurt Institute for Advanced Studies
Goethe-University Frankfurt Germany
shelton,bornschein,sheikh}@fias.uni-frankfurt.de
Pietro Berkes
Volen Center for Complex Systems
Brandeis University Boston USA
J?org Lucke
Frankfurt Institute for Advanced Studies
Goethe-University Frankfurt Germany
berkes@brandeis.edu
luecke@fias.uni-frankfurt.de
Abstract
An increasing number of experimental studies indicate that perception encodes a
posterior probability distribution over possible causes of sensory stimuli which
is used to act close to optimally in the environment One outstanding difficulty
with this hypothesis is that the exact posterior will in general be too complex to
be represented directly and thus neurons will have to represent an approximation
of this distribution Two influential proposals of efficient posterior representation
by neural populations are neural activity represents samples of the underlying distribution or they represent a parametric representation of a variational
approximation of the posterior We show that these approaches can be combined
for an inference scheme that retains the advantages of both it is able to represent
multiple modes and arbitrary correlations a feature of sampling methods and it
reduces the represented space to regions of high probability mass a strength of
variational approximations Neurally the combined method can be interpreted as
a feed-forward preselection of the relevant state space followed by a neural dynamics implementation of Markov Chain Monte Carlo MCMC to approximate
the posterior over the relevant states We demonstrate the effectiveness and efficiency of this approach on a sparse coding model In numerical experiments on
artificial data and image patches we compare the performance of the algorithms
to that of exact EM variational state space selection alone MCMC alone and
the combined select and sample approach The select and sample approach integrates the advantages of the sampling and variational approximations and forms
a robust neurally plausible and very efficient model of processing and learning
in cortical networks For sparse coding we show applications easily exceeding a
thousand observed and a thousand hidden dimensions
Introduction
According to the recently quite influential statistical approach to perception our brain represents
not only the most likely interpretation of a stimulus but also its corresponding uncertainty In
other words ideally the brain would represent the full posterior distribution over all possible interpretations of the stimulus which is statistically optimal for inference and learning a
hypothesis supported by an increasing number of psychophysical and electrophysiological results
Although it is generally accepted that humans indeed maintain a complex posterior representation
one outstanding difficulty with this approach is that the full posterior distribution is in general very
complex as it may be highly correlated due to explaining away effects multimodal multiple
possible interpretations and very high-dimensional One approach to address this problem in neural
circuits is to let neuronal activity represent the parameters of a variational approximation of the real
posterior Although this approach can approximate the full posterior the number of neurons
explodes with the number of variables for example approximation via a Gaussian distribution
requires parameters to represent the covariance matrix over variables Another approach
is to identify neurons with variables and interpret neural activity as samples from their posterior
13 This interpretation is consistent with a range of experimental observations including
neural variability which would result from the uncertainty in the posterior and spontaneous activity
corresponding to samples from the prior in the absence of a stimulus The advantage of
using sampling is that the number of neurons scales linearly with the number of variables and
it can represent arbitrarily complex posterior distributons given enough samples The latter part
is the issue collecting a sufficient number of samples to form such a complex high-dimensional
representation is quite time-costly Modeling studies have shown that a small number of samples
are sufficient to perform well on low-dimensional tasks intuitively this is because taking a lowdimensional marginal of the posterior accumulates samples over all dimensions However
most sensory data is inherently very high-dimensional As such in order to faithfully represent
visual scenes containing potentially many objects and object parts one requires a high-dimensional
latent space to represent the high number of potential causes which returns to the problem sampling
approaches face in high dimensions
The goal of the line of research pursued here is to address the following questions can we find
a sophisticated representation of the posterior for very high-dimensional hidden spaces as this
goal is believed to be shared by the brain can we find a biologically plausible solution reaching it
In this paper we propose a novel approach to approximate inference and learning that addresses the
drawbacks of sampling as a neural processing model yet maintains its beneficial posterior representation and neural plausibility We show that sampling can be combined with a preselection of
candidate units Such a selection connects sampling to the influential models of neural processing
that emphasize feed-forward processing and many more and is consistent with the popular view of neural processing and learning as an interplay between feed-forward and recurrent stages
of processing 19 Our combined approach emerges naturally by interpreting feedforward selection and sampling as approximations to exact inference in a probabilistic framework
for perception
A Select and Sample Approach to Approximate Inference
Inference and learning in neural circuits can be regarded as the task of inferring the true hidden
causes of a stimulus An example is inferring the objects in a visual scene based on the image
projected on the retina We will refer to the sensory stimulus the image as a data point
yD and we will refer to the hidden causes the objects as sH with sh
denoting hidden variablePor hidden unit The data distribution can then be modeled by a generative
data model with denoting the parameters of the model1 If we
assume that the data distribution can be optimally modeled by the generative distribution for optimal
parameters then the posterior probability represents optimal inference given a data
point The parameters given a set of data points yN are given by the
maximum likelihood parameters argmax
A standard procedure to find the maximum likelihood solution is expectation maximization
EM iteratively optimizes a lower bound of the data likelihood by inferring the posterior distribution
over hidden variables given the current parameters the E-step and then adjusting the parameters to
maximize the likelihood of the data averaged over this posterior the M-step The M-step updates
typically depend only on a small number of expectation values of the posterior as given by
hg(~s)ip(~s
where is usually an elementary function of the hidden variables or s~sT
in the case of standard sparse coding For any non-trivial generative model the computation of
In the case of continuous variables the sum is replaced by an integral For a hierarchical model the prior
distribution may be subdivided hierarchically into different sets of variables
expectation values is the computationally demanding part of EM optimization Their exact computation is often intractable and many well-known algorithms rely on estimations
The EM iterations can be associated with neural processing by the assumption that neural activity represents the posterior over hidden variables E-step and that synaptic plasticity implements
changes to model parameters M-step Here we will consider two prominent models of neural processing on the ground of approximations to the expectation values and show how they can be
combined
Selection Feed-forward processing has frequently been discussed as an important component of
neural processing 24 17 One perspective on this early component of neural activity is
as a preselection of candidate units or hypotheses for a given sensory stimulus 26
and many more with the goal of reducing the computational demand of an otherwise too complex
computation In the context of probabilistic approaches it has recently been shown that preselection
can be formulated as a variational approximation to exact inference The variational distribution
in this case is given by a truncated sum over possible hidden states
qn
Kn
Kn
Kn
Kn
where Kn if Kn and zero otherwise The subset Kn represents the preselected
latent states Given a data point Eqn. results in good approximations to the posterior if Kn
contains most posterior mass Since for many applications the posterior mass is concentrated in
small volumes of the state space the approximation quality can stay high even for relatively small
sets Kn This approximation can be used to compute efficiently the expectation values needed in the
M-step
s?Kn
hg(~s)ip(~s hg(~s)iqn
Kn
Eqn. represents a reduction in required computational resources as it involves only summations
integrations over the smaller state space Kn The requirement is that the set Kn needs to be selected
prior to the computation of expectation values and the final improvement in efficiency relies on such
selections being efficiently computable As such a selection function Sh needs to be carefully
chosen in order to define Kn Sh efficiently selects the candidate units sh that are most likely
to have contributed to a data point Kn can then be defined by
Kn for all I sh
where I contains the indices with the highest values of Sh compare For sparse
coding models for instance we can exploit that the posterior mass lies close to low dimensional
subspaces to define the sets Kn and appropriate Sh can be found by deriving efficiently computable upper-bounds for probabilities p(sh or by derivations
based on taking limits for no data noise For more complex models see Sec
for a discussion of suitable selection functions Often the precise form of Sh has limited influence on the final approximation accuracy because its values are not used for the approximation
itself and the size of sets Kn can often be chosen generously to easily contain the regions with
large posterior mass The larger Kn the less precise the selection has to be For Kn equal to the
entire state space no selection is required and the approximations and fall back to the case of
exact inference
Sampling An alternative way to approximate the expectation values in eq is by sampling from
the posterior distribution and using the samples to compute the average
PM
hg(~s)ip(~s
with
The challenging aspect of this approach is to efficiently draw samples from the posterior In a
high-dimensional sample space this is mostly done by Markov Chain Monte Carlo MCMC This
class of methods draws samples from the posterior distribution such that each subsequent sample is
drawn relative to the current state and the resulting sequence of samples form a Markov chain In
the limit of a large number of samples Monte Carlo methods are theoretically able to represent any
probability distribution However the number of samples required in high-dimensional spaces can
be very large sampling
A
MAP estimate
exact EM
preselection
qn
s?Kn
selected units
Sh
select and
sample
Kn
Kn
smax
smax
sampling
with
with
qn
selected units
s1
s1
sH
Wdh
y1
sH
Wdh
yD
y1
yD
Figure A Simplified illustration of the posterior mass and the respective regions each approximation approach uses to compute the expectation values Graphical model showing each connection Wdh between the observed variables and hidden variables and how hidden
variables/units are selected to form a set Kn Graphical model resulting from the selection of
hidden variables and associated weights Wdh black
Select and Sample Although preselection is a deterministic approach very different than the
stochastic nature of sampling its formulation as approximation to expectation values allows for
a straight-forward combination of both approaches given a data point we first approximate
the expectation value using the variational distribution qn as defined by preselection
Second we approximate the expectations qn using sampling The combined approach
is thus given by
PM
with qn
hg(~s)ip(~s hg(~s)iqn
where denote samples from the truncated distribution qn Instead of drawing from a distribution
over the entire state space approximation requires only samples from a potentially very small
subspace Kn In the subspace Kn most of the original probability mass is concentrated in a
smaller volume thus MCMC algorithms perform more efficiently which results in a smaller space
to explore shorter burn-in times and a reduced number of required samples Compared to selection
alone the select and sample approach will represent an increase in efficiency as soon as the number
of samples required for a good approximation is less then the number of states in Kn
Sparse Coding An Example Application
We systematically investigate the computational efficiency performance and biological plausibility
of the select and sample approach in comparison with selection and sampling alone using a sparse
coding model of images The choice of a sparse coding model has numerous advantages First it
is a non-trivial model that has been extremely well-studied in machine learning research and for
which efficient algorithms exist Second it has become a standard albeit somewhat
simplistic model of the organization of receptive fields in primary visual cortex 31 Here
we consider a discrete variant of this model known as Binary Sparse Coding BSC also
compare which has binary hidden variables but otherwise the same features as standard sparse
coding versions The generative model for BSC is expressed by
QH
sh
where RD?H denotes the basis vectors and parameterizes the sparsity and as above
The M-step updates of the BSC learning algorithm are given by
PN
PN
new
h~s iqn
qn
new
ND
q,n new
qn where
xh
The only expectation values needed for the M-step are thus h~siqn and s~sT qn We will compare
learning and inference between the following algorithms
BSCexact An EM algorithm without approximations is obtained if we use the exact posterior for
the expectations qn We will refer to this exact algorithm as BSCexact Although
directly computable the expectation values for BSCexact require sums over the entire state space
over 2H terms For large numbers of latent dimensions BSCexact is thus intractable
BSCselect An algorithm that more efficiently scales with the number of hidden dimensions is
obtained by applying preselection PFor the BSC model we use qn as given in and Kn
for all I sh or
sh Note that in addition to states as in we include all states with one non-zero unit all singletons Including them avoids EM iterations in the
initial phases of learning that leave some basis functions unmodified As selection function Sh to define Kn we use
PD Wdh
Sh
with
as a component
A large value of Sh strongly indicates that contains the basis function
Note that can be related to a deterministic ICA-like selection of a hidden state
in the limit case of no noise compare Further restrictions of the state space are possible
but require modified M-step equations which will not be considered here
BSCsample An alternative non-deterministic approach can be derived using Gibbs sampling Gibbs
sampling is an MCMC algorithm which systematically explores the sample space by repeatedly
drawing samples from the conditional distributions of the individual hidden dimensions In other
words the transition probability from the current sample to a new candidate sample is given by
current
p(snew
s\h
In our case of a binary sample space this equates to selecting one random axis
and toggling its bit value thereby changing the binary state in that dimension
leaving the remaining axes unchanged Specifically the posterior probability computed for each
candidate sample is expressed by
p(sh
p(sh
p(sh p(sh
where we have introduced a parameter that allows for smoothing of the posterior distribution
To ensure an appropriate mixing behavior of the MCMC chains over a wide range of note that
is a model parameter that changes with learning we define where is a temperature
parameter that is set manually and selected such that good mixing is achieved The samples drawn
in this manner can then be used to approximate the expectation values in to using
BSCs+s The EM learning algorithm given by combining selection and sampling is obtained by
applying First note that inserting the BSC generative model into results in
BernoulliKn
Kn
BernoulliKn
Kn
where BernoulliKn h?I sh The remainder of the Bernoulli distribution
cancels out If we define to be the binary vector consisting of all entries of of the selected
RD?H contains all basis functions of those selected we observe that the
dimensions and if
distribution is equal to the posterior a BSC model with instead of hidden dimensions
Bernoulli(~s
Bernoulli(~s
qn
Instead of drawing samples from qn we can thus draw samples from the exact posterior
the BSC generative model with dimensions The sampling procedure for BSCsample can thus
be applied simply by ignoring the non-selected dimensions and their associated parameters For
different data points different latent dimensions will be selected such that averaging over data points
can update all model parameters For selection we again use Sh defining Kn as in
where I now contains the indices with the highest values of Sh and two randomly
selected dimensions drawn from a uniform distribution over all non-selected dimensions The
two randomly selected dimensions fulfill the same purpose as the inclusion of singleton states for
BSCselect Preselection and Gibbs sampling on the selected dimensions define an approximation to
the required expectation values and result in an EM algorithm referred to as BSCs+s
Complexity Collecting the number of operations necessary to compute the expectation values for
all four BSC cases we arrive at
si
sT
where denotes the number of hidden states that contribute to the calculation of the expectation
values For the approaches with preselection BSCselect BSCs+s all the calculations of the expectation values can be performed on the reduced latent space therefore the is replaced by For
BSCexact this number scales exponentially in exact 2H and in in the BSCselect case it scales
exponentially in the number of preselected hidden variables select 2H However for the sampling based approaches BSCsample and BSCs+s the number directly corresponds to the number
of samples to be evaluated and is obtained empirically As we will show later s+s is
a reasonable choice for the interval of that we investigate in this paper
Numerical Experiments
We compare the select and sample approach with selection and sampling applied individually on
different data sets artifical images and natural image patches For all experiments using the two
sampling approaches we draw independent chains that are initialized at random states in order to
increase the mixing of the samples Also of the samples drawn per chain 13 were used to as burn-in
samples and 23 were retained samples
Artificial data Our first set of experiments investigate the select and sample approach?s convergence properties on artificial data sets where ground truth is available As the following experiments
were run on a small scale problem we can compute the exact data likelihood for each EM step in all
four algorithms BSCexact BSCselect BSCsample and BSCs+s to compare convergence on ground
truth likelihood
A
EM step
BSCsample
BSCselect
BSCexact
EM step
EM step
BSCs+s
EM step
Figure Experiments using artificial bars data with Dotted line indicates the
ground truth log-likelihood value A Random selection of the training data points
Learned basis functions Wdh after a successful training run Development of the log-likelihood
over a period of EM steps for all investigated algorithms
gt
Data for these experiments consisted of images generated by creating basis functions
in the form of horizontal and vertical bars on a 36 pixel grid Each bar was randomly
gt
assigned to be either positive Wdh
or negative Whgt0
data points
were generated by linearly combining these basis functions Using
a sparseness value of gt
resulted in on average two active bars per data point According to
the model we added Gaussian noise to the data
We applied all algorithms to the same dataset and monitored the exact likelihood over a period of
EM steps Although the calculation of the exact likelihood requires O(N 2H operations this is feasible for such a small scale problem For models using preselection BSCselect and
BSCs+s we set to effectively halving the number of hidden variables participating in the
calculation of the expectation values For BSCsample and BSCs+s we drew samples from the
posterior of each data point as such the number of states evaluated totaled sample
and s+s respectively To ensure an appropriate mixing
behavior annealing temperature was set to In each experiment the basis functions were
initialized at the data mean plus Gaussian noise the prior probability to init H1 and the data
noise to the variance of the data All algorithms recover the correct set of bases functions in
of the trials and the sparseness prior and the data noise with high accuracy Comparing the
computational costs of algorithms shows the benefits of preselection already for this small scale
problem while BSCexact evaluates the expectation values using the full set of 2H hidden
states BSCselect only considers 2H 70 states The pure sampling based approaches
performs evaluations while BSCs+s requires evaluations
Image patches We test the select and sample approach on natural image data at a more challenging scale to include biological plausibility in the demonstration of its applicability to larger scale
problems We extracted patches of size 26 26 pixels from the van
Hateren image database and preprocessed them using a Difference of Gaussians DoG filter
which approximates the sensitivity of center-on and center-off neurons found in the early stages of
the mammalian visual processing Filter parameters where chosen as in For the following
experiments we ran EM iterations to ensure proper convergence The annealing temperature
was set to
SC
el
SC
of states
ec
le
SC
am
of states
A
Figure Experiments on image patches with 26 26 and A Random
selection of used patches after DoG preprocessing Random selection of learned basis functions
number of samples set to End approx log-likelihood after EM-steps number of
samples per data point Number of states that had to be evaluated for the different approaches
The first series of experiments investigate the effect of the number of drawn samples on the performance of the algorithm as measured by the approximate data likelihood across the entire range
of values between and 36 We observe with BSCs+s that samples per hidden dimension
total states are sufficient the final value of the likelihood after EM steps begins
to saturate Particularly increasing the number of samples does not increase the likelihood by more
than In 3C we report the curve for but the same trend is observed for all other
values of In another set of experiments we used this number of samples in the pure
sampling case BSCsample in order to monitor the likelihood behavior We observed two consistent
trends the algorithm was never observed to converge to a high-likelihood solution and even
when initialized at solutions with high likelihood the likelihood always decreases This example
demonstrates the gains of using select and sample above pure sampling while BSCs+s only needs
samples to robustly reach a high-likelihood solutions by following the same
regime with BSCsample not only did the algorithm poorly converge on a high-likelihood solution
but it used samples to do so
Large scale experiment on image patches Comparison of the above results shows that the most
efficient algorithm is obtained by a combination of preselection and sampling our select and sample approach BSCs+s with no or only minimal effect on the performance of the algorithm as
depicted in and This efficiency allows for applications to much larger scale problems
than would be possible by individual approximation approaches To demonstrate the efficiency of
the combined approach we applied BSCs+s to the same image dataset but with a very high number of observed and hidden dimensions We extracted from the database patches of
size pixels BSCs+s was applied with the number of hidden units set to
and with 34 Using the same conditions as in the previous experiments notably
64 samples and EM iterations we again obtain a set of Gabor-like
basis functions with relatively very few necessary states To our knowledge
the presented results illustrate the largest application of sparse coding with a reasonably complete
representation of the posterior
Discussion
We have introduced a novel and efficient method for unsupervised learning in probabilistic models one which maintains a complex representation of the posterior for problems consistent with
We restricted the set of images to images without man-made structures Fig The brightest
of the pixels were clamped to the max value of the remaining reducing influences of light-reflections
A
of states
BSCselect 2H
BSCs+s
H0
34
Figure A Large-scale application of BSCs+s with 34 to image patches
pixels and hidden dimensions A random selection of the inferred basis functions is
shown Suppl for all basis functions and model parameters Comparison the of computational
complexity BSCselect scales exponentially with whereas BSCs+s scales linearly Note the large
difference at 34 as used in A.
real-world scales Furthermore our approach is biologically plausible and models how the brain
can make sense of its environment for large-scale sensory inputs Specifically the method could
be implemented in neural networks using two mechanisms both of which have been independently
suggested in the context of a statistical framework for perception feed-forward preselection
and sampling 13 We showed that the two seemingly contrasting approaches can be combined based on their interpretation as approximate inference methods resulting in a considerable
increase in computational efficiency Figs
We used a sparse coding model of natural images a standard model for neural response properties
in V1 in order to investigate both numerically and analytically the applicability and efficiency of the method Comparisons of our approach with exact inference selection alone and sampling alone showed a very favorable scaling with the number of observed and hidden dimensions To
the best of our knowledge the only other sparse coding implementation that reached a comparable
problem size assumed a Laplace prior and used a MAP estimation of the
posterior However with MAP estimations basis functions have to be rescaled compare
and data noise or prior parameters cannot be inferred instead a regularizer is hand-set Our method
does not require any of these artificial mechanisms because of its rich posterior representation Such
representations are furthermore crucial for inferring all parameters such as data noise and sparsity
learned in all of our experiments and to correctly act when faced with uncertain input
Concretely we used a sparse coding model with binary latent variables This allowed for a systematic comparison with exact EM for low-dimensional problems but extension to the continuous case
should be straight-forward In the model the selection step results in a simple local and neurally
plausible integration of input data given by We used this in combination with Gibbs sampling
which is also neurally plausible because neurons can individually sample their next state based on
the current state of the other neurons as transmitted through recurrent connections The idea
of combining sampling with feed-forward mechanisms has previously been explored but in other
contexts and with different goals Work by Beal used variational approximations as proposal
distributions within importance sampling and Zhu guided a Metropolis-Hastings algorithm by a data-driven proposal distribution Both approaches are different from selecting subspaces
prior to sampling and are more difficult to link to neural feed-forward sweeps
We expect the select and sample strategy to be widely applicable to machine learning models whenever the posterior probability masses can be expected to be concentrated in a small sub-space of the
whole latent space Using more sophisticated preselection mechanisms and sampling schemes could
lead to a further reduction in computational efforts although the details will depend in general on
the particular model and input data
Acknowledgements We acknowledge funding by the German Research Foundation DFG in the project
LU by the German Federal Ministry of Education and Research BMBF project
JAS JB ASS by the Swartz Foundation and the Swiss National Science Foundation Furthermore
support by the Physics Dept and the Center for Scientific Computing CSC in Frankfurt are acknowledged

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4331-unsupervised-learning-models-of-primary-cortical-receptive-fields-and-receptive-field-plasticity.pdf

Unsupervised learning models of primary cortical
receptive fields and receptive field plasticity
Andrew Saxe Maneesh Bhand Ritvik Mudur Bipin Suresh Andrew Y. Ng
Department of Computer Science
Stanford University
asaxe mbhand rmudur bipins ang}@cs.stanford.edu
Abstract
The efficient coding hypothesis holds that neural receptive fields are adapted to
the statistics of the environment but is agnostic to the timescale of this adaptation
which occurs on both evolutionary and developmental timescales In this work we
focus on that component of adaptation which occurs during an organism?s lifetime and show that a number of unsupervised feature learning algorithms can
account for features of normal receptive field properties across multiple primary
sensory cortices Furthermore we show that the same algorithms account for
altered receptive field properties in response to experimentally altered environmental statistics Based on these modeling results we propose these models as
phenomenological models of receptive field plasticity during an organism?s lifetime Finally due to the success of the same models in multiple sensory areas we
suggest that these algorithms may provide a constructive realization of the theory
first proposed by Mountcastle that a qualitatively similar learning algorithm
acts throughout primary sensory cortices
Introduction
Over the last twenty years researchers have used a number of unsupervised learning algorithms to
model a range of neural phenomena in early sensory processing These models have succeeded in
replicating many features of simple cell receptive fields in primary visual cortex as well as
cochlear nerve fiber responses in the subcortical auditory system Though these algorithms do
not perfectly match the experimental data they continue to improve in recent work
However each phenomenon has generally been fit by a different algorithm and there has
been little comparison of an individual algorithm?s breadth in simultaneously capturing different
types of data In this paper we test whether a single learning algorithm can provide a reasonable
fit to data from three different primary sensory cortices Further we ask whether such algorithms
can account not only for typical data from normal environments but also for experimental data from
animals raised with drastically different environmental statistics
Our motivation for exploring the breadth of each learning algorithm?s applicability is partly biological Recent reviews of the experimental literature regarding the functional consequences of plasticity have remarked on the surprising similarity in plasticity outcomes across sensory cortices
These empirical results raise the possibility that a single phenomenological model of plasticity
learning algorithm in our terminology might account for receptive field properties independent of
modality Finding such a model if it exists could yield broad insight into early sensory processing
strategies As an initial step in this direction we evaluate the match between current unsupervised
learning algorithms and receptive field properties in visual auditory and somatosensory cortex We
find that many current algorithms achieve qualitatively similar matches to receptive field properties
in all three modalities though differences between the models and experimental data remain
In the second part of this paper we examine the sensitivity of these algorithms to changes in their
input statistics Most previous work that uses unsupervised learning algorithms to explain neural
receptive fields makes no claim about the relative contributions of adaptation on evolutionary as
compared to developmental timescales but rather models the end point of these complex processes
that is the receptive field ultimately measured in the adult animal In this work we consider the alternative view that significant adaptation occurs during an organism?s lifetime that the learning
algorithm operates predominantly during development rather than over the course of evolution
One implication of lifetime adaptation is that experimental manipulations of early sensory experience should result in altered receptive field properties We therefore ask whether current unsupervised learning algorithms can reproduce appropriately altered receptive field properties in response
to experimentally altered inputs Our results show that the same unsupervised learning algorithm can
model normal and altered receptive fields yielding an account of sensory receptive fields focused
heavily on activity dependent plasticity processes operating during an organism?s lifetime
Modeling approach
We use the same three stage processing pipeline to model each modality the first stage models peripheral end-receptors namely rods and cones in the retina hair cells in the cochlea and mechanoreceptors in glabrous skin the second stage crudely models subcortical processing as a whitening
transformation of the data and the third stage models cortical receptive field plasticity mechanisms
as an unsupervised learning algorithm We note that the first two stages cannot do justice to the
complexities of subcortical processing and the simple approximation built into these stages limits
the quality of fit we can expect from the models
We consider five unsupervised learning algorithms independent component analysis sparse
autoencoder neural networks restricted Boltzmann machines RBMs K-means and
sparse coding These algorithms were chosen on two criteria First all of the algorithms share the
property of learning a sparse representation of the input though they clearly differ in their details
and have at least qualitatively been shown to yield Gabor-like filters when applied to naturalistic
visual input Second we selected algorithms to span a number of reasonable approaches and popular
formalisms efficient coding ideas backpropagation in artificial neural networks probabilistic
generative models and clustering methods As we will show in the rest of the paper in fact these
five algorithms turn out to yield very similar results with no single algorithm being decisively better
Each algorithm contains a number of parameters which control the learning process which we fit to
the experimental data by performing extensive grid searches through the parameter space To obtain
an estimate of the variability in our results we trained multiple models at each parameter setting but
with different randomly drawn datasets and different initial weights All error bars are the standard
error of the mean The results reported in this paper are for the best-fitting parameter settings for
each algorithm per modality We worried that we might overfit the experimental data due to the
large number of models we trained As one check against this we performed a crossvalidation-like experiment by choosing the parameters of each algorithm to maximize the fit to one
modality and then evaluating the performance of these parameters on the other two modalities We
found that though quantitatively the results are slightly worse as expected qualitatively the results
follow the same patterns of which phenomena are well-fit supplementary material Because
we have fit model parameters to experimental data we cannot assess the efficiency of the resulting
code Rather our aim is to evaluate the single learning algorithm hypothesis which is orthogonal to
the efficient coding hypothesis A learning algorithm could potentially learn a non-efficient code for
instance but nonetheless describe the establishment of receptive fields seen in adult animals Details
of the algorithms parameters and fitting methods can be found in the supplementary information
Results from our grid searches are available at http://www.stanford.edu/?asaxe/rf
plasticity.html
Naturalistic experience and normal receptive field properties
In this section we focus on whether first-order linear properties of neural responses can be captured
by current unsupervised learning algorithms applied to naturalistic visual auditory and somatosensory inputs Such a linear description of neural responses has been broadly studied in all sensory
cortices Though a more complete model would incorporate nonlinear components
these more sophisticated nonlinear models often have as their first step a convolution with a linear
kernel for an overview and it is this kernel which we suggest might be learned over the
course of development by a qualitatively similar learning algorithm across modalities
Figure Top left K-means bases learned from natural images Histograms Black lines show
population statistics for K-means bases gray bars show V1 simple cell data from Macaque Far
right Distribution of receptive field shapes Red triangles are V1 simple cells from blue circles
are K-means bases
Primary visual cortex
A number of studies have shown that response properties in V1 can be successfully modeled using
a variety of unsupervised learning algorithms 19 We replicate these findings for
the particular algorithms we employ and make the first detailed comparisons to experiment for the
sparse autoencoder sparse RBM and K-means algorithms
Our natural image dataset consists of ten gray scale images of outdoor scenes Multiple nonoverlapping patches were sampled to form the first stage of our model meant to approximate the
response of rods and cones This raw data was then whitened using PCA whitening in the second stage of the model corresponding to retinal ganglion or LGN responses.1 These inputs were
supplied to each of the five learning algorithms
shows example bases learned by the K-means algorithm All five algorithms learn localized
band-pass receptive field structures for a broad range of parameter settings in qualitative agreement
with the spatial receptive fields of simple cells in primary visual cortex To better quantify the match
we compare five properties of model neuron receptive fields to data from macaque namely the
spatial frequency bandwidth orientation tuning bandwidth length aspect ratio and peak spatial
frequency of the receptive fields We compare population histograms of these metrics to those
measured in macaque V1 by as reported in shows these histograms for the bestfitting K-means bases according to the average L1 distance between model and data histograms For
all five algorithms the histograms show general agreement with the distribution of parameters in
primary visual cortex except for the peak spatial frequency consistent with the results of previous
studies for ICA and sparse coding Additional plots for the other algorithms can be found in
the supplementary materials
Next we compare the shape of simulated receptive fields to experimentally-derived receptive fields
As had been done for the experimental data we fit Gabor functions to our simulated receptive fields
and calculated the normalized receptive field sizes nx and ny where is the
standard deviation of the gaussian envelope along the axis with sinusoidal modulation is the
stardard deviation of the gaussian envelope along the axis in which the filter is low pass and is
the frequency of the sinusoid The parameters nx and ny measure the number of sinusoidal cycles
that fit within an interval of length and respectively Hence they capture the number of
excitatory and inhibitory lobes of significant power in each receptive field The right panel of
shows the distribution of nx and ny for K-means compared to those reported experimentally
The model bases lie within the experimentally derived values though our models fail to exhibit as
much variability in shape as the experimentally-derived data As had been noted for ICA and sparse
coding in all five of our algorithms fail to capture low frequency bases near the origin These
low frequency bases correspond to blobs with just a single excitatory region
Taking the log of the image intensities before whitening as in yielded similar fits to V1 data
Figure Comparison to A1. Left RBM bases Second from left top Composite MTF in cat
reproduced from Bottom Composite MTF for RBM. Second from right top temporal MTF
in A1 dashed gray and for our model black Bottom spectral MTF. Right top frequency sweep
preference Bottom Spectrum width vs center frequency for A1 neurons red triangles and model
neurons blue circles
Primary auditory cortex
In contrast to the large amount of work in the visual system few efficient coding studies have
addressed response properties in primary auditory cortex but see We base our comparison
on natural sound data consisting of a mixture of data from the Pittsburgh Natural Sounds database
and the TIMIT speech corpus A mix of speech and natural sounds was reported to be necessary
to achieve a good match to auditory nerve fiber responses in previous sparse coding work We
transform the raw sound waveform into a representation of its frequency content over time meant
to approximate the response of the cochlea In particular we pass the input sound signal to a
gammatone filterbank which approximates auditory nerve fiber responses The energy of the
filter responses is then summed within fixed time-bins at regular intervals yielding a representation
similar to a spectrogram We then whiten the data to model subcortical processing Although there
is evidence for temporal whitening in the responses of afferents to auditory cortex this is certainly
a very poor aproximation of subcortical auditory processing After whitening we applied
unsupervised learning models yielding the bases shown in for RBMs These bases map from
our spectrogram input to the model neuron output and hence represent the spectrotemporal receptive
field STRF of the model neurons
We then compared properties of our model STRFs to those measured in cortex First based on
the experiments reported in O?Connor we analyze the relationship between spectrum
bandwidth and center frequency O?Connor found a nearly linear relationship between these
which matches well with the scaling seen in our model bases bottom right Next we
compared model receptive fields to the composite cortical modulation transfer function reported in
The modulation transfer function MTF of a neuron is the amplitude of the 2D Fourier transform of its STRF The STRF contains one spectral and one temporal axis and hence its 2D Fourier
transform contains one spectral modulation and one temporal modulation axis The composite MTF
is the average of the MTFs computed for each neuron and for all five algorithms it has a characteristic inverted shape evident in Summing the composite MTF over time yields the
spectral MTF which is low-pass for our models and well-matched to the spectral MTF reported in
cat Summing over the spectral dimension yields the temporal MTF which is low-pass in
our models but band-pass in the experimental data Finally we investigate the preference of neurons
for upsweeps in frequency versus downsweeps which can be cast in terms of the MTF by measuring
the energy in the left half compared to the right half The difference in these energies normalized
by their sum is the spectrotemporal asymmetry shown in top right All algorithms showed
qualitatively similar distributions of spectrotemporal asymmetry to that found in cat A1. Hence
the model bases are broadly consistent with receptive field properties measured in primary auditory
cortex such as a roughly linear scaling of center frequency with spectrum bandwidth a low-pass
Figure Left Data collection pipeline Center Top two rows sparse autoencoder bases Bottom
two rows first six PCA components Right Histograms of receptive field structure for the sparse
autoencoder algorithm Black model distribution Gray experimental data from Best viewed
in color
spectral MTF of appropriate slope and a similar distribution of spectrotemporal asymmetry The
models differ from experiment in their temporal structure which is band-pass in the experimental
data but low-pass in our models
Primary somatosensory cortex
Finally we test whether these learning algorithms can model somatosensory receptive fields on
the hand To enable this comparison we collected a naturalistic somatosensory dataset meant to
capture the statistics of contact points on the hand during normal primate grasping behavior A
variety of objects were dusted with fine white powder and then grasped by volunteers wearing blue
latex gloves To match the natural statistics of primate grasps we performed the same grip types
in the same proportions as observed ecologically in a study of semi-free ranging Macaca mulatta
Points of contact were indicated by the transfer of powder to the gloved hand which was then
placed palm-up on a green background and imaged using a digital camera The images were then
post-processed to yield an estimate of the pressure applied to the hand during the grasp left
The dataset has a number of limitations it contains no temporal information but rather records all
areas of contact for the duration of the grip Most significantly it contains only individual
grasps due to the high effort required to collect such data minutes/sample and hence is an
order of magnitude smaller than the datasets used for the vision and auditory analyses Given these
limitations we decided to compare our receptive fields to those found in area 3b of primary somatosensory cortex Neurons in area 3b respond to light cutaneous stimulation of restricted regions
of glabrous skin the same sort of contact that would transfer powder to the glove Area 3b
neurons also receive a large proportion of inputs from slowly adapting mechanoreceptor afferents
with sustained responses to static skin indentation making the lack of temporal information
less problematic
Bases learned by the algorithms are shown in These exhibit a number of qualitative features
that accord with the biology As in area the model receptive fields are localized to a single digit
and receptive field sizes are larger on the palm than on the fingers These qualitative
features are not shared by PCA bases which typically span multiple fingers As a more quantitative
assesment we compared model receptive fields on the finger tips to those derived for area 3b neurons
in We computed the ratio between excitatory and inhibitory area for each basis and plot a
population histogram of this ratio shown for the sparse autoencoder algorithm in the right panel of
Importantly because this comparison is based on the ratio of the areas it is not affected by the
unknown scale factor between the dimensions of our glove images and those of the macaque hand
We also plot the ratio of the excitatory and inhibitory mass where excitatory and inhibitory mass is
defined as the sum of the positive and negative coefficients in the receptive field respectively We
find good agreement for all the algorithms we tested
Figure Top row Input image Resulting goggle image reproduced from Our simulated
goggle image Bottom row Natural image Simulated goggle image Bases learned by sparse
coding Right Orientation histogram for model neurons is biased towards goggle orientation
Adaptation to altered environmental statistics
Numerous studies in multiple sensory areas and species document plasticity of receptive field properties in response to various experimental manipulations during an organism?s lifetime In visual
cortex for instance orientation selectivity can be altered by rearing animals in unidirectionally oriented environments In auditory cortex pulsed-tone rearing results in an expansion in the area
of auditory cortex tuned to the pulsed tone frequency And in somatosensory cortex surgically
fusing digits and the middle and ring fingers of the hand to induce an artificial digital syndactyly
webbed finger condition results in receptive fields that span these digits In this section we
ask whether the same learning algorithms that explain features of normal receptive fields can also
explain these alterations in receptive field properties due to manipulations of sensory experience
Goggle-rearing alters V1 orientation tuning
The preferred orientations of neurons in primary visual cortex can be strongly influenced by altering
visual inputs during development Tanaka fitted goggles that severly restricted orientation
information to kittens at postnatal week three and documented a massive overrepresentation of the
goggle orientation subsequently in primary visual cortex Hsu and Dayan have shown
that an unsupervised learning algorithm the product-of-experts model closely related to ICA can
reproduce aspects of the goggle-rearing experiment Here we follow their methods extending the
analysis to the other four algorithms we consider
To simulate the effect of the goggles on an input image we compute the 2D Fourier transform of
the image and remove all energy except at the preferred orientation of the goggles We slightly
blur the resulting image with a small Gaussian filter Because the kittens receive some period of
natural experience we trained the models on mixtures of patches from natural and altered images
adding one parameter in addition to the algorithmic parameters shows resulting receptive
fields obtained using the sparse coding algorithm After learning the preferred orientations of the
bases were derived using the analysis described in Section All five algorithms demonstrated an
overrepresentation of the goggle orientation consistent with the experimental data
Pulsed-tone rearing alters A1 frequency tuning
Early sensory experience can also profoundly alter properties of neural receptive fields in primary
auditory cortex Along similar lines to the results for V1 in Section early exposure to a pulsed
tone can induce shifts in the preferred center frequency of A1 neurons In particular de VillersSidani raised rats in an environment with a free field speaker emitting a tone with amplitude modulation that repeatedly cycled on for then off for Mapping the preferred
center frequencies of neurons in tone-exposed rats revealed a corresponding overrepresentation in
A1 around the pulsed-tone frequency
We instantiated this experimental paradigm by adding a pulsed tone to the raw sound waveforms
of the natural sounds and speech before computing the gammatone responses Example bases for
ICA are shown in the center panel of many of which are tuned to the pulsed-tone frequency
We computed the preferred frequency of each model receptive field by summing the square of each
patch along the temporal dimension The right panel of shows population histograms of the
Figure Left Example spectrograms before and after adding a 4kHz pulsed tone Center ICA
bases learned from pulsed tone data Right Population histograms of preferred frequency reveal a
strong preference for the pulsed-tone frequency of 4kHz
preferred center frequencies for models trained on natural and pulsed-tone data for ICA and Kmeans We find that all algorithms show an overrepresentation in the frequency band containing the
tone in qualitative agreement with the results reported in Intuitively this overrepresentation
is due to the fact that many bases are necessary to represent the temporal information present in
the pulsed-tone that is the phase of the amplitude modulation and the onset or offset time of the
stimulus
Artificial digital syndactyly in S1
Allard surgically fused adjacent skin on
digits and in adult owl monkeys to create an artificial sydactyly or webbed finger condition After
or 33 weeks many receptive fields of neurons in area 3b of S1 were found to span digits
and a qualitative change from the normally strict
localization of receptive fields to a single digit Additionally at the tips of digits and where there
is no immediately adjacent skin on the other digit
some neurons showed discontinuous double-digit receptive fields that responded to stimulation on either Figure Bases trained on artificial synfinger tip In contrast to the shifts in receptive dactyly data Top row Sparse coding Botfield properties described in the preceding two sec tom row K-means
tions these striking changes are qualitatively different and as such provide an important test for functional models of plasticity
We modeled the syndactyly condition by fusing digits and of our gloves and collecting additional grip samples according to the method in Section Bases learned from this syndactyly
dataset are shown in All models learned double-digit receptive fields that spanned digits
and in qualitative agreement with the findings reported in Additionally a small number
of bases contained discontinuous double-digit receptive fields consisting of two well-separated excitatory regions on the extreme finger tips top right In contrast to the experimental
findings model receptive fields spanning digits and also typically have a discontinuity along the
seam We believe this reflects a limitation of our dataset although digits and of our data collection glove are fused together and must move in concert the seam between these digits remains inset
from the neighboring fingers and hence grasps rarely transfer powder to this area In the experiment
the skin was sutured to make the seam flush with the neighboring fingers
Discussion
Taken together our results demonstrate that a number of unsupervised learning algorithms can account for certain normal and altered linear receptive field properties across multiple primary sensory
cortices Each of the five algorithms we tested obtained broadly consistent fits to experimental data
in A1 and S1. Although these fits were not perfect?notably missing blob receptive fields
in V1 and bandpass temporal structure in A1?they demonstrate the feasibility of applying a single
learning algorithm to experimental data from multiple modalities
In no setting did one of our five algorithms yield qualitatively different results from any other This
finding likely reflects the underlying similarities between the algorithms which all attempt to find
a sparse representation of the input while preserving information about it The relative robustness
of our results to the details of the algorithms offers one explanation of the empirical observation of
similar plasticity outcomes at a functional level despite potentially very different underlying mechanisms Even if the mechanisms differ provided that they still incorporate some version of
sparsity they can produce qualitatively very similar outcomes
The success of these models in capturing the effects of experimental manipulations of sensory input
suggests that the adaptation of receptive field properties to natural statistics as proposed by efficient
coding models may occur significantly on developmental timescales If so this would allow the
extensive literature on plasticity to constrain further modeling efforts
Furthermore the ability of a single algorithm to capture responses in multiple sensory cortices shows
that in principle a qualitatively similar plasticity process could operate throughout primary sensory
cortices Experimentally such a possibility has been addressed most directly by cortical rewiring
experiments where visual input is rerouted to either auditory or somatosensory cortex 31 32
33 34 In neonatal ferrets visual input normally destined for lateral geniculate nucleus can
be redirected to the auditory thalamus which then projects to primary auditory cortex Roe
and Sharma found that rewired ferrets reared to adulthood had neurons in auditory
cortex responsive to oriented edges with orientation tuning indistinguishable from that in normal
V1. Further Von Melchner found that rewired auditory cortex can mediate behavior such
as discriminating between different grating stimuli and navigating toward a light source Rewiring
experiments in hamster corroborate these results and in addition show that rewiring visual input to
somatosensory cortex causes S1 to exhibit light-evoked responses similar to normal V1
Differences between rewired and normal cortices do exist?for example the period of the orientation
map is larger in rewired animals However these experiments are consistent with the hypothesis
that sensory cortices share a common learning algorithm and that it is through activity dependent
development that they specialize to a specific modality Our results provide a possible explanation
of these experiments as we have shown constructively that the exact same algorithm can produce
or S1-like receptive fields depending on the type of input data it receives
Acknowledgements We give warm thanks to Andrew Maas Cynthia Henderson Daniel Hawthorne
and Conal Sathi for code and ideas This work is supported by the DARPA Deep Learning program
under contract number Andrew Saxe is supported by a NDSEG and Stanford
Graduate Fellowship

<<----------------------------------------------------------------------------------------------------------------------->>

title: 224-acoustic-imaging-computations-by-echolocating-bats-unification-of-diversely-represented-stimulus-features-into-whole-images.pdf

Simmons
Acoustic-Imaging Computations by Echolocating Bats
Unification of Diversely-Represented Stimulus
Features into Whole Images
James A. Simmons
Department of Psychology
and Section of Neurobiology
Division of Biology and Medicine
Brown University Providence RI
ABSTRACT
The echolocating bat Eptesicus fuscus perceives the distance to
sonar targets from the delay of echoes and the shape of targets
from the spectrum of echoes However shape is perceived in
terms of the target's range proftle The time separation of echo
components from parts of the target located at different distances
is reconstructed from the echo spectrum and added to the
estimate of absolute delay already derived from the arrival-time
of echoes The bat thus perceives the distance to targets and
depth within targets along the same psychological range
dimension which is computed The image corresponds to the
crosscorrelation function of echoes Fusion of physiologically
distinct time and frequency-domain representations into a fmal
common time-domain image illustrates the binding of withinmodality features into a unified whole image To support the
structure of images along the dimension of range bats can
perceive echo delay with a hyperacuity of nanoseconds
Acoustic-Imaging Computations by Echolocating Bats
THE SONAR
BATS
Bats are flying mammals whose lives are largely nocturnal They have evolved
the capacity to orient in darkness using a biological sonar called echolocation
which they use to avoid obstacles to flight and to detect identify and track flying
insects for interception Griffm Echolocating bats emit brief mostly
ultrasonic sonar sounds and perceive objects from echoes that return to their ears
The bat's auditory system acts as the sonar receiver processing echoes to
reconstruct images of the objects themselves
Many bats emit frequencymodulated signals the big brown bat Eptesicus fuscus transmits sounds
with durations of several milliseconds containing frequencies from about to
kHz arranged in two or three hannonic sweeps The images that
Eptesicus ultimately perceives retain crucial features of the original sonar wave100
I
Figure I Spectrogram of a
sonar sound emitted by the
big brown bat Eptesicus
fuscus Simmons
cO
msec
forms thus revealing how echoes are processed to reconstruct a display of the
object itself Several important general aspects of perception are embodied in
specific echo-processing operations in the bat's sonar By recognizing constraints
imposed when echoes are encoded in terms of neural activity in the bat's auditory
system recent experiments have identified a nove use of time and frequencydomain techniques as the basis for acoustic imaging in FM echolocation The
intrinsically reciprocal properties of time and frequency-domain representations
are exploited in the neural algorithms which the bat uses to unify disparate
features into whole images
IMAGES OF SINGLE-GI.JNT TARGETS
A simple sonar target consists of a single reflecting point or glint located at a
discrete range and reflecting a single replica of the incident sonar signal A
complex target consists of several glints at slightly different ranges It thus reflects
compound echoes composed of individual replicas of the incident sound arriving
Simmons
at slightly different delays To dctennine the distance to a target or target range
echolocating bats estimate the delay of echoes Simmons The bat's image
of a single-glint target is constructed around its estimate of echo delay and the
shape of the image can be measured behaviorally The performance of bats
trained to discriminate between echoes that jitter in delay and echoes that are
stationary in delay yields a graph of the image itself Altes together with
an indication of the accuracy of the delay estimate that underlies it Simmons
Simmons Perragamo Moss Stevenson Altes in press shows
Jitter Performonce
Crasscorrelatian Function
Time mIcroseconds
JO
JO
Time microseconds
Figure Graphs showing the bat's image of a single-glint target
from jitter discrimination experilnents left for comparison with
the crosscorrelation function of echoes right The zero point
on each time axis corresponds to the objective arrival-time of the
echoes about msec in this experiment Sinlmons Perragamo
in press
the image of a single-glint target perceived by Eptesicus expressed in terms of
echo delay Ilsec/cm of range
Prom the bat's jitter discrimination
performance the target is perceived at its true range Also the image has a fme
structure consisting of a central peak corresponding to the location of the target
and two prominent side-peaks as ghost images located about 35 lsec or cm
nearer and farther than the main peak This image fme structure reflects the
composition of the waveform of the echoes themselves it approximates the
crosscorrelation function of echoes
The discovery that the bat perceives an image corresponding to the crosscorrelation function of echoes provides a view of the hidden machinery of the
bat's sonar receiver The bat's estimate of echo delay evidently is based upon a
capacity of the auditory system to represent virtually all of the information
available in echo waveforms that is relevant to determining delay including the
phase of echoes relative to emissions Simmons Ferragamo al in press The
bat's initial auditory representation of these FM signals resembles spectrograms
Acoustic-Imaging Computations by Echolocating Bats
that consist of neural impulses marking the time-of-occurrence of succeSSlve
frequencies in the FM sweeps of the sounds Each nerve
I
I
I
time msec
Hgure Neural spectrograms
representing a sonar emission
left and an echo from a target
located about I away right
The individual dots are neural
impulses
conveying
the
instantaneous frequency of the
FM sweeps The 6msec time separation of the two
spectrograms indicates target
range in the bat's sonar receiver
Simmons Kick
pulse travels in a channel that is tuned to a particular excitatory frequency
Bodenhamer Pollak as a consequence of the frequency analyzing
properties of the cochlea The cochlear filters are followed by rectification and
low-pass filtering so in a conventional sense the phase of the filtered signals is
destroyed in the course of forming the spectrograms However shows that
the bat is able to reconstruct the crosscorrclation function of echoes from its
spectrogram-like auditory representation The individual neural points in the
spectrogram signify instantaneous frequency and the recovery of the fIne
structure in the image may exploit properties of instantaneous frequency when
the images are assembled by integrating numerous separate delay measurements
across different frequencies The fact that the crosscorrelation function emerges
from these neural computations is provocative from theoretical and technological
viewpoints--the bat appears to employ novel real-time algorithms that can
transform echoes into spectrograms and then into the sonar ambiguity function
itself
The range-axis image of a single-glint target has a fIne structure surrounding a
central peak that constitutes the bat's estimate of echo delay The width
of this peak corresponds to the limiting accuracy of the bat's delay estimate
allowing for the ambiguity represented by the side-peaks located about 35 Jlsec
away In the data-points arc spaced Jlsec apart along the time axis
approximately the Nyquist sampling interval for the bat's signals and the true
width of the central peak is poorly shown shows the performance of three
Eptesicus in an experiment to measure this width with smaller delay steps The
Simmons
90
70
Oeloy line
Bot I
Bot.
Bot.
Cable
Bat.3
Bot'5
35 45 55
TIme nanosetonds
Figure A graph of the
pelformance
of
Eptesicus
discriminating
echo-delay
jitters that change small
steps
The bats limiting
acuity IS about nsec for
correct
responses
Simmons Perragamo
in press
bats can detect a shift of as little as nsec as a hyperacuity Altes for
echo delay in the jitter task In estimating echo delay the bat must integrate
spectrogram delay estimates across separate frequencies in the FM sweeps of
emissions and echoes and it arrives at a very accurate composite
estimate indeed Timing accuracy in the nanosecond range is a previously
unsuspected capahility of the nervous system and it is likely that more complex
algorithms than just integration of information across frequencies lie behind this
fine acuity below on amplitude-latency trading and perceived delay
IMAGES
lWO-GLINT TARGETS
Complex targets such as airborne insects reflect echoes composed of several
replicas of the incident sound separated by short intervals of time Simmons
Chen Por insect-sized targets with dimensions of a few centimeters this
time separation of echo components is unlikely to exceed to Jlsec
Because the bat's signals arc several milliseconds long the echoes from complex
targets thus will contain echo components that largely overlap The auditory
system of Eptesicus has an integration-time of about Jlsec for reception of
sonar echoes Simmons Freedman Two echo components that
arrive together within this integration-time will merge together into a single
compound echo having an arrival-time as a whole that indicates the delay of the
first echo component and having a series of notches in its spectrum that indicates
the time separation of the first and second components In the bat's auditory
representation echo delay corresponds to the time separation of the emission and
echo spectrograms while the notches in the compound echo
spectrum appear as in the spectrogram--that is as frequencies that fail to
appear in echoes The location and spacing of these notches or holes in
frequency is related to the separation of the two echo components in lime The
crucial point is that the constraint imposed by the 350-Jlsec integration-time for
echo reception disperses the information required to reconstruct the detailed range
Acoustic-Imaging Computations by Echolocating Bats
structure of the complex target into both the time and the frequency dimensions
of the neural spectrograms
FptesicuJ extracts an estimate of the overall delay of the waveform of compound
echoes from two-glint targets This time estimate leads to a range-axis image of
the closer of the two glints in the target the target's leading edge This part of
the image exhibits the same properties as the image of a single-glint target--it is
encoded by the time-of-occurrence of neural discharges in the spectrograms and it
resembles the crosscorrclation function for the first echo component Simmons
Moss Perragamo Simmons Ferragamo in press see Simmons
The bat also perceives a range-axis image of the farther of the two glints
the target's trailing edge This image is located at a perceived distance that
corresponds to the bat's estimate of the time separation of the two echo
components that make up the compound echo shows the performance of
EpleJicuJ in a jitter discrimination experiment in which one of the
a'i
I
I
I
lime psec
I
Figure A graph comparing
the crosscorrelation function of
echoes from a two-glint target
with a delay separation of
Jlsec top with the bat's jitter
discrimination
performance
using tlus compound echo as a
stimulus bottom The two
glints arc indicated as a I and
aI Simmons
jittering stimulus echoes contained two replicas of the bat's emitted sound
separated by Jlsec The bat perceives two distinct reflecting points along the
range axis Both glints appear as events along the range axis in a time-domain
image even though the existence of the second glint could only be inferred from
the frequency domain because the delay separation of Jlsec is much shorter
than the receiver's integration time The image of the second glint resembles the
crosscorrelation function of the later of the two echo components The bat adds
it to the crosscorrelation function for the earlier component when the whole
image is formed
Simmons
ACOUSTIC-IMA(;E PROCESSING BY FM BATS
Somehow Eptesicus recovers sufficient information from the timing of neural
discharges across the frequencies in the PM sweeps of emissions and echoes to
reconstruct the crosscorrelation function of echoes from the flfst glint in the
complex target and to estimate delay with nanosecond accuracy
This
fundamentally time-domain image is derived from the processing of information
initially also represented in the time domain as demonstrated by the occurrence
of changes in apparent delay as echo amplitude increases or decreases The
location of the perceived crosscorrelation function for the flfst glint can be shifted
by predictable amounts along the time axis according to the separately-measured
amplitude-latency trading relation for Eptesicus about 17 lsec/dB Simmons
Moss Perragamo Simmons Ferragamo in press indicating that
neural response latency--that is neural discharge timing--conveys the crucial
information about delay in the bat's auditory system
The second glint in the complex target manifests itself as a crosscorrelation-like
image component too However the bat must transform spectral information
into the time domain to arrive at such a time or range-axis representation for the
second glint This transformed time-domain image is added to the time-domain
image for the first glint in such a way that the absolute range of the second glint
is referred to that of the first glint Shifts in the apparent range of the flfst glint
caused by neural discharges undergoing amplitude-latency trading will carry the
image of the second glint along with it to a new range value Simmons Moss
Perragamo Evidently the psychological dimension of absolute range
supports the image of the target as a whole This helps to explain the bat's
extraordinary IO-nsec accuracy for perceiving delay For the psychological range
or delay axis to accept fine-grain range infonnation about the separation of glints
in complex targets its intrinsic accuracy must be adequate to receive the
information that is transformed from the frequency domain The bat achieves
fusion of image components by transfonning one component into the numerical
fonnat for the other and then adding them together
The experimental
dissociation of the images of the first and second glints from different effects of
latency shifts demonstrates the independence of their initial physiological
representations Furthennore the expected latency shift does not occur for
frequencies whose amplitudes are low because they coincide with spectral
notches the bat's fine nanosecond acuity thus seems to involve removal of
discharges at untrustworthy frequencies prior to integration of discharge timing
across frequencies The delay-tuning of neurons is usually thought to represent
the conversion of a temporal code timing of neural discharges into a place
code the location of activity on the neural map The bat's unusual acuity of
nsec suggests that this conversion of a temporal to a place code is only partial
Acoustic-Imaging Computations by EchoIocating Bats
Not. only does the site of activity on the neural map convey information about
delay but the timing of discharges in map neurons may also play a critical role in
the map-reading operation The bat's fIne acuity may emerge in the behavioral
data because initial neural encoding of the stimulus conditions in the jitter task
involves the same parameter of neural rcsponses--timing--that later is intimately
associated with map-reading in the brain Echolocation may thus fortuitously be
a good system in which to explore this basic perceptual process
Ackllowledgmen ts
Research supported by grants from ONR NIH NIMH ORF and SOF.

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6257-joint-line-segmentation-and-transcription-for-end-to-end-handwritten-paragraph-recognition.pdf

Joint Line Segmentation and Transcription for
End-to-End Handwritten Paragraph Recognition
Th?odore Bluche
A2iA SAS
39 rue de la Bienfaisance
Paris
tb@a2ia.com
Abstract
Offline handwriting recognition systems require cropped text line images for both
training and recognition On the one hand the annotation of position and transcript
at line level is costly to obtain On the other hand automatic line segmentation
algorithms are prone to errors compromising the subsequent recognition In this
paper we propose a modification of the popular and efficient Multi-Dimensional
Long Short-Term Memory Recurrent Neural Networks MDLSTM-RNNs to
enable end-to-end processing of handwritten paragraphs More particularly we
replace the collapse layer transforming the two-dimensional representation into
a sequence of predictions by a recurrent version which can select one line at a
time In the proposed model a neural network performs a kind of implicit line
segmentation by computing attention weights on the image representation The
experiments on paragraphs of Rimes and IAM databases yield results that are
competitive with those of networks trained at line level and constitute a significant
step towards end-to-end transcription of full documents
Introduction
Offline handwriting recognition consists in recognizing a sequence of characters in an image of
handwritten text Unlike printed texts images of handwriting are difficult to segment into characters
Early methods tried to compute segmentation hypotheses for characters for example by performing a
heuristic over-segmentation followed by a scoring of groups of segments in In the nineties
this kind of approach was progressively replaced by segmentation-free methods where a whole
word image is fed to a system providing a sequence of scores A lexicon constrains a decoding step
allowing to retrieve the character sequence Some examples are the sliding window approach in
which features are extracted from vertical frames of the line image or space-displacement neural
networks In the last decade word segmentations were abandoned in favor of complete text line
recognition with statistical language models
Nowadays the state of the art handwriting recognition systems are Multi-Dimensional Long ShortTerm Memory Recurrent Neural Networks MDLSTM-RNNs which consider the whole image
alternating MDLSTM layers and convolutional layers The transformation of the 2D structure into
a sequence is computed by a simple collapse layer summing the activations along the vertical axis
Connectionist Temporal Classification CTC allows to train the network to both align and
recognize sequences of characters These models have become very popular and won the recent
evaluations of handwriting recognition 34
However current models still need segmented text lines and full document processing pipelines
should include automatic line segmentation algorithms Although the segmentation of documents
into lines is assumed in most descriptions of handwriting recognition systems several papers or
Conference on Neural Information Processing Systems NIPS Barcelona Spain
surveys state that it is a crucial step for handwriting text recognition systems The need
of line segmentation to train the recognition system has also motivated several efforts to map a
paragraph-level or page-level transcript to line positions in the image recently
Handwriting recognition systems evolved from character to word segmentation and to complete
line processing nowadays The performance has always improved by making less segmentation
hypotheses In this paper we pursue this traditional tendency We propose a model for multiline recognition based on the popular MDLSTM-RNNs augmented with an attention mechanism
inspired from the recent models for machine translation image caption generation or speech
recognition In the proposed model the collapse layer is modified with an attention
network providing weights to modulate the importance given at different positions in the input By
iteratively applying this layer to a paragraph image the network can transcribe each text line in turn
enabling a purely segmentation-free recognition of full paragraphs
We carried out experiments on two public datasets of handwritten paragraphs Rimes and IAM. We
report results that are competitive with the state-of-the-art systems which use the ground-truth line
segmentation The remaining of this paper is organized as follows Section presents methods related
to the one presented here in terms of the tackled problem and modeling choices In Section we
introduce the baseline model MDLSTM-RNNs We expose in Section the proposed modification
and we give the details of the system Experimental results are reported in Section and followed by
a short discussion in Section in which we explain how the system could be improved and present
the challenge of generalizing it to complete documents
Related Work
Our work is clearly related to MDLSTM-RNNs which we improve by replacing the simple
collapse layer by a more elaborated mechanism itself made of MDLSTM layers The model we
propose iteratively performs an implicit line segmentation at the level of intermediate representations
Classical text line segmentation algorithms are mostly based on image processing techniques and
heuristics However some methods were devised using statistical models and machine learning
techniques such as hidden Markov models conditional random fields or neural networks
31 In our model the line segmentation is performed implicitly and integrated in the neural
network The intermediate features are shared by the transcription and the segmentation models and
they are jointly trained to minimize the transcription error
Recently many attention-based models were proposed to iteratively select in an encoded signal
the relevant parts to make the next prediction This paradigm already suggested by Fukushima
in was successfully applied to various problems such as machine translation image
caption generation speech recognition or cropped words in scene text Attention
mechanisms were also parts of systems that can generate or recognize small pieces of handwriting
a few digits with DRAW or RAM or short online handwritten sequences Our
system is designed to handle long sequences and multiple lines
In the field of computer vision and particularly object detection and recognition many neural
architectures were proposed to both locate and recognize the objects such as OverFeat or spatial
transformer networks STN In a sense our model is quite related to the DenseCap model for
image captioning itself similar to STNs However we do not aim at explicitly predicting line
positions and STNs are not as good with a large amount of small objects
We recently proposed an attention-based model to transcribe full paragraphs of handwritten text
which predicts each character in turn Outputting one token at a time turns out to be prohibitive in
terms of memory and time consumption for full paragraphs which typically contain about hundreds
of characters In the proposed system the encoded image is not summarized as a single vector at each
timestep but as a sequence of vectors representing full text lines It represents a huge speedup and
a comeback to the original MDLSTM-RNN architecture in which the collapse layer is augmented
with an MDLSTM attention network similar to the one presented in
Handwriting Recognition with MDLSTM and CTC
MDLSTM-RNNs were first introduced in the context of handwriting recognition The Multi2
Figure MDLSTM-RNN architecture for handwriting recognition LSTM layers in four scanning
directions are followed by convolutions The feature maps of the top layer are are summed in the
vertical dimension and character predictions are obtained after a softmax normalization
Dimensional Long Short-Term Memory layers scan the input in the four possible directions The
LSTM cell inner state and output are computed from the states and outputs of previous positions in
the considered horizontal and vertical directions Each MDLSTM layer is followed by a convolutional
layer At the top of this network there is one feature map for each character These maps are collapsed
into a sequence of prediction vectors normalized with a softmax activation The whole architecture
is depicted in Figure The Connectionist Temporal Classification CTC algorithm which
considers all possible labellings of the sequence may be applied to train the network to recognize
text lines
The 2D to 1D conversion happens in the collapsing layer which computes a simple aggregation of
the feature maps into vector sequences maps of height This is achieved by a simple sum across
the vertical dimension
zi
aij
where zi is the i-th output vector and aij is the input feature vector at coordinates All the
information in the vertical dimension is reduced to a single vector regardless of its position in the
feature maps preventing the recognition of multiple lines within this framework
An Iterative Weighted Collapse for End-to-End Handwriting Recognition
In this paper we replace the sum of Eqn. by a weighted sum in order to focus on a specific part of
the input The weighted collapse is defined as follows
zi
aij
where are scalar weights between and computed at every time for each position The
weights are provided by a recurrent neural network illustrated in Figure enabling the recognition
of a text line at each timestep
Figure Proposed modification of the collapse layer While the standard collapse left top computes
a simple sum the weighted collapse right bottom includes a neural network to predict the weights
of a weighted sum
This collapse weighted with a neural network may be interpreted as the attention module of an
attention-based neural network similar to those of This mechanism is differentiable and can
be trained with backpropagation The complete architecture may be described as follows
An encoder extracts feature maps from the input image I
a aij Encoder(I
where are coordinates in the feature maps In this work the Encoder module is an MDLSTM
network with same architecture as the model presented in Section
A weighted collapse provides a view of the encoded image at each timestep in the form of a weighted
sum of feature vector sequences The attention network computes a score for the feature vectors at
every position
Attention(a
We refer to as the attention map at time which computation depends
not only on the encoded image but also on the previous attention features A softmax normalization
is applied to each column
e?ij
ij0
j0
In this work the Attention module is an MDLSTM network
This module is applied several times to the features from the encoder The output of the attention
module at iteration computed with Eqn. is a sequence of feature vectors intended to represent
a text line Therefore we may see this module as a soft line segmentation neural network The
advantages over the neural networks trained for line segmentation 24 32 are that it works
on the same features as those used for the transcription multi-task encoder and it is trained to
maximize the transcription accuracy more closely related to the goal of handwriting recognition
systems and easily interpretable
A decoder predicts a character sequence from the feature vectors
Decoder(z
where is the concatenation of Alternatively the decoder may be applied to
sub-sequences to get and is the concatenation of
In the standard MDLSTM architecture of Section the decoder is a simple softmax However a
Bidirectional LSTM BLSTM decoder could be applied to the collapsed representations This is
particularly interesting in the proposed model as the BLSTM would potentially process the whole
paragraph allowing a modeling of dependencies across text lines
This model can be trained with CTC. If the line breaks are known in the transcript the CTC could
be applied to the segments corresponding to each line prediction Otherwise one can directly apply
CTC to the whole paragraph In this work we opted for that strategy with a BLSTM decoder applied
to the concatenation of all collapsing steps
Experiments
Experimental Setup
We carried out the experiments on two public databases The IAM database is made of
handwritten English texts copied from the LOB corpus There are documents lines in the
training set documents lines in the validation set and documents lines in the
test set The Rimes database contains handwritten letters in French The data consist of a training
set of paragraphs lines and a test set of paragraphs lines We held out the
last paragraphs of the training set as a validation set
The networks have the following architecture The encoder first computes a tiling of the input
and alternate MDLSTM layers of and units and convolutions of and 32 filters
with no overlap The last layer is a linear layer with outputs for IAM and for Rimes The
attention network is an MDLSTM network with units in each direction followed by a linear
layer with one output and a softmax on columns Eqn The decoder is a BLSTM network with
units Dropout is applied after each LSTM layer The networks are trained with RMSProp
with a base learning rate of and mini-batches of examples to minimize the CTC loss over
entire paragraphs The measure of performance is the Character Word Error Rate
corresponding to the edit distance between the recognition and ground-truth normalized by the
number of ground-truth characters
Impact of the Decoder
In our model the weighted collapse method is followed by a BLSTM decoder In this experiment
we compare the baseline system standard collapse followed by a softmax with the proposed model
In order to dissociate the impact of the weighted collapse from that of the BLSTM decoder we also
trained an intermediate architecture with a BLSTM layer after the standard collapse but still limited
to text lines
Table Character Error Rates of CTC-trained RNNs on dpi images The Standard models
are trained on segmented lines The Attention models are trained on paragraphs
Collapse
Standard
Standard
Attention
Decoder
Softmax
BLSTM Softmax
BLSTM Softmax
IAM
Rimes
The character error rates on the validation sets are reported in Table for images
We observe that the proposed model outperforms the baseline by a large margin relative
improvement on IAM on Rimes and that the gain may be attributed to both the BLSTM
decoder and the attention mechanism
Impact of Line Segmentation
Our model performs an implicit line segmentation to transcribe paragraphs The baseline considered
in the previous section is somehow cheating because it was evaluated on the ground-truth line
segmentation In this experiment we add to the comparison the baseline models evaluated in a real
scenario where they are applied to the result of an automatic line segmentation algorithm
Table Character Error Rates of CTC-trained RNNs on ground-truth lines and automatic
segmentation of paragraphs with different resolutions The last column contains the error rate of the
attention-based model presented in this work without an explicit line segmentation
Database
IAM
Rimes
Resolution
dpi
dpi
dpi
dpi
GroundTruth
Line segmentation
Projection Shredding
Energy
This work
In Table we report the CERs obtained with the ground-truth line positions with three different
segmentation algorithms and with our end-to-end system on the validation sets of both databases with
different input resolutions We see that applying the baseline networks on automatic segmentations
increases the error rates by an absolute in the best case We also observe that the models are
better with higher resolutions
Our models yield better performance than methods based on an explicit and automatic line segmentation and comparable or better results than with ground-truth segmentation even with a resolution
divided by two Two factors may explain why our model yields better results than the line recognition
from ground-truth segmentation First the ground-truth line positions are bounding boxes that may
include some parts of adjacent lines and include irrelevant data whereas the attention model will
focus on smaller areas But the main reason is probably that the proposed model includes a BLSTM
operating on the whole paragraph which may capture linguistic dependencies across text lines
In Figure we display a visualisation of the implicit line segmentation computed by the network
Each color corresponds to one step of the iterative weighted collapse On the images the color
represents the weights given by the attention network the transparency encodes their intensity The
texts below are the predicted transcriptions and chunks are colored according to the corresponding
timestep of the attention mechanism
Figure Transcription of full paragraphs of text and implicit line segmentation learnt by the network
on IAM left and Rimes right Best viewed in color
Comparison to Published Results
In this section we also compute the word error rates and evaluate our models on the test
sets to compare the proposed approach to existing systems For IAM we applied a 3-gram language
model with a lexicon of words trained on the LOB Brown and Wellington corpora.1 This
language model has a perplexity of and out-of-vocabulary rate of on the validation set
and on the test set
The results are presented in Table for different input resolutions When comparing the error rates it
is important to note that all systems in the literature used an explicit ground-truth line segmentation
and a language model 26 used a hybrid character/word language model to tackle the issue
of out-of-vocabulary words Moreover all systems except carefully pre-processed the line
image corrected the slant or skew normalized the height whereas we just normalized the
pixel values to zero mean and unit variance Finally is a combination of four systems
Table Final results on Rimes and IAM databases
dpi
dpi
no language model
with language model
no language model
with language model
Bluche
Doetsch
Kozielski
Pham
Messina Kermorvant
Rimes
WER CER
IAM
WER CER
The parts of the LOB corpus used in the validation and evaluation sets were removed
On Rimes the system applied to dpi images already outperforms the state of the art in CER
while being competitive in terms of WER The system for dpi images is comparable to the best
single system in WER with a significantly better CER
On IAM the language model turned out to be quite important probably because there is more
variability in the language.2 On dpi images the results are not too far from the state of the art
results The WER does not improve much on dpi images but we get a lower CER When
analysing the errors we noticed that there is a lot of punctuation in IAM which was often missed by
the attention mechanism It may happen because punctuation marks are significantly smaller than
characters With the attention-based collapse and the weighted sum they will be more easily missed
than with the standard collapse which gives the same weight to all vertical positions
Discussion
Table Comparison of decoding times of different methods using ground-truth line information
with explicit segmentation with the attention-based method of and with the system presented in
this paper
Method
GroundTruth
Shredding
Scan Attend and Read
This Work
crop+reco
segment+crop+reco
reco
reco
Processing time
The proposed model can transcribe complete paragraphs without segmentation and is orders of
magnitude faster that the model of Table However the mechanism cannot handle
arbitrary reading orders Rather it implements a sort of implicit line segmentation In the current
implementation the iterative collapse runs for a fixed number of timesteps Yet the model can handle
a variable number of text lines and interestingly the focus is put on interlines in the additional steps
A more elegant solution should include the prediction of a binary variable indicating when to stop
reading
Our method was applied to paragraph images so a document layout analysis is required to detect
those paragraphs before applying the model Naturally the next step should be the transcription of
complex documents without an explicit or assumed paragraph extraction The limitation to paragraphs
is inherent to this system Indeed the weighted collapse always outputs sequences corresponding to
the whole width of the encoded image which in paragraphs may correspond to text lines In order to
switch to full documents several issues arise On the one hand the size of the lines is determined by
the size of the text block Thus a method should be devised to only select a smaller part of the feature
maps representing only the considered text line This is not possible in the presented framework A
potential solution could come from spatial transformer networks performing a differentiable
crop On the other hand training will in practice become more difficult not only because of the
complexity of the task but also because the reading order of text blocks in complex documents cannot
be exactly inferred in many cases even defining arbitrary rules may be tricky
Conclusion
We have presented a model to transcribe full paragraphs of handwritten texts without an explicit
line segmentation Contrary to classical methods relying on a two-step process segment then
recognize our system directly considers the paragraph image without an elaborated pre-processing
and outputs the complete transcription We proposed a simple modification of the collapse layer
in the standard MDLSTM architecture to iteratively focus on single text lines This implicit line
segmentation is learnt with backpropagation along with the rest of the network to minimize the
CTC error at the paragraph level We reported error rates comparable to the state of the art on two
public databases After switching from explicit to implicit character then word segmentation for
handwriting recognition we showed that line segmentation can also be learnt inside the transcription
model The next step towards end-to-end handwriting recognition is now at the full page level
A simple language model yields a perplexity of 18 on Rimes

<<----------------------------------------------------------------------------------------------------------------------->>

