query sentence: Self-organization of associative database and its applications
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 1-self-organization-of-associative-database-and-its-applications.pdf

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
AND ITS APPLICATIONS
Hisashi Suzuki and Suguru Arimoto
Osaka University Toyonaka Osaka Japan
ABSTRACT
An efficient method of self-organizing associative databases is proposed together with
applications to robot eyesight systems The proposed databases can associate any input
with some output In the first half part of discussion an algorithm of self-organization is
proposed From an aspect of hardware it produces a new style of neural network In the
latter half part an applicability to handwritten letter recognition and that to an autonomous
mobile robot system are demonstrated
INTRODUCTION
Let a mapping be given Here is a finite or infinite set and is another
finite or infinite set A learning machine observes any set of pairs sampled randomly
from Y. means the Cartesian product of and And it computes some
estimate of to make small the estimation error in some measure
Usually we say that the faster the decrease of estimation error with increase of the number of samples the better the learning machine However such expression on performance
is incomplete Since it lacks consideration on the candidates of of assumed preliminarily Then how should we find out good learning machines To clarify this conception
let us discuss for a while on some types of learning machines And let us advance the
understanding of the self-organization of associative database
Parameter Type
An ordinary type of learning machine assumes an equation relating x's and y's with
parameters being indefinite namely a structure of It is equivalent to define implicitly a
set of candidates of
is some subset of mappings from to And it computes
values of the parameters based on the observed samples We call such type a parameter
type
For a learning machine defined well if approaches as the number of samples
increases In the alternative case however some estimation error remains eternally Thus
a problem of designing a learning machine returns to find out a proper structure of in this
sense
On the other hand the assumed structure of is demanded to be as compact as possible
to achieve a fast learning In other words the number of parameters should be small Since
if the parameters are few some can be uniquely determined even though the observed
samples are few However this demand of being proper contradicts to that of being compact
Consequently in the parameter type the better the compactness of the assumed structure
that is proper the better the learning machine This is the most elementary conception
when we design learning machines
Universality and Ordinary Neural Networks
Now suppose that a sufficient knowledge on is given though itself is unknown In
this case it is comparatively easy to find out proper and compact structures of J. In the
alternative case however it is sometimes difficult A possible solution is to give up the
compactness and assume an almighty structure that can cover various A combination
of some orthogonal bases of the infinite dimension is such a structure Neural networks
are its approximations obtained by truncating finitely the dimension for implementation
American Institute of Physics
A main topic in designing neural networks is to establish such desirable structures of
This work includes developing practical procedures that compute values of coefficients from
the observed samples Such discussions are flourishing since while many efficient methods have been proposed Recently even hardware units computing coefficients in parallel
for speed-up are sold ANZA Mark III Odyssey and E-1.
Nevertheless in neural networks there always exists a danger of some error remaining
eternally in estimating Precisely speaking suppose that a combination of the bases of a
finite number can define a structure of essentially In other words suppose that or
is located near F. In such case the estimation error is none or negligible However if
is distant from the estimation error never becomes negligible Indeed many researches
report that the following situation appears when is too complex Once the estimation
error converges to some value as the number of samples increases it decreases hardly
even though the dimension is heighten This property sometimes is a considerable defect of
neural networks
Recursi ve Type
The recursive type is founded on another methodology of learning that should be as
follows At the initial stage of no sample the set Fa instead of notation of candidates
of I equals to the set of all mappings from to Y. After observing the first sample
Yl Fa is reduced to Fi so that I(xt Yl for any I F. After observing
the second sample Fl is further reduced to F2 so that i(xt Yl and
Y2 for any I F. Thus the candidate set becomes gradually small as observation
of samples proceeds The after observing i-samples which we write
is one of the most
likelihood estimation of selected in Hence contrarily to the parameter type the
recursive type guarantees surely that approaches to as the number of samples increases
The recursive type if observes a sample yd rewrites values to for
some x's correlated to the sample Hence this type has an architecture composed of a rule
for rewriting and a free memory space Such architecture forms naturally a kind of database
that builds up management systems of data in a self-organizing way However this database
differs from ordinary ones in the following sense It does not only record the samples already
observed but computes some estimation of for any We call such database an
associative database
The first subject in constructing associative databases is how we establish the rule for
rewri ting For this purpose we adap a measure called the dissimilari ty Here a dissimilari ty
means a mapping reals such that for any
whenever However it is not necessarily defined with a single formula It is
definable with for example a collection of rules written in forms of then
The dissimilarity defines a structure of locally in Y. Hence even though
the knowledge on is imperfect we can re:flect it on in some heuristic way Hence
contrarily to neural networks it is possible to accelerate the speed of learning by establishing
well Especially we can easily find out simple d's for those l's which process analogically
information like a human See the applications in this paper And for such the
recursive type shows strongly its effectiveness
We denote a sequence of observed samples by Yd One of the simplest
constructions of associative databases after observing i-samples is as follows
I
Algorithm At the initial stage let So be the empty set For every
let for any equal some such that and
min
Furthermore add to to produce Sa
Another version improved to economize the memory is as follows
Algorithm At the initial stage let So be composed of an arbitrary element
in Y. For every let ii-lex for any equal some such
that Si-l and
min
Furthermore if ii-l(Xi Yi then let Si Si-l or add Yi to Si-l to
produce Si Si Si-l
In either construction ii approaches to as increases However the computation time
grows proportionally to the size of Si. The second subject in constructing associative
databases is what addressing rule we should employ to economize the computation time In
the subsequent chapters a construction of associative database for this purpose is proposed
It manages data in a form of binary tree
SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
Given a sample sequence the algorithm for constructing associative
database is as follows
Algorithm
Step I(Initialization Let x[root y[root Yd. Here and are
variables assigned for respective nodes to memorize data Furthermore let
Step Increase by and put in After reset a pointer to the root repeat
the following until arrives at some terminal node leaf
Notations nand
d(xt let
mean the descendant nodes of
Otherwise let
If
Step Display yIn as the related information Next put in If yIn back
to step Otherwise first establish new descendant nodes and Secondly
let
yIn
yIn
Finally back to step Here the loop of step can be stopped at any time
and also can be continued
Now suppose that gate elements namely artificial synapses that play the role of branching by are prepared Then we obtain a new style of neural network with gate elements
being randomly connected by this algorithm
LETTER RECOGNITION
Recen tly the vertical slitting method for recognizing typographic English letters3 the
elastic matching method for recognizing hand written discrete English letters4 the global
training and fuzzy logic search method for recognizing Chinese characters written in square
styleS etc are published The self-organization of associative database realizes the recognition of handwritten continuous English letters
NOV
xk La.t
dw1lo
Source document
Windowing
Number of samples
NUAlber of sampl es
An experiment result
An image scanner takes a document image The letter recognizer uses a parallelogram window that at least can cover the maximal letter and processes the
sequence of letters while shifting the window That is the recognizer scans a word in a
slant direction And it places the window so that its left vicinity may be on the first black
point detected Then the window catches a letter and some part of the succeeding letter
If recognition of the head letter is performed its end position namely the boundary line
between two letters becomes known Hence by starting the scanning from this boundary
and repeating the above operations the recognizer accomplishes recursively the task Thus
the major problem comes to identifying the head letter in the window
Considering it we define the following
Regard window images as and define accordingly
For a denote by a black point in the left area from the boundary on
window image Project each onto window image Then measure the Euclidean
distance between fj and a black point on being the closest to B. Let be
the summation of for all black points B's on divided by the number of B's.
Regard couples of the reading and the position of boundary as and define
accordingly
An operator teaches the recognizer in interaction the relation between window image and
reading boundary with algorithm Precisely if the recalled reading is incorrect the
operator teaches a correct reading via the console Moreover if the boundary position is
incorrect he teaches a correct position via the mouse
shows partially a document image used in this experiment shows the
change of the number of nodes and that of the recognition rate defined as the relative
frequency of correct answers in the past trials Speciiications of the window are height
width and slant angular In this example the levels of tree
were distributed in at time and the recognition rate converged to about
Experimentally the recognition rate converges to about in most cases and to at
a rare case However it does not attain since and are not distinguishable
because of excessive lluctuation in writing If the consistency of the y-relation is not
assured like this the number of nodes increases endlessly Hence it is clever to
stop the learning when the recognition rate attains some upper limit To improve further
the recognition rate we must consider the spelling of words It is one of future subjects
OBSTACLE AVOIDING MOVEMENT
Various systems of camera type autonomous mobile robot are reported flourishingly6-1O
The system made up by the authors also belongs to this category Now in mathematical methodologies we solve usually the problem of obstacle avoiding movement as
a cost minimization problem under some cost criterion established artificially Contrarily
the self-organization of associative database reproduces faithfully the cost criterion of an
operator Therefore motion of the robot after learning becomes very natural
Now the length width and height of the robot are all about and the weight is
about The visual angle of camera is about The robot has the following three
factors of motion It turns less than advances less than and controls speed less
than The experiment was done on the passageway of wid th inside a building
which the authors laboratories exist in Because of an experimental intention we
arrange boxes smoking stands gas cylinders stools handcarts etc on the passage way at
random We let the robot take an image through the camera recall a similar image and
trace the route preliminarily recorded on it For this purpose we define the following
Let the camera face 28deg downward to take an image and process it through a low
pass filter Scanning vertically the filtered image from the bottom to the top search
the first point where the luminance changes excessively Then su bstitu te all points
from the bottom to for white and all points from to the top for black
If no obstacle exists just in front of the robot the white area shows the area
where the robot can move around Regard binary 32 32dot images processed thus
as and define accordingly
For every let be the number of black points on the exclusive-or
image between and
Regard as y's the images obtained by drawing routes on images and define
accordingly
The robot superimposes on the current camera image the route recalled for and
inquires the operator instructions The operator judges subjectively whether the suggested
route is appropriate or not In the negative answer he draws a desirable route on with the
mouse to teach a new to the robot This opera.tion defines implicitly a sample sequence
of reflecting the cost criterion of the operator
IibUBe
22
Roan
13
Stationary uni
Configuration of
autonomous mobile robot system
I
23
24
North
rmbi Ie unit robot
Roan
Experimental
environment
Wall
Camera image
Preprocessing
A
fa
Preprocessing
Course
suggest ion
Search
A
Processing for
obstacle avoiding movement
Processing for
position identification
We define the satisfaction rate by the relative frequency of acceptable suggestions of
route in the past trials In a typical experiment the change of satisfaction rate showed
a similar tendency to and it attains about around time Here notice that
the rest does not mean directly the percentage of collision In practice we prevent the
collision by adopting some supplementary measure At time the number of nodes was
and the levels of tree were distributed in
The proposed method reflects delicately various characters of operator For example a
robot trained by an operator moves slowly with enough space against obstacles while one
trained by another operator brushes quickly against obstacles This fact gives us a hint
on a method of printing characters into machines
POSITION IDENTIFICATION
The robot can identify its position by recalling a similar landscape with the position data
to a camera image For this purpose in principle it suffices to regard camera images and
position data as x's and respectively However the memory capacity is finite in actual
compu ters Hence we cannot but compress the camera images at a slight loss of information
Such compression is admittable as long as the precision of position identification is in an
acceptable area Thus the major problem comes to find out some suitable compression
method
In the experimental environment juts are on the passageway at intervals of
and each section between adjacent juts has at most one door The robot identifies
roughly from a surrounding landscape which section itself places in And it uses temporarily
a triangular surveying technique if an exact measure is necessary To realize the former task
we define the following
Turn the camera to take a panorama image of Scanning horizontally the
center line substitute the points where the luminance excessively changes for black
and the other points for white Regard binary line images processed
thus as and define accordingly
For every project each black point A on onto And measure the
Euclidean distance between A and a black point A on being the closest to A. Let
the summation of be S. Similarly calculate by exchanging the roles of and
Denoting the numbers of A's and A's respectively by nand define
Regard positive integers labeled on sections as y's and define accordingly
In the learning mode the robot checks exactly its position with a counter that is reset periodically by the operator The robot runs arbitrarily on the passageways within area
and learns the relation between landscapes and position data Position identification beyond area is achieved by crossing plural databases one another This task is automatic
excepting the periodic reset of counter namely it is a kind of learning without teacher
We define the identification rate by the relative frequency of correct recalls of position
data in the past trials In a typical example it converged to about around time
At time the number of levels was and the levels oftree were distributed in Since the identification failures of can be rejected by considering the trajectory no
pro blem arises in practical use In order to improve the identification rate the compression
ratio of camera images must be loosened Such possibility depends on improvement of the
hardware in the future
shows an example of actual motion of the robot based on the database for obstacle
avoiding movement and that for position identification This example corresponds to a case
of moving from to 23 in Here the time interval per frame is about
I
I
Actual motion of the robot
CONCLUSION
A method of self-organizing associative databases was proposed with the application to
robot eyesight systems The machine decomposes a global structure unknown into a set of
local structures known and learns universally any input-output response This framework
of problem implies a wide application area other than the examples shown in this paper
A defect of the algorithm of self-organization is that the tree is balanced well only
for a subclass of structures of A subject imposed us is to widen the class A probable
solution is to abolish the addressing rule depending directly on values of and instead to
establish another rule depending on the distribution function of values of It is now under
investigation

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1105-cholinergic-suppression-of-transmission-may-allow-combined-associative-memory-function-and-self-organization-in-the-neocortex.pdf

Cholinergic suppression of transmission may
allow combined associative memory function and
self-organization in the neocortex
Michael E. Hasselmo and Milos Cekic
Department of Psychology and Program in Neurosciences
Harvard University 33 Kirkland Cambridge MA
hasselmo@katIa.harvard.edu
Abstract
Selective suppression of transmission at feedback synapses during
learning is proposed as a mechanism for combining associative feedback with self-organization of feed forward synapses Experimental
data demonstrates cholinergic suppression of synaptic transmission in
layer I feedback synapses and a lack of suppression in layer IV feedforward synapses A network with this feature uses local rules to learn
mappings which are not linearly separable During learning sensory
stimuli and desired response are simultaneously presented as input
Feedforward connections form self-organized representations of input
while suppressed feedback connections learn the transpose of feedforward connectivity During recall suppression is removed sensory input
activates the self-organized representation and activity generates the
learned response
INTRODUCTION
The synaptic connections in most models of the cortex can be defined as either associative
or self-organizing on the basis of a single feature the relative infl uence of modifiable synapses on post-synaptic activity during learning figure In associative memories postsynaptic activity during learning is determined by nonmodifiable afferent input connections with no change in the storage due to synaptic transmission at modifiable synapses
Anderson McNaughton and Morris In self-organization post-synaptic
activity is predominantly influenced by the modifiable synapses such that modification of
synapses influences subsequent learning Von der Malsburg Miller
Models of cortical function must combine the capacity to form new representations and
store associations between these representations Networks combining self-organization
and associative memory function can learn complex mapping functions with more biologically plausible learning rules Hecht-Nielsen Carpenter Dayan
M. E. HASSELMO M. CEKIC
but must control the influence of feedback associative connections on self-organization Some networks use special activation dynamics which prevent feedback from
influencing activity unless it coincides with feedforward activity Carpenter
A new network alternately shuts off feedforward and feedback synaptic transmission
Dayan
A.
Self-organizing
Afferent
Self-organizing
feedforward
Associative
feedback
Figure Defining characteristics of self-organization and associative memory A. At
self-organizing synapses post-synaptic activity during learning depends predominantly
upon transmission at the modifiable synapses B. At synapses mediating associative memory function post-synaptic activity during learning does not depend primarily on the modifiable synapses but is predominantly influenced by separate afferent input C. Selforganization and associative memory function can be combined if associative feedback
synapses are selectively suppressed during learning but not recall
Here we present a model using selective suppression of feedback synaptic transmission
during learning to allow simultaneous self-organization and association between two
regions Previous experiments show that the neuromodulator acetylcholine selectively
suppresses synaptic transmission within the olfactory cortex Hasselmo and Bower
and hippocampus Hasselmo and Schnell If the model is valid for neocortical structures cholinergic suppression should be stronger for feedback but not feedforward synapses Here we review experimental data Hasselmo and Cekic comparing
cholinergic suppression of synaptic transmission in layers with predominantly feedforward or feedback synapses
BRAIN SLICE PHYSIOLOGY
As shown in Figure we utilized brain slice preparations of the rat somatosensory neocortex to investigate whether cholinergic suppression of synaptic transmission is selective
for feedback but not feedforward synaptic connections This was possible because feedforward and feedback connections show different patterns of termination in neocortex As
shown in Figure Layer I contains primarily feedback synapses from other cortical
regions Cauller and Connors whereas layer IV contains primarily afferent synapses from the thalamus and feedforward synapses from more primary neocortical structures Van Essen and Maunsell Using previously developed techniques Cauller
and Connors Li and Cauller for testing of the predominantly feedback connections in layer I we stimulated layer I and recorded in layer I cut prevented spread of
Cholinergic Suppression of Transmission in the Neocortex
activity from layers and III For testing the predominantly feedforward connections
terminating in layer IV we elicited synaptic potentials by stimulating the white matter
deep to layer VI and recorded in layer IV. We tested suppression by measuring the change
in height of synaptic potentials during perfusion of the cholinergic agonist carbachol at
lOOJ,1M Figure shows that perfusion of carbachol caused much stronger suppression of
synaptic transmission in layer I as compared to layer IV Hasselmo and Cekic suggesting that cholinergic suppression of transmission is selective for feedback synapses and
not for feedforward synapses
I
ll-ill
I
IV
V-VI
Layer IV
recording
Region
Foedback
I
ll-ill
IV
V-VI
Region
White matter
stimulation
Figure A. Brain slice preparation of somatosensory cortex showing location of stimulation and recording electrodes for testing suppression of synaptic transmission in layer I
and in layer IV. Experiment based on procedures developed by Cauller Cauller and Connors Li and Cauller B. Anatomical pattern of feedforward and feedback
connectivity within cortical structures based on Van Essen and Maunsell
Feedforward layer IV
Control
Carbachol OOJlM
Wash
I
5ms
Feedback layer I
Control
Carbachol OOJlM
Wash
Figure Suppression of transmission in somatosensory neocortex Top Synaptic potentials recorded in layer IV where feedforward and afferent synapses predominate show
little effect of l00J.tM carbachol Bottom Synaptic potentials recorded in layer I where
feedback synapses predominate show suppression in the presence of lOOJ,1M carbachol
M. E. HASSELMO M. CEKIC
COMPUTATIONAL MODELING
These experimental results supported the use of selective suppression in a computational
model Hasselmo and Cekic with self-organization in its feedforward synaptic connections and associative memory function in its feedback synaptic connections Figs and
The proposed network uses local Hebb-type learning rules supported by evidence on
the physiology of long-tenn potentiation in the hippocampus Gustafsson and Wigstrom
The learning rule for each set of connections in the network takes the fonn
tlWS:'Y
Where W(x. designates the connections from region to region is the threshold of
synaptic modification in region is the rate of modification and the output function is
where represents the constraint to positive values only
Feedforward connections have self-organizing properties while feedback connections have associative memory properties This difference depends entirely
upon the selective suppression of feedback synapses during learning which is implemented in the activation rule in the form For the entire network the activation rule
takes the fonn
II(X
II(X
x=lk=l
II(Y
x=lk=l
k=l
where represents the activity of each of the neurons in region is the activity of each of the neurons in other regions is the total number of regions providing feedforward input is the total number of regions providing feedback input Aj(y is
the input pattern to region represents the inhibition between neurons in region
and represents the suppression of synaptic transmission During learning takes a
value between and During recall suppression is removed In this network synapses between regions only take positive values reflecting the fact that long-range
connections between cortical regions consist of excitatory synapses arising from pyramidal cells Thus inhibition mediated by the local inhibitory interneurons within a region is
represented by a separate inhibitory connectivity matrix H.
After each step of learning the total weight of synaptic connections is nonnalized pre-synaptically for each neuron in each region
ij
Wij(t
Wij(t l1Wij
Synaptic weights are then normalized post-synaptically for each neuron in each region
replacing with in the sum in the denominator in equation This nonnalization of
synaptic strength represents slower cellular mechanisms which redistribute pre and postsynaptic resources for maintaining synapses depending upon local influences
In these simulations both the sensory input stimuli and the desired output response to be
learned are presented as afferent input to the neurons in region Most networks using
error-based learning rules consist of feedforward architectures with separate layers of
input and output units One can imagine this network as an auto-encoder network folded
back on itself with both input and output units in region and hidden units in region
Cholinergic Suppression of Transmission in the Neocortex
As an example of its functional properties the network presented here was trained on the
XOR problem The XOR problem has previously been used as an example of the capability of error based training schemes for solving problems which are not linearly separable
The specific characteristics of the network and patterns used for this simulation are shown
in figure The two logical states of each component of the XOR problem are represented
by two separate units designated on or off in figures and ensuring that activation of
the network is equal for each input condition The problem has the appearance of two
XOR problems with inverse logical states being solved simultaneously
As shown in figure the input and desired output of the network are presented simultaneously during learning to region The six neurons in region project along feedforward connections to four neurons in region the hidden units of the network These four
neurons project along feedback connections to the six neurons in region All connections take random initial weights During learning the feedforward connections undergo
self-organization which ultimately causes the hidden units to become feature detectors
responding to each of the four patterns of input to region Thus the rows of the feedforward synaptic connectivity matrix gradually take the form of the individual input patterns
on
STIMULUS
off on off
RESPONSE
yes no
oeeo eo
oeoe oe
eooe eo
Afferent
input
Region
Figure Network for learning the XOR problem with units in region and units in
region Four different patterns of afferent input are presented successively to region
The input stimuli of the XOR problem are represented by the four units on the left and the
desired output designation of XOR or not-XOR is represented by the two units on the
right The XOR problem has four basic states on-off and off-on on the input is categorized by yes on the output while on-on and off-off on the input is categorized by no on the
output
Modulation is applied during learning in the form of selective suppression of synaptic
transmission along feedback connections this suppression need not be complete giving
these connections associative memory function Hebbian synaptic modification causes
these connections to link each of the feature detecting hidden units in region with the
cells in region activated by the pattern to which the hidden unit responds Gradually the
feedback synaptic connectivity matrix becomes the transpose of the feedforward connectivity matrix parameters used in simulation Aj(l or I
and Function was similar and convergence
was obtained more rapidly with Feedback synaptic transmission prevented con
M. E. HASSELMO M. CEKIC
vergence during learning when
During recall modulation of synaptic transmission is removed and the various input stimuli of the XOR problem are presented to region without the corresponding output pattern Activity spreads along the self-organized feedforward connections to activate the
specific hidden layer unit responding to that pattern Activity then spreads back along
feedback connections from that particular unit to activate the desired output units The
activity in the two regions settles into a final pattern of recall Figure shows the settled
recall of the network at different stages of learning It can be seen that the network initially may show little recall activity or erroneous recall activity but after several cycles of
learning the network settles into the proper response to each of the XOR problem states
Convergence during learning and recall have been obtained with other problems including recognition of whether on units were on the left or right symmetry of on units and
number of on units In addition larger scale problems involving multiple feedforward and
feedback layers have been shown to converge
on on no
I.
26
off on yes
off off no
en
Region
I
on off yes
Region
Figure Output neuronal activity in the network shown at different learning steps The
four input patterns are shown at top Below these are degraded patterns presented during
recall missing the response components of the input pattern The output of the region
units and the region units are shown at each stage of learning As learning progresses
gradually one region unit starts to respond selectively to each input pattern and the correct output unit becomes active in response to the degraded input Note that as learning
progresses the response to pattern changes gradually from incorrect yes to correct
Cholinergic Suppression of Transmission in the Neocortex

<<----------------------------------------------------------------------------------------------------------------------->>

title: 251-a-self-organizing-associative-memory-system-for-control-applications.pdf

Hormel
A Sell-organizing Associative
Memory System lor Control
Applications
Michael Bormel
Department of Control Theory and Robotics
Technical University of Darmstadt
Schlossgraben
Darmstadt/W.-Ger.any
ABSTRACT
The CHAC storage scheme has been used as a basis
for a software implementation of an associative
emory system AHS which itself is a major part
of the learning control loop LERNAS A major
disadvantage of this CHAC-concept is that the
degree of local generalization area of interpolation is fixed This paper deals with an algorithm for self-organizing variable generalization for the AKS based on ideas of T. Kohonen
INTRODUCTION
For several years research at the Department of Control Theory and Robotics at the Technical University of Darmstadt
has been concerned with the design of a learning real-time
control loop with neuron-like associative memories LERNAS
A Self-organizing Associative Memory System for Control Applications
for the control of unknown nonlinear processes Ersue
Tolle This control concept uses an associative memory system AHS based on the cerebellar cortex model CHAC by
Albus Albus for the storage of a predictive nonlinear process model and an appropriate nonlinear control strategy
e&;ected process response
I>.
planne
control inputs
red setpoint
co
predictive
process
IF
opti.iud
control input
eValuation
opti.ization
actual/past
process infor.ation
control strate
actual control input
I
unknown process
I>.
I
short ter
e.ory
process infor.ation
I
I
laSSOCialive lIe.ory syste
Figure The learning control loop LERNAS
One problem for adjusting the control loop to a process is
however to find a suitable set of parameters for the associative memory The parameters in question determine the
degree of generalization within the memory and therefore
have a direct influence on the number of training steps required to learn the process behaviour For a good performance of the control loop it is desirable to have a very
small generalization around a given setpoint but to have a
large generalization elsewhere Actually the amount of collected data is small during the transition phase between two
Hormel
setpoints but is large during setpoint control Therefore a
self-organizing variable generalization adapting itself to
the amount of available data would be very advantageous
Up to now when working with fixed generalization finding
the right parameters has meant to find the best compromise
between performance and learning time required to generate a
process model This paper will show a possibility to introduce a self-organizing variable generalization capability
into the existing AMS/CMAC algorithm
THE AMS-CONCEPT
The associative memory syste AMS is based on the Cerebellar Model Articulation Controller CMAC as presented by J.S.
Albus The information processing structure of AMS can be
divided into three stages
Each component of a n-dimensional input vector stimulus activates a fixed number of sensory cells the
receptive fields of which are overlapping So n?p sensory cells become active
The active sensory cells are grouped to form n-dimensional vectors These vectors are mapped to association cells The merged receptive fields of the sensory
cells described by one vector can be seen as a hypercube
in the n-dimensional input space and therefore as the
receptive field of the association cell In normal applications the total number of available association
cells is about
The association cells are connected to the output cells
by modifiable synaptic weights The output cell computes
the mean value of all weights that are connected to active association cells active weights
Figure shows the basic principle of the associative memory
system AMS.
A Self-organizing Associative Memory System for Control Applications
output value
input space
adjustable weights
Figure The basic aechanism of AMS
During training the generated output is compared with a desired output the error is computed and equally distributed
over all active weights For the mapping of sensory cells to
association cells a hash-coding mechanism is used
THE SELF-ORGANIZING FEATURE MAP
An approach for explaining the self-organizing capabilities
of the nervous system has been presented by T. Kohonen Kohonen
In his self-organizing feature mapft a network of laterally
interconnected neurons can adapt itself according to the
density of trained points in the input space Presenting a
n-diaensional input vector to the network causes every neuron to produce an output signal which is correlated with the
similarity between the input vector and a template vector
which may be stored in the synaptic weights of the neuron
Due to the mexican-hat coupling function between the neurons the one with the maximum output activity will excite
its nearest neighbours but will inhibit neurons farther away therefore generating a localized response in the network The active cells can now adapt their input weights in
order to increase their similarity to the input vector If
we define the receptive field of a neuron by the number of
input vectors for which the neurons activity is greater than
Hormel
that of any other neuron in the net this yields the effect
that in areas with a high density of trained points the receptive fields become small whereas in areas with a low density of trained points the size of the receptive fields is
large Is mentioned above this is a desired effect when
workin with a learning control loop
SELF-ORGANIZING VARIABLE GENERALIZATION
Both of the approaches above have several advantages and
disadvantages when using them for real-time control applications
In the AKS algorithm one does not have to care for predefining a network and the coupling functions or coupling matrices among the elements of the network Association and
weight cells are generated when they are needed during
training and can be adressed very quietly to produce a memory response One of the disadvantages is the fixed generalization once the parameters of a eaory unit have been
chosen
Unlike AHS the feature map allows the adaption of the network according to the input data This advantage has to be
payed for by extensive search for the best matching neuron
in the network and therefore the response time of the network aay be too large for real-tiae control when working
with big networks
These problems can be overcome when allowing that the mapping of sensory cells to association cells in AKS is no
longer fixed but can be changed during training
To accomplish this a template vector is introduced for
every association cell This vector serves as an indicator
for the stimuli by which the association cell has been accessed previously During an associative recall for a stimulus a preliminary set of association cells is activated
by the hash coding mechanism Due to the self-organizing
process during training the template vectors do not need to
correspond to the input vector For the search for the
A Self-organizing Associative Memory System for Control Applications
best aatching cell the template vector of the accessed
association cell is compared to the stiaulus and a difference vector is calculated
L.v
number of searching steps
This vector can now be used to compute a virtual stimulus
which compensates the mapping errors of the hash-coding
mechanism
The best matching cell is found for
ain
ns
and can be adressed by the virtual stimulus when using
the hash coding mechanism This search mechanism ensures
that the best matching cell is found even if self organization is in effect
During training the template
cells are updated by
vectors of
the
association
lateral distance of neurons in the network
where denotes the value of the teaplate vector at time
and denotes the stimulus is a monotonic decreasing function of time and the lateral distance between
neurons in the network
SIMULATION RESULTS
Figure and show some simulation results of the presented
algorithm for the dase of a two dimensional stimulus vector
Hormel
Figure shows the expected positions in input space of the
untrained template vectors denotes untrained association
cells
Figure Untrained etwork
Figure shows the network after training steps with
stimuli of gaussian distribution in input space The position of the template vectors of trained cells has shifted
into the direction of the better trained areas so that more
association cells are used to represent this area than before Therefore the stored information will be more exact in
this area
Figure Network after training steps
A Self-organizing Associative Memory System for Control Applications
CONCLUSION
The ney algorithm presented above introduces the capability
to adapt the storage mechanisms of a CMAC-type associative
memory according to the arriving stimuli This will result
in various degrees of generalization depending on the number
of trained points in a given area It therefore will make it
unnecessary to choose a generalization factor as a compromise between several constraints when representing nonlinear
functions by storing them in this type of associative memory Some results on tests will be presented together with a
comparison on respective results for the original AMS.
Acknowledgements
This work was sponsored by the German inistry for
and Technology BMFT under grant no ITR
Research

<<----------------------------------------------------------------------------------------------------------------------->>

title: 972-a-model-of-the-hippocampus-combining-self-organization-and-associative-memory-function.pdf

A model of the hippocampus combining selforganization and associative memory function
Michael E. Hasselmo Eric Schnell
Joshua Berke and Edi Barkai
Dept of Psychology Harvard University
33 Kirkland Cambridge MA
hasselmo@katla.harvard.edu
Abstract
A model of the hippocampus is presented which forms rapid self-organized representations of input arriving via the perforant path performs
recall of previous associations in region and performs comparison
of this recall with afferent input in region CA This comparison drives
feedback regulation of cholinergic modulation to set appropriate
dynamics for learning of new representations in region CA3 and CA
The network responds to novel patterns with increased cholinergic modulation allowing storage of new self-organized representations but
responds to familiar patterns with a decrease in acetylcholine allowing
recall based on previous representations This requires selectivity of the
cholinergic suppression of synaptic transmission in stratum radiatum of
regions CA3 and CAl which has been demonstrated experimentally
INTRODUCTION
A number of models of hippocampal function have been developed Burgess
Myers and Gluck Touretzky but remarkably few simulations have
addressed hippocampal function within the constraints provided by physiological and anatomical data Theories of the function of specific subregions of the hippocampal formation often do not address physiological mechanisms for changing dynamics between
learning of novel stimuli and recall of familiar stimuli For example the afferent input to
the hippocampus has been proposed to form orthogonal representations of entorhinal
activity Marr McNaughton and Morris Eichenbaum and Buckingham
but simulations have not addressed the problem of when these representations
78
Michael E. Hasselmo Eric Schnell Joshua Berke Edi Barkai
should remain stable and when they should be altered In addition models of autoassociative memory function in region CA3 Marr McNaughton and Morris
Levy Eichenbaum and Buckingham and heteroassociative memory function
at the Schaffer collaterals projecting from region CA3 to CAl Levy McNaughton
require very different activation dynamics during learning versus recall
Acetylcholine may set appropriate dynamics for storing new information in the cortex
Hasselmo Hasselmo Hasselmo and Bower Acetylcholine has been shown to selectively suppress synaptic transmission at intrinsic but
not afferent fiber synapses Hasselmo and Bower to suppress the neuronal adaptation of cortical pyramidal cells Hasselmo Barkai and Hasselmo and
to enhance long-term potentiation of synaptic potentials Hasselmo Models
show that suppression of synaptic transmission during learning prevents recall of previously stored information from interfering with the storage of new information Hasselmo
Hasselmo while cholinergic enhancement of synaptic
modification enhances the rate of learning Hasselmo
Feedback regulation of cholinergic modulation may set the appropriate level of cholinergic modulation dependent upon the novelty or familiarity of a particular input pattern
We have explored possible mechanisms for the feedback regulation of cholinergic modulation in simulations of region CAl Hasselmo and Schnell and region CA3. Here
we show that self-regulated learning and recall of self-organized representations can be
obtained in a network simulation of the hippocampal formation This model utilizes selective cholinergic suppression of synaptic transmission in stratum radiatum of region
which has been demonstrated in brain slice preparations of the hippocampus
METHODS
SIMPLIFIED REPRESENTA nON OF HIPPOCAMPAL NEURONS
In place of the sigmoid input-output functions used in many models this model uses a
simple representation in which the output of a neuron is not explicitly constrained but the
total network activity is regulated by feedback from inhibitory interneurons and adaptation due to intracellular calcium concentration Separate variables represent pyramidal
cell membrane potential a intracellular calcium concentration and the membrane potential of inhibitory interneurons
l1a Ai
Wijg(aj Hikg(h
I1c?I Vg(a
I
I1hk
Qc
IWkjg(aj-eo)-l1hk IHk/g(h/-e
where A afferent input
passive decay of membrane potential Il strength of cal
A Model of Hippocampus
79
cium-dependent potassium current proportional to intracellular calcium Wij excitatory
recurrent synapses longitudinal association path tenninating in stratum radiatum gO is a
threshold linear function proportional to the amount by which membrane potential
exceeds an output threshold or threshold for calcium current Oc strength of voltagedependent calcium current diffusion constant of calcium Wki excitatory synapses
inhibitory interneurons Hilc inhibitory synapses from interneurons to pyramidal cells
inhibitory synapses between interneurons This representation gives neurons adaptation characteristics similar to those observed with intracellular recording Barkai and Hasselmo including a prominent afterhyperpolarization potential Figure
An
N~JJL
lO
Figure Comparison of pyramidal cell model with experimental data
In Figure I A shows the membrane potential of a modeled pyramidal cell in response to
simulated current injection Output of this model is a continuous variable proportional to
how much membrane potential exceeds threshold This is analogous to the reciprocal of
interspike interval in real neuronal recordings Note that the model displays adaptation
during current injection and afterhyperpolarization afterwards due to the calcium-dependent potassium current shows the intracellularly recorded membrane potential in a pirifonn cortex pyramidal cell demonstrating adaptation of firing frequency due to
activation of calcium-dependent potassium current The firing rate falls off in a manner
similar to the smooth decrease in firing rate in the simplified representation shows an
intracellular recording illustrating long-tenn afterhyperpolarization caused by calcium
influx induced by spiking of the neuron during current injection
NETWORK CONNECTIVITY
A schematic representation of the network simulation of the hippocampal fonnation is
shown in Figure The anatomy of the hippocampal fonnation is summarized on the left
in A and the function of these different subregions in the model is shown on the right in
B. Each of the subregions in the model contained a population of excitatory neurons with
a single inhibitory interneuron mediating feedback inhibition and keeping excitatory
activity bounded Thus the local activation dynamics in each region follow the equations
presented above The connectivity of the network is further summarized in Figure in the
Results section A learning rule of the Hebbian type was utilized at all synaptic connections with the exception of the mossy fibers from the dentate gyrus to region and the
connections to and from the medial septum Self-organization of perforant path synapses
was obtained through decay of synapses with only pre or post-synaptic activity and
growth of synapses with combined activity Associative memory function at synapses
Michael E. Hasse/mo Eric Schnell Joshua Berke Edi Barkai
arising from region CA3 was obtained through synaptic modification during cholinergic
suppression of synaptic transmission
Entorhinal cortex
Self-organized
representation
Comparison
Feedback regulation of
cholinergic modulation
Regulation of
learning dynamics
Figure Schematic representation of hippocampal circuitry
and the corresponding function of connections in the model
CHOLINERGIC MODULAnON
The total output from region CAl determined the level of cholinergic modulation within
both region CA3 and CAl with increased output causing decreased modulation This is
consistent with experimental evidence suggesting that activity in region CAl and region
CA3 can inhibit activity in the medial septum and thereby downregulate cholinergic modulation This effect was obtained in the model by excitatory connections from region CAl
to an inhibitory interneuron in the medial septum which suppressed the activity of a cholinergic neuron providing modulation to the full network When levels of cholinergic
modulation were high there was strong suppression of synaptic transmission at the excitatory recurrent synapses in CA3 and the Schaffer collaterals projecting from region CA3 to
CAL This prevented the spread of activity due to previous learning from interfering with
self-organization When levels of cholinergic modulation were decreased the strength of
synaptic transmission was increased allowing associative recall to dominate Cholinergic
modulation also increased the rate of synaptic modification and depolarized neurons
TESTS OF SELF-REGULATED LEARNING AND RECALL
Simulations of the full hippocampal network evaluated the response to the sequential presentation of a series of highly overlapping activity patterns in the entorhinal cortex Recall
was tested with interspersed presentation of degraded versions of previously presented
activity patterns For effective recall the pattern of activity in entorhinal cortex layer IV
evoked by degraded patterns matched the pattern evoked by the full learned version of
these patterns The function of the full network is illustrated in Figure In simulations
A Model of Hippocampus
81
focused on region activity patterns were induced sequentially in region representing afferent input from the entorhinal cortex Different levels of external activation of
the cholinergic neuron resulted in different levels of learning of new overlapping patterns
These results are illustrated in Figure
BRAIN SLICE EXPERIMENTS
The effects in the simulations of region CA3 depended upon the cholinergic suppression
of synaptic transmission in stratum radiatum of this region The cholinergic suppression of
glutamatergic synaptic transmission in region CA3 was tested in brain slice preparations
by analysis of the influence of the cholinergic agonist carbachol on the size of field potentials elicited by stimulation of stratum radiatum These experiments used techniques similar to previously published work in region CAl Hasselmo and Schnell
RESULTS
In the full hippocampal simulation input of an unfamiliar pattern to entorhinal cortex
layer resulted in high levels of acetylcholine This allowed rapid self-organization of
the perforant path input to the dentate gyrus and region CAl. Cholinergic suppression of
synaptic transmission in region CAl prevented recall from interfering with self-organization Instead recurrent collaterals in region CA3 stored an autoassociative representation
of the input from the dentate gyrus to region and connections from CA3 to CA
stored associations between the pattern of activity in CA3 and the associated self-organized representation in region CAl.
identity
self-org matrix
auto
assoc
Self-org
iden~ity
hetero.9 hetero8
matrix
assoc
assoc
I I If
I I I
I I
ld
I I
I I I
I I I I
3d
4d
ld
2d
n,n
I I I
I
I
I
I
1I
I
I
I
I I Itt
I.Ll
I
I
I
I I
U.
I I
I
I I
I I I
I I
I I
I
I J1
III I
I
I
I
I I
I
I
I III I I
W.
Jl
lU
Neuron
Figure Activity in each subregion of the full network simulation of the hippocampal
formation during presentation of a sequence of activity patterns in entorhinal cortex
82
Michael E. Hasselmo Eric Schnell Joshua Berke Edi Barkai
In Figure width of the lines represents the activity of each neuron at a particular time
step As seen here the network forms a self-organized representation of each new pattern
consisting of active neurons in the dentate gyrus and region CAL At the same time an
association is formed between the self-organized representation in region CAl and the
same afferent input pattern presented to entorhinal cortex layer IV. Four overlapping patterns are presented sequentially each of which results in learning of a separate selforganized representation in the dentate gyrus and region CAl. with an association formed
between this representation and the full input pattern in entorhinal cortex
The recall characteristics of the network are apparent when degraded versions of the afferent input patterns are presented in the sequence This degraded afferent input
weakly activates the same representations previously formed in the dentate gyrus Recurrent excitation in region CA3 enhances this activity giving robust recall of the full version
of this pattern This activity then reaches CA where it causes strong activation if it
matches the pattern of afferent input from the entorhinal cortex Strong activation in
region CAl decreases cholinergic modulation preventing formation of a new representation and allowing recall to dominate Strong activation of the representation stored in
region CAl then activates the full representation of the pattern in entorhinal cortex layer
IV. Thus the network can accurately recall each of many highly overlapping patterns
The effect of cholinergic modulation on the level of learning or recall can be seen more
clearly in a simulation of auto-associative memory function in region CA3 as shown in
Figure Each box shows the response of the network to sequential presentation of full
and degraded versions of two highly overlapping input patterns The width of the black
traces represents the activity of each of CA3 pyramidal cells during each simulation
step In the top row level of cholinergic modulation ACh is plotted In A. external activation of the cholinergic neuron is absent so there is no cholinergic suppression of synaptic transmission In this case the first pattern is learned and recalled properly but
subsequent presentation of a second overlapping pattern results only in recall of the previously learned pattern In B. with greater cholinergic suppression recall is suppressed sufficiently to allow learning of a combination of the two input patterns Finally in C. strong
cholinergic suppression prevents recall allowing learning of the new overlapping pattern
to dominate over the previously stored pattern
A
Stored
patterns
ACh
Inhib
ACh input
ACh input
ACh input
I
Figure Increased cholinergic suppression of synaptic transmission in region CA3
causes greater learning of new aspects of afferent input patterns
A Model of Hippocampus
83
Extracellular recording in brain slice preparations of hippocampal region CA3 have demonstrated that perfusion of the cholinergic agonist carbachol strongly suppresses synaptic
potentials recorded in stratum radiatum as shown in Figure In contrast suppression of
synaptic transmission at the afferent fiber synapses arising from entorhinal cortex is much
weaker At a concentration of carbachol suppressed synaptic potentials in stratum
radiatum on average by Synaptic potentials elicited in stratum lacunosum
were more weakly suppressed with an average suppression
Control
Carbachol
Wash
Figure Cholinergic suppression of synaptic transmission in stratum radiatum of CA3.
DISCUSSION
In this model of the hippocampus self-organization at perforant path synapses forms compressed representations of specific patterns of cortical activity associated with events in
the environment Feedback regulation of cholinergic modulation sets appropriate dynamics for learning in response to novel stimuli allowing predominance of self-organization
and appropriate dynamics for recall in response to familiar stimuli allowing predominance of associative memory function This combination of self-organization and associative memory function may also occur in neocortical structures The selective cholinergic
suppression of feedback and intrinsic synapses has been proposed to allow self-organization of feedforward synapses while feedback synapses mediate storage of associations
between higher level representations and activity in primary cortical areas Hasselmo
This previous proposal could provide a physiological justification for a similar
mechanism utilized in recent models Dayan Detailed modeling of cholinergic effects in the hippocampus provides a theoretical framework for linking the considerable behavioral evidence for a role of acetylcholine in memory function Hagan and
Morris to the neurophysiological evidence for the effects of acetylcholine within
cortical structures Hasselmo and Bower Hasselmo
Acknowledgements
This work supported by a pilot grant from the Massachusetts Alzheimer's Disease
Research Center and by an NIMH FIRST award

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1210-self-organizing-and-adaptive-algorithms-for-generalized-eigen-decomposition.pdf

Self-Organizing and Adaptive Algorithms for
Generalized Eigen-Decomposition
Chanchal Chatterjee
Vwani P. Roychowdhury
Newport Corporation
Deere Avenue Irvine CA
Electrical Engineering Department
UCLA Los Angeles CA
ABSTRACT
The paper is developed in two parts where we discuss a new approach
to self-organization in a single-layer linear feed-forward network First
two novel algorithms for self-organization are derived from a two-layer
linear hetero-associative network performing a one-of-m classification
and trained with the constrained least-mean-squared classification error
criterion Second two adaptive algorithms are derived from these selforganizing procedures to compute the principal generalized
eigenvectors of two correlation matrices from two sequences of
random vectors These novel adaptive algorithms can be implemented
in a single-layer linear feed-forward network We give a rigorous
convergence analysis of the adaptive algorithms by using stochastic
approximation theory As an example we consider a problem of online
signal detection in digital mobile communications
INTRODUCTION
We study the problems of hetero-associative trammg linear discriminant analysis
generalized eigen-decomposition and their theoretical connections The paper is divided
into two parts In the first part we study the relations between hetero-associative training
with a linear feed-forward network and feature extraction by the linear discriminant
analysis LOA criterion Here we derive two novel algorithms that unify the two
problems In the second part we generalize the self-organizing algorithm for LOA to
obtain adaptive algorithms for generalized eigen-decomposition for which we provide a
rigorous proof of convergence by using stochastic approximation theory
HETERO-ASSOCIATION AND LINEAR DISCRIMINANT ANALYSIS
In this discussion we consider a special case of hetero-association that deals with the
classification problems Here the inputs belong to a finite m-set of pattern classes and the
Self-Organizing and Adaptive Generalized Eigen-Decomposition
outputs indicate the classes to which the inputs belong Usually the ith standard basis
vector ei is chosen to indicate that a particular input vector belongs to class
The LDA problem on the other hand aims at projecting a multi-class data in a lower
dimensional subspace such that it is grouped into well-separated clusters for the
classes The method is based upon a set of scatter matrices commonly known as the
mixture scatter Sm and between class scatter Sb Fukunaga These matrices are
used to formulate criteria such as tr(Sm-ISb and det(Sb)1 det(Sm which yield a linear
transform that satisfy the generalized eigenvector problem where A is the
generalized eigenvalue matrix If Sm is positive definite we obtain a such that
and Furthermore the significance of each eigenvector for class
separability is determined by the corresponding generalized eigenvalue
A relation between hetero-association and LDA was demonstrated by Gallinari
Their work made explicit that for a linear multi-layer perceptron performing a
one-from-m classification that minimized the total mean square error MSE at the
network output also maximized a criterion det(Sb)/det(Sm for LDA at the final hidden
layer This study was generalized by Webb and Lowe by using a nonlinear
transform from the input data to the final hidden units and a linear transform in the final
layer This has been further generalized by Chatterjee and Roychowdhury by
including the Bayes cost for misclassification into the criteria tr(Sm-ISb
Although the above studies offer useful insights into the relations between heteroassociation and LDA they do not suggest an algorithm to extract the optimal LDA
transform Since the criteria for class separability are insensitive to multiplication by
nonsingular matrices the above studies suggest that any training procedure that
minimizes the MSE at the network output will yield a nonsingular transformation of
we obtain where is a nonsingular matrix Since does not satisfy the
generalized eigenvector problem for any arbitrary nonsingular matrix we
need to determine an algorithm that will yield Q=I.
In order to obtain the optimum linear transform we constrain the training of a twolayer linear feed-forward network such that at convergence the weights for the first
layer simultaneously diagonalizes Sm and Sb. Thus the hetero-associative network is
trained by minimizing a constrained MSE at the network output This training procedure
yields two novel algorithms for LDA.
LDA AND GENERALIZED EIGEN-DECOMPOSITION
Since the LDA problem is a generalized eigen-decomposition problem for the
symmetric-definite case the self-organizing algorithms derived from the heteroassociative networks lead us to construct adaptive algorithms for generalized eigendecomposition Such adaptive algorithms are required in several applications of image
and signal processing As an example we consider the problem of online interference
cancellation in digital mobile communications
Similar to the LDA problem the generalized eigen-decomposition problem
involves the matrix pencil A where A and are assumed to be real
symmetric and positive definite Although a solution to the problem can be obtained by a
conventional method there are several applications in image and signal processing where
an online solution of generalized eigen-decomposition is desired In these real-time
situations the matrices A and are themselves unknown Instead there are available two
C. Chatterjee and V. Roychowdhury
sequences of random vectors and with limk~ooE[x~/J A and limk~oo
E[Yky/'I=B where xk and Yk represent the online observations of the application For
every sample we need to obtain the current estimates and Ak of and A
respectively such that and Ak converge strongly to their true values
The conventional approach for evaluating and A requires the computation of
after collecting all of the samples and then the application of a numerical procedure
the approach works in a batch fashion There are two problems with this approach
Firstly the dimension of the samples may be large so that even if all of the samples are
available performing the generalized eigen-decomposition may take prohibitively large
amount of computational time Secondly the conventional schemes can not adapt to slow
or small changes in the data So the approach is not suitable for real-time applications
where the samples come in an online fashion
Although the adaptive generalized eigen-decomposition algorithms are natural
generalizations of the self-organizing algorithms for LDA their derivations do not
constitute a proof of convergence We therefore give a rigorous proof of convergence
by stochastic approximation theory that shows that the estimates obtained from our
adaptive algorithms converge with probability one to the generalized eigenvectors
In summary the study offers the following contributions we present two novel
algorithms that unify the problems of hetero-associative training and LDA feature
extraction and we discuss two single-stage adaptive algorithms for generalized eigendecomposition from two sequences of random vectors
In our experiments we consider an example of online interference cancellation in digital
mobile communications In this problem the signal from a desired user at a far distance
from the receiver is corrupted by another user very near to the base The optimum linear
transform for weighting the signal is the first principal generalized eigenvector of the
signal correlation matrix with respect to the interference correlation matrix Experiments
with our algorithm suggest a rapid convergence within four bits of transmitted signal and
provides a significant advantage over many current methods
HETERO-ASSOCIATIVE TRAINING AND LDA
We consider a two-layer linear network performing a one-from-m classification Let XE
9t be an input to the network to be classified into one out of classes ro If ro
then the desired output d=e ith std basis vector Without loss of generality we assume
the inputs to be a zero-mean stationary process with a nonsingular covariance matrix
EXTRACTING THE PRINCIPAL LDA COMPONENTS
In the two-layer linear hetero-associative network let there be neurons in the hidden
layer and output units The aim is to develop an algorithm so that indi",idual weight
vectors for the first layer converge to the first p~m generalized eigenvectors
corresponding to the significant generalized eigenvalues arranged in decreasing order
Let WjE9t be the weight vectors for the input layer and VjE9t be
the weight vectors for the output layer
The neurons are trained sequentially the training of the jlh neuron is started only
after the weight vector of the j_I)fh neuron has converged Assume that all the j-I
previous neurons have already been trained and their weights have converged to the
Self-Organizing and Adaptive Generalized Eigen-Decomposition
optimal weight vectors for To extract the J'h generalized eigenvector in the
output of the neuron the updating model for this neuron should be constructed by
subtracting the results from all previously computed j-I generalized eigenvectors from
the desired output dj as below
dj
dj
j-I
vi
This process is equivalent to the deflation of the desired output
The scatter matrices Sm and Sb can be obtained from and as Sm=E[xx and Sb
MMT where We need to extract the j1h LOA transform Wj that satisfies the
generalized eigenvector equation SbWj=AlmWj such that Aj is the J'h largest generalized
eigenvalue The constrained MSE criterion at the network output is
Jh,Vj
v;wTx-vjWJxr
p{wJSmw
Using we obtain the update equation for Wj as
hI
w(J)v(i)T
L..
Differentiating with respect to vi and equating it to zero we obtain the optimum
value ofvj as MTWj Substituting this Vj in we obtain
w(J)(w(J)TS wU)w(i)T
L..
Let Wk be the matrix whose ith column is Then can be written in matrix form as
Wk r{SbWk SmWkU~W[SbWk
where sets all elements below the diagonal of its matrix argument to zero thereby
making it upper triangular
ANOTHER SELF-ORGANIZING ALGORITHM FOR LDA
In the previous analysis for a two-layer linear hetero-associative network we observed
that the optimum value for V=WTM where the jlh column of Wand row of are formed
by Wi and Vi respectively It is therefore worthwhile to explore the gradient descent
procedure on the error function below instead of
E[lld MTWWTxI12
By differentiating this error function with respect to and including the deflation
process we obtain the following update procedure for instead of
Wk 2SbWk
Sm Wk UT SbWk SbWkUT SmWk
LDA AND GENERALIZED EIGEN-DECOMPOSITION
Since LOA consists of solving the generalized eigenvector problem Sb<P=Sm<PA we can
naturally generalize algorithms and to obtain adaptive algorithms for the
generalized eigen-decomposition problem A<P=B<PA where A and are assumed to be
symmetric and positive definite Here we do not have the matrices A and B. Instead
C. Chatterjee and V. P. Roychowdhury
there are available two sequences of random vectors and with limk~ooE[xp
A and limk~~[Yky/]=B where xk and Yk represent the online observations
From we obtain the following adaptive algorithm for generalized eigendecomposition
Here is a sequence of scalar gains whose properties are described in Section The
sequences and are instantaneous values of the matrices A and respectively
Although the Ak and Bk values can be obtained from xk and Yk as xp and YkY
respectively our algorithm requires that at least one of the or sequences have a
dominated convergence property Thus the and Bk sequences may be obtained
from xp and YkY from the following algorithms
Ak
Yk(XkXk A I
and Bk
Bk I
Yk(YkYk
where Ao and Bo are symmetric and is a scalar gain sequence
As done before we can generalize to obtain the following adaptive algorithm for
generalized eigen-decomposition from a sequence of samples and
Wk Wk
BkWkUT AkWk AkWkUT BkWk
Although algorithms and were derived from the network MSE by the gradient
descent approach this derivation does not guarantee their convergence In order to prove
their convergence we use stochastic approximation theory We give the convergence
results only for algorithm
STOCHASTIC APPROX CONVG PROOF FOR ALG.
In order to prove the con vergence of we use stochastic approximation theory due to
Ljung In stochastic approximation theory we study the asymptotic properties of
in terms of the ordinary differential equation ODE
E[2AkW
BkWUT AkW AkWUT
where is the continuous time counterpart of Wk with denoting continuous time The
method of proof requires the following steps establishing a set of conditions to be
imposed on A and finding the stable stationary points of the ODE and
demonstrating that Wk visits a compact subset of the domain of attraction of a stable
stationary point infinitely often
We use Theorem of Ljung for the convergence proof The following is a general
set of assumptions for the convergence proof of
Assumption Each xk and Yk is bounded with probability one and limk~ooE[xp
A and limk~ooE[y kY where A and are positive definite
Assumption satisfies Lk=Ol7k OO,Lk=Ol7k for some and
limk~oo sup(l7i
Assumption The largest generalized eigenvalues of A with respect to are each
of unit mUltiplicity
Lemma Let Al and A2 hold Let be a locally asymptotically stable the sense of
Liapunov solution to the ordinary differential equation
Self-Organizing and Adaptive Generalized Eigen-Decomposition
BW(t)U4W(t AW(t)U4W(t
with domain of attraction Then if there is a compact subset of such that
Wk infinitely often then we have Wk with probability one as
We denote A Ap An as the generalized eigenvalues of A with
respect to and as the generalized eigenvector corresponding to A such that
are orthonormal with respect to B. Let and A=diag(A An denote the
matrix of generalized eigenvectors and eigenvalues of A with respect to B. Note that if
is a generalized eigenvector then is also a generalized eigenvector
In the next two lemmas we first prove that all the possible equilibrium points ofthe ODE
are up to an arbitrary permutation of the generalized eigenvectors of A with
respect to corresponding to the largest generalized eigenvalues We next prove that
all these equilibrium points of the ODE are unstable equilibrium points except for
nl where for
Lemma For the ordinary differential equation let Al and A3 hold Then W=<l>DP
are equilibrium points of where D=[D\IOV is a nXp matrix with DI being a pXp
diagonal matrix with diagonal elements such that Id;l or and is a nXn
arbitrary permutation matrix
Lemma Let Al and A3 hold Then where diag(d
Id;I=I are stable equilibrium points of the ODE In addition W=<l>DP for i~p
or are unstable equilibrium points of the ODE
Lemma For the ordinary differential equation let Al and A3 hold Then the
points where diag(d dp for are
asymptotically stable
Lemma Let AI-A3 hold Then there exists a uniform upper boundfor such that Wk
is uniformly bounded w.p I
The convergence of alg can now be established by referring to Theorem of Ljung
Theorem Let A hold Assume that with probability one the process Wk visits
infinitely often a compact subset of the domain of attraction of one of the asymptotically
stable points Then with probability one
lim Wk
k~OCl
Proof By Lemma are asymptotically stable points of the ODE Since
we assume that Wk visits a compact subset of the domain of attraction of infmitely
often Lemma then implies the theorem
EXPERIMENT AL RESULTS
We describe the performance of algorithms and with an example of online
interference cancellation in a high-dimensional signal in a digital mobile communication
problem The problem occurs when the desired user transmits a signal from a far distance
to the receiver while another user simultaneously transmits very near to the base For
common receivers the quality of the received signal from the desired user is dominated
by interference from the user close to the base Due to the high rate and large dimension
of the data the system demands an accurate detection method for just a few data samples
C. Chatterjee and V. P. Roychowdhury
If we use conventional numerical analysis methods signal detection will require a
significant part of the time slot allotted to a receiver accordingly reducing the effective
communication rate Adaptive generalized eigen-decomposition algorithms on the other
hand allow the tracking of slow changes and directly performs signal detection
The details of the data model can be found in Zoltowski In this application
the duration for each transmitted code is IlS within which we have lOllS of signal
and of interference We take frequency samples equi-spaced between O.4MHz
to Using antennas the signal and interference correlation matrices are of
dimension in the complex domain
We use both algorithms and for the cancellation of the interference Figure
shows the convergence of the principal generalized eigenvector and eigenvalue The
closed form solution is obtained after collecting all of the signal and interference
samples In order to measure the accuracy of the algorithms we compute the direction
cosine of the estimated principal generalized eigenvector and the generalized eigenvector
computed by the conventional method The optimum value is one We also show the
estimated principal generalized eigenvalue in Figure The results show that both
algorithms converge after the 4th bit of signal
Algonthm
Algonlhm
Algonthm
Algonlhm
35
CLOSl!D FORM SOUlTlON
lll
08
07
I
I
I
Iii
04
03
Iii
OJ
13
I
I
III
Iii
ii
NUMBER OF SAMPLES
NUMBER OF SAMPLES
Figure Direction Cosine of Estimated First Principal Generalized Eigenvector and
Estimated First Principal Generalized Eigenvalue

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1774-effective-learning-requires-neuronal-remodeling-of-hebbian-synapses.pdf

Effective Learning Requires Neuronal
Remodeling of Hebbian Synapses
Gal Chechik Isaac Meilijson Eytan Ruppin
School of Mathematical Sciences
Tel-Aviv University Tel Aviv Israel
ggal@math.tau.ac.il isaco@math.tau.ac.il ruppin@math.tau.ac.il
Abstract
This paper revisits the classical neuroscience paradigm of Hebbian
learning We find that a necessary requirement for effective associative memory learning is that the efficacies of the incoming
synapses should be uncorrelated This requirement is difficult to
achieve in a robust manner by Hebbian synaptic learning since it
depends on network level information Effective learning can yet be
obtained by a neuronal process that maintains a zero sum of the incoming synaptic efficacies This normalization drastically improves
the memory capacity of associative networks from an essentially
bounded capacity to one that linearly scales with the network's size
It also enables the effective storage of patterns with heterogeneous
coding levels in a single network Such neuronal normalization can
be successfully carried out by activity-dependent homeostasis of the
neuron's synaptic efficacies which was recently observed in cortical
tissue Thus our findings strongly suggest that effective associative learning with Hebbian synapses alone is biologically implausible and that Hebbian synapses must be continuously remodeled by
neuronally-driven regulatory processes in the brain
Introduction
Synapse-specific changes in synaptic efficacies carried out by long-term potentiation
LTP and depression LTD are thought to underlie cortical self-organization and
learning in the brain In accordance with the Hebbian paradigm LTP and LTD
modify synaptic efficacies as a function of the firing of pre and post synaptic neurons
This paper revisits the Hebbian paradigm showing that synaptic learning alone
cannot provide effective associative learning in a biologically plausible
manner and must be complemented with neuronally-driven synaptic
remodeling
The importance of neuronally driven normalization processes has already been
demonstrated in the context of self-organization of cortical maps and in continuous unsupervised learning as in principal-component-analysis networks In
these scenarios normalization is necessary to prevent the excessive growth of synap

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1014-associative-decorrelation-dynamics-a-theory-of-self-organization-and-optimization-in-feedback-networks.pdf

Associative Decorrelation Dynamics
A Theory of Self-Organization and
Optimization in Feedback Networks
Dawei W. Dong
Lawrence Berkeley Laboratory
University of California
Berkeley CA
Abstract
This paper outlines a dynamic theory of development and adaptation in neural networks with feedback connections Given input ensemble the connections change in strength according to an
associative learning rule and approach a stable state where the
neuronal outputs are decorrelated We apply this theory to primary visual cortex and examine the implications of the dynamical
decorrelation of the activities of orientation selective cells by the
intracortical connections The theory gives a unified and quantitative explanation of the psychophysical experiments on orientation
contrast and orientation adaptation Using only one parameter we
achieve good agreements between the theoretical predictions and
the experimental data
Introduction
The mammalian visual system is very effective in detecting the orientations of lines
and most neurons in primary visual cortex selectively respond to oriented lines and
form orientation columns Why is the visual system organized as such We
Present address Rockefeller University York Avenue NY NY
Dawei Dong
believe that the visual system is self-organized in both long term development and
short term adaptation to ensure the optimal information processing
Linsker applied Hebbian learning to model the development of orientation selectivity and later proposed a principle of maximum information preservation in early
visual pathways The focus of his work has been on the feedforward connections
and in his model the feedback connections are isotropic and unchanged during the
development of orientation columns but the actual circuitry of visual cortex involves extensive columnar specified feedback connections which exist even before
functional columns appear in cat striate cortex
Our earlier research emphasized the important role of the feedback connections in
the development of the columnar structure in visual cortex We developed a theoretical framework to help understand the dynamics of Hebbian learning in feedback networks and showed how the columnar structure originates from symmetry
breaking in the development of the feedback connections intracortical or lateral
connections within visual cortex
Figure illustrates our theoretical predictions The intracortical connections break
symmetry and develop strip-like patterns with a characteristic wave length which
is comparable to the developed intracortical inhibitory range and the LGN-cortex
afferent range left The feedforward LGN-cortex connections develop under the
influence of the symmetry breaking development of the intracortical connections
The developed feedforward connections for each cell form a receptive field which
is orientation selective and nearby cells have similar orientation preference right
Their orientations change in about the same period as the strip-like pattern of the
intracortical connections
Figure The results of the development of visual cortex with feedback connections The
simulated cortex consists of 48 48 neurons each of which connects to other cortical
neurons left and receives inputs from LGN neurons right In this figure white
inclicates positive connections and black inclicates negative connections One can see that
the change of receptive field's orientation right is highly correlated with the strip-like
pattern of intracortical connections left
Many aspects of our theoretical predictions agree qualitatively with neurobiological observations in primary visual cortex Another way to test the idea of optimal
Associative Correlation Dynamics
information processing or any self-organization theory is through quantitative psychophysical studies The idea is to look for changes in perception following changes
in input environments The psychophysical experiments on orientation illusions
offer some opportunities to test our theory on orientation selectivity
Orientation illusions are the effects that the perceived orientations of lines are affected by the neighboring time or space oriented stimuli which have been
observed in many psychophysical experiments and were attributed to the inhibitory
interactions between channels tuned to different orientations But there is no unified and quantitative explanation Neurophysiological evidences support our earlier
computational model in which intracortical inhibition plays the role of gain-control
in orientation selectivity But in order for the gain-control mechanism to be
effective to signals of different statistics the system has to develop and adapt in
different environments
In this paper we examine the implication of the hypothesis that the intracortical
connections dynamically decorrelate the activities of orientation selective cells
the intracortical connections are actively adapted to the visual environment such
that the output activities of orientation selective cells are decorrelated The dynamics which ensures such decorrelation through associative learning is outlined in the
next section as the theoretical framework for the development and the adaptation
of intracortical connections We only emphasize the feedback connections in the
following sections and assume that the feedforward connections developed orientation selectivities based on our earlier works The quantitative comparisons of the
theory and the experiments are presented in section
Associative Decorrelation Dynamics
There are two different kinds of variables in neural networks One class of variables
represents the activity of the nerve cells or neurons The other class of variables
describes the synapses or connections between the nerve cells A complete model
of an adaptive neural system requires two sets of dynamical equations one for each
class of variables to specify the evolution and behavior of the neural system
The set of equations describing the change of the state of activity of the neurons is
dVi
adt
I
ViI
I
in which a is a time constant Tij is the strength of the synaptic connection from
neuron to neuron and Ii is the additional feedforward input to the neuron besides
those described by the feedback connection matrix nj A second set of equations
describes the way the synapses change with time due to neuronal activity The
learning rule proposed here is
dnj
dt
in which is a time constant and
in the following
Vi
I
I
is the feedback learning signal as described
The feedback learning signal Vi is generated by a Hopfield type associative memory
network Vi Lj T/j Vi in which T/j is the strength of the associative connection
Dawei Dong
from neuron to neuron which is the recent correlation between the neuronal
activities Vi and Vj determined by Hebbian learning with a decay term
dTfj
dt Iij ViVj
in which is a time constant The Vi and T[j are only involved in learning and
do not directly affect the network outputs
It is straight forward to show that when the time constants
dynamics reduces to
dT
dt
VVT
a the
VIT
where bold-faced quantities are matrices and vectors and denotes ensemble
average It is not difficult to show that this equation has a Lyapunov or energy
function
VV VVT
which is lower bounded and satisfies
dL
dt
and
dL
dt
dTij
dt
I'lor
Thus the dynamics is stable When it is stable the output activities are decorrelated
VVT
The above equation shows that this dynamics always leads to a stable state where
the neuronal activities are decorrelated and their correlation matrix is orthonormal
Yet the connections change in an associative fashion equation and are
almost Hebbian That is why we call it associative decorrelation dynamics From information processing point of view a network self-organized to satisfy equation
is optimized for Gaussian input ensembles and white output noises
Linear First Order Analysis
In applying our theory of associative decorrelation dynamics to visual cortex to
compare with the psychophysical experiments on orientation illusions the linear
first-order approximation is used which is
TO
Va
TO 6T ex I IT
Va I 6V TI
where it is assumed that the input correlations are small It is interesting to notice
that the linear first-order approximation leads to anti-Hebbian feedback connections Iij ex which is guarantteed to be stable around
Quantitative Predictions of Orientation Illusions
The basic phenomena of orientation illusions are demonstrated in figure left
On the top is the effect of orientation contrast also called tilt illusion within the
two surrounding circles there are tilted lines the orientation of a center rectangle
Associative Correlation Dynamics
appears rotated to the opposite side of its surrounding tilt Both the two rectangles and the one without surround at the left-center of this figure are in fact
exactly same On the bottom is the effect of orientation adaptation also called
tilt aftereffect if one fixates at the small circle in one of the two big circles with
tilted lines for seconds or so and then look at the rectangle without surround
the orientation of the lines of the rectangle appears tilted to the opposite side
These two effects of orientation illusions are both in the direction of repulsion the
apparent orientation of a line is changed to increase its difference from the inducing
line Careful experimental measurements also revealed that the angle with the
inducing line is
for maximum orientation adaptation effect but for
orientation contrast
90
45
Stimulus orientation
45
90
degree
Figure The effects of orientation contrast upper-left and orientation adaptation lowerleft are attributed to feedback connections between cells tuned to different orientations
upper-right network lower-right tuning curve
Orientation illusions are attributed to the feedback connections between orientation selective cells This is illustrated in figure right On the top is the network
of orientation selective cells with feedback connections Only four cells are shown
From the left they receive orientation selective feedforward inputs optimal at 45
45 and 90 respectively The dotted lines represent the feedback connections
only the connections from the second cell are drawn On the bottom is the orientation tuning curve of the feedforward input for the second cell optimally tuned to
stimulus of vertical which is assumed to be Gaussian of width Because of the feedback connections the output of the second cell will have different
tuning curves from its feedforward input depending on the activities of other cells
For primary visual cortex we suppose that there are orientation selective neurons
tuned to all orientations It is more convenient to use the continuous variable
instead of the index to represent neuron which is optimally tuned to the orientation
of angle The neuronal activity is represented by and the feedforward input
to each neuron is represented by The feedforward input itself is orientation
Dawei W. Dong
selective given a visual stimulus of orientation
eo the input is
This kind of the orientation tuning has been measured by experiments for

<<----------------------------------------------------------------------------------------------------------------------->>

title: 161-neural-approach-for-tv-image-compression-using-a-hopfield-type-network.pdf

NEURAL APPROACH FOR TV IMAGE COMPRESSION
USING A HOPFIELD TYPE NETWORK
Martine NAILLON
Jean-Bernard THEETEN
Laboratoire d'Electronique de Physique Appliquee
Avenue DESCARTES BP
LIMEIL BREVANNES Cedex FRANCE
ABSTRACT
A self-organizing Hopfield network has been
developed in the context of Vector Ouantiza-tion aiming at compression of television
images The metastable states of the spin
glass-like network are used as an extra
storage resource using the Minimal Overlap
learning rule Krauth and Mezard to
optimize the organization of the attractors
The sel f-organi zi ng scheme that we have
devised results in the generation of an
adaptive codebook for any qiven TV image
I NTRODOCTI ON
The ability of an Hopfield network
Hopfield Amit and Personnaz and
Hertz to behave as an associative memory
usua aSSlJ11es a pri ori knowl edge of the patterns to be
stored As in many applications they are unknown the aim
of this work is to develop a network capable to learn how
to select its attractors TV image compression using
Vector Quantization a key issue for
HOTV transmission is a typical case since the non
neural algorithms which generate the list of codes the
codebookl are suboptimal As an alternative to the
prani si ng neural canpressi on techni ques Jackel
Kohonen Grossberg Cottrel
our idea is to use the metastability in a spin
glass-like net as an additional storage resource and to
cl usteri nq a1gori thm a
derive after a cl assi cal
sel f-organi zi ng sheme for generatf ng adaptively the
codebook We present the illustrative case of 2D-vectors
LEP A member of the Philips Research Organization
Neural Approach for TV Image Compression
NON NEURAL APPROACH
In the image is divided into blocks named vectors
of pixels typically pixels Given the codebook
each vector is coded by associating it with the nearest
element of the list Nearest Neighbour Classifier
fi ure
EMCaD
INPUT
YEtTa
COP1PARE
INDEX
ftECDNINDEX CODEBOOK STRUCTED
VECTOR
CODE BOOK
Figure Basic scheme of a vector quantizer
For designing an optimal codebook a clustering algorithm
is app1 ied to a training set of vectors figure the
criterium of optimality being a distorsion measure
between the training set and the codebook The algorithm
is actua subopt ima1 especi for non connex
training set as it is based on an iterative computation
of centers of grav ty whi ch tends to overcode the dense
regions of poi nts whereas the 1ight ones are undercoded
figure
PIXEl
Figure Training set of two pixels vectors and the
associated codebook canputed by a non neural c1 ustering
algorithm overcoding of the dense regions pixel
and subcoding of the light ones
Naillon and Theeten
NEURAL APPROACH
In a Hopfield neural network the code vectors are the
attractors of the net and the neural dynamics resolution
phase
is
substituted to the nearest neighbourg
classification
en patterns referred to as prototypes and named
here explicit memory
are prescribed in a spin
glass-like net other attractors
referred to as
me tastable states are induced in the net Sherrington
and Kirkpatrick Toulouse Hopfield
Mezard and We consider those induced
attractors as additional memory named here impl icit
memory whi ch can be used by the network to code the
previously mentioned light regions of points This
provides a higher flexibility to the net during the
self-organization process as it can choose in a large
basis of explicit and implicit attractors the ones which
will optimize the coding task
NEURAL NOTATION
A vector of pixels with bits per pel is a vector of
dimensions in an Eucl idean space where each dimension
corresponds to grey levels To preserve the Euclidean
di stance we use the well-known themometri notati on
neurons for level per dimens on the number of
neurons set to one th a reg ul ar orderi ng iv ng the
pixel luminance For vectors of
dimension neurons will be used
INDUCTION PROCESS
The induced impl icit memory depends on the prescription
rule We have compared the Projection rule Personnaz and
and the Minimal Overlap rule Krauth and
Mezard
The metastable states are detected by relaxing any point
of the training set of the figure to its
corresponding prescribe or induced attractor marked in
figure with a small diamond
For the two rules the induction process is rather
detenni ni stic generati ng an orthogonal mesh if two
prototypes and are prescribed a
metastable state is induced at the cross-points namely
and figure
Neural Approach for TV Image Compression
PDIB
PDCEI
Figure Comparaison of the induction process for
prescription rules The prescribed states are the full
squares the induced states the open diamonds
What differs between the two rul es the number of
induced attractors For prototypes and a training set
of 2d-vectors the projection rule induces about
metastable states ratio whereas Min
Over induces only ratio This is due to the
different stabil ity of the prescribed and the induced
states in the case of Min Over Naillon and Theeten to
be published
GENERALIZED ATTRACTORS
Some attractors are induced out of the image space
Figure as the neurons space has
configurations to be compared with the image
configurati ons
We extend the image space by defi n1 ng a genera 1i ze
attractor as the class of patterns having the same
number of neurons set to one for each pixel whatever
thei orderi ng Such a notati on corresponds to a random
thermometri neural representati on The simul ati on has
shown that the generalized attractors correspond to
acceptable states Figure they are located at the
place when one would like to obtain a normal attractor
NsiIlon and Theeten
NO GENERAUZATION
WITI-I GENERAUZATJON
wrTHOVT AT
CII!JeMIJZm
AJ'TAACT'OR
A
fl
I
j.J
I
PIXEL
I't hjlt
to
PIXEL
Figure The induced bassins of attractions are
represented with arrows In the left plot some training
vectors have no attractor in the image space After
generalization randon thermometric notation the right
ot shows their corresponding attractors
ADAPTIVE NEURAL CODEBOOK LEARNING
An iterative sel organi zi ng process has been developed
to optimi ze the codebook For a given TV image the
codebook is defined at each step of the process as the
set of prescribed and induced attractors selected by the
training set of vectors The self-organizing scheme is
controlled by a cost function the distorsion measure
between the training set and the codebook Having a
target of code vectors we have to prescri be at each
step as discussed above typically
prototypes As seen in figure Sa we choose initial
prototypes uniformly distributed along the bisecting
line Using the training set of vectors of the figure
the induced metastable states are detected with their
corresponding bassins of attraction The most
frequent prescribed or induced attractors are selected
and the
centers of gravi ty of thei bassi ns of
attracti on are taken as new prototypes figure 5b
After iterations the distorsion measure stabilizes
Table
Neural Approach for TV Image Compression
INmALIZATION
PlXB.1
PIXEl
Initialization of the self-organizing scheme
Fi gure 5a
ITERATION
FAST ORGANIZATION
PROTOTYPES
I
Figure 5b
scheme
First iteration of the self-organizinq
Iobal
codebook
dislofsion size
itrrllioM
97
97
98
53
57
79
84
68
eneralized
aUraclors
I
Table Evolution of the distorsion measure versus the
iterations of the self-organizing scheme It stabilizes
in iterations
NOOllon and Theeten
Fourty 1i nes of a TV image the port of Ba 1timore of
bits per pel has been coded with an adaptive neural
codebook of 20-vectors The coherence of the coding is
visible from the apparent continuity of the image
Figure
The coded image has bits per pel
I
Figure Neural coded image with bits per pel
CONCLUSION
Using
a
classical
clusterinq
algorithm
a
self-organizing scheme has been developed in a Hopfield
network f.or the adaptive design of a codebook of small
imensi on vectors ina Vector Quanti zati on techni Que. It
has been shown that using the Minimal Overlap
prescription rule the metastable states induced in a
spin gl ass-like network can be used as extra-codes The
optimal organization of the prescribed and induced
attractors has been defined as the limit organization
obtained from the iterative learning process It is an
example of learning by selection as already proposed by
physicists and biologists Toulouse and ale
Hard~re
impl ementation on the neural VLSI ci rcuit
curren~y
designed at LEP should allow for on-line
codebook computations
We woul like to thank J.J. Hopfield who has inspired
this study as well H. Bosma and W. Kreuwel from Phil ips
Research Laboratories Eindhoven who have allow to
initialize this research
Neural Approach for TV Image Compression

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1554-neuronal-regulation-implements-efficient-synaptic-pruning.pdf

Neuronal Regulation Implements
Efficient Synaptic Pruning
Gal Chechik and Isaac Meilijson
School of Mathematical Sciences
Tel Aviv University Tel Aviv Israel
ggal@math.tau.ac.il isaco@math.tau.ac.il
Eytan Ruppin
Schools of Medicine and Mathematical Sciences
Tel Aviv University Tel Aviv Israel
ruppin@math.tau.ac.il
Abstract
Human and animal studies show that mammalian brain undergoes
massive synaptic pruning during childhood removing about half of
the synapses until puberty We have previously shown that maintaining network memory performance while synapses are deleted
requires that synapses are properly modified and pruned removing the weaker synapses We now show that neuronal regulation a
mechanism recently observed to maintain the average neuronal input field results in weight-dependent synaptic modification Under
the correct range of the degradation dimension and synaptic upper bound neuronal regulation removes the weaker synapses and
judiciously modifies the remaining synapses It implements near
optimal synaptic modification and maintains the memory performance of a network undergoing massive synaptic pruning Thus
this paper shows that in addition to the known effects of Hebbian
changes neuronal regulation may play an important role in the
self-organization of brain networks during development
Introduction
This paper studies one of the fundamental puzzles in brain development the massive synaptic pruning observed in mammals during childhood removing more than
half of the synapses until puberty for review This phenomenon is observed in various areas of the brain both in animal studies and human studies How
can the brain function after such massive synaptic elimination what could be the
computational advantage of such a seemingly wasteful developmental strategy In
G. Chechik I. Meilijson and E. Ruppin
98
previous work we have shown that synaptic overgrowth followed by judicial
pruning along development improves the performance of an associative memory
network with limited synaptic resources thus suggesting a new computational explanation for synaptic pruning in childhood The optimal pruning strategy was
found to require that synapses are deleted according to their efficacy removing the
weaker synapses first
But is there a mechanism that can implement these theoretically-derived synaptic
pruning strategies in a biologically plausible manner To answer this question we
focus here on studying the role of neuronal regulation a mechanism operating
to maintain the homeostasis of the neuron membrane potential NR has been recently identified experimentally by who showed that neurons both up-regulate
and down-regulate the efficacy of their incoming excitatory synapses in a multiplicative manner maintaining their membrane potential around a baseline level
Independently have studied NR theoretically showing that it can efficiently
maintain the memory performance of networks undergoing synaptic degradation
Both and have hypothesized that NR may lead to synaptic pruning during
development
In this paper we show that this hypothesis is both computationally feasible and
biologically plausible by studying the modification of synaptic values resulting from
the operation of NR. Our work thus gives a possible account for the way brain
networks maintain their performance while undergoing massive synaptic pruning
The Model
NR-driven synaptic modification NRSM results from two concomitant processes
synaptic degradation which is the inevitable consequence of synaptic turnover
and neuronal regulation operating to compensate for the degradation
We therefore model NRSM by a sequence of degradation-strengthening steps At
each time step synaptic degradation stochastically reduces the synaptic strength
to W't+l by
W't+l
where is noise term with positive mean and the power a defines the degradation
dimension parameter chosen in the range Neuronal regulation is modeled by
letting the post-synaptic neuron multiplicatively strengthen all its synapses by a
common factor to restore its original input field
W'tH li
Ii
where If is the input field of neuron at time The excitatory synaptic efficacies are
assumed to have a viability lower bound below which a synapse degenerates and
vanishes and a soft upper bound beyond which a synapse is strongly degraded
reflecting their maximal efficacy To study of the above process in a network a
model incorporating a segregation between inhibitory and excitatory neurons
obeying Dale's law is required To generate this essential segregation we modify
the standard low-activity associative memory model proposed by by adding a
small positive term to the synaptic learning rule In this model memories
are stored in an excitatory neuron network forming attractors of the network
dynamics The synaptic efficacy Wij between the jth pre-synaptic neuron and
the ith post-synaptic neuron is
Wij
I p)(ej a
99
Neuronal Regulation Implements Efficient Synaptic Prnning
where are I memory patterns with coding level fraction of firing
neurons and a is some positive constant The updating rule for the state Xf of
the ith neuron at time is
If
L9(Wij)Xj
j=l
sign(J
j=l
where is the neuronal threshold and I is the inhibition strength is a general
modification function over the excitatory synapses which is either derived explicitly
See Section or determined implicitly by the operation of NRSM If is linear
and I Mathe model reduces to the original model described by The overlap
mil similarity between the network activity pattern and the memory
serves to measure memory performance retrieval acuity and is defined as mil
Ef=l
p)Xj
euronally Regulated Synaptic Modification
NRSM was studied by simulating the degradation-strengthening sequence in a network in which memory patterns were stored according to Figure la plots a
typical distribution of synaptic values as traced along a sequence of degradationstrengthening steps As evident the synaptic values diverge some of the
weights are strengthened and lie close to the upper synaptic bounds while the other
synapses degenerate and vanish Using probabilistic considerations it can be shown
that the synaptic distribution converge to a meta-stable state where it remains for
long waiting times Figure Ib describes the metastable synaptic distribution as
calculated for different values
Evolving distribution of synaptic efficacies
a Simulation results
Numerical results
CJ
CJ
I
I
I
I
ctl
I
Alpha=O.O
Alpha=O.5
Alpha=O.9
I
Figure Distribution of synaptic strengths following a degradation-strengthening
process Synaptic distribution after and degradationstrengthening steps of a neurons network with stored memory patterns
18 and Qualitatively similar
results were obtained for a wide range of simulation parameters The synaptic
distribution of the remaining synapses at the meta-stable state was calculated as
the main eigen vector of the transition probability matrix
the weights are normally distributed with expectation a and standard deviation O(VM the probability of a negative synapse vanishes as goes to infinity and
is negligible already for several dozens of memories in the parameters range used here
As
G. Chechik I Meilijson and E. Ruppin
a NRSM functions at the
Metastable state
NRSM and
random deletion
NR modification
Random deletion
Original synaptic strength
04
Network's Connectivity
Figure NRSM functions at the metastable state for different a values Results were obtained in a 400-neurons network after performing degradationstrengthening steps Parameter values are as in Figure except
Performance of NR modification and random deletion The retrieval acuity of
memories stored in a network of neurons is portrayed as a function of network
connectivity as the network undergoes continuous pruning until NR reaches the
metastable state a rna a
and N(O.OI
To further investigate which synapses are strengthened and which are pruned we
study the resulting synaptic modification function Figure 2a plots the value of
synaptic efficacy at the metastable state as a function of the initial synaptic efficacy for different values of the degradation dimension a As observed a sigmoidal
dependency is obtained where the slope of the sigmoid s.trongly depends on the
degradatiori dimension In the two limit cases additive degradation results
in a step function at the metastable state while multiplicative degradation
results in random diffusion of the synaptic weights toward a memory less mean value
Different values of a and result in different levels of synaptic pruning When
the synaptic upper bound is high the surviving synapses assume high values
leading to massive pruning to maintain the neuronal input field which in turn reduces network performance Low values lead to high connectivity but limit
synapses to a small set of possible values again reducing memory performance Our
simulations show that optimal memory retrieval is obtained for values that lead
to deletion levels of in which NR indeed maintains the network performance Figure 2b traces the average retrieval acuity of a network throughout the
operation of NR versus a network subject to random deletion at the same pruning
levels While the retrieval of a randomly pruned network collapses already at low
deletion levels of about a network undergoing NR performs well even in high
deletion levels
Optimal Modification In Excitatory-Inhibitory Networks
To obtain a a comparative yardstick to evaluate the efficiency of NR as a selective
pruning mechanism we derive optimal modification functions maximizing memory
performance in our excitatory-inhibitory model and compare them to the NRSM
functions
Neuronal Regulation Implements Efficient Synaptic Pruning
We study general synaptic modification functions which prune some of the synapses
and possibly modify the rest while satisfying global constraints on synapses such
as the number or total strength of the synapses These constraints reflect the
observation that synaptic activity is strongly correlated with energy consumption
in the brain and synaptic resources may hence be inherently limited in the adult
brain
We evaluate the impact of these functions on the network's retrieval performance
by deriving their effect on the signal to noise ratio SIN of the neuron's input field
Eqs known to be the primary determinant of retrieval capacity This
analysis conducted in a similar manner to yields
where and is the modification function of but is now explicitly
applied to the synapses To derive optimal synaptic modification functions with
limited synaptic resources we consider functions that zero all synapses except
those in some set A and keep the integral
OVz A
limited We then maximize the SIN under this constraint using the Lagrange
method Our results show that without any synaptic constraints the optimal function is the identity function that is the original Hebbian rule is optimal When
the number of synapses is restricted the optimal modification function is a
linear function for all the remaining synapses
a
aW J.ta+b
where
Ta
dz
A
for any deletion set A. To find the synapses that should be deleted we have numerically searched for a deletion set maximizing SIN while limiting to positive
values as required by the segregation between excitatory and inhibitory neurons
The results show that weak synapses pruning a modification strategy that removes the weakest synapses and modifies the rest according to is optimal
at deletion levels above For lower deletion levels the above function fails
to satisfy the positivity constraint for any set A. When the positivity constraint is
ignored SIN is maximized if the weights closest to the mean are deleted and the
remaining synapses are modified according to Eq We name this strategy mean
synapses pruning Figure plots the memory capacity under weak-synapses
pruning compared with random deletion and mean-synaptic pruning showing that
pruning the weak synapses performs at least near optimally for lower deletion levels
as well Even more interesting under the correct parameter values weak-synapses
pruning results in a modification function that has a similar form to the NR-driven
modification function studied in the previous Section both strategies remove the
weakest synapses and linearly modify the remaining synapses in a similar manner
In the case of limited overall synaptic strength in the optimal
satisfies
and thus for and the optimal modification function is again linear For
a sublinear modification function is optimal where is a function of
G. Chechik I. Meilijson and E. Ruppin
Capacity of different synaptic modification functions
a Analysis results
Simulations results
I
I
I
Figure Comparison between performance of different modification strategies as a
function of the deletion level percentage of synapses pruned Capacity is measured
as the number of patterns that can be stored in the network and be
recalled almost correctly rn from a degraded pattern rna
and is thus unbounded for all Therefore in our model bounds on the synaptic
efficacies are not dictated by the optimization process Their computational advantage arises from their effect on preserving memory capacity in face of ongoing
synaptic pruning
Discussion
By studying NR-driven synaptic modification in the framework of associative memory networks we show that NR prunes the weaker synapses and modifies the remaining synapses in a sigmoidal manner The critical variables that govern the
pruning process are the degradation dimension and the upper synaptic bound Our
results show that in the correct range of these parameters NR implements
a near optimal strategy maximizing memory capacity in the sparse connectivity levels observed in the brain
A fundamental requirement of central nervous system development is that the system should continuously function while undergoing major structural and functional developmental changes It has been proposed that a major functional role
of neuronal down-regulation during early infancy is to maintain neuronal activity
at its baseline levels while facing continuous increase in the number and efficacy
of synapses Focusing on up-regulation our work shows that NR has another
important interesting effect that of modifying and pruning synapses in a continuously optimal manner Neuronally regulated synaptic modifications may play the
same role also in the peripheral nervous system It was recently shown that in the
neuro-muscular junction the muscle regulates its incoming synapses in a way similar to NR Our analysis suggests this process may be the underlying cause for
the finding that synapses in the neuro-muscular junction are either strengthened or
pruned according to their initial efficacy
The significance of our work goes beyond understanding synaptic organization and
remodeling in the associative memory models studied in this paper Our analysis
bears relevance to two other fundamental paradigms Hetero Associative memory
and self organizing maps sharing the same basic synaptic structure of storing as
Neuronal Regulation Implements Efficient Synaptic Pruning
sociations between sets of patterns via a Hebbian learning rule
Combining the investigation of a biologically identified mechanism with the analytic study of performance optimization in neural network models this paper shows
the biologically plausible and beneficial role of weight dependent synaptic pruning
Thus in addition to the known effects of Hebbian learning neuronal regulation may
play an important role in the self-organization of brain networks during development

<<----------------------------------------------------------------------------------------------------------------------->>

title: 353-self-organization-of-hebbian-synapses-in-hippocampal-neurons.pdf

Self-organization of Hebbian Synapses
in Hippocampal Neurons
Thomas H. Brown,t Zachary F. Mainen,t Anthony M. Zador,t and Brenda J. Claiborne
Department of Psychology
Division of Life Sciences
Yale University
University of Texas
New Haven cr
San Antonio TX
ABSTRACT
We are exploring the significance of biological complexity for neuronal
computation Here we demonstrate that Hebbian synapses in realistically-modeled hippocampal pyramidal cells may give rise to two novel
forms of self-organization in response to structured synaptic input First
on the basis of the electrotonic relationships between synaptic contacts
a cell may become tuned to a small subset of its input space Second the
same mechanisms may produce clusters of potentiated synapses across
the space of the dendrites The latter type of self-organization may be
functionally significant in the presence of nonlinear dendritic conductances
INTRODUCTION
Long-term potentiation LTP is an experimentally observed form of synaptic plasticity
that has been interpreted as an instance of a Hebbian modification Kelso al
Brown al The induction ofLTP requires synchronous presynaptic activity and
postsynaptic depolarization Kelso al We have previously developed a detailed
biophysical model of the LTP observed at synapses onto hippocampal region CAl pyrami
39
Brown Mainen Zador and Claiborne
Figure Two-dimensional projection of a reconstructed hippocampal CAl pyramidal cell
dal neurons Zador al The synapses at which this form of LTP occurs are distributed across an extensive dendritic arbor During synaptic stimulation the
membrane voltage at each synapse is different In this way a biological neuron differs
from the processing elements typically used in neural network models where the postsynaptic activity can be represented by a single state variable We have developed an electrotonic model based on an anatomically reconstructed neuron We have used this model to
explore how the spatial distribution of inputs and the temporal relationships of their activation affect synaptic potentiation
THE NEURONAL MODEL
Standard compartmental modeling techniques were used to represent the electrical structure of hippocampal CAl pyramidal cells
MORPHOLOGY AND ELECTRICAL PARAMETERS
Morphometric data were obtained from three-dimensional reconstructions Brown
of hippocampal neurons A correction factor was applied to the membrane
area based on an estimate for spine density of The original measurements divided
a single neuron into cylinders with an average length of For simulation
purposes this structure was collapsed into compartments preserving the connectivity pattern and changes in process diameter Electrical constants were Rm 70 ID-cm
em JlF'lcrrll Ri n-cm Spruston Johnston The membrane was electrically passive Synaptic currents were modeled as the sum of fast AMPA and slow NMDA
conductances on the head of a two-compartment spine Zador The AMPA
conductance was represented by an alpha function Jack with time constant of
msec Brown and Johnston The NMDA conductance was represented by a
more complicated function with two time constants and a voltage dependence due to voltage-sensitive channel blocking by ions Zador Brown
The initial peak conductances gAMPA and gNMDA were set to and nS respectively
Self-organization of Hebbian Synapses in Hippocampal Neurons
SIMULATION AND SYNAPTIC MODIFICATION
Simulations were run on a Sun workstation using a customized version of NEURON
a simulator developed by Michael Hines Hines Prior to a simulation patterns of
synapses were selected at random from a pool of synapses distributed unifonnly over
the apical and basal dendrites Simulations were divided into trials of msec At the
beginning of each trial a particular pattern of synapses was activated synchronously
stimuli at intervals of msec The sequential presentation of all selected patterns
constituted an epoch An entire simulation consisted of presentation epochs Over the
course of each trial membrane potential was computed at each location in the dendritic
tree and these voltages were used to compute weight changes according to the
Hebbian algorithm described below After each trial the actual peak AMPA conductances
gAMPA hereafter denoted were scaled by the sigmoidal function
gmax
where detennines the steepness of the sigmoid and gfM was set to nS
The rule for synaptic modification was based on a biophysical interpretation Kairiss aI
Brown aI of a generalized bilinear fonn of Hebbian algorithm Brown
where a and are functionals.l is a constant represents postsynaptic activity and
a represents presynaptic activity This equation specifies an interactive fonn of synaptic
ehhancement combined with three noninteractive forms of synaptic depression all of
which have possible neurobiological analogs Brown aI The interactive tenn was
derived from a biophysical model of LTP induction in a spine Zador A simplified version of this model was used to compute the concentration of Ca bound calmodulin It has been suggested that CaM-C84 may trigger protein kinases
responsible for LTP induction In general was a nonlinear function of subsynaptic voltage Zador al
The biophysical mechanisms underlying synaptic depression are less well understood The
constant represents a passive decay process and was generally set to zero The functional
represents heterosynaptic depression based on postsynaptic activity In these simulations was proportional the amount of depolarization of the subsynaptic membrane from
resting potential The functional represents homosynaptic depression based
on presynaptic activity Were was proportional to the AMPA conductance which can
be considered a measure of exclusively presynaptic activity because it is insensitive to
postsynaptic voltage The three activity-dependent tenns were integrated over the period
of the trial in order to obtain a measure of weight change Reinterpreting a and Yas constants the equation is thus
ial
a CamCa
V.rYII
YgAMPA dt
41
42
Brown Mainen Zador and Claiborne
tOO
m.sec
tOO
m.sec
epochs
Figure Interactions among Hebbian synapses produce differing global effects winning and
losing patterns on the basis of the spatial distribution of synapses The PSP always measured
at the soma due to two different patterns of synapses are plotted as a function of the presentation
epoch Initially pattern solid line evoked a slightly greater PSP than pattern dotted line inset top right Mter epochs these responses were reversed thePSP due to pattern was depressed while the PSP due to pattern was potentiated inset top left
RESULTS
Analysis of the simulations revealed self-organization in the form of differential modification of synaptic strengths Mainen Two aspects of the self-organization phenomena were distinguished In some simulations a form of pattern selection was observed
in which clear winners and losers emerged In other simulations the average synaptic
efficacy remained about the same but spatial heterogeneities~lustering~f synaptic
strength developed Different measures were used to assess these phenomena
PATTERNSELECTION
The change in the peak postsynaptic potential recorded at the soma SP provided one useful measure of pattern selection In many simulations pattern selection resulted in a
marked potentiation of the PSP due to some patterns and a depression of the PSP due to
others The PSP can be regarded as an indirect measure of the functional consequence of
self-organization In the simulation illustrated in patterns of synapses produced
an average PSP of mV before learning After learning responses ranged from to
of this amount Underlying.pattern selection was a ch8!!ge in the average peak synaptic conductance for the patterng8YIIO).1 The initial value of g8YII was same for all patterns and its final value was bounded by eq In many simulations g8YII approached the
upper bound for some patterns and the lower bound for other patterns In this way
the neuron became selectively tuned to a subset of its original set of inputs The specificity
Self-organization of Hebbian Synapses in Hippocampal Neurons
epochs
Figure The mean synaptic conductance gSy"of two patterns is plotted as a function of the presentation epoch Both patterns began with iaenucal total synaptic strength synapses with gs,r
Synaptic conductances were constrained to the range nS Mter twenty epochs
gSY of pattern solid line approached the minimum ofO.OnS while gsy of pattern dotted line
approached the maximum of nS
of this tuning was dependent on the parameter values of the neuronal model learning rule
and stimulus set
CLUSTER FORMAnON
Heterogeneity in the spatial distribution of strengthened and weakened synapses was often
observed After learning spatial clusters of synapses with similar conductances formed
These spatial heterogeneities can be illustrated in several ways In one convenient method
Brown synapses are represented as colored points superimposed on a rendition of the neuronal morphology as illustrated in By COlor-coding gsyn for each
synapse in a pattern correlations in synaptic strength across dendritic space are immediately apparent In a second method better suited to the monochrome graphics available in the
present text the evolution of the variance of gsyn is plotted as a function of time
In the simulation illustrated here the increase in variance was due to the formation of a single relatively large cluster of strengthened synapses Within other parameter regimes multiple clusters of smaller size were formed
DISCUSSION
The important differences between synaptic modifications in the biophysically-modeled
neuron and those in simple processing elements arise from voltage gradients present in the
realistic model Brown Kairiss In standard processing elements
Although SJ and the somatic PSP were generally correlated the relationship between the two is
not linear as was often evident in simulations compare initial trials in Figs and
43
44
Brown Mainen Zador and Claiborne
til
epochs
Figure Synaptic heterogeneity is indicated by increases in the variance of the set of synaptic
conductances for each pattern The variances of the peak synaptic conductances of patterns
are plotted as ajy)lction of the epoch The variance of all patterns approached the theoretical
maximum of In this parameter regime the variance was due to the potentiation of a single
large cluster of synapes combined with the depression of other synapses
a single state variable represents postsynaptic activity In contrast the critical subsynaptic
voltages which represent postsynaptic activity in the neuron are correlated but are not strictly equal The structure and electrical properties of the cell interact with its synaptic input
to detennine the precise spatiotemporal pattern of membrane voltage Thus the voltage at
any synapse depends strongly on its electrotonic relationships to other active synapses The
way in which this local depolarization affects the nature of self-organization depends on the
specific mechanisms of the synaptic modification rule We have modeled a pair of opposing voltage-dependent mechanisms An interactive potentiation mechanism the functional
ex promotes cooperativity between spatially proximal synapses with temporally correlated
activity A heterosynaptic depression mechanism the functional which is independent
of presynaptic activity promotes competition among spatially proximal synapses
Through mechanisms such as these the specific electrotonic structure of a neuron predetennines a complex set of interactions between any given spatial distribution of synaptic
inputs We have shown that these higher-order interactions can give rise to self-organization with at least two interesting effects
SPARSE REPRESENTATION
The phenomenon of pattern selection demonstrates how Hebbian self-organization may
naturally tune neurons to respond to a subset of their input space This tuning mechanism
might allow a large field of neurons to develop a sparse coding of the activity in a set of
input fibers since each neuron would respond to a particular small portion of the input
space Sparse coding may be advantageous to associative learning and other types of neural
computation Kanerva
Self-organization of Hebbian Synapses in Hippocampal Neurons
CLUSTERING AND NONLINEAR COMPUTATION
The fonnation of clusters of strengthened synapses illustrates a property of Hebbian selforganization whose functional significance might only be appreciated in the presence of
nonlinear voltage-dependent dendritic conductances We have examined the self-organization process in an electrically passive neuron Under these conditions the presence of
clustering within patterns has little effect on the observed output In fact it is known that
hippocampal cells of the type modeled possess a variety of spatially heterogeneous nonlinear dendritic conductances Jones The computational role of such nonlinearities is just beginning to be explored It is possible that interactions between synaptic
clustering and nonlinear membrane patches may significantly affect both the perfonnance
of dendritic computations and the process of self-organization itself
Acknowledgments
This research was supported by grants from the Office of Naval Research the Defense Advanced Research Projects Agency and the Air Force Office of Scientific Research

<<----------------------------------------------------------------------------------------------------------------------->>

