query sentence: search only non-malicious real-world web-pages
---------------------------------------------------------------------
title: 2718-semi-supervised-learning-on-directed-graphs.pdf

Semi-supervised Learning on Directed Graphs
Dengyong Zhou Bernhard Sch?olkopf and Thomas Hofmann
Max Planck Institute for Biological Cybernetics
Tuebingen Germany
dengyong.zhou bernhard.schoelkopf}@tuebingen.mpg.de
Department of Computer Science Brown University
Providence RI USA
th@cs.brown.edu
Abstract
Given a directed graph in which some of the nodes are labeled we investigate the question of how to exploit the link structure of the graph to infer
the labels of the remaining unlabeled nodes To that extent we propose a
regularization framework for functions defined over nodes of a directed
graph that forces the classification function to change slowly on densely
linked subgraphs A powerful yet computationally simple classification
algorithm is derived within the proposed framework The experimental
evaluation on real-world Web classification problems demonstrates encouraging results that validate our approach
Introduction
We consider semi-supervised classification problems on weighted directed graphs in which
some nodes in the graph are labeled as positive or negative and where the task consists in
classifying unlabeled nodes Typical examples of this kind are Web page categorization
based on hyperlink structure and document classification or recommendation based
on citation graphs yet similar problems exist in other domains such as computational
biology For the sake of concreteness we will mainly focus on the Web graph in the sequel
the considered graph represents a subgraph of the Web where nodes correspond to Web
pages and directed edges represent hyperlinks between them
We refrain from utilizing attributes or features associated with each node which may or
may not be available in applications but rather focus on the analysis of the connectivity of
the graph as a means for classifying unlabeled nodes Such an approach inevitably needs
to make some a priori premise about how connectivity and categorization of individual
nodes may be related in real-world graphs The fundamental assumption of our framework
is the category similarity of co-linked nodes in a directed graph This is a slightly more
complex concept than in the case of undirected weighted graphs 18 where
a typical assumption is that an edge connecting two nodes will more or less increase the
likelihood of the nodes belonging to the same category Co-linkage on the other hand
seems a more suitable and promising concept in directed graphs as is witnessed by its
successful use in Web page categorization as well as co-citation analysis for information
retrieval Notice that co-linkage comes in two flavors sibling structures nodes
with common parents and co-parent structures nodes with common children In most
Web and citation graph related application the first assumption namely that nodes with
highly overlapping parent sets are likely to belong to the same category seems to be more
relevant but in general this will depend on the specific application
One possible way of designing classifiers based on graph connectivity is to construct a
kernel matrix based on pairwise links and then to adopt a standard kernel method
Support Vector Machines SVMs as a learning algorithm However a kernel
matrix as the one proposed in only represents local relationships among nodes but
completely ignores the global structure of the graph The idea of exploiting global rather
than local graph structure is widely used in other Web-related techniques including Web
page ranking finding similar Web pages detecting Web communities
and so on The major innovation of this paper is a general regularization framework on
directed graphs in which the directionality and global relationships are considered and
a computationally attractive classification algorithm which is derived from the proposed
regularization framework
Regularization Framework
Preliminaries
A directed graph consists of a set of vertices denoted by and a set of edges
denoted by Each edge is an ordered pair of nodes representing a
directed connection from to We do not allow self loops for all
In a weighted directed graph a weight function is associated with
satisfying if and only if E. Typically we can equip a directed graph
with a canonical weight function by defining if and only if E. The
in-degree and out-degree of a vertex respectively are defined as
and
Let H(V denote the space of functions which assigns a real value to
each vertex The function can be represented as a column vector in R|V where
denotes the number of the vertices in The function space H(V can be endowed with
the usual inner product
hf gi
Accordingly the norm of the function induced from the inner product is kf
hf
Bipartite Graphs
A bipartite graph A is a special type of directed graph that consists of two
sets of vertices denoted by and A respectively and a set of edges links denoted by
A. In a bipartite graph each edge connects a vertex in to a vertex in A. Any
directed graph can be regarded as a bipartite graph using the following simple
construction A and E.
Figure depicts the construction of the bipartite graph Notice that vertices of the original
graph may appear in both vertex sets and A of the constructed bipartite graph
The intuition behind the construction of the bipartite graph is provided by the so-called hub
and authority web model introduced by Kleinberg The model distinguishes between
two types of Web pages authoritative pages which are pages relevant to some topic
and hub pages which are pages pointing to relevant pages Note that some Web pages can
Figure Constructing a bipartite graph from a directed one Left directed graph Right
bipartite graph The hub set and the authority set A Notice
that the vertex indexed by is simultaneously in the hub and authority set
simultaneously be both hub and authority pages Figure Hubs and authorities exhibit
a mutually reinforcement relationship a good hub node points to many good authorities
and a good authority node is pointed to by many good hubs It is interesting to note that in
general there is no direct link from one authority to another It is the hub pages that glue
together authorities on a common topic
According to Kleinberg?s model we suggestively call the vertex set in the bipartite graph
the hub set and the vertex set A the authority set
Smoothness Functionals
If two distinct vertices and in the authority set A are co-linked by vertex in the hub
set as shown in the left panel of Figure then we think that and are likely to be
related and the co-linkage strength induced by between and can be measured by
ch
In addition we define ch for all in the authority set A and for all in the hub
set H. Such a relevance measure can be naturally understood in the situation of citation
networks If two articles are simultaneously cited by some other article then this should
make it more likely that both articles deal with a similar topics Moreover the more articles
cite both articles together the more significant the connection A natural question arising
in this context is why the relevance measure is further normalized by out-degree Let us
consider the following two web sites Yahoo and kernel machines General interest portals
like Yahoo consists of pages having a large number of diverse hyperlinks The fact that
two web pages are co-linked by Yahoo does not establish a significant connection between
them In contrast the pages on the kernel machine Web site have much fewer hyperlinks
but the Web pages pointed to are closely related in topic
Let denote a function defined on the authority set A. The smoothness of function can
be measured by the following functional
XX
ch
A
u,v
The smoothness functional penalizes large differences in function values for vertices in the
authority set A that are strongly related Notice that the function values are normalized by
Figure Link and relevance Left panel vertices and in the authority set A are colinked by vertex in the hub set H. Right panel vertices and in the hub set co-link
vertex a in the authority set A.
in-degree For the Web graph the explanation is similar to the one given before Many
web pages contain links to popular sites like the Google search engine This does not mean
though that all these Web pages share a common topic However if two web pages point to
web page like the one of the Learning with Kernels book it is likely to express a common
interest for kernel methods
Now define a linear operator by
a
Then its adjoint is given by
These two operators and were also implicitly suggested by for developing a new
Web page ranking algorithm Further define the operator SA by composing and
SA
and the operator A by
A I A
where I denotes the identity operator Then we can show the following See Appendix A
for the proof
Proposition A hf A
Comparing with the combinatorial Laplace operator defined on undirected graphs we
can think of the operator A as a Laplacian but defined on the authority set of directed
graphs Note that Proposition also shows that the Laplacian A is positive semi-definite
In fact we can further show that the eigenvalues of the operator SA are scattered in
and accordingly the eigenvalues of the Laplacian A fall into
Similarly if two distinct vertices and co-link vertex a in the authority set A as shown in
right panel of Figure then and are also thought to be related The co-linkage strength
between and induced by a can be measured by
ca
and the smoothness of function on the hub set can be measured by
XX
ca
u,v a
As before one can define the operators SH and I SH leading to the
corresponding statement
Proposition hf
Convexly combining together the two smoothness functionals and we obtain a
smoothness measure of function defined on the whole vertex set
where the parameter weighs the relative importance between A and Extend
the operator to H(V by defining if is only in the authority set A and
not in the hub set H. Similarly extend by defining if is only in the
hub set and not in the authority set A. Then if the remaining operators are extended
correspondingly one can define the operator H(V H(V by
SA
and the Laplacian on directed graphs H(V H(V by
I
Clearly By Proposition and it is easy to see that
Proposition hf
Regularization
Define a function in H(V in which or if vertex is labeled as positive
or negative and if it is not labeled The classification problem can be regarded as the
problem of finding a function which reproduces the target function to a sufficient
degree of accuracy while being smooth in a sense quantified by the above smoothness
functional A formalization of this idea leads to the following optimization problem
argmin kf yk2
The final classification of vertex is obtained as sign The first term in the bracket
is called the smoothness term or regularizer which measures the smoothness of function
and the second term is called the fitting term which measures its closeness to the given
function The trade-off between these two competitive terms is captured by a positive
parameter Successively smoother solutions can be obtained by decreasing
Theorem The solution of the optimization problem satisfies
Proof By Proposition we have
Differentiating the cost function in the bracket of with respect to function completes
the proof
Corollary The solution of the optimization problem is
where
It is worth noting that the closed form solution presented by Corollary shares the same
appearance as the algorithm proposed by which operates on undirected graphs
Experiments
We considered the Web page categorization task on the WebKB dataset We only
addressed a subset which contains the pages from the four universities Cornell Texas
Washington Wisconsin We removed pages without incoming or outgoing links resulting in and pages respectively for a total of These pages were
manually classified into the following seven categories student faculty staff department
course project and other We investigated two different classification tasks The first is
used to illustrate the significance of connectivity information in classification whereas the
second one stresses the importance of preserving the directionality of edges We may assign a weight to each hyperlink according to the textual content of web pages or the anchor
text contained in hyperlinks However here we are only interested in how much we can
obtain from link structure only and hence adopt the canonical weight function defined in
Section
We first study an extreme classification problem predicting which university the pages
belong to from very few labeled training examples Since pages within a university are
well-linked and cross links between different universities are rare we can imagine that
few training labels are enough to exactly classify pages based on link information only For
each of the universities we in turn viewed corresponding pages as positive examples and
the pages from the remaining universities as negative examples We have randomly draw
two pages as the training examples under the constraint that there is at least one labeled
instance for each class Parameters were set to and In fact in
this experiment the tuning parameters have almost no influence on the result Since the
Web graph is not connected some small isolated subgraphs possibly do not contain labeled
instances The values of our classifying function on the pages contained in these subgraphs
will be zeros and we simply think of these pages as negative examples This is consistent
with the search engine ranking techniques We compare our method with SVMs
using a kernel matrix constructed as where denotes the adjacency
matrix of the web graph and denotes the transpose of The test errors averaged over
training sample sets for both our method and SVMs are summarized into the following
table
our method
SVMs
Cornel
Texas
Washington
Wisconsin
However to be fair we should state that the kernel matrix that we used in the SVM may not
be the best possible kernel matrix for this task this is an ongoing research issue which is
not the topic of the present paper
The other investigated task is to discriminate the student pages in a university from the
non-student pages in the same university As a baseline we have applied our regularization
method on the undirected graph obtained by treating links as undirected or bidirectional the affinity matrix is defined to be We use the AUC scores to
measure the performances of the algorithms The experimental results in Figure
clearly demonstrate that taking the directionality of edges into account can yield substantial accuracy gains In addition we also studied the influence of different choices for the
parameters and we used the Cornell Web for that purpose and sampled labeled
training pages Figure show that relatively small values of are more suitable We
think that is because the subgraphs in each university are quite small limiting the information conveyed in the graph structure The influence of is shown in Figure The
performance curve shows that large values for are preferable This confirms the conjecture that co-link structure among authority pages is much more important than within the
hub set
directed
undirected
labeled points
18
directed
undirected
Cornell
labeled points
18
directed
undirected
labeled points
Wisconsin
18
labeled points
18
AUC
AUC
AUC
Washington
directed
undirected
Texas
AUC
AUC
AUC
parameter values
Cornell
parameter values
Cornell
Figure Classification on the WebKB dataset Figure depict the AUC scores of
the directed and undirected regularization methods on the classification problem student
non-student in each university Figure illustrate the influences of the different
choices of the parameters and
Conclusions
We proposed a general regularization framework on directed graphs which has been validated on a real-word Web data set The remaining problem is how to choose the suitable
parameters contained in this approach In addition it is worth noticing that this framework
can be applied without any essential changes to bipartite graphs to graphs describing
customers purchase behavior in market basket analysis Moreover in the absence of labeled instances this framework can be utilized in an unsupervised setting as a spectral
clustering method for directed or bipartite graphs Due to lack of space we have not been
able to give a thorough discussion of these topics
Acknowledgments We would like to thank David Gondek for his help on this work

----------------------------------------------------------------

title: 3807-tracking-dynamic-sources-of-malicious-activity-at-internet-scale.pdf

Tracking Dynamic Sources of Malicious Activity at
Internet-Scale
Shobha Venkataraman Avrim Blum Dawn Song Subhabrata Sen Oliver Spatscheck
AT&T Labs Research shvenk,sen,spatsch}@research.att.com
Carnegie Mellon University avrim@cs.cmu.edu
University of California Berkeley dawnsong@cs.berkeley.edu
Abstract
We formulate and address the problem of discovering dynamic malicious regions
on the Internet We model this problem as one of adaptively pruning a known
decision tree but with additional challenges severe space requirements since
the underlying decision tree has over billion leaves and a changing target
function since malicious activity on the Internet is dynamic We present a novel
algorithm that addresses this problem by putting together a number of different
experts algorithms and online paging algorithms We prove guarantees on our
algorithm?s performance as a function of the best possible pruning of a similar
size and our experiments show that our algorithm achieves high accuracy on large
real-world data sets with significant improvements over existing approaches
Introduction
It is widely acknowledged that identifying the regions that originate malicious traffic on the Internet
is vital to network security and management in throttling attack traffic for fast mitigation isolating infected sub-networks and predicting future attacks 18 19 24 In this paper we show
how this problem can be modeled as a version of a question studied by Helmbold and Schapire
of adaptively learning a good pruning of a known decision tree but with a number of additional challenges and difficulties These include a changing target function and severe space requirements due
to the enormity of the underlying IP address-space tree We develop new algorithms able to address
these difficulties that combine the underlying approach of with the sleeping experts framework
of and the online paging problem of We show how to deal with a number of practical
issues that arise and demonstrate empirically on real-world datasets that this method substantially
improves over existing approaches of prefixes and network-aware clusters in correctly
identifying malicious traffic Our experiments on data sets of million IP addresses demonstrate
that our algorithm is able to achieve a clustering that is both highly accurate and meaningful
Background
Multiple measurement studies have indicated that malicious traffic tends to cluster in a way that
aligns with the structure of the IP address space and that this is true for many different kinds of
malicious traffic spam scanning botnets and phishing 18 19 Such clustered behaviour
can be easily explained most malicious traffic originates from hosts in poorly-managed networks
and networks are typically assigned contiguous blocks of the IP address space Thus it is natural
that malicious traffic is clustered in parts of the IP address space that belong to poorly-managed
networks
From a machine learning perspective the problem of identifying regions of malicious activity can
be viewed as one of finding a good pruning of a known decision tree the IP address space may be
naturally interpreted as a binary tree and the goal is to learn a pruning of this tree that
is not too large and has low error in classifying IP addresses as malicious or non-malicious The
structure of the IP address space suggests that there may well be a pruning with only a modest number of leaves that can classify most of the traffic accurately Thus identifying regions of malicious
activity from an online stream of labeled data is much like the problem considered by Helmbold and
Schapire of adaptively learning a good pruning of a known decision tree However there are a
number of real-world challenges both conceptual and practical that must be addressed in order to
make this successful
One major challenge in our application comes from the scale of the data and size of a complete
decision tree over the IP address space A full decision tree over the IPv4 address space would
have leaves and over the IPv6 address space which is slowly being rolled out leaves
With such large decision trees it is critical to have algorithms that do not build the complete tree
but instead operate in space comparable to the size of a good pruning These space constraints are
also important because of the volume of traffic that may need to be analyzed ISPs often collect
terabytes of data daily and an algorithm that needs to store all its data in memory simultaneously
would be infeasible
A second challenge comes from the fact that the regions of malicious activity may shift longitudinally over time This may happen for many reasons administrators may eventually
discover and clean up already infected bots and attackers may target new vulnerabilities and attack
new hosts elsewhere Such dynamic behaviour is a primary reason why individual IP addresses tend
to be such poor indicators of future malicious traffic Thus we cannot assume that the data
comes from a fixed distribution over the IP address space the algorithm needs to adapt to dynamic
nature of the malicious activity and track these changes accurately and quickly That is we must
consider not only an online sequence of examples but also a changing target function
While there have been a number of measurement studies that have examined the origin
of malicious traffic from IP address blocks that are kept fixed apriori none of these have focused on
developing online algorithms that find the best predictive IP address tree Our challenge is to develop
an efficient high-accuracy online algorithm that handles the severe space constraints inherent in this
problem and accounts for the dynamically changing nature of malicious behavior We show that
we can indeed do this both proving theoretical guarantees on adaptive regret and demonstrating
successful performance on real-world data
Contributions
In this paper we formulate and address the problem of discovering and tracking malicious regions of
the IP address space from an online stream of data We present an algorithm that adaptively prunes
the IP address tree in a way that maintains at most leaves and performs nearly as well as the
optimum adaptive pruning of the IP address tree with a comparable size Intuitively we achieve the
required adaptivity and the space constraints by combining several experts algorithms together
with a tree-based version of paging Our theoretical results prove that our algorithm can predict
nearly as well as the best adaptive decision tree with leaves when using O(k log leaves
Our experimental results demonstrate that our algorithm identifies malicious regions of the IP address space accurately with orders of magnitude improvement over previous approaches Our experiments focus on classifying spammers and legitimate senders on two mail data sets one with
million messages collected over 38 days from the mail servers of a tier-1 ISP and a second with
28 million messages collected over months from an enterprise mail server Our experiments also
highlight the importance of allowing the IP address tree to be dynamic and the resulting view of the
IP address space that we get is both compelling and meaningful
Definitions and Preliminaries
We now present some basic definitions as well as our formal problem statement
The IP address hierarchy can be naturally interpreted as a full binary tree as in the leaves of
the tree correspond to individual IP addresses and the non-leaf nodes correspond to the remaining
IP prefixes Let denote the set of all IP prefixes and I denote the set of all IP addresses We also
use term clusters to denote the IP prefixes
We define an IPTree TP to be a pruning of the full IP address tree a tree whose nodes are IP
prefixes and whose leaves are each associated with a label malicious or non-malicious
An IPtree can thus be interpreted as a classification function for the IP addresses I an IP address
gets the label associated with its longest matching prefix in shows an example of an IPtree
We define the size of an IPtree to be the number of leaves it has For example in the size
of the IPtree is
As described in Sec. we focus on online learning in this paper A typical point of comparison
used in the online learning model is the error of the optimal offline fixed algorithm In this case
the optimal offline fixed algorithm is the IPtree of a given size the tree of size that makes
An example IPTree
A real IPTree Color coding explained in Sec.
Figure IPTrees example and real Recall that an IP address is interpreted as a string read
from left to right This defines a path on the binary tree going left for and right for An IP prefix
is denoted by IP/n where indicates the number of bits relevant to the prefix
the fewest mistakes on the entire sequence However if the true underlying IPtree may change over
time a better point of comparison would allow the offline tree to also change over time To make
such a comparison meaningful the offline tree must pay an additional penalty each time it changes
otherwise the offline tree would not be a meaningful point of comparison it could change for each
IP address in the sequence and thus make no mistakes We therefore limit the kinds of changes the
offline tree can make and compare the performance of our algorithm to every IPtree with leaves
as a function of the errors it makes and the changes it makes
We define an adaptive IPtree of size to be an adaptive tree that can grow nodes over time so
long as it never has more than leaves change the labels of its leaf nodes and occasionally
reconfigure itself completely Our goal is to develop an online algorithm such that for any sequence of IP addresses for every adaptive tree of size the number of mistakes made by
is bounded by a small function of the mistakes and the changes of types and made by
and uses no more than
space In the next section we describe an algorithm meeting
these requirements
Algorithms and Analysis
In this section we describe our main algorithm TrackIPTree and present theoretical guarantees on
its performance At a high-level our approach keeps a number of experts in each prefix of the
IPtree and combines their predictions to classify every IP address The inherent structure in the
IPtree allows us to decompose the problem into a number of expert problems and provide lower
memory bounds and better guarantees than earlier approaches
We begin with an overview Define the path-nodes of an IP address to be the set of all prefixes of
in and denote this set by Pi,T To predict the label of an IP the algorithm looks up all the pathnodes in Pi,T considers their predictions and combines these predictions to produce a final label
for To update the tree the algorithm rewards the path-nodes that predicted correctly penalizes the
incorrect ones and modifies the tree structure if necessary
To fill out this overview there are four technical questions that we need to address Of all the
path-nodes in Pi,T how do we learn the ones that are the most important How do we learn the
correct label to predict at a particular path-node in Pi,T positive or negative How do we
grow the IPtree appropriately ensuring that it grows primarily the prefixes needed to improve the
classification accuracy How do we ensure that the size of the IPtree stays bounded by We
address these questions by treating them as separate subproblems and we show how they fit together
to become the complete algorithm in Figure
Subproblems of TrackIPTree
We now describe our algorithm in detail Since our algorithm decomposes naturally into the four
subproblems mentioned above we focus on each subproblem separately to simplify the presentation
We use the following notation in our descriptions Recall from Sec. that is the maximum number
of leaves allowed to our algorithm is the size of the optimal offline tree and Pi,T denotes the set
of path-nodes the prefixes of IP in the current IPtree
Relative Importance of the Path Nodes First we consider the problem of deciding which of the
prefix nodes in the path Pi,T is most important We formulate this as a sleeping experts problem
We set an expert in each node and call them the path-node experts and for an IP we consider
the set of path-node experts in Pi,T to be the awake experts and the rest to be asleep The
x3
x4
x5
x6
Sleeping Experts Relative Importance
of Path-Nodes
Shifting Experts Determining
Node Labels
Figure Decomposing the TrackIPTree Algorithm
sleeping experts algorithm makes predictions using the awake experts and intuitively has the goal
of predicting nearly as well as the best awake expert on the instance In our context the best
awake expert on the IP corresponds to the prefix of in the optimal IPtree which remains sleeping
until the IPtree grows that prefix illustrates the sleeping experts framework in our context
the shaded nodes are awake and the rest are asleep
Specifically let denote the weight of the path-node expert at node and let Si,T t?Pi,T
To predict on IP address the algorithm chooses the expert at node with probability Si,T To
update the algorithm penalizes all incorrect experts in Pi,T reducing their weight to
It then renormalizes the weights of all the experts in Pi,T so that their sum Si,T does not
change In our proof we use a slightly different version of the sleeping experts algorithm
Deciding Labels of Individual Nodes Next we need to decide whether the path-node expert at
a node should predict positive or negative We use a different experts algorithm to address this
subproblem the shifting experts algorithm Specifically we allow each node to have two
additional experts a positive expert which always predicts positive and a negative expert which
always predicts negative We call these experts node-label experts
Let and denote the weights of the positive and negative node-label experts respectively
with The algorithm operates as follows to predict the node predicts positive with
probability and negative with probability To update when the node receives a label it
increases the weight of the correct node-label expert by and decreases the weight of the incorrect
node-label expert by upto a maximum of and a minimum of Note that this algorithm naturally
adapts when a leaf of the optimal IPtree switches labels the relevant node in our IPtree will slowly
shift weights from the incorrect node-label expert to the correct one making an expected mistakes
in the process illustrates the shifting experts setting on an IPtree each node has two
experts a positive and a negative shows how it fits in with the sleeping experts algorithm
Building Tree Structure We next address the subproblem of building the appropriate structure for
the IPtree The intuition here is when a node in the IPtree makes many mistakes then either
that node has a subtree in the optimal IPtree that separates the positive and negative instances
or the optimal IPtree must also make the same mistakes Since TrackIPTree cannot distinguish
between these two situations it simply splits any node that makes sufficient mistakes In particular
TrackIPTree starts with only the root node and tracks the number of mistakes made at every node
Every time a leaf makes mistakes TrackIPTree splits that leaf into its children and instantiates
and initializes the relevant path-node experts and node-label experts of the children In effect it is
as if the path-node experts of the children had been asleep till this point but will now be awake
for the appropriate IP addresses
TrackIPTree waits for mistakes at each node before growing it so that there is a little resilence
with noisy data otherwise it would split a node every time the optimal tree made a mistake and the
IPtree would grow very quickly Note also that it naturally incorporates the optimal IPtree growing
a leaf our tree will grow the appropriate nodes when that leaf has made mistakes
Bounding Size of IPtree Since TrackIPTree splits any node after it makes mistakes it is likely
that the IPtree it builds is split much farther than the optimal IPtree TrackIPTree does not know
when to stop growing a subtree and it splits even if the same mistakes are made by the optimal
IPtree While this excessive splitting does not impact the predictions of the path-node experts or the
node-label experts significantly we still need to ensure that the IPtree built by our algorithm does
not become too large
We leave the exact statement of the guarantee to the proof in
RACK IPT REE
Input tree size learning rate penalty factor
Initialize
Set root
InitializeNode(root
Update Rule Contd
Update
path-node experts
n?P
t,T
for Pi,T
if predict[n
penalize
mistakes[xn
if mistakes[xn and
is leaf GrowT ree(n
Renormalize
Prediction Rule Given IP
Select a node-label expert
for Pi,T
flip coin of bias
if heads predict[n
else predict[n
Select a path-node expert
rval
predict[n with weight
t?P
Return rval
j?Pi,T
sub I NITIALIZE ODE
Input node
mistakes[t
sub ROW REE
Input leaf
if size(T
Select nodes to discard with
paging algorithm
Split leaf into children lc rc
InitializeNode(lc InitializeNode(rc
Update Rule Given IP label
Update node-label experts
for Pi,T
for label
if yn,z yn,z
else yn,z yn,z
Figure The Complete TrackIPTree Algorithm
We do this by framing it as a paging problem consider each node in the IPtree to be a page
and the maximum allowed nodes in the IPtree to be the size of the cache The offline IPtree which
has leaves needs a cache of size The IPtree built by our algorithm may have at most leaves
and thus 2m nodes since it is a binary tree and so the size of its cache is 2m and the offline
cache is We may then select nodes to be discarded as if they were pages in the cache once the
IPtree grows beyond 2m nodes so for example we may choose the least recently used nodes in
the IPtree with LRU as the paging algorithm Our analysis shows that setting log
suffices when TrackIPTree uses LUSH HEN ULL FWF as its paging algorithm this is a
simple paging algorithm that discards all the pages in the cache when the cache is full and restarts
with an empty cache We use FWF here for a clean analysis and especially since in simple paging
models many algorithms achieve no better guarantees For our experiments we implement
LRU and our results show that this approach while perhaps not sophisticated still maintains an
accurate predictive IPtree
Analysis
In this section we present theoretical guarantees on TrackIPTree?s performance We show our
algorithm performs nearly as well as best adaptive k-IPtree bounding the number of mistakes made
by our algorithm as a function of the number of mistakes number of labels changes and number of
complete reconfigurations of the optimal such tree in hindsight
Theorem Fix Set the maximum number of leaves allowed to the TrackIPTree algorithm to
be
log Let be an adaptive k-IPtree Let denote the number of times changes labels
on the its leaves over the sequence and RT,z denote the number of times times has completely
reconfigured itself over
The algorithm TrackIPTreeensures that on any sequence of instances for each the number of
mistakes made by TrackIPTree is at most
log RT,z with
probability at least k1
In other words if there is an offline adaptive k-IPtree that makes few changes and few mistakes
on the input sequence of IP addresses then TrackIPTree will also make only a small number of
mistakes Due to space constraints we present the proof in the technical report
Evaluation Setup
We now describe our evaluation set-up data practical changes to the algorithm and baseline
schemes that compare against While there are many issues that go into converting the algorithm in
Sec. for practical use we describe here those most important to our experiments and defer the
rest to the technical report
Data We focus on IP addresses derived from mail data since spammers represent a significant fraction of the malicious activity and compromised hosts on the Internet and labels are relatively
easy to obtain from spam-filtering run by the mail servers For our evaluation we consider labels
from the mail servers spam-filtering to be ground truth Any errors in the spam-filtering will influence the tree that we construct and our experimental results are limited by this assumption
One data set consists of log extracts collected at the mail servers of a tier-1 ISP with million
active mailboxes The extracts contain the IP addresses of the mail servers that send mail to the
ISP the number of messages they sent and the fraction of those messages that are classified as
spam aggregated over minute intervals The mail server?s spam-filtering software consists of a
combination of hand-crafted rules DNS blacklists and Brightmail and we take their results as
labels for our experiments The log extracts were collected over 38 days from December to
January and contain million IP addresses of which million are spam and million
are legitimate
The second data set consists of log extracts from the enterprise mail server of a large corporation with
active mailboxes These extracts also contain the IP addresses of mail servers that attempted to
send mail along with the number of messages they sent and the fraction of these messages that were
classified spam by SpamAssassin aggregated over minute intervals The extracts contain 28
million IP addresses of which around million are legitimate and the rest are spammers
Note that in both cases our data only contains aggregate information about the IP addresses of the
mail servers sending mail to the ISP and enterprise mail servers and so we do not have the ability
to map any information back to individual users of the ISP or enterprise mail servers
TrackIPTree For the experimental results we use LRU as the paging algorithm when nodes need
to be discarded from the IPtree Sec In our implementation we set TrackIPTree to discard
of the maximum leaves allowed every time it needs to expire nodes The learning rate is
set to and the penalty factor for sleeping experts is set to respectively Our results are not
affected if these parameters are changed by a factor of
While we have presented an online learning algorithm in practice it will often need to predict
on data without receiving labels of the instances right away Therefore we study TrackIPTree?s
accuracy on the following day?s data to compute prediction accuracy of day TrackIPTree is
allowed to update until day We choose intervals of a day?s length to allow the tree?s predictions
to be updated at least every day
Apriori Fixed Clusters We compare TrackIPTree to two sets of apriori fixed clusters networkaware clusters which are a set of unique prefixes derived from BGP routing table snapshots and
prefixes We choose these clusters as a baseline as they have been the basis of measurement
studies discussed earlier Sec prior work in IP-based classification and are even used
by popular DNS blacklists
We use the fixed clusters to predict the label of an IP in the usual manner we simply assign an
IP the label of its longest matching prefix among the clusters.Of course we first need to assign
these clusters their own labels To ensure that they classify as well as possible we assign them the
optimal labeling over the data they need to classify we do this by allowing them to make multiple
passes over the data That is for each day we assign labels so that the fixed clusters maximize their
accuracy on spam for a given required accuracy on legitimate mail It is clear that this experimental
set-up is favourable to the apriori fixed clusters
We do not directly compare against the algorithm in as it requires every unique IP address in
the data set to be instantiated in the tree In our experiments with the ISP logs this means that
it requires over 90 million leaves in the tree We instead focus on practical prior approaches with
more cluster sizes in our experiments
Results
We report three sets of experimental results regarding the prediction accuracy of TrackIPTree using
the experimental set-up of Section While we do not provide an extensive evaluation of our algorithm?s computational efficiency we note that our unoptimized implementation of TrackIPTree
takes under a minute to learn over a million IP addresses on a Sparc64-VI core
For space reasons we defer the details of how we assign this labeling to the technical report
Coverage on Legit IPs
Error on Legit IPs
Accuracy on Spam IPs
Coverage on Legit IPs
5k
1k
Dynamic
Static Days
Static Days
Coverage on Legit IPs
Expt ISP logs
TrackIPTree
Network?Aware
Prefixes
Expt Enterprise logs
Expt ISP logs
Error on Spam IPs
TrackIPTree
Network?Aware
Prefixes
Accuracy on Spam IPs
Accuracy on Spam IPs
Accuracy on Spam IPs
Dynamic
Static Days
Static Days
Coverage on Legit IPs
Expt Enterprise logs
Time in days
Expt Legitimate IPs
Time in days
Expt Spam IPs
Figure Results for Experiments and
Our results compare the fraction of spamming IPs that the clusters classify correctly subject to
the constraint that they classify at least legitimate mail IPs correctly we term this to be the
coverage of the legitimate IPs required Thus we effectively plot the true positive rate against
the true negative rate This is just the ROC curve with the x-axis reversed since we plot the true
positive against the true negative instead of plotting the true positive against the false positive
Experiment Comparisons with Apriori Fixed Clusters Our first set of experiments compares
the performance of our algorithm with network-aware clusters and IP prefixes Figs
illustrate the accuracy tradeoff of the three sets of clusters on the two data sets Clearly the accuracy
of TrackIPTree is a tremendous improvement on both sets of apriori fixed clusters for any choice
of coverage on legitimate IPs the accuracy of spam IPs by TrackIPTree is far higher than the apriori
fixed clusters even by as much as a factor of In particular note that when the coverage required
on legitimate IPs is TrackIPTree achieves accuracy in classifying spam on both data sets
compared to the 35 achieved by the other clusters
In addition TrackIPTree gains this classification accuracy using a far smaller tree Table shows
the median number of leaves instantiated by the tree at the end of each day To be fair to the fixed
clusters we only instantiate the prefixes required to classify the day?s data rather than all possible
prefixes in the clustering scheme Table shows that the tree produced by TrackIPTree is a factor
of smaller with the ISP logs and a factor of smaller with the enterprise logs These
numbers highlight that the apriori fixed clusters are perhaps too coarse to classify accurately in parts
of the IP address space and also are insufficiently aggregated in other parts of the address space
Experiment Changing the Maximum Leaves Allowed Next we explore the effect of changing
the maximum number of leaves allowed to TrackIPTree show the accuracycoverage tradeoff for TrackIPTree when ranges between leaves for the ISP logs
and leaves for the enterprise logs Clearly in both cases the predictive accuracy
increases with only until is sufficiently large once is large enough to capture all the
distinct subtrees in the underlying optimal IPtree the predictive accuracy will not increase While
the actual values of are specific to our data sets the results highlight the importance of having a
space-efficient and flexible algorithm both and are very modest sizes compared to
the number of possible apriori fixed clusters or the size of the IPv4 address space and this suggests
that the underlying decision tree required is indeed of a modest size
Experiment Does a Dynamic Tree Help In this experiment we demonstrate empirically that
our algorithm?s dynamic aspects do indeed significantly enhance its accuracy over static clustering
schemes The static clustering that we compare to is a tree generated by our algorithm but one that
learns over the first days and then stays unchanged For ease of

----------------------------------------------------------------

title: 3454-predictive-indexing-for-fast-search.pdf

Predictive Indexing for Fast Search
Sharad Goel
Yahoo Research
New York NY
goel@yahoo-inc.com
John Langford
Yahoo Research
New York NY
jl@yahoo-inc.com
Alex Strehl
Yahoo Research
New York NY
strehl@yahoo-inc.com
Abstract
We tackle the computational problem of query-conditioned search Given a
machine-learned scoring rule and a query distribution we build a predictive index by precomputing lists of potential results sorted based on an expected score
of the result over future queries The predictive index datastructure supports an
anytime algorithm for approximate retrieval of the top elements The general approach is applicable to webpage ranking internet advertisement and approximate
nearest neighbor search It is particularly effective in settings where standard techniques inverted indices are intractable We experimentally find substantial
improvement over existing methods for internet advertisement and approximate
nearest neighbors
Introduction
The Problem The objective of web search is to quickly return the set of most relevant web pages
given a particular query string Accomplishing this task for a fixed query involves both determining
the relevance of potential pages and then searching over the myriad set of all pages for the most
relevant ones Here we consider only the second problem More formally let Rn be an input
space Rm a finite output space of size and a known scoring function
Given an input search query the goal is to find or closely approximate the top-k output
objects web pages p1 pk in the top objects as ranked by
The extreme speed constraint often or less and the large number of web pages
makes web search a computationally-challenging problem Even with perfect parallelization on modern machines there is far too little time to directly evaluate against every page when a
particular query is submitted This observation limits the applicability of machine-learning methods
for building ranking functions The question addressed here is Can we quickly return the highest
scoring pages as ranked by complex scoring rules typical of learning algorithms
Predictive Indexing We describe a method for rapidly retrieving the top elements over a large set
as determined by general scoring functions The standard method for mitigating the computational
difficulties of search is to pre-process the data so that far less computation is necessary at runtime
Taking the empirical probability distribution of queries into account we pre-compute collections of
web pages that have a large expected score conditioned on the query falling into particular sets of
related queries Qi For example we may pre-compute and store the list of web pages that have the
highest average score when the query contains the phrase machine learning To yield a practical
algorithm these sets should form meaningful groups of pages with respect to the scoring function
and query distribution At runtime we then optimize only over those collections of top-scoring web
pages for sets Qi containing the submitted query
Our main contribution is optimizing the search index with respect to the query distribution The empirical evidence presented shows that predictive indexing is an effective technique making general
machine learning style prediction methods viable for quickly ranking over large numbers of objects
The general methodology applies to other optimization problems as well including approximate
nearest neighbor search
In the remainder of Section we describe existing solutions to large-scale search and their applicability to general scoring functions Section describes the predictive indexing algorithm and covers
an example and lemma suggesting that predictive indexing has significant advantages over existing
techniques We present empirical evaluation of the method in Section using both proprietary web
advertising data and public data for nearest neighbor search
Feature Representation
One concrete way to map web search into the general predictive index framework is to represent
both queries and pages as sparse binary feature vectors in a high-dimensional Euclidean space
Specifically we associate each word with a coordinate A query page has a value of for that
coordinate if it contains the word and a value of otherwise We call this the word-based feature
representation because each query and page can be summarized by a list of its features words
that it contains The general predictive framework supports many other possible representations
including those that incorporate the difference between words in the title and words in the body of
the web page the number of times a word occurs or the IP address of the user entering the query
Related Work
Given the substantial importance of large-scale search a variety of techniques have been developed
to address the rapid ranking problem Past work that has referenced the query distribution includes
Cheng Chierichetti Here we describe two commonly applied methods
related to the predictive index approach
Fagin?s Threshold Algorithm Fagin?s threshold algorithmP
Fagin supports the top-k
problem for linear scoring functions of the form qi gi where qi is the
th
coordinate of the query and gi are partial scores for pages as determined by the ith
feature1 For each query feature construct an ordered list Li containing every web page sorted
in descending order by their partial scores gi We refer to this as the projective order since it
is attained by projecting the scoring rule onto individual coordinates Given a query we evaluate
web pages in the lists Li that correspond to features of The algorithm maintains two statistics
upper and lower bounds on the score of the top-k th page halting when these bounds cross The
lower bound is the score of the th best page seen so far the upper bound is the sum of the partial
scores gi for the next-to-be-scored page in each list Since the lists are ordered by the
partial scores the upper threshold does in fact bound the score of any page yet to be seen
The threshold algorithm is particularly effective when a query contains a small number of features
facilitating fast convergence of the upper bound In our experiments we find that the halting condition is rarely satisfied within the imposed computational restrictions One can of course simply
halt the algorithm when it has expended the computational budget Fagin which we refer to
as the Halted Threshold Algorithm
Inverted Indices An inverted index is a datastructure that maps every page feature to a list of
pages that contain When a new query arrives a subset of page features relevant to the query is
first determined For instance when the query contains the page feature set might be
canine collar Note that a distinction is made between query features and page features and
in particular the relevant page features may include many more words than the query itself Once a
set of page features is determined their respective lists inverted indices are searched and from
them the final list of output pages is chosen One method for searching over these lists is to execute
Fagin?s threshold algorithm Other methods such as the Weighted-And algorithm Broder
use one global order for pages in the lists and walk down the lists synchronously to compute
page scores See Zobel Moffat for an overview of inverted indices applied to web search
Standard approaches based on inverted indices suffer from a shortcoming The resulting algorithms
are efficient only when it is sufficient to search over a relatively small set of inverted indices for each
More general monotone scoring functions coordinate-wise product and max are in fact supported
for clarity however we restrict to the linear case
query They require for each query that there exists a small set2 Xq of page features such that the
score of any page against depends only on its intersection with Xq In other words the scoring
rule must be extremely sparse with most words or features in the page having zero contribution to
the score for In Section we consider a machine-learned scoring rule derived from internet
advertising data with the property that almost all page features have substantial influence on the
score for every query making any straightforward approach based on inverted indices intractable
Furthermore algorithms that use inverted indices do not typically optimize the datastructure against
the query distribution and our experiments suggest that doing so may be beneficial
An Algorithm for Rapid Approximate Ranking
Suppose we are provided with a categorization of possible queries into related potentially overlapping sets For example these sets might be defined as queries containing the word France
or queries with the phrase car rental For each query set the associated predictive index is an
ordered list of web pages sorted by their expected score for random queries drawn from that set In
particular we expect web pages at the top of the France list to be good on average for queries
containing the word France In contrast to an inverted index the pages in the France list need not
themselves contain the word France To retrieve results for a particular query France car
rental we optimize only over web pages in the relevant pre-computed lists Note that the predictive index is built on top of an already existing categorization of queries a critical and potentially
difficult initial step In the applications we consider however we find that predictive indexing works
well even when applied to naively defined query sets Furthermore in our application to approximate nearest neighbor search we found predictive indexing to be robust to cover sets generated via
random projections whose size and shape were varied across experiments
We represent queries and web pages as points in respectively Rn and Rm This setting
is general but for the experimental application we consider with any given page or
query having about non-zero entries Section for details Thus pages and points are
typically sparse vectors in very high dimensional spaces A coordinate may indicate for example
whether a particular word is present in the page/query or more generally the number of times that
word appears Given a scoring function and a query we attempt to rapidly find
the top-k pages p1 pk Typically we find an approximate solution a set of pages p?k
that are among the top for We assume queries are generated from a probability distribution
that may be sampled
Predictive Indexing for General Scoring Functions
Consider a finite collection of sets Qi that cover the query space Qi For each
Qi define the conditional probability distribution Di over queries in Qi by Di and
define fi as fi Eq?Di The function fi is the expected score of the
web page for the related queries in Qi The hope is that any page has approximately the same
score for any query Qi If for example Qi is the set of queries that contain the word we
may expect every query in Qi to score high against pages about dogs and to score low against those
pages not about dogs
For each set of queries Qi we pre-compute a sorted list Li of pages pi1 pi2 piN ordered in
descending order of fi At runtime given a query we identify the query sets Qi containing
and compute the scoring function only on the restricted set of pages at the beginning of their
associated lists Li We search down these lists for as long as the computational budget allows
In general it is difficult to compute exactly the conditional expected scores of pages fi One can
however approximate these scores by sampling from the query distribution D. Algorithm outlines
the construction of the sampling-based predictive indexing datastructure Algorithm shows how
the method operates at run time
Note that in the special case where we cover with a single set we end up with a global ordering
of web pages independent of the query which is optimized for the underlying query distribution
The size of these sets are typically on the order of or smaller
Algorithm Construct-Predictive-Index(Cover Dataset
Lj for all objects and query sets Qj
for random queries do
for all objects in the data set do
for all query sets Qj containing do
Lj Lj
end for
end for
end for
for all lists Lj do
sort Lj
end for
return
Algorithm Find-Top(query count
top-k list
while time remains do
for each query set Qj containing do
Lj
if th best seen so far then
insert into ordered top-k list
end if
end for
end while
return
While this global ordering may not be effective in isolation it could perhaps be used to order pages
in traditional inverted indices
Discussion
We present an elementary example to help develop intuition for why we can sometimes expect
predictive indexing to improve upon projective datastructures such as those used in Fagin?s threshold
algorithm Suppose we have two query features t1 and t2 three possible queries q1
q2 and q3 t2 and three web pages p1 p2 and p3 Further suppose we have a simple
linear scoring function defined by
p1 It1 It2
p2 It2 It1
p3 It2 It1
where I is the indicator function That is pi is the best match for query qi but p3 does not score
highly for either query feature alone Thus an ordered projective datastructure would have
t1 p3 p2
t2 p3 p1
Suppose however that we typically only see query q3 In this case if we know t1 is in the query
we infer that t2 is likely to be in the query and vice versa and construct the predictive index
t1 p1 p2
t2 p2 p1
On the high probability event namely query q3 we see the predictive index outperforms the projective query independent index
We expect predictive indices to generally improve on datastructures that are agnostic to the query
distribution In the simple case of a single cover set a global web page ordering and when
we wish to optimize the probability of returning the highest-scoring object Lemma shows that
a predictive ordering is the best ordering relative to any particular query distribution
Lemma Suppose we have a set of points a query distribution and a function that scores
queries against points in S. Further assume that for each query there is a unique highest scoring
point Hq For let Prq?D Hq and let s1 s2 sN be ordered according to
For any fixed
Pr Hq sk
max
Pr Hq
q?D
permutations q?D
Proof For any ordering of points the probability of the highest scoring point apPk
pearing in the top entries equals This sum is clearly maximized by ordering the
list according to
Empirical Evaluation
We evaluate predictive indexing for two applications Internet advertising and approximate nearest
neighbor
Internet Advertising
We present results on Internet advertising a problem closely related to web search We have obtained proprietary data both testing and training from an online advertising company The data are
comprised of logs of events where each event represents a visit by a user to a particular web page
from a set of web pages Rn From a large set of advertisements Rm the commercial
system chooses a smaller ordered set of ads to display on the page generally around The set of
ads seen and clicked by users is logged Note that the role played by web pages has switched from
result to query The total number of ads in the data set is Each ad contains on
average ad features and a total of ad features are observed The training data consist
of million events web page ad displays The total number of distinct web pages is
Each page consists of approximately page features and a total of total page features
are observed
We used a sparseP
feature representation Section and trained a linear scoring rule of the
form i,j wi,j pi aj to approximately rank the ads by their probability of click Here
wi,j are the learned weights parameters of the linear model The search algorithms we compare
were given the scoring rule the training pages and the ads for the necessary pre-computations
They were then evaluated by their serving of ads under a time constraint for each page
in the test set There was a clear separation of test and training We measured computation time
in terms of the number of full evaluations by the algorithm the number of ads scored against
a given page Thus the true test of an algorithm was to quickly select the most promising ads
to fully score against the page where was externally imposed and
varied over the experiments These numbers were chosen to be in line with real-world computational
constraints
We tested four methods halted threshold algorithm as described in Section two variants
of predictive indexing PI-AVG and PI-DCG and a fourth method called best global ordering
which is a degenerate form of PI discussed in Section An inverted index approach is
prohibitively expensive since almost all ad features have substantial influence on the score for every
web page Section
PI-AVG and PI-DCG require a covering of the web page space We used the natural covering suggested by the binary features?each page feature corresponds to a cover set consisting of precisely
those pages that contain The resulting datastructure is therefore similar to that maintained by
the TA algorithm?lists
for each page feature containing all the ads However while TA orders ads
by partial score wi,j pi aj for each fixed page feature the predictive methods order by expected
score PI-AVG sorts ad lists by expected score of Ep?Di Ep?D conditioned on the page containing feature PI-DCG and BO optimize the expected value of a modified
scoring rule DCGf log2 where is the rank function and I is the
indicator function Here indicates that ad a has rank according to over all ads
in BO stores a single list of all ads sorted by expected DCGf while PI-DCG stores a list
for each page feature sorted by Ep?Di DCGf We chose this measure because
Compared with using the average score of we empirically observe that expected DCGf
greatly improves the performance of BO on these data
It is related to discounted cumulative gain a common measure for evaluating search
results in the information retrieval literature J?arvelin Kek?al?ainen
Expected DCGf is zero for many ads enabling significant compression of the predictive
index
Lemma suggests ordering by the probability an ad is in the top The DCGf score is
a softer version of indicator of top
All three predictive methods were trained by sampling from the training set as described in
Figure plots the results of testing the four algorithms on the web advertising data Each point in
the figure corresponds to one experiment which consisted of executing each algorithm on test
pages Along the x-axis we vary the time constraint imposed on the algorithm The y-axis plots the
frequency over the test pages that the algorithm succeeded in serving the top scoring ad for position
Figure and for position Figure Thus vertical slices through each plot show the
difference in performance between the algorithms when they are given the same amount of serving
time per page The probabilities were computed by off-line scoring of all ads for each test
page and computing the true ads Serving correctly for position is more difficult than for
position because it also requires correctly serving ads for positions through We see that all
three methods of predictive indexing are superior to Fagin?s halted threshold algorithm In addition
the use of a richer covering for PI-DCG and PI-AVG provides a large boost in performance These
latter two predictive indexing methods attain relatively high accuracy even when fully evaluating
only of the potential results
PI?AVG
PI?DCG
Fixed Ordering
Halted TA
Number of Full Evaluations
PI?AVG
PI?DCG
Fixed Ordering
Halted TA
Probability of Exact Retrieval?10th Result
Comparison of Serving Algorithms
Probability of Exact Retrieval?1st Result
Comparison of Serving Algorithms
Number of Full Evaluations
Figure Comparison of the first and tenth results returned from the four serving algorithms on the
web advertisement dataset
Our implementation of the predictive index and also the halted threshold algorithm required about
per display event when ad evaluations are allowed The RAM use for the predictive index
is also reasonable requiring about a factor of more RAM than the ads themselves
Approximate Nearest Neighbor Search
A special case application of predictive indexing is approximate nearest neighbor search Given a set
of points in n-dimensional Euclidean space and a query point in that same space the nearest
neighbor problem is to quickly return the top-k neighbors of This problem is of considerable
interest for a variety of applications including data compression information retrieval and pattern
recognition In the predictive indexing framework the nearest neighbor problem corresponds to
minimizing a scoring function defined by Euclidean distance We assume
query points are generated from a distribution that can be sampled
To start we define a covering of the input space Rn which we borrow from locality-sensitive
hashing LSH Gionis Datar a commonly suggested scheme for the
approximate nearest neighbor problem Fix positive integer parameters First we form
random partitions of the input space Geometrically each partition splits the n-dimensional space
on random hyperplanes Formally for all and generate a random unitnorm n-vector ij Y1ij Ynij Rn from the Gaussian normal distribution For fixed
and subset define the cover set Qi,J Rn ij
if and only if J}. Note that for fixed the set Qi,J partitions the space by
random planes
Given a query point consider the union Ux Qi,J Qi,J Qi,J of all cover sets containing Standard LSH approaches to the nearest neighbor problem work by scoring points in the set
Qx Ux That is LSH considers only those points in that are covered by at least one of
the same sets as Predictive indexing in contrast maps each cover set Qi,J to an ordered list
of points sorted by their probability of being a nearest point to points in Qi,J That is the
lists are sorted by hQi,J Prq?D|Qi,J is one of the nearest points to For the query
we then consider those points in with large probability hQi,J for at least one of the sets Qi,J that
cover
We compare LSH and predictive indexing over four data sets training and
test points in dimensions points in 32 dimensions split randomly
into training and test subsets Pendigits?7494 training and test points in 17
dimensions and Optdigits?3823 training and test points in 65 dimensions The MNIST
data is available at http://yann.lecun.com/exdb/mnist and the remaining three data
sets are available at the UCI Machine Learning Repository http://archive.ics.uci.edu
Random projections were generated for each experiment inducing a covering of the space that
was provided to both LSH and predictive indexing The predictive index was generated by sampling
over the training set as discussed in Section The number of projections per partition was set to
24 for the larger sets Corel and MNIST and 63 for the smaller sets Pendigits and Optdigits while
the number of partitions was varied as an experimental parameter Larger corresponds to more
full evaluations per query resulting in improved accuracy at the expense of increased computation
time Both algorithms were restricted to the same average number of full evaluations per query
Predictive indexing offers substantial improvements over LSH for all four data sets Figure
displays the true rank of the first point returned by LSH and predictive indexing on the MNIST data
set as a function of averaged over all points in the test set and over multiple trials Predictive
indexing outperforms LSH at each parameter setting with the difference particularly noticeable
when fewer full evaluation are permitted small Figure displays the performance of
LSH and predictive indexing for the tenth point returned over all four data sets with values of
varying from to 70 averaged over the test sets and replicated by multiple runs In over trials
we did not observe a single instance of LSH outperforming predictive indexing
Recent work has proposed more sophisticated partitionings for LSH Andoni Indyk Approaches based on metric trees Liu which take advantage of the distance metric structure have also been shown to perform well for approximate nearest neighbor Presumably taking
advantage of the query distribution could further improve these algorithms as well although that is
not studied here
Conclusion
Predictive indexing is the first datastructure capable of supporting scalable rapid ranking based on
general purpose machine-learned scoring rules In contrast existing alternatives such as the Threshold Algorithm Fagin and Inverted Index approaches Broder are either substantially slower inadequately expressive or both for common machine-learned scoring rules In the
special case of approximate nearest neighbors predictive indexing offers substantial and consistent
improvements over the Locality Sensitive Hashing algorithm
70
Rank of 1st Result
LSH
Predictive Indexing
Number of Partitions
LSH Predictive Indexing All Data Sets
Predictive Indexing Rank of Result
LSH Predictive Indexing on MNIST Data
LSH Rank of Result
The y-axis Rank of 1st Result measures the Each point represents the outcome of a single extrue rank of the first result returned by each method periment for one of the four data sets at various paAs the number of partitions is increased improved rameter settings
accuracy is achieved at the expense of longer computation time
Figure Comparison of the first and tenth results returned from LSH and predictive indexing

----------------------------------------------------------------

title: 6064-end-to-end-goal-driven-web-navigation.pdf

End-to-End Goal-Driven Web Navigation
Rodrigo Nogueira
Tandon School of Engineering
New York University
rodrigonogueira@nyu.edu
Kyunghyun Cho
Courant Institute of Mathematical Sciences
New York University
kyunghyun.cho@nyu.edu
Abstract
We propose a goal-driven web navigation as a benchmark task for evaluating an
agent with abilities to understand natural language and plan on partially observed
environments In this challenging task an agent navigates through a website
which is represented as a graph consisting of web pages as nodes and hyperlinks as
directed edges to find a web page in which a query appears The agent is required
to have sophisticated high-level reasoning based on natural languages and efficient
sequential decision-making capability to succeed We release a software tool
called WebNav that automatically transforms a website into this goal-driven web
navigation task and as an example we make WikiNav a dataset constructed from
the English Wikipedia We extensively evaluate different variants of neural net
based artificial agents on WikiNav and observe that the proposed goal-driven web
navigation well reflects the advances in models making it a suitable benchmark
for evaluating future progress Furthermore we extend the WikiNav with questionanswer pairs from Jeopardy and test the proposed agent based on recurrent neural
networks against strong inverted index based search engines The artificial agents
trained on WikiNav outperforms the engined based approaches demonstrating the
capability of the proposed goal-driven navigation as a good proxy for measuring
the progress in real-world tasks such as focused crawling and question-answering
Introduction
In recent years there have been many exciting advances in building an artificial agent which can be
trained with one learning algorithm to solve many relatively large-scale complicated tasks see
In much of these works target tasks were computer games such as Atari games and
racing car game
These successes have stimulated researchers to apply a similar learning mechanism to language-based
tasks such as multi-user dungeon MUD games Instead of visual perception an agent
perceives the state of the world by its written description A set of actions allowed to the agent is
either fixed or dependent on the current state This type of task can efficiently evaluate the agent?s
ability of not only in planning but also language understanding
We however notice that these MUD games do not exhibit the complex nature of natural languages
to the full extent For instance the largest game world tested by Narasimhan uses a
vocabulary of only unique words and the largest game tested by He uses only
words Furthermore the description of a state at each time step is almost always limited to the visual
description of the current scene lacking any use of higher-level concepts present in natural languages
In this paper we propose a goal-driven web navigation as a large-scale alternative to the text-based
games for evaluating artificial agents with natural language understanding and planning capability
The proposed goal-driven web navigation consists of the whole website as a graph in which the web
pages are nodes and hyperlinks are directed edges An agent is given a query which consists of one
Conference on Neural Information Processing Systems NIPS Barcelona Spain
or more sentences taken from a randomly selected web page in the graph and navigates the network
starting from a predefined starting node to find a target node in which the query appears Unlike
the text-based games this task utilizes the existing text as it is resulting in a large vocabulary with
a truly natural language description of the state Furthermore the task is more challenging as the
action space greatly changes with respect to the state in which the agent is
We release a software tool called WebNav that converts a given website into a goal-driven web
navigation task As an example of its use we provide WikiNav which was built from English
Wikipedia We design artificial agents based on neural networks called NeuAgents trained with
supervised learning and report their respective performances on the benchmark task as well as the
performance of human volunteers We observe that the difficulty of a task generated by WebNav is
well controlled by two control parameters the maximum number of hops from a starting to a
target node Nh and the length of query Nq
Furthermore we extend the WikiNav with an additional set of queries that are constructed from
Jeopardy questions to which we refer by WikiNav-Jeopardy We evaluate the proposed NeuAgents
against the three search-based strategies SimpleSearch Apache Lucene and Google
Search API. The result in terms of document recall indicates that the NeuAgents outperform those
search-based strategies implying a potential for the proposed task as a good proxy for practical
applications such as question-answering and focused crawling
Goal-driven Web Navigation
A task of goal-driven web navigation is characterized by
sS
The world in which an agent A navigates is represented as a graph E). The graph consists
NN
of a set of nodes si
and a set of directed edges ei,j connecting those nodes Each
node represents a page of the website which in turn is represented by the natural language text
D(si in it There exists an edge going from a page si to sj if and only if there is a hyperlink in D(si
that points to sj One of the nodes is designated as a starting node sS from which any navigation
begins A target node is the one whose natural language description contains a query and there
may be more than one target node
At each time step the agent A reads the natural language description D(st of the current node in
which the agent has landed At no point the whole world consisting of the nodes and edges nor its
structure or map graph structure without any natural language description is visible to the agent
thus making this task partially observed
Once the agent A reads the description D(si of the current node si it can take one of the actions
available A set of possible actions is defined as a union of all the outgoing edges and the stop
action thus making the agent have state-dependent action space
Each edge ei,k corresponds to the agent jumping to a next node sk while the stop action corresponds
to the agent declaring that the current node si is one of the target nodes Each edge ei,k is represented
by the description of the next node D(sk In other words deciding which action to take is equivalent
to taking a peek at each neighboring node and seeing whether that node is likely to lead ultimately to
a target node
The agent A receives a reward R(si when it chooses the stop action This task uses a simple
binary reward where
if D(si
R(si
otherwise
Constraints It is clear that there exists an ultimate policy for the agent to succeed at every trial
which is to traverse the graph breadth-first until the agent finds a node in which the query appears To
avoid this kind of degenerate policies the task includes a set of four rules/constraints
An agent can follow at most Nn edges at each node
An agent has a finite memory of size smaller than
Table Dataset Statistics of WikiNav-4 WikiNav-8 WikiNav-16 and WikiNav-Jeopardy
Train
WikiNav-4
WikiNav-8
1M
WikiNav-16
WikiNav-Jeopardy
Valid
1k
Test
1k
An agent moves up to Nh hops away from sS
A query of size Nq comes from at least two hops away from the starting node
The first constraint alone prevents degenerate policies such as breadth-first search forcing the agent
to make good decisions as possible at each node The second one further constraints ensure that the
agent does not cheat by using earlier trials to reconstruct the whole graph structure during test time
or to store the entire world in its memory during training The third constraint which is optional is
there for computational consideration The fourth constraint is included because the agent is allowed
to read the content of a next node
WebNav Software
As a part of this work we build and release a software tool which turns a website into a goal-driven
web navigation task.1 We call this tool WebNav Given a starting URL the WebNav reads the whole
website constructs a graph with the web pages in the website as nodes Each node is assigned a
unique identifier si The text content of each node D(si is a cleaned version of the actual HTML
content of the corresponding web page The WebNav turns intra-site hyperlinks into a set of edges
ei,j
In addition to transforming a website into a graph from the WebNav automatically selects
queries from the nodes texts and divides them into training validation and test sets We ensure that
there is no overlap among three sets by making each target node from which a query is selected
belongs to only one of them
Each generated example is defined as a tuple
where is a query from a web page which was found following a randomly selected path
sS In other words the WebNav starts from a starting page sS random-walks the
graph for a predefined number of steps Nh in our case reaches a target node and selects a
query from A query consists of Nq sentences and is selected among the top-5 candidates
in the target node with the highest average TF-IDF thus discouraging the WebNav from choosing a
trivial query
For the evaluation purpose alone it is enough to use only a query itself as an example However
we include both one target node among potentially many other target nodes and one path from the
starting node to this target node again among many possible connecting paths so that they can be
exploited when training an agent They are not to be used when evaluating a trained agent
WikiNav A Benchmark Task
With the WebNav we built a benchmark goal-driven navigation task using Wikipedia as a target
website We used the dump file of the English Wikipedia from September which consists of
more than five million web pages We built a set of separate tasks with different levels of difficulty by
varying the maximum number of allowed hops Nh and the size of query Nq
We refer to each task by WikiNav-Nh Nq
For each task we generate training validation and test examples from the pages half as many hops
away from a starting page as the maximum number of hops allowed.2 We use Category:Main topic
classifications as a starting node sS
The source code and datasets are publicly available at github.com/nyu-dl/WebNav
This limit is an artificial limit we chose for computational reasons
Table Sample query-answer pairs from WikiNav-Jeopardy
Query
Answer
For the last years of his life Galileo was under
house arrest for espousing this man?s theory
Copernicus
In the winter of a record inches of snow fell
at Rainier Paradise Ranger Station in this state
Washington
This company?s Accutron watch introduced in
had a guarantee of accuracy to within one minute a month
Bulova
As a minimal cleanup procedure we excluded meta articles whose titles start with Wikipedia
Any hyperlink that leads to a web page outside Wikipedia is removed in advance together with the
following sections References External Links Bibliography and Partial Bibliography
In Table we present basic per-article statistics of the
English Wikipedia It is evident from these statistics that
the world of WikiNav-Nh Nq is large and complicated
even after the cleanup procedure
We ended up with a fairly small dataset for WikiNav-4
but large for WikiNav-8 and WikiNav-16 See Table
for details
Related Work Wikispeedia
Avg.
Var
Max
Min
Hyperlinks
Words
Table Per-page statistics of English
Wikipedia
This work is indeed not the first to notice the possibility of a website or possibly the whole web as a
world in which intelligent agents explore to achieve a certain goal One most relevant recent work to
ours is perhaps Wikispeedia from
West proposed the following game called Wikispeedia The game?s world is nearly
identical to the goal-driven navigation task proposed in this work More specifically they converted
Wikipedia for Schools which contains approximately articles as of into a graph whose
nodes are articles and directed edges are hyperlinks From this graph a pair of nodes is randomly
selected and provided to an agent
The agent?s goal is to start from the first node navigate the graph and reach the second node Similarly
to the WikiNav the agent has access to the text content of the current nodes and all the immediate
neighboring nodes One major difference is that the target is given as a whole article meaning that
there is a single target node in the Wikispeedia while there may be multiple target nodes in the
proposed WikiNav
From this description we see that the goal-driven web navigation is a generalization and re-framing
of the Wikispeedia First we constrain a query to contain less information making it much more
difficult for an agent to navigate to a target node Furthermore a major research question by West and
Leskovec was to understand how humans navigate and find the information they are looking
for whereas in this work we are fully focused on proposing an automatic tool to build a challenging
goal-driven tasks for designing and evaluating artificial intelligent agents
WikiNav-Jeopardy Jeopardy on WikiNav
One of the potential practical applications utilizing the goal-drive navigation is question-answering
based on world knowledge In this Q&A task a query is a question and an agent navigates a given
information network website to retrieve an answer In this section we propose and describe
an extension of the WikiNav in which query-target pairs are constructed from actual Jeopardy
question-answer pairs We refer to this extension of WikiNav by WikiNav-Jeopardy
We first extract all the question-answer pairs from Archive3 which has more than such
pairs We keep only those pairs whose answers are titles of Wikipedia articles leaving us with
pairs We divide those pairs into training validation and test examples while carefully
www.j-archive.com
ensuring that no article appears in more than one partition Additionally we do not shuffle the original
pairs to ensure that the train and test examples are from different episodes
For each training pair we find one path from the starting node Main Topic Classification to the
target node and include it for supervised learning For

----------------------------------------------------------------

title: 2244-parametric-mixture-models-for-multi-labeled-text.pdf

Parametric Mixture Models for
Multi-Labeled Text
Naonori Ueda
Kazumi Saito
NTT Communication Science Laboratories
Hikaridai Seikacho Kyoto Japan
ueda,saito}@cslab.kecl.ntt.co.jp
Abstract
We propose probabilistic generative models called parametric mixture models PMMs for multiclass multi-labeled text categorization problem Conventionally the binary classification approach
has been employed in which whether or not text belongs to a category is judged by the binary classifier for every category In contrast our approach can simultaneously detect multiple categories of
text using PMMs We derive efficient learning and prediction algorithms for PMMs We also empirically show that our method could
significantly outperform the conventional binary methods when applied to multi-labeled text categorization using real World Wide
Web pages
Introduction
Recently as the number of online documents has been rapidly increasing automatic text categorization is becoming a more important and fundamental task in
information retrieval and text mining Since a document often belongs to multiple
categories the task of text categorization is generally defined as assigning one or
more category labels to new text This problem is more difficult than the traditional
pattern classification problems in the sense that each sample is not assumed to be
classified into one of a number of predefined exclusive categories When there are
categories the number of possible multi-labeled classes becomes 2L Hence this
type of categorization problem has become a challenging research theme in the field
of machine learning
Conventionally a binary classification approach has been used in which the multicategory detection problem is decomposed into independent binary classification
problems This approach usually employs the state-of-the-art methods such as support vector machines SVMs and naive Bayes classifiers However
since the binary approach does not consider a generative model of multi-labeled text
we think that it has an important limitation when applied to the multi-labeled text
categorization
In this paper using independent word-based representation known as Bag-of-Words
BOW representation we present two types of probabilistic generative models
for multi-labeled text called parametric mixture models where
PMM2 is a more flexible version of PMM1 The basic assumption under PMMs is
that multi-labeled text has a mixture of characteristic words appearing in singlelabeled text that belong to each category of the multi-categories This assumption
leads us to construct quite simple generative models with a good feature the objective function of PMM1 is convex the global optimum solution can be easily
found We present efficient learning and prediction algorithms for PMMs We also
show the actual benefits of PMMs through an application of WWW page categorization focusing on those from the yahoo.com domain
Parametric Mixture Models
Multi-labeled Text
According to the BOW representation which ignores the order of word occurrence
in a document the nth document dn can be represented by a word-frequency
vector xnV where xni denotes the frequency of word occurrence
in dn among the vocabulary w1 wV Here is the total number
be a category vector for
of words in the vocabulary Next let yL
where yl takes a value of when belongs does not belong to the lth
category is the total number of categories Note that categoriesP
are pre-defined
and that a document always belongs to at least one category yl
In the case of multi-class and single-labeled text it is natural that in the lth catQV
egory should be generated from a multinomial distribution
PV
Here and is a probability that the ith word appears
in a ducument belonging to the lth class We generalize this to multi-class and
multi-labeled text as
where and
Here is a class-dependent probability that the ith word appears in a document
belonging to class Clearly it is impractical to independently set a multinomial
parameter vector to each distinct since there are 2L possible classes Thus
we try to efficiently parameterize them
PMM1
In general words in a document belonging to a multi-category class can be regarded
as a mixture of characteristic words related to each of the categories For example a
document that belongs to both sports and music would consist of a mixture of
characteristic words mainly related to both categories Let The
above assumption indicates that can be represented by
the following parametric mixture
hl where hl for such that yl
PL
Here hl is a mixing proportion hl Intuitively hl can
also be interpreted as the degree to which has the lth category Actually by
experimental verification using about real Web pages we confirmed that the
above assumption was reasonable
Based on the parametric mixture assumption we can construct a simple parametric
PL
mixture model PMM1 in which the degree is uniform hl yl l0 yl0
For example in the case of and
Substituting into PMM1 can be defined by
PL
yl
PL
l0 yl0
A set of unknown model paramters in PMM1 is
Of course multi-category text may sometimes be weighted more toward one category than to the rest of the categories among multiple categories However being
averaged over all biases they could be canceled and therefore PMM1 would be
reasonable This motivates us to construct PMM1
PMMs are different from usual distributional mixture models in the sense that the
mixing is performed in a parameter space while the latter several distributional
components are mixed Since the latter models assume that a sample is generated
from one component they cannot represent multiplicity On the other hand
PMM1 can represent 2L multi-category classes with only parameter vectors
PMM2
In PMM1 shown in is approximated by which can be regarded
as the first-order approximation We consider the second order model PMM2 as
a more flexible model in which parameter vectors of duplicate-category l,m are
also used to approximate
hl y)hm l,m where l,m
Here is a non-negative bias parameter satisfying
Clearly For example in the case of
In PMM2 unlike in PMM1 the category biases themselves
can be estimated from given training data
Based on PMM2 can be defined by
yl ym
PL
PL
yl
ym
A set of unknown parameters in PMM2 becomes
Related Model
Very recently as a more general probabilistic model for multi-latent-topics text
called Latent Dirichlet Allocation has been proposed However LDA is
formulated in an unsupervised manner Blei also perform single-labeled text
categorization using LDA in which individual LDA is fitted to each class Namely
they do not explain how to model the observed class labels in LDA.
In contrast our PMMs can efficiently model class depending on other classes
through the common basis vectors Moreover based on the PMM assumtion models
much simpler than LDA can be constructed as mentioned above Moreover unlike
in LDA it is feasible to compute the objective functions for PMMs exactly as shown
below
Learning Prediction Algorithms
Objective functions
Let
denote the given training data labeled documents The
unknown parameter is estimated by maximizing posterior Assuming
map arg max log log
that is independent of
Here is prior over the parameters We used the following conjugate priors
QL QV
for PMM1
Dirichlet distributions over and as
QL QV
and for PMM2 Here and are
hyperparameters and in this paper we set and each of which is
equivalent to Laplace smoothing for and respectively
map is given by
Consequently the objective function to find
log
log
Of course the third term on the RHS of is just ignored for PMM1 The
likelihood term is given by
PMM1
xn,i log
PMM2
hnl
xn,i log
hnl hnm
Note that
Update formulae
The optimization problem given by cannot be solved analytically therefore
some iterative method needs to be applied Although the steepest ascend algorithms
involving Newton?s method are available here we derive an efficient algorithm in a
similar manner to the EM algorithm First we derive parameter update formulae
for PMM2 because they are more general than those for PMM1 We then explain
those for PMM1 as a special case
Suppose that is obtained at step We then attmpt to derive by using
For convenience we define gl,m,i
and as follows
gl,m,i
hnl hnm
hnl hnm
l,m l,m
PL
Noting that gl,m,i
for PMM2 can be rewritten as
hnl hnm
hnl hnm
n,i
l,m
xn,i
gl,m,i
log hnl hnm
xn,i
gl,m,i
log gl,m,i
n,i
xn,i
gl,m,i
log
n,i
l,m
l,m
Moreover noting that l,m l,m we rewrite the first term on
the RHS of as
xn,i
gl,m,i
l,m log
n,i
l,m
l,m log
hl hm
From and we obtain the following important equation
Here and are defined by
l,m log hnl hnm
xn,i gl,m,i
n,i,l,m
xn,i
n,i,l,m
l,m log hnl hnm
gl,m,i
log gl,m,i
l,m log l,m
l,m log l,m
From Jensen?s inequality holds Thus we just maximize
log to derive the parameter update formula Noting that
we can derive the following formulae
qm,l,i
and ql,m,i
PN
xni ql,m,i
PN
xni ql,m,i
PV
ql,m,i
PV
ql,m,i
These parameter updates always converge to a local optimum of given by
In PMM1 since unknown parameter is just by modifying as
hn
gl,i
PL
hl
and rewriting in a similar manner we obtain
xn,i
xn,i
gl,i
log hnl
gl,i
log gl,i
n,i
n,i
In this case becomes a simpler form as
xn,i
gl,i
log hnl
PV
Therefore
maximizing log under the
constraint we can obtain the following update formula for PMM1
PN
xn,i gl,i
li
PV PN
xn,i gl,i
Remark The parameter update given by of PMM1 always converges to
the global optimum solution
Proof The Hessian matrix of the objective function of PMM1 becomes
d2
li
li
li
hi li
n,i
l,i
Here is an arbitrary vector in the space Noting that xni and
is negative definite therefore is a strictly convex
Pfunction of Moreover since
the feasible region defined by and constraints
is a convex set
the maximization problem here becomes a convex programming problem and has
a unique global solution Since always increases at each iteration the
learning algorithm given above always converges to the global optimum solution
irrespective of any initial parameter value
Prediction
denote the estimated parameter Then applying Bayes rule the opLet
timum category vector for of a new document is defined as
under a uniform class prior assumption Since this maxiarg maxy
mization problem belongs to the zero-one integer problem NP-hard problem
an exhaustive search is prohibitive for a large L. Therefore we solve this problem
approximately with the help of the following greedy-search algorithm That is first
is maximized Then for the reonly one yl1 value is set to so that
is set to
maining elements only one yl2 value which mostly increases
cannot increase
under a fixed yl1 value This procedure is repeated until
any further This algorithm successively determines an element in to increase the
posterior probability until its value does not improve This is very efficient because
it requires the calculation of the posterior probability at most L(L times
while the exhaustive search needs 2L times
Experiments
Automatic Web Page Categorization
We tried to categorize real Web pages linked from the yahoo.com domain1 More
specifically Yahoo consists of top-level categories Arts Humanities
Business Economy Computers Internet and so and each category is
classified into a number of second-level subcategories By focusing on the secondlevel categories we can make independent text categorization problems We used
of these problems2 In those problems mininum maximum values of
and were and respectively About of the pages
are multi-labeled over the problems To collect a set of related Web pages for
each problem we used a software robot called GNU Wget version A text
multi-label can be obtained by following its hyperlinks in reverse toward the page
of origin
We compared our PMMs with the convetional methods naive Bayes SVM
k-nearest neighbor and three-layer neural networks We used linear
SVMlight version tuning the penalty cost and cost-factor for negative
and positive samples parameters for each binary classification to improve the SVM
results In addition it is worth mentioning that when performing the SVM
PV
each was normalized to be xni because discrimination is much easier
in the 1-dimensional simplex than in the original dimensional space In other
words classification is generally not determined by the number of words on the
page actually normalization could also significantly improve the performance
This domain is a famous portal site and most related pages linked from the domain
are registered by site recommendation and therefore category labels would be reliable
We could not collect enough pages for three categories due to our communication
network security However we believe that independent problems are sufficient for
evaluating our method
Since the ratio of the number of positive samples to negative samples per category
was quite small in our web pages SVM without the option provided poor results
No.
Table Performance for test data using training data
NB
SVM
kNN
NN
PMM1
PMM2
We employed the cosine similarity for kNN method for more details As for
NNs an NN consists of input units and output units for estimating a category
vector from each frequency vector We used hidden units An NN was trained
to maximize the sum of cross-entropy functions for target and estimated category
vectors of training samples together with a regularization term consisting of a sum
of squared NN weights Note that we did not perform any feature transformations
such as TFIDF for an example see because we wanted to evaluate the
basic performance of each detection method purely
We used the F-measure as the performance measure which is defined as the weighted
harmonic average of two well-known statistics precision and recall R. Let
yL
and
y1n y?L
be actual and predicted category vecn
tors for respectively Subsequently the Fn 2Pn Rn Rn where
PL
PL
PL
PL
Pn yln y?ln y?ln and Rn yln y?ln yln We evaluated the
formance by
Fn using test data independent of the training
data Although micro and macro-averages can be used we think that the samplebased measure is the most suitable for evaluating the generalization performance
since it is natural to consider the assumption for documents
Results
For each of the problems we used five pairs of training and test data sets In
Table Table we compared the mean of the values over five trials by using
training documents Each number in parenthesis in the Tables denotes
the standard deviation of the five trials PMMs took about five minutes for training
data and only about one minute for the test data on Pentium
PC averaged over the problmes The PMMs were much faster than the k-NN
and NN. In the binary approach SVMs with optimally tuned parameters produced
rather better results than the NB method The performance by SVMs however
was inferior to those by PMMs in almost all problems These experimental results
support the importance of considering generative models of multi-category text
When the training sample size was kNN provided comparable results to the
NB method On the other hand when the training sample size was the kNN
method obtained results similar to or slightly better than those of SVM. However
in both cases PMMs significantly outperformed kNN We think that the memorybased approach is limited in its generalization ability for multi-labeled text categorization
The results of well-regularized NN were fair although it took an intolerable amount
of training time indicating that flexible discrimination would not be necessary for
Table Performance for test
No.
NB
SVM
kNN
data using training data
NN
PMM1
PMM2
discriminating high-dimensional sparse-text data The results obtained by PMM1
were better than those by PMM2 which indicates that a model with a fixed l,m
seems sufficient at least for the WWW pages used in the experiments
Concluding Remarks
We have proposed new types of mixture models PMMs for multi-labeled text
categorization and also efficient algorithms for both learning and prediction We
have taken some important steps along the path and we are encouraged by our
current results using real World Wide Web pages Moreover we have confirmed
that studying the generative model for multi-labeled text is beneficial in improving
the performance

----------------------------------------------------------------

title: 2291-improving-a-page-classifier-with-anchor-extraction-and-link-analysis.pdf

Improving a Page Classifier with Anchor
Extraction and Link Analysis
William W. Cohen
Center for Automated Learning and Discovery
Carnegie-Mellon University
Forbes Ave Pittsburgh PA
william@wcohen.com
Abstract
Most text categorization systems use simple models of documents and
document collections In this paper we describe a technique that improves a simple web page classifier?s performance on pages from a new
unseen web site by exploiting link structure within a site as well as
page structure within hub pages On real-world test cases this technique
significantly and substantially improves the accuracy of a bag-of-words
classifier reducing error rate by about half on average The system uses
a variant of co-training to exploit unlabeled data from a new site Pages
are labeled using the base classifier the results are used by a restricted
wrapper-learner to propose potential main-category anchor wrappers
and finally these wrappers are used as features by a third learner to find
a categorization of the site that implies a simple hub structure but which
also largely agrees with the original bag-of-words classifier
Introduction
Most text categorization systems use simple models of documents and document collections For instance it is common to model documents as bags of words and to model
a collection as a set of documents drawn from some fixed distribution An interesting
question is how to exploit more detailed information about the structure of individual documents or the structure of a collection of documents
For web page categorization a frequently-used approach is to use hyperlink information
to improve classification accuracy Often hyperlink structure is used to
smooth the predictions of a learned classifier so that documents that say are pointed to
by the same hub page will be more likely to have the same classification after smoothing
This smoothing can be done either explicitly or implicitly for instance by representing examples so that the distance between examples depends on hyperlink connectivity
The structure of individual pages as represented by HTML markup structure or linguis
tic structure is less commonly used in web page classification however page structure is
often used in extracting information from web pages Page structure seems to be particularly important in finding site-specific extraction rules wrappers since on a given site
formatting information is frequently an excellent indication of content
This paper is based on two practical observations about web page classification The first
is that for many categories of economic interest product pages job-posting pages
and press releases many sites contain hub or index pages that point to essentially all
pages in that category on a site These hubs rarely link exclusively to pages of a single
category?instead the hubs will contain a number of additional links such as links back to
a home page and links to related hubs However the page structure of a hub page often
gives strong indications of which links are to pages from the main category associated
with the hub and which are ancillary links that exist for other navigational purposes
As an example refer to Figure Links to pages in the main category associated with this
hub previous NIPS conference homepages are in the left-hand column of the table and
hence can be easily identified by the page structure
The second observation is that it is relatively easy to learn to extract links from hub pages
to main-category pages using existing wrapper-learning methods Wrapper-learning
techniques interactively learn to extract data of some type from a single site using userprovided training examples Our experience in a number of domains indicates that maincategory links on hub pages like the NIPS-homepage links from Figure can almost
always be learned from two or three positive examples
Exploiting these observations we describe in this paper a web page categorization system
that exploits link structure within a site as well as page structure within hub pages to
improve classification accuracy of a traditional bag-of-words classifier on pages from a
previously unseen site The system uses a variant of co-training to exploit unlabeled
data from a new previously unseen site Specifically pages are labeled using a simple
bag-of-words classifier and the results are used by a restricted wrapper-learner to propose
potential main-category link wrappers These wrappers are then used as features by a
decision tree learner to find a categorization of the pages on the site that implies a simple
hub structure but which also largely agrees with the original bag-of-words classifier
One-step co-training and hyperlink structure
Consider a binary bag-of-words classifier that has been learned from some set of labeled
web pages We wish to improve the performance of on pages from an unknown web
site by smoothing its predictions in a way that is plausible given the hyperlink of
and the page structure of potential hub pages in S. As background for the algorithm let
us consider first co-training a well-studied approach for improving classifier performance
using unlabeled data
In co-training one assumes a concept learning problem where every instance can be
written as a pair such that is conditionally independent of given the class
One also assumes that both and are sufficient for classification in the sense that
the target function can be written either as a function of or that there exist
functions f1 and f2 Finally one assumes that both f1 and f2 are
learnable that f1 H1 and f2 H2 and noise-tolerant learning algorithms A1 and
A2 exist for H1 and H2
Webpages and Papers for Recent NIPS Conferences
A. David Redish dredish@cs.cmu.edu created and maintained these web pages from
until L. Douglas Baker ldbapp+nips@cs.cmu.edu maintained these web pages from
until They were maintained in by L. Douglas Baker and Alexander Gray
agray+nips@cs.cmu.edu
NIPS 13 the conference proceedings for Advances in Neural
Information Processing Systems edited by Leen Todd Dietterich
Thomas G. and Tresp Volker will be available to all attendees in June
Abstracts and papers from this forthcoming volume are available
on-line
BibTeX entries for all papers from this forthcoming volume are available
on-line
NIPS*99
NIPS is available from MIT Press
Abstracts and papers from this volume are available on-line
NIPS*98
NIPS is available from MIT Press
Abstracts and some papers from this volume are available on-line
Figure Part of a hub page Links to pages in the main category associated with this
hub are in the left-hand column of the table
In this setting a large amount of unlabeled data Du can be used to improve the accuracy
of a small set of labeled data as follows First use A1 to learn an approximation
to f1 using Then use to label the examples in Du and use A2 to learn from this
training set Given the assumptions above errors on Du will appear to A2 as random
uncorrelated noise and A2 can in principle learn an arbitrarily good approximation to
given enough unlabeled data in Du We call this process one-step co-training using A1
A2 and Du
Now consider a set DS of unlabeled pages from a unseen web site S. It seems not unreasonable to assume that the words on a page and the hub pages
that hyperlink to are independent given the class of This suggests that one-step cotraining could be used to improve a learned bag-of-words classifier using the following
algorithm
Algorithm One-step co-training
Parameters Let be a web site be a bag-of-words page classifier and DS be
the pages on the site S.
Instance generation and labeling For each page DS represent as a vector
of all pages in that hyperlink to Call this vector xi2 Let
Learning Use a learner A2 to learn from the labeled examples D2
Labeling Use as the final label for each page DS
This one-step use of co-training is consistent with the theoretical results underlying cotraining In experimental studies co-training is usually done iteratively alternating between using and for tagging the unlabeled data The one-step version seems more
appropriate in this setting in which there are a limited number of unlabeled examples over
which each is defined
Anchor Extraction and Page Classification
Learning to extract anchors from web pages
Algorithm has some shortcomings Co-training assumes a large pool of unlabeled data
however if the informative hubs for pages on are mostly within very plausible
assumption then the amount of useful unlabeled data is limited by the size of S. With limited amounts of unlabeled data it is very important that A2 has a strong and appropriate
statistical bias and that A2 has some effective method for avoiding overfitting
As suggested by Figure the informativeness of hub features can be improved by using
knowledge of the structure of hub pages themselves To make use of hub page structure
we used a wrapper-learning system called WL2 which has experimentally proven to be
effective at learning substructures of web pages The output of WL is an extraction
predicate a binary relation between pages and substrings a within As an example
WL2 might output is the page of Figure and a is an anchor appearing
in the first column of the table An anchor is a substring of a web page that defines a
hyperlink
This suggests a modification of Algorithm in which one-step co-training is carried out on
the problem of extracting anchors rather than the problem of labeling web pages Specifically one might map f1 predictions from web pages to anchors by giving a positive label
to anchor a iff a links to a page such that then use WL2 algorithm A2 to learn
a predicate and finally map the predictions of from anchors back to web pages
One problem with this approach is that WL2 was designed for user-provided data sets
which are small and noise-free Another problem is that it unclear how to map class labels from anchors back to web pages since a page might be pointed to by many different
anchors
Bridging the gap between anchors and pages
Based on these observations we modified Algorithm as follows As suggested we map
the predictions about page labels made by to anchors Using these anchor labels we then
produce many small training sets that are passed to WL2 The intuition here is that some of
these training sets will be noise-free and hence similar to those that might be provided by
a user Finally we use the many wrappers produced by WL2 as features in a representation
of a page and again use a learner to combine the wrapper-features and produce a single
classification for a page
Algorithm
Parameters Let be a web site be a bag-of-words page classifier and DS be
the pages on the site
Link labeling For each anchor a on a page label a as tentatively-positive
if a points to a page such that and
Wrapper proposal Let be the set of all pairs where a is a tentativelypositive link and is the page on which a is found Generate a number of small
sets D1 Dk containing such pairs and for each subset Di use WL2 to produce a number of possible extraction predicates pi,ki See appendix for
details
Instance generation and labeling We will say that the wrapper predicate ij
links to iff pij includes some pair such that DS and a is a hyperlink
to page For each page DS represent as a vector of all wrappers pij
that link to Call this vector xi2 Let
Learning Use a learner A2 to learn from the labeled examples DS
Labeling Use as the final label for each page DS
A general problem in building learning systems for new problems is exploiting existing
knowledge about these problems In this case in building a page classifier one would
like to exploit knowledge about the related problem of link extraction Unfortunately this
knowledge is not in any particularly convenient form a set of well-founded parametric
assumptions about the data instead we only know that experimentally a certain learning
algorithm works well on the problem In general it is often the case that this sort of
experimental evidence is available even when a learning problem is not formally wellunderstood
The advantage of Algorithm is that one need make no parametric assumptions about
the anchor-extraction problem The bagging-like approach of feeding WL many small
training sets and the use of a second learning algorithm to aggregate the results of WL
are a means of exploiting prior experimental results in lieu of more precise statistical assumptions
Experimental results
To evaluate the technique we used the task of categorizing web pages from company sites
as executive biography or other We selected nine company web sites with non-trivial
hub structures These were crawled using a heuristic spidering strategy intended to find
executive biography pages with high recall.1 The crawl found pages of which
were labeled positive A simple bag-of-words classifier was trained using a disjoint set
of sites different from the nine above obtaining an average accuracy of recall
precision on the nine held-out sites Using an implemention of Winnow
as A2 Algorithm obtained an average accuracy of on the nine held-out
sites Algorithm improves over the baseline classifier on six of the nine sites and
obtains the same accuracy on two more This difference is significant at the level with
a 2-tailed paired sign test and at the level with a 2-tailed paired test
Similar results were also obtained using a sparse-feature implementation of a C4.5-like
decision tree learning algorithm for learner A2 Note that both Winnow and are
known to work well when data is noisy irrelevant attributes are present and the underlying
concept is simple These results are summarized in Table
The authors wish to thank Vijay Boyaparti for assembling this data set
Site
avg
Classifier
Accuracy
Algorithm
Accuracy
Algorithm Winnow
Accuracy
Table Experimental results with Algorithm Paired tests indicate that both versions of
Algorithm significantly improve on the baseline classifier
Related work
The introduction discusses the relationship between this work and a number of previous
techniques for using hyperlink structure in web page classification The WL based method for finding document structure has antecedents in other techniques for learning and automatically detecting structure in web pages
In concurrent work Blei al introduce a probabilistic model called scoped learning
which gives a generative model for the situation described here collections of examples
in which some subsets documents from the same site share common local features
and all documents share common content features Blei al do not address the specific
problem considered here of using both page structure and hyperlink structure in web page
classification However they do apply their technique to two closely related problems
they augment a page classification method with local features based on the page?s URL
and also augment content-based classification of text nodes specific substrings of a web
page with page-structure-based local features
We note that Algorithm could be adapted to operate in Blei al?s setting specifically
the vectors produced in Steps could be viewed as local features In fact Blei
al generated page-structure-based features for their extraction task in exactly this way
the only difference is that WL2 was parameterized differently The co-training framework
adopted here clearly makes different assumptions than those adopted by Blei More experimentation is needed to determine which is preferable?current experimental evidence
is ambiguous as to when probabilistic approaches should be prefered to co-training
Conclusions
We have described a technique that improves a simple web page classifier by exploiting
link structure within a site as well as page structure within hub pages The system uses
a variant of co-training called one-step co-training to exploit unlabeled data from a new
site First pages are labeled using the base classifier Next results of this labeling are
propogated to links to labeled pages and these labeled links are used by a wrapper-learner
called WL2 to propose potential main-category link wrappers Finally these wrappers
are used as features by another learner A2 to find a categorization of the site that implies a
simple hub structure but which also largely agrees with the original bag-of-words classifier
Experiments suggest the choice of A2 is not critical
On a real-world benchmark problem this technique substantially improved the accuracy
of a simple bag-of-words classifier reducing error rate by about half This improvement is
statistically significant
Acknowledgments
The author wishes to thank his former colleagues at Whizbang Labs for many helpful discussions and useful advice
Appendix A Details on Wrapper Proposal
Extraction predicates are constructed by WL2 using a rule-learning algorithm and a configurable set of components called builders Each builder corresponds to a language of
extraction predicates Builders support a certain set of operations relative to in particular the least general generalization LGG operation Given a set of pairs
such that each is a substring of LGGB is the least general LB such that
Intuitively LGGB encodes common properties of the positive examples in D. Depending on these properties might be membership in a particular
syntactic HTML structure a specific table column common visual properties
being rendered in boldface etc
To generate subsets Di in Step of Algorithm we used every pair of links that pointed
to the two most confidently labeled examples every pair of adjacent tentatively-positive
links and every triple and every quadruple of tentatively-positive links that were separated
by at most intervening tokens These heuristics were based on the observation that in
most extraction tasks the items to be extracted are close together Careful implementation
allows the subsets Di to be generated in time linear in the size of the site We also note
that these heuristics were initially developed to support a different set of experiments
and were not substantially modified for the experiments in this paper
Normally WL2 is parameterized by a list of builders which are called by a master
rule-learning algorithm In our use of WL2 we simply applied each builder Bj to a dataset
Di to get the set of predicates pij LGGBj Di instead of running the full WL2
learning algorithm

----------------------------------------------------------------

title: 3986-optimal-web-scale-tiering-as-a-flow-problem.pdf

Optimal Web-scale Tiering as a Flow Problem
Novi Quadrianto
SML-NICTA RSISE-ANU
Canberra ACT Australia
novi.quad@gmail.com
Gilbert Leung
eBay Inc.
San Jose CA USA
gleung@alum.mit.edu
Kostas Tsioutsiouliklis
Yahoo Labs
Sunnyvale CA USA
kostas@yahoo-inc.com
Alexander J. Smola
Yahoo Research
Santa Clara CA USA
alex@smola.org
Abstract
We present a fast online solver for large scale parametric max-flow problems as
they occur in portfolio optimization inventory management computer vision and
logistics Our algorithm solves an integer linear program in an online fashion It
exploits total unimodularity of the constraint matrix and a Lagrangian relaxation to
solve the problem as a convex online game The algorithm generates approximate
solutions of max-flow problems by performing stochastic gradient descent on a set
of flows We apply the algorithm to optimize tier arrangement of over 84 million
web pages on a layered set of caches to serve an incoming query stream optimally
Introduction
Parametric flow problems have been well-studied in operations research It has received a significant amount of contributions and has been applied in many problem areas such as database
record segmentation energy minimization for computer vision critical load factor determination in two-processor systems end-of-session baseball elimination and most recently by
18 in product portfolio selection In other words it is a key technique for many estimation and assignment problems Unfortunately many algorithms proposed in the literature are geared
towards thousands to millions of objects rather than billions as is common in web-scale problems
Our motivation for solving parametric flow is the problem of webpage tiering for search engine
indices While our methods are entirely general and could be applied to a range of other machine
learning and optimization problems we focus on webpage tiering as the illustrative example in this
paper The rationale for choosing this application is threefold firstly it is a real problem in search
engines Secondly it provides very large datasets Thirdly in doing so we introduce a new problem
to the machine learning community That said our approach would also be readily applicable to
very large scale versions of the problems described in
The specific problem that will provide our running example is that of assigning webpages to several
tiers of a search engine cache such that the time to serve a query is minimized For a given query
a search engine returns a number of documents typically The time it takes to serve a query
depends on where the documents are located The first tier cache is the fastest using premium
hardware etc thus also often the smallest and retrieves its documents with little latency If even
just a single document is located in a back tier the delay is considerably increased since now we
need to search the larger and slower tiers until the desired document is found Hence it is our
goal to assign the most popular documents to the fastest tiers while taking the interactions between
documents into account
The Tiering Problem
We would like to allocate documents into tiers of storage at our disposal Moreover let
be the queries arriving at a search engine with finite values vq the probability of
the query possibly weighted by the relevance of the retrieved results and a set of documents Dq
retrieved for the query This input structure is stored in a bipartite graph with vertices
and edges whenever document should be retrieved for query
The tiers with tier as the most desirable and the least most costly for retrieval form an
increasing sequence of cummulative capacities Ct with Ct indicating how many pages can be stored
by tiers t0 together Without loss of generality assume that is the last tier is
required to hold all documents or the problem can be reduced Finally for each we assume
that there is a penalty incurred by a tier-miss at level known as fallthrough from tier
to tier And since we have to access tier regardless we set p0 for convenience For
instance retrieving a page in tier incurs a total penalty of p1 p2
Background
Optimization of index structures and data storage is a key problem in building an efficient search
engine Much work has been invested into building efficient inverted indices which are optimized for
query processing These papers all deal with the issue of optimizing the data representation
for a given query and how an inverted index should be stored and managed for general queries In
particular address the problem of computing the top-k results without scanning over the
entire inverted lists Recently machine learning algorithms have been proposed to improve the
ordering within a given collection beyond the basic inverted indexing setup
A somewhat orthogonal strategy to this is to decompose the collection of webpages into a number
of disjoint tiers ordered in decreasing level of relevance That is documents are partitioned
according to their relevance for answering queries into different tiers of typically increasing size
This leads to putting the most frequently retrieved or the most relevant according to the value of
query the market or other operational parameters pages into the top tier with the smallest latency
and relegating the less frequently retrieved or the less relevant pages into bottom tiers Since queries
are often carried out by sequentially searching this hierarchy of tiers an improved ordering minimizes latency improves user satisfaction and it reduces computation
A naive implementation of this approach would simply assign a value to each page in the index and
arrange them such that the most frequently accessed pages reside in the highest levels of the cache
Unfortunately this approach is suboptimal in order to answer a given query well a search engine
typically does not only return a single page as a result but rather returns a list of typically
pages This means that if even just one of these pages is found at a much lower tier we either need
to search the backtiers to retrieve this page or alternatively we need to sacrifice result relevance
At first glance the problem is daunting we need to take all correlations among pages induced by
user queries into account Moreover for reasons of practicality we need to design an algorithm
which is linear in the amount of data presented the number of queries and whose storage
requirements are only linear in the number of pages Finally we would like to obtain guarantees in
terms of performance for the assignment that we obtain from the algorithm Our problem even for
is closely related to the weighted k-densest subgraph problem which is NP hard
Optimization Problem
Since the problem we study is somewhat more general than the parametric flow problem we give a
self-contained derivation of the problem and derive the more general version beyond For brevity
we relegate all proofs to the Appendix
We denote the result set for query by Dq and similarly the set of queries seeking for a document by Qd G}. For a document we denote by zd
the tier storing Define
uq max zd
d?Dq
as the number of cache levels we need to traverse to answer query In other words it is the
document found in the worst tier which determines the cost of access Integrating the optimization
over uq we may formulate the tiering problem as an integer program
uq
minimize
z,u
vq
pt subject to zd uq for all and
q?Q
zd Ct
d?D
Note that we replaced the maximization condition by a linear inequality in preparation for a
reformulation as an integer linear program Obviously the optimal uq for a given will satisfy
Lemma
Assume that Ck Then there exists an optimal solution of such that
Ct for all
In the following we address several issues associated with the optimization problem A is an
integer program and consequently it is discrete and nonconvex We show that there exists a convex
reformulation of the problem It is at a formidable scale often Section presents
a stochastic gradient descent procedure to solve the problem in few passes through the database
We have insufficient data for an accurate tier assignment for pages associated with tail queries This
can be addressed by a smoothing estimator for the tier index of a page
Integer Linear Program
We now replace the selector variables zd and uq by binary variables via a thermometer code Let
subject to xdt for all
subject to yqt for all
be index variables Thus we have the one-to-one mapping zd xdt and xdt zd
between and For instance for a middle tier maps into requiring
two fallthroughs and the best tier corresponds to The mapping between
and is analogous The constraint uq zd can simply be rewritten coordinate-wise yqt xdt
Finally the capacity constraints assume the form xdt Ct That is the number of pages
allocated to higher tiers are at least Ct Define remaining capacities C?t Ct and use
the variable transformation we have the following integer linear program
minimize yp
x,y
subject to xdt and yqt and yqt xdt for all
xdt Ct for all
where and are column vectors and a matrix yqt The
advantage of is that while still discrete we now have linear constraints and a linear objective
function The only problem is that the variables and need to be binary
Lemma The solutions of and are equivalent
Hardness
Before discussing convex relaxations and approximation algorithms it is worthwhile to review the
hardness of the problem consider only two tiers and a case where we retrieve only two pages
per query The corresponding graph has vertices and edges d0 whenever and d0 are
displayed together to answer a query In this case the tiering problem reduces to one of finding a
subset of vertices D0 such that the induced subgraph has the largest number possibly weighted
of edges subject to the capacity constraint C.
For the case of pages per query simply assume that of the pages are always the same Hence
the problem of finding the best subset reduces to the case of pages per query This problem is
identical to the k-densest subgraph problem which is known to be NP hard
URL
que
ry
ry
que
URL
Figure k-densest subgraph reduction Vertices
correspond to URLs and queries correspond to
edges Queries can be served whenever the corresponding URLs are in the cache This is the case
whenever the induced subgraph contains the edge
Convex Programming
The key idea in solving is to relax the capacity constraints for the tiers This renders the problem
totally unimodular and therefore amenable to a solution by a linear program We replace the capacity
constraint by a partial Lagrangian This does not ensure that we will be able to meet the capacity
constraints exactly anymore Instead we will only be able to state ex-post that the relaxed solution
is optimal for the observed capacity distribution Moreover we are still able to control capacity by
a suitable choice of the associated Lagrange multipliers
Linear Program
Instead of solving we study the linear program
minimize yp subject to xdt and yqt
x,y
yqt xdt for and xdt yqt
Here act as Lagrange multipliers for enforcing capacity constraints
and denotes a column of ones We now relate the solution of to that of
Lemma For any choice of with the linear program has an integral solution
Pi.e there
which minimize Moreover for C?t x?dt the
exists some satisfying x?dt yqt
solution also solves
We have succeeded in reducing the complexity of the problem to that of a linear program yet it is still
formidable and it needs to be solved to optimality for an accurate caching prescription Moreover
we need to adjust such that we satisfy the desired capacity constraints approximately
Lemma Denote by the value of at the solution of and let C?t
Hence is concave in and moreover is maximized for a choice of where the solution
of satisfies the constraints of
Note that while the above two lemmas provide us with a guarantee that for every and for every
associated integral solution of there exists a set of capacity constraints for which this is optimal
and that such a capacity satisfying constraint can be found efficiently by concave maximization
they do not guarantee the converse not every capacity constraint can be satisfied by the convex
relaxation as the following example demonstrates
Example Consider the case of tiers hence we drop the index a single query and documents Set the capacity constraint of the first tier to In this case it is impossible to avoid a cache
miss in the ILP. In the LP relaxation of however the optimal non-integral solution is to set all
xd 31 and yq 13 The partial Lagrangian is maximized for Moreover for
the optimization problem has as its solution whereas for the
solution is For the critical value any convex combination of those two values is valid
This example shows why the optimal tiering problem is NP hard it is possible to design cases
where the tier assignment for a page is highly ambiguous Note that for the integer programming
problem with capacity constraint we could allocate an arbitrary pair of pages to the cache
This does not change the objective function total cache miss or feasibility
pages
pages
queries
queries
Figure Left maximum flow problem for a problem of pages and queries The minimum cut
of the directed graph needs to sever all pages leading to a query or alternatively it needs to sever the
corresponding query incurring a penalty of vq This is precisely the tiering objective function
for the case of two tiers Right the same query graph for three tiers Here the black nodes and
dashed edges represent a copy of the original graph additionally each page in the original graph
also has an infinite-capacity link to the corresponding query in the additional graph
Graph Cut Equivalence
It is well known that the case of two tiers can be relaxed to a min-cut max-flow problem
The transformation works by designing a bipartite graph between queries and documents
All documents are connected to the source by edges with capacity and queries are connected
to the sink with capacity vq Documents retrieved for a query are connected to with
capacity
Figure provides an example of such a maximum-flow minimum-cut graph from source to sink
The conversion to several tiers is slightly more involved Denote by vdi vertices associated with
document and tier and moreover denote by wqi vertices associated with a query and tier Then
the graph is given by edges vdi with capacities edges vdi wqi0 for all document query
pairs and for all i0 endowed with infinite capacity and edges wqi with capacity vq
As with the simple caching problem we need to impose a cut on any query edge for which not all
incoming page edges have been cut The key difference is that in order to benefit from storing pages
in a better tier we need to guarantee that the page is contained in the lower tier too
Variable Reduction
We now simplify the relaxed problem further by reducing the number of variables without
sacrificing integrality of the solution A first step is to substitute yqt maxd?Dq xdt to obtain an
optimization problem over the documents alone
minimize max xdt subject to xdt xdt0 for t0 and xdt
d?Dq
Note that the monotonicity condition yqt yqt0 for t0 is automatically inherited from that of
The solution of is still integral since the problem is equivalent to one with integral solution
Lemma We may scale pt and together by constants such that p0t pt
The resulting solution of this new problem with is unchanged
Essentially problem as parameterized by yields solutions which form equivalence classes
Consequently for the convenience of solving we may assume p0t for We only need to
consider the original for evaluating the objective using solution thus same observed capacities
Ct
Since is a relaxation of this reformulation can be extended to the integer linear program too
Moreover under reasonable conditions on the capacity constraints there is more structure in
Lemma Assume that C?t is monotonically decreasing and that pt for Then any choice
of satisfying the capacity constraints is monotonically non-increasing
Algorithm Tiering Optimization
Initialize all zd
Initialize
for to MAXITER do
for all do
learning rate
increment counter
Update
Project to k]D via
zd min(k zd
end for
end for
Algorithm Deferred updates
Observe current time n0
Read timestamp for document
Compute update steps
repeat
bzd 1c next largest tier
zd change needed to reach next tier
if then
and zd zd partial step we are done
else
and zd zd full step next tier
end if
until no more updates or zd bottom tier
One interpretation of this is that unless the tiers are increasingly inexpensive the optimal solution would assign pages in a fashion yielding empty middle tiers the remaining capacities C?t not
strictly decreasing This monotonicity simplifies the problem Consequently we exploit this fact to
complete the variable reduction
Define for all non-negative by virtue of Lemma and
for
Note that by construction whenever The function is clearly
convex which helps describe our tiering problem via the following convex program
minimize max zd
zd for zd
d?Dq
We now use only one variable per document Moreover the convex constraints are simple box
constraints This simplifies convex projections as needed for online programming
Lemma The solution of is equivalent to that of
Online Algorithm
We now turn our attention to a fast algorithm for minimizing While greatly simplified relative
to it still remains a problem of billions of variables The key observation is that the objective
function of can be written as sum over the following loss functions
lq vq max zd
zd
d?Dq
where denotes the cardinality of the query set The transformation suggests a simple stochastic
gradient descent optimization algorithm traverse the input stream by queries and update the values
of xd of all those documents that would need to move into the next tier in order to reduce service
time for a query Subsequently perform a projection of the page vectors to the set to ensure
that we do not assign pages to non-existent tiers
Algorithm proceeds by processing the input query-result records vq Dq as a stream comprising the set of pages that need to be displayed to answer a given query More specifically it updates
the tier preferences of the pages that have the lowest tier scores for each level and it decrements the
preferences for all other pages We may apply results for online optimization algorithms to show
that a small number of passes through the dataset suffice
Lemma The solution obtained by Algorithm converges at rate log to its minimum
value Here is the number of queries processed
Deferred and Approximate Updates
The naive implementation of algorithm is infeasible as it would require us to update all coordinates of xd for each query However it is possible to defer the updates until we need to inspect zd
directly The key idea is to exploit that for all zd with Dq the updates only depend on the value
of zd at update time Section and that is piecewise linear and monotonically decreasing
Path Following
The tiering problem has the appealing property that the solutions for increasing form a nested
subset In other words relaxing capacity constraints never demotes but only promotes pages This
fact can be used to design specialized solvers which work well at determining the entire solution path
at once for moderate-sized problems Alternatively we can simply take advantage of solutions
for successive values of in determining an approximate solution path by using the solution for
as initialization for This strategy is well known as path-following in numerical optimization
In this context it is undesirable to solve the optimization for a particular value of to optimality
Instead we simply solve it approximately using a small number of passes and readjust Due to
the nesting property and the fact that the optimal solutions are binary via total unimodularity
the average over solutions on the entire path provides an ordering of pages into tiers Thus
Lemma Denote by xd the solution of the two-tier optimization problem for a given value of
Moreover denote by xd the average value over a range of Lagrange
multipliers Then provides an order for sorting documents into tiers for the entire range
In practice1 we only choose a finite number of steps for near-optimal solutions This yields
Algorithm Path Following
Initialize all xdt zd
for each do
Refine variables xdt by Algorithm using a
small number of iterations
end for
Average the variables xdt xdt
Sort the documents with the resulting total scores zd
Fill the ordered documents to tier then tier etc
Experiments show that using synthetic data
where it was feasible to compute and compare with the optimal LP solution pointwise
even values of produce nearoptimal results in the two-tier case Moreover we may carry out the optimization
procedure for several parameters simultaneously This is advantageous since the main
cost is sequential RAM read-write access
rather than CPU speed
Experiments
To examine the efficacy of our algorithm at web-scale we tested it with real data from a major
search engine The results of our proposed methods are compared to those of the max and sum
heuristics in Section A.2. We also performed experiments on small synthetic data 2-tier and
where we were able to show that our algorithm converges to exact solution given by an LP solver
Appendix C). However since LP solvers are very slow it is not feasible for web-scale problems
We processed the logs for one week of September containing results from the top geographic
regions which include a majority of the search engine?s user base To simplify the heavy processing
involved for collecting such a massive data set we only record whether a particular result defined
as a query document pair appears in top first result page for a given session and we aggregate
the view counts of such results which will be used for the session value vq once In its entirety
this subset contains about viewed documents and distinct queries We excluded results
viewed only once yielding a final data set of documents.2 For simplicity our experiments
are carried out for a two-tier single cache system such that the only design parameter is the relative
This result can be readily extended to and any probability measure over a set of Lagrangian values
so long as there are positive weights around the values yielding all the nested solutions
The search results for any fixed query vary for a variety of reasons database updates We approximate
the session graph by treating queries with different result sets as if they were different This does not change
Figure Left Experimental results for real web-search data with pages and
queries Session miss rate for the online procedure the max and sum heuristics The yaxis is normalized such that SUM-tier?s first point is at As seen the max heuristic cannot be
optimal for any but small cache sizes but it performs comparably well to Online Right Online
is outperforming MAX for cache size larger than sometimes more than twofold
size of the prime tier the cache The ranking variant of our online Algorithm passes over the
data consistently outperforms the max and sum heuristics over a large span of cache sizes Figure
Direct comparison can now be made between our online procedure and the max and sum heuristics
since each one induces a ranking on the set of documents We then calculate the session miss rate
of each procedure at any cache size and report the relative improvement of our online algorithm as
ratios of miss rates in Figure 3?Right
The optimizer fits well in a desktop?s RAM since values of only amount to about 2GB of singleprecision We measure a throughput of approximately million query-sessions per second
qps for this version and about million qps for smaller problems as they incur fewer memory
page faults Billion-scale problems can readily fit in of RAM by serializing computation one
value at a time We also implemented a multi-thread version utilizing CPU cores although its
performance did not improve since memory and disk bandwidth limits have already been reached
Discussion
We showed that very large tiering and densest subset optimization problems can be solved efficiently
by a relatively simple online optimization procedure Some extensions are in Appendix B). It came
somewhat as a surprise that the max heuristic often works nearly as well as the optimal tiering
solution Since we experienced this correlation on both synthetic and real data we believe that it
might be possible to prove approximation guarantees for this strategy whenever the bipartite graphs
satisfy certain power-law properties
Some readers may question the need for a static tiering solution given that data could in theory
be reassigned between different caching tiers on the fly The problem is that in production systems
of a search engine such reassignment of large amounts of data may not always be efficient for
operational reasons different versions of the ranking algorithm different versions of the index
different service levels constraints on transfer bandwidth In addition to that tiering is a problem
not restricted to the provision of webpages It occurs in product portfolio optimization and other
resource constrained settings We showed that it is possible to solve such problems at several orders
of magnitude larger scale than what was previously considered feasible
Acknowledgments We thank Kelvin Fong for providing computer facilities NICTA is funded by
the Australian Government as represented by the Department of Broadband Communications and
the Digital Economy and the Australian Research Council through the ICT Centre of Excellence
program This work was carried out while GL and NQ were with Yahoo Labs
the optimization problem and keeps the model accurate Moreover we remove rare results by maintaining that
the lowest count of a document is at least as large as the square root of the highest within the same session

----------------------------------------------------------------

title: 2047-the-intelligent-surfer-probabilistic-combination-of-link-and-content-information-in-pagerank.pdf

The Intelligent Surfer:\n Probabilistic Combination of Link and\n Content Information in PageRank\n Matthew Richardson\n Pedro Domingos\n Department of Computer Science and Engineering\n University of Washington\n Box Seattle WA USA\n mattr pedrod}@cs.washington.edu\n Abstract\n The PageRank algorithm used in the Google search engine greatly\n improves the results of Web search by taking into account the link\n structure of the Web. PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page,\n if it surfed indefinitely from page to page following all outlinks\n from a page with equal probability We propose to improve PageRank by using a more intelligent surfer one that is guided by probabilistic model of the relevance of a page to a query Efficient\n execution of our algorithm at query time is made possible by precomputing at crawl time and thus once for all queries the necessary terms Experiments on two large subsets of the Web indicate\n that our algorithm significantly outperforms PageRank in the human-rated quality of the pages returned while remaining efficient\n enough to be used in today?s large search engines.\n Introduction\n Traditional information retrieval techniques can give poor results on the Web with\n its vast scale and highly variable content quality Recently however it was found\n that Web search results can be much improved by using the information contained the link structure between pages The two best-known algorithms which do this are\n HITS and PageRank The latter is used in the highly successful Google search\n engine The heuristic underlying both of these approaches is that pages with many\n inlinks are more likely to be of high quality than pages with few inlinks given that\n the author of a page will presumably include in it links to pages that s/he believes are\n of high quality Given a query set of words or other query terms HITS invokes traditional search engine to obtain a set of pages relevant to it expands this set with\n its inlinks and outlinks and then attempts to find two types of pages hubs pages\n that point to many pages of high quality and authorities pages of high quality Because this computation is carried out at query time it is not feasible for today?s\n search engines which need to handle tens of millions of queries per day In contrast,\n PageRank computes a single measure of quality for a page at crawl time This meas-\n x0cure is then combined with a traditional information retrieval score at query time.\n Compared with HITS this has the advantage of much greater efficiency but the disadvantage that the PageRank score of a page ignores whether or not the page is relevant to the query at hand.\n Traditional information retrieval measures like TFIDF rate a document highly the query terms occur frequently in it PageRank rates a page highly if it is at the center of a large sub-web if many pages point to it many other pages point those Intuitively however the best pages should be those that are at the center\n of a large sub-web relevant to the query If one issues a query containing the word\n jaguar then pages containing the word jaguar that are also pointed to by many other\n pages containing jaguar are more likely to be good choices than pages that contain\n jaguar but have no inlinks from pages containing it This paper proposes a search\n algorithm that formalizes this intuition while like PageRank doing most of its computations at crawl time The PageRank score of a page can be viewed as the rate which a surfer would visit that page if it surfed the Web indefinitely blindly jumping from page to page Our algorithm does something closer to what a human surfer\n would do jumping preferentially to pages containing the query terms.\n A problem common to both PageRank and HITS is topic drift Because they give the\n same weight to all edges the pages with the most inlinks in the network being considered either at crawl or query time tend to dominate whether or not they are the\n most relevant to the query Chakrabarti and Bharat and Henzinger propose heuristic methods for differentially weighting links Our algorithm can viewed as a more principled approach to the same problem It can also be viewed an analog for PageRank of Cohn and Hofmann?s variation of HITS Rafiei and\n Mendelzon'\n algorithm which biases PageRank towards pages containing a specific word is a predecessor of our work Haveliwala proposes applying an optimized version of PageRank to the subset of pages containing the query terms and\n suggests that users do this on their own machines.\n We first describe PageRank We then introduce our query-dependent contentsensitive version of PageRank and demonstrate how it can be implemented efficiently Finally we present and discuss experimental results.\n PageRank The Random Surfer\n Imagine a web surfer who jumps from web page to web page choosing with uniform\n probability which link to follow at each step In order to reduce the effect of deadends or endless cycles the surfer will occasionally jump to a random page with some\n small probability or when on a page with no out-links To reformulate this graph terms consider the web as a directed graph where nodes represent web pages,\n and edges between nodes represent links between web pages Let be the set nodes Fi be the set of pages page links to and be the set pages which\n link to page For pages which have no outlinks we add a link to all pages in the\n graph1 In this way rank which is lost due to pages with no outlinks is redistributed\n uniformly to all pages If averaged over a sufficient number of steps the probability\n the surfer is on page at some point in time is given by the formula:\n For each page with no outlinks we set Fs={all nodes and for all other nodes augment with x0cThe PageRank score for node is defined as this probability Because\n equation is recursive it must be iteratively evaluated until converges Typically the initial distribution for is uniform PageRank is equivalent to the primary eigenvector of the transition matrix with\n NxN\n ji if there is an edge from to otherwise\n One iteration of equation is equivalent to computing xt+1=Zxt where xjt=P(j iteration After convergence we have or xT=ZxT which means xT is eigenvector of Z. Furthermore since the columns of are normalized has an eigenvalue of Directed Surfer Model\n We propose a more intelligent surfer who probabilistically hops from page to page,\n depending on the content of the pages and the query terms the surfer is looking The resulting probability distribution over pages Pq Pq Pq Pq where Pq(i?j is the probability that the surfer transitions to page given that he on page and is searching for the query specifies where the surfer chooses jump when not following links Pq(j is the resulting probability distribution over\n pages and corresponds to the query-dependent PageRank score QD-PageRankq(j As with PageRank QD-PageRank is determined by iterative evaluation equation from some initial distribution and is equivalent to the primary eigenvector of the transition matrix Zq where ji Pq Pq Although\n and are arbitrary distributions we will focus on the case where both\n probability distributions are derived from a measure of relevance of page query Rq Rq Pq Rq Rq In other words when choosing among multiple out-links from a page the directed\n surfer tends to follow those which lead to pages whose content has been deemed\n relevant to the query according to Rq). Similarly to PageRank when a page?s outlinks all have zero relevance or has no outlinks we add links from that page to all\n other pages in the network On such a page the surfer thus chooses a new page jump to according to the distribution Pq When given a multiple-term query the surfer selects a according some probability distribution and uses that term to guide its behavior according to equation for a large number of steps1 It then selects another term according\n to the distribution to determine its behavior and so on The resulting distribution\n over visited web pages is QD-PageRankQ and is given However many steps are needed to reach convergence of equation x0cQD PageRank PQ Pq For standard PageRank the PageRank vector is equivalent to the primary eigenvector\n of the matrix Z. The vector of single-term QD-PageRankq is again equivalent to the\n primary eigenvector of the matrix Zq. An interesting question that arises is whether\n the QD-PageRankQ vector is equivalent to the primary eigenvector of a matrix\n ZQ corresponding to the combination performed by equation fact this is not the case Instead the primary eigenvector of ZQ corresponds to the\n QD-PageRank obtained by a random surfer who at each step selects a new query\n according to the distribution However QD-PageRankQ is approximately equal\n to the PageRank that results from this single-step surfer for the following reason.\n Let xq be the L2-normalized primary eigenvector for matrix Zq note element of is QD-PageRankq(j thus satisfying xi=Tixi Since xq is the primary eigenvector for\n we have Thus to a first degree of approximation Suppose Consider xQ q)x equation\n Then\n ZQ q?Q q?Q r?Q and thus xQ is approximately an eigenvector for ZQ. Since xQ is equivalent to QDPageRankQ and ZQ describes the behavior of the single-step surfer QD-PageRankQ\n is approximately the same PageRank that would be obtained by using the single-step\n surfer The approximation has the least error when the individual random surfers defined by Zq are very similar or are very dissimilar.\n The choice of relevance function Rq(j is arbitrary In the simplest case Rq(j)=R independent of the query term and the document and QD-PageRank reduces to PageRank One simple content-dependent function could be if the term appears\n on page and otherwise Much more complex functions could be used such as the\n well-known TFIDF information retrieval metric a score obtained by latent semantic\n indexing or any heuristic measure using text size positioning etc It is important\n to note that most current text ranking functions could be easily incorporated into the\n directed surfer model.\n Scalability\n The difficulty with calculating a query dependent PageRank is that a search engine\n cannot perform the computation which can take hours at query time when it is expected to return results in seconds less We surmount this problem by precomputing the individual term rankings QD-PageRankq and combining them at query\n time according to equation We show that the computation and storage requirements for QD-PageRankq for hundreds of thousands of words is only approximately\n times that of a single query independent PageRank.\n Let qm be the set of words in our lexicon That is we assume all\n search queries contain terms in or we are willing to use plain PageRank for those\n terms not in W. Let be the number of documents which contain the term Then\n is the number of unique document-term pairs.\n Disk St ra For each term we must store the results of the computation We add the minor restriction that a search query will only return documents containing all of the terms Thus when merging QD-PageRankq?s we need only to know the QD-PageRankq for\n documents that contain the term Each QD-PageRankq is a vector of values Thus,\n the space required to store all of the PageRanks is a factor of S/N times the query\n independent PageRank alone recall is the number of web pages Further note\n that the storage space is still considerably less than that required for the search engine?s reverse index which must store information about all document-term pairs opposed to our need to store information about every unique document term pair.\n Time Requirement If for some document the directed surfer will never arrive at that page this case we know QD-PageRankq(j)=0 and thus when calculating QD-PageRankq,\n we need only consider the subset of nodes for which We add the reasonable\n constraint that if term does not appear in document which is common for\n most information retrieval relevance metrics such as TFIDF The computation for\n term then only needs to consider dq documents Because it is proportional to the\n number of documents in the graph the computation of QD-PageRankq for all in will require time a factor of S/N times the computation of the query independent PageRank alone Furthermore we have noticed in our experiments that the computation converges in fewer iterations on these smaller sub-graphs empirically reducing the computational requirements to Additional speedup may be derived from the fact that for most words the sub-graph will completely fit in memory,\n unlike PageRank which for any large corpus must repeatedly read the graph structure from disk during computation.\n Empirica Sca la bilit The fraction S/N is critical to determining the scalability of QD-PageRank If every\n document contained vastly different words S/N would be proportional to the number\n of search terms However this is not the case Instead there are a very few words\n that are found in almost every document and many words which are found in very\n few documents2 in both cases the contribution to is small.\n In our database of million pages section we let be the set of all unique\n words and removed the most common words3 This results in million\n words and the ratio S/N was found to be We expect that this ratio will remain\n relatively constant even for much larger sets of web pages This means QDPageRank requires approximately times the storage space and times the\n computation time to allow for arbitrary queries over any of the million words\n which is still less storage space than is required by the search engine?s reverse index\n Google has this feature as well See http://www.google.com/technology/whyuse.html.\n This is because the distribution of words in text tends to follow an inverse power law\n We also verified experimentally that the same holds true for the distribution of the\n number of documents a word is found It is common to remove stop words such as the is etc as they do not affect the\n search.\n x0cTable Results on educrawl\n Query\n Table Results on WebBase\n Query\n chinese association\n computer labs\n financial aid\n intramural\n maternity\n president office\n sororities\n student housing\n visitor visa\n Average\n alcoholism\n architecture\n bicycling\n rock climbing\n shakespeare\n stamp collecting\n vintage car\n Thailand tourism\n Zen Buddhism\n Average\n Results\n We give results on two data sets educrawl and WebBase Educrawl is a crawl of the\n web restricted to edu domains The crawler was seeded with the first 18 results of search for University on Google www.google.com Links containing or cgibin were ignored and links were only followed if they ended with html The\n crawl contains million pages over different domains WebBase is the first\n million pages of the Stanford WebBase repository which contains over million pages For both datasets HTML tags were removed before processing.\n We calculated QD-PageRank as described above using Rq(j the fraction of words\n equal to in page and We compare our algorithm to the standard PageRank algorithm For content ranking we used the same Rq(j function as for QDPageRank but similarly to TFIDF weighted the contribution of each search term the log of its inverse document frequency As there is nothing published about merging PageRank and content rank into one list the approach we follow is to normalize\n the two scores and add them This implicitly assumes that PageRank and content rank\n are equally important This resulted in poor PageRank performance which we found\n was because the distribution of PageRanks is much more skewed than the distribution\n of content ranks normalizing the vectors resulted in PageRank primarily determining\n the final ranking To correct this problem we scaled each vector to have the same\n average value in its top ten terms before adding the two vectors This drastically improved PageRank.\n For educrawl we requested a single word and two double word search queries from\n each of three volunteers resulting in a total of nine queries For each query we randomly mixed the top results from standard PageRank with the top results from\n QD-PageRank and gave them to four volunteers who were asked to rate each search\n result as a not relevant somewhat relevant not very good or good search\n result based on the contents of the page it pointed to In Table we present the final\n rating for each method per query This rating was obtained by first summing the ratings for the ten pages from each method for each volunteer and then averaging the\n individual ratings A similar experiment for WebBase is given in Table For WebBase we randomly selected the queries from Bharat and Henzinger The four\n volunteers for the WebBase evaluation were independent from the four for the\n educrawl evaluation and none knew how the pages they were asked to rate were obtained.\n x0cQD-PageRank performs better than PageRank accomplishing a relative improvement\n in relevance of on educrawl and on WebBase The results are statistically\n significant for educrawl and for WebBase using a two-tailed paired ttest one sample per person per query Averaging over queries every volunteer\n found QD-PageRank to be an improvement over PageRank though not all differences were statistically significant.\n One item to note is that the results on multiple word queries are not as positive as the\n results on single word queries As discussed in section the combination of single\n word QD-PageRanks to calculate the QD-PageRank for a multiple word query is only\n an approximation made for practical reasons This approximation is worse when the\n words are highly dependent Further some queries such as financial aid have different intended meaning as a phrase than simply the two words financial and\n aid For queries such as these the words are highly dependent We could partially\n overcome this difficulty by adding the most common phrases to the lexicon thus\n treating them the same as single words.\n Conclusions\n In this paper we introduced a model that probabilistically combines page content and\n link structure in the form of an intelligent random surfer The model can accommodate essentially any query relevance function in use today and produces higherquality results than PageRank while having time and storage requirements that are\n within reason for today large scale search engines.\n Ackno ledg ment We would like to thank Gary Wesley and Taher Haveliwala for their help with WebBase Frank McSherry for eigen-help and our experiment volunteers for their time.\n This work was partially supported by NSF CAREER and IBM Faculty awards to the\n second author.\n

----------------------------------------------------------------

title: 4861-on-the-complexity-and-approximation-of-binary-evidence-in-lifted-inference.pdf

On the Complexity and Approximation of
Binary Evidence in Lifted Inference
Guy Van den Broeck and Adnan Darwiche
Computer Science Department
University of California Los Angeles
guyvdb,darwiche}@cs.ucla.edu
Abstract
Lifted inference algorithms exploit symmetries in probabilistic models to speed
up inference They show impressive performance when calculating unconditional
probabilities in relational models but often resort to non-lifted inference when
computing conditional probabilities The reason is that conditioning on evidence
breaks many of the model?s symmetries which can preempt standard lifting techniques Recent theoretical results show for example that conditioning on evidence which corresponds to binary relations is P-hard suggesting that no lifting
is to be expected in the worst case In this paper we balance this negative result
by identifying the Boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference In particular we show
that conditioning on binary evidence with bounded Boolean rank is efficient This
opens up the possibility of approximating evidence by a low-rank Boolean matrix
factorization which we investigate both theoretically and empirically
Introduction
Statistical relational models are capable of representing both probabilistic dependencies and relational structure Due to their first-order expressivity they concisely represent probability distributions over a large number of propositional random variables causing inference in these models
to quickly become intractable Lifted inference algorithms attempt to overcome this problem by
exploiting symmetries found in the relational structure of the model
In the absence of evidence exact lifted inference algorithms can work well For large classes of
statistical relational models they perform inference that is polynomial in the number of objects
in the model and are therein exponentially faster than classical inference algorithms When
conditioning a query on a set of evidence literals however these lifted algorithms lose their advantage over classical ones The intuitive reason is that evidence breaks the symmetries in the model
The technical reason is that these algorithms perform an operation called shattering which ends
up reducing the first-order model to a propositional one This issue is implicitly reflected in the
experiment sections of exact lifted inference papers Most report on experiments without evidence
Examples include publications on FOVE and WFOMC Others found ways to efficiently deal with evidence on only unary predicates They perform experiments without evidence
on binary or higher-arity relations There are examples for FOVE WFOMC PTP
and CP
This evidence problem has largely been ignored in the exact lifted inference literature until recently
when Bui and Van den Broeck and Davis showed that conditioning on unary evidence
is tractable More precisely conditioning on unary evidence is polynomial in the size of evidence
This type of evidence expresses attributes of objects in the world but not relations between them
Unfortunately Van den Broeck and Davis also showed that this tractability does not extend to
evidence on binary relations for which conditioning on evidence is P-hard Even if conditioning is
hard in general its complexity should depend on properties of the specific relation that is conditioned
on It is clear that some binary evidence is easy to condition on even if it talks about a large number
of objects for example when all atoms are true or false As
our first main contribution we formalize this intuition and characterize the complexity of conditioning more precisely in terms of the Boolean rank of the evidence We show that it is a measure of
how much lifting is possible and that one can efficiently condition on large amounts of evidence
provided that its Boolean rank is bounded
Despite the limitations useful applications of exact lifted inference were found by sidestepping the
evidence problem For example in lifted generative learning the most challenging task is to
compute partition functions without evidence Regardless the lack of symmetries in real applications is often cited as a reason for rejecting the idea of lifted inference entirely informally called
the death sentence for lifted inference This problem has been avoided for too long and as
lifted inference gains maturity solving it becomes paramount As our second main contribution
we present a first general solution to the evidence problem We propose to approximate evidence
by an over-symmetric matrix and will show that this can be achieved by minimizing Boolean rank
The need for approximating evidence is new and specific to lifted inference in undirected probabilistic graphical models more evidence typically makes inference easier Practically we will show
that existing tools from the data mining community can be used for this low-rank Boolean matrix
factorization task
The evidence problem is less pronounced in the approximate lifted inference literature These algorithms often introduce approximations that lead to symmetries in their computation even when
there are no symmetries in the model Also for approximate methods however the benefits of lifting will decrease with the amount of symmetry-breaking evidence Kersting We
will show experimentally that over-symmetric evidence approximation is also a viable technique for
approximate lifted inference
Encoding Binary Relations in Unary
Our analysis of conditioning is based on a reduction turning evidence on a binary relation into
evidence on several unary predicates We first introduce some necessary background
Background
An atom tn consists of a predicate of arity followed by arguments which are either lowercase constants or uppercase logical variables A literal is an atom a or its negation
A formula combines atoms with logical connectives A formula is ground if it does
not contain any logical variables A possible world assigns a truth value to each ground atom Statistical relational languages define a probability distribution over possible words where ground atoms
are individual random variables Numerous languages have been proposed in recent years and our
analysis will apply to many including MLNs parfactors and WFOMC problems
Example The following MLNs model the dependencies between web pages A first peer-to-peer
model says that student web pages are more likely to link to other student pages
studentpage(X linkto(X studentpage(Y
It increases the probability of a world by a factor ew with every pair of pages that satisfies the
formula A second hierarchical model says that professors are more likely to link to course pages
profpage(X linkto(X coursepage(Y
In this context evidence is a truth-value assignment to a set of ground atoms and is often represented as a conjunction of literals In unary evidence atoms have one argument
studentpage(a while in binary evidence they have two linkto(a Without loss of generality we assume full evidence on certain predicates all their ground atoms are in We will
sometimes represent unary evidence as a Boolean vector and binary evidence as a Boolean matrix
Partial evidence on the relation can be encoded as full evidence on predicates p0 and p1 by adding
formulas p1 and p0 to the model
Example Evidence is represented by
p(X,Y
X=a
X=b
X=c
X=d
a
We will look at computing conditional probabilities Pr(q for single ground atoms Finally we
assume a representation language that can express universally quantified logical constraints
Vector-Product Binary Evidence
Certain binary relations can be represented by a pair of unary predicates By adding the formula
r(Y
to our statistical relational model and conditioning on the and relations we can condition on
certain types of binary relations Assuming that we condition on the and predicates adding
this formula as hard clauses to the model does not change the probability distribution over the
atoms in the original model It is merely an indirect way of conditioning on the relation
If we now represent these unary relations by vectors and and the binary relation by the binary
matrix the above technique allows us to condition on any relation that can be factorized in the
outer vector product
Example Consider the following outer vector factorization of the Boolean matrix P.
In a model containing Formula this factorization indicates that we can condition on the binary
evidence literals of by conditioning on the the
unary literals represented by and
Matrix-Product Binary Evidence
This idea of encoding a binary relation in unary relations can be generalized to pairs of unary
relations by adding the following formula to our model
r1 r2 qn rn
By conditioning on the qi and ri relations we can now condition on a much richer set of binary
relations The relations that can be expressed this way are all the matrices that can be represented
by the sum of outer products Boolean algebra where is and
q1 q2 qn r|n
where the columns of and are the qi and ri vectors respectively and the matrix multiplication
is performed in Boolean algebra that is
Qi,r Rj,r
Example Consider the following its decomposition into a sum/disjunction of outer vector
products and the corresponding Boolean matrix multiplication
This factorization shows that we can condition on the binary evidence literals of Example
by conditioning on the unary literals
q1 q1 q1 q1 r1 r1 r1
q2 q2 q2 r2 r2 r2
q3 q3 q3 q3 r3 r3 r3 r3
Boolean Matrix Factorization
Matrix factorization decomposition is a popular linear algebra tool Some well-known instances
are singular value decomposition and non-negative matrix factorization NMF NMF
factorizes into a product of non-negative matrices which are more easily interpretable and therefore
attracted much attention for unsupervised learning and feature extraction These factorizations all
work with real-valued matrices We instead consider Boolean-valued matrices with only entries
Boolean Rank
Factorizing a matrix as in Boolean algebra is a known problem called Boolean Matrix
Factorization BMF BMF factorizes a matrix into a matrix and a
matrix where potentially and and we always have that min(k
Any Boolean matrix can be factorized this way and the smallest number for which it is possible is
called the Boolean rank of the matrix Unlike textbook real-valued rank computing the Boolean
rank is NP-hard and cannot be approximated unless P=NP The Boolean and real-valued rank
are incomparable and the Boolean rank can be exponentially smaller than the real-valued rank
Example The factorization in Example is a BMF with Boolean rank It is only a decomposition in Boolean algebra and not over the real numbers Indeed the matrix product over the reals
contains an incorrect value of
real
Note that is of full real-valued rank having four non-zero singular values and that its Boolean
rank is lower than its real-valued rank
Approximate Boolean Factorization
Computing Boolean ranks is a theoretical problem Because most real-world matrices will have
nearly full rank almost min(k applications of BMF look at approximate factorizations The
goal is to find a pair of small Boolean matrices Qk?n and Rl?n such that Pk?l Qk?n R|l?n
or more specifically to find matrices that optimize some objective that trades off approximation
error and Boolean rank When and this approximation extracts interesting structure
and removes noise from the matrix This has caused BMF to receive considerable attention in the
data mining community recently as a tool for analyzing high-dimensional data It is used to find
important and interpretable Boolean concepts in a data matrix
Unfortunately the approximate BMF optimization problem is NP-hard as well and inapproximable However several algorithms have been proposed that work well in practice Algorithms exist that find good approximations for fixed values of or when is sparse
BMF is related to other data mining tasks such as biclustering and tiling databases whose
algorithms could also be used for approximate BMF. In the context of social network analysis BMF
is related to stochastic block models and their extensions such as infinite relational models
Complexity of Binary Evidence
Our goal in this section is to provide a new complexity result for reasoning with binary evidence
in the context of lifted inference Our result can be thought of as a parametrized complexity result similar to ones based on treewidth in the case of propositional inference To state the new
result however we must first define formally the computational task We will also review the key
complexity result that is known about this computation now the one we will be improving
Consider an MLN and let contain a set of ground literals representing binary evidence That is
for some binary predicate evidence contains precisely one literal positive or negative
for each grounding of predicate Here represents the number of objects that parameters
and may take.2 Therefore evidence must contain precisely m2 literals
We assume without loss of generality that all logical variables range over the same set of objects
Suppose now that Prm is the distribution induced by MLN over objects and is a ground
literal Our analysis will apply to classes of models that are domain-liftable which means that
the complexity of computing Prm without evidence is polynomial in One such class is the
set of MLNs with two logical variables per formula
Our task is then to compute the posterior probability Prm q|em where em is a conjunction of the
ground literals in binary evidence Moreover our goal here is to characterize the complexity of
this computation as a function of evidence size
The following recent result provides a lower bound on the complexity of this computation
Theorem Suppose that evidence is binary Then there exists a domain-liftable MLN with
a corresponding distribution Prm and a posterior marginal Prm q|em that cannot be computed
by any algorithm whose complexity grows polynomially in evidence size unless
This is an analogue to results according to which for example the complexity of computing posterior probabilities in propositional graphical models is exponential in the worst case Yet for these
models the complexity of inference can be parametrized allowing one to bound the complexity of
inference on some models Perhaps the best example of such a parametrized complexity is the one
based on treewidth which can be thought of as a measure of the model?s sparsity tree-likeness
In this case inference can be shown to be linear in the size of the model and exponential only in its
treewidth Hence this parametrized complexity result allows us to state that inference can be done
efficiently on models with bounded treewidth
We now provide a similar parameterized complexity result but for evidence in lifted inference In
this case the parameter we use to characterize complexity is that of Boolean rank
Theorem Suppose that evidence is binary and has a bounded Boolean rank Then for every
domain-liftable MLN and corresponding distribution Prm the complexity of computing posterior
marginal Prm q|em grows polynomially in evidence size
The proof of this theorem is based on the reduction from binary to unary evidence which is described
in Section In particular our reduction first extends the MLN with Formula leading to the new
MLN and new pairs of unary predicates qi and ri This does not change the domain-liftability
of as Formula is itself liftable We then replace binary evidence by unary evidence That
is the ground literals of the binary predicate are replaced by ground literals of the unary predicates
qi and ri Example This unary evidence is obtained by Boolean matrix factorization As the
matrix size in our reduction is m2 the following Lemma implies that the first step of our reduction
is polynomial in for bounded rank evidence
Lemma Miettinen The complexity of Boolean matrix factorization for matrices with
bounded Boolean rank is polynomial in their size
The main observation in our reduction is that Formula has size which is the Boolean rank of the
given binary evidence Hence when the Boolean rank is bounded by a constant the size of the
extended MLN is independent of the evidence size and is proportional to the size of the original
MLN
We have now reduced inference on MLN and binary evidence into inference on an extended
MLN and unary evidence The second observation behind the proof is the following
Lemma Van den Broeck and Davis Van den Broeck Suppose that evidence is
unary Then for every domain-liftable MLN and corresponding distribution Prm the complexity
of computing posterior marginal Prm q|em grows polynomially in evidence size
Hence computing posterior probabilities can be done in time which is polynomial in the size of
unary evidence which completes our proof
We can now identify additional similarities between treewidth and Boolean rank Exact inference algorithms for probabilistic graphical models typically perform two steps namely to compute a tree
decomposition of the graphical model a corresponding variable order and perform inference
that is polynomial in the size of the decomposition but potentially exponential in its tree)width The
analogous steps for conditioning are to perform a BMF and perform inference that is polynomial in the size of the BMF but potentially exponential in its rank The steps are both NP-hard
yet are efficient assuming bounded treewidth or bounded Boolean rank Lemma Whereas
treewidth is a measure of tree-likeness and sparsity of the graphical model Boolean rank seems to
be a fundamentally different property more related to the presence of symmetries in evidence
Over-Symmetric Evidence Approximation
Theorem opens up many new possibilities Even for evidence with high Boolean rank it is possible
to find a low-rank approximate BMF of the evidence as is commonly done for other data mining
and machine learning problems Algorithms already exist for solving this task Section
Example The evidence matrix from Example has Boolean rank three Dropping the third pair
of vectors reduces the Boolean rank to two
@
@
@
0@
@
This factorization is approximate as it flips the evidence for atom from true to false represented by the bold By paying this price the evidence has more symmetries and we can condition
on the binary relation by introducing only two instead of three new pairs qi ri of unary predicates
Low-rank approximate BMF is an instance of a more general idea that of over-symmetric evidence
approximation This means that when we want to compute Pr(q we approximate it by computing Pr(q e0 instead with evidence e0 that permits more efficient inference In this case it is more
efficient because it maintains more symmetries of the model and permits more lifting Because all
lifted inference algorithms exact or approximate exploit symmetries we expect this general idea
and low-rank approximate BMF in particular to improve the performance of any lifted inference
algorithm
Having a small amount of incorrect evidence in the approximation need not be a problem As these
literals are not covered by the first most important vector pairs they can be considered as noise in
the original matrix Hence a low-rank approximation may actually improve the performance of for
example a lifted collective classification algorithm On the other hand the approximation made in
Example may not be desirable if we are querying attributes of the constant and we may prefer
to approximate other areas of the evidence matrix instead There are many challenges in finding
appropriate evidence approximations which makes the task all the more interesting
Empirical Evaluation
To complement the theoretical analysis from the previous sections we will now report on experiments that investigate the following practical questions
Q1 How well can we approximate a real-world relational data set by a low-rank Boolean matrix
Q2 Is Boolean rank a good indicator of the complexity of inference as suggested by Theorem
Q3 Is over-symmetric evidence approximation a viable technique for approximate lifted inference
To answer we compute approximations of the linkto binary relation in the WebKB data set
using the ASSO algorithm for approximate BMF The WebKB data set consists of web pages
from the computer science departments of four universities The data has information about
words that appear on pages labels of pages and links between web pages linkto relation There
are four folds one for each university The exact evidence matrix for the linkto relation ranges in
size from by to by Its real-valued rank ranges from to Performing a
BMF approximation in this domain adds or removes hyperlinks between web pages so that more
web pages can be grouped together that behave similarly
Figure plots the approximation error for increasing Boolean ranks measured as the number of
incorrect evidence literals The error goes down quickly for low rank and is reduced by half after
Boolean rank 70 to even though the matrix dimensions and real-valued rank are much higher
Note that these evidence matrices contain around a million entries and are sparse Hence these
approximations correctly label to of the atoms
Error
Rank
cornell
texas
washington
wisconsin
Boolean rank
Circuit Size
24
Figure First-order NNF circuit size number
of nodes for increasing Boolean rank and
the peer to peer and hierarchical model
Figure Approximation BMF error in terms
of the number of incorrect literals for the
WebKB linkto relation
Relative KLD
Relative KLD
Circuit Size
18
58
Iteration
Texas Data Set
Iteration
Wisconsin Data Set
Figure KLD of LMCMC on different BMF approximations relative to the KLD of vanilla MCMC
on the same approximation From top to bottom the lines represent exact evidence blue and
approximations red of rank 75 and
To answer we perform two sets of experiments Firstly we look at exact lifted inference and
investigate the influence of adding Formula to the peer-to-peer and hierarchical MLNs from
Example The goals is to condition on linkto relations with increasing rank These models
are compiled using the WFOMC algorithm into first-order NNF circuits which allow for exact
domain-lifted inference Lemma Table shows the sizes of these circuits As expected
circuit sizes grow exponentially with Evidence breaks more symmetries in the peer-to-peer model
than in the hierarchical model causing the circuit size to increase more quickly with Boolean rank
Since the connection between rank and exact inference is obvious from Theorem the more
interesting question in Q2 is whether Boolean rank is indicative of the complexity of approximate lifted inference as well Therefore we investigate its influence on the Lifted MCMC algorithm LMCMC with Rao-Blackwellized probability estimation LMCMC interleaves
standard MCMC steps here Gibbs sampling with jumps to states that are symmetric in the graphical model in order to speed up mixing of the chain We run LMCMC on the WebKB MLN of Davis
and Domingos which has first-order formulas and over million random variables It
classifies web pages into categories based on their link structure and the most predictive words
they contain We learn its parameters with the Alchemy package and obtain evidence sets of varying
Boolean rank from the factorizations of Figure For these we run both vanilla and lifted MCMC
and measure the KL divergence KLD between the marginal distribution at each iteration4 and a
ground truth obtained from million iterations on the corresponding evidence set Figure plots the
KLD of LMCMC divided by the KLD of MCMC It shows that the improvement of LMCMC over
MCMC goes down with Boolean rank answering Q2 positively
To answer we look at the KLD between different evidence approximations Pr(. e0n of rank
and the true marginals Pr(. conditioned on exact evidence As this requires a good estimate
of Pr(. we make our learned WebKB model more tractable by removing formulas about word
content For two approximations e0a and e0b such that rank a we expect LMCMC to converge
faster to Pr(. e0a than to Pr(. e0b as suggested by Figure However because Pr(. e0a is a more
crude approximation of Pr(. than Pr(. e0b is the KLD at convergence should be worse for a
When synthetically generating evidence of these ranks results are comparable
Runtime per iteration is comparable for both algorithms BMF runtime is negligible
Ground MCMC
Lifted MCMC
Lifted MCMC Rank
Lifted MCMC Rank
KL Divergence
KL Divergence
Iteration
Ground MCMC
Lifted MCMC
Lifted MCMC Rank
Lifted MCMC Rank
Cornell Ranks and
Iteration
Cornell Ranks 75 and
Ground MCMC
Lifted MCMC
Lifted MCMC Rank
Lifted MCMC Rank
KL Divergence
KL Divergence
Iteration
Ground MCMC
Lifted MCMC
Lifted MCMC Rank
Lifted MCMC Rank
Washington Ranks 75 and
Iteration
Wisconsin Ranks 75 and
Figure Error for different low-rank approximations of WebKB in KLD from true marginals
than for Hence we expect to see a trade-off where the lowest ranks are optimal in the beginning
higher ranks become optimal later one and the exact model is optimal at convergence
Figure shows exactly that for a representative sample of ranks and data sets In Figure rank
and outperform LMCMC with the exact evidence at first Exact evidence overtakes rank
after iterations and rank after After iterations even non-lifted MCMC outperforms
these crude approximations Figure shows the other side of the spectrum where a rank 75
and approximation are overtaken at iterations and Figure is representative of
other datasets Note here that at around iteration rank 75 in turn outperforms the rank
approximation which has fewer symmetries and does not permit as much lifting Finally Figure
shows the ideal case for low-rank approximation This is the largest dataset and therefore the most
challenging inference task Here LMCMC on converges slowly compared to its approximations e0
and e0 results in almost perfect marginals The crossover point where exact inference outperforms
the approximation is never reached in practice This answers Q3 positively
Conclusions
We presented two main results The first is a more precise complexity characterization of conditioning on binary evidence in terms of its Boolean rank The second is a technique to approximate
binary evidence by a low-rank Boolean matrix factorization This is a first type of over-symmetric
evidence approximation that can speed up lifted inference We showed empirically that low-rank
BMF speeds up approximate inference leading to improved approximations
For future work we want to evaluate the practical implications of the theory developed for other
lifted inference algorithms such as lifted BP and look at the performance of over-symmetric evidence approximation on machine learning tasks such as collective classification There are many
remaining challenges in finding good evidence-approximation schemes including ones that are
query-specific de Salvo Braz or that incrementally run inference to find better approximations Kersting Furthermore we want to investigate other subsets of binary
relations for which conditioning could be efficient in particular functional relations where
each has at most a limited number of associated values
Acknowledgments
We thank Pauli Miettinen Mathias Niepert and Jilles Vreeken for helpful suggestions This work
was supported by ONR grant NSF grant NSF grant and the Research Foundation-Flanders FWO-Vlaanderen

----------------------------------------------------------------

title: 3860-filtering-abstract-senses-from-image-search-results.pdf

Filtering Abstract Senses From Image Search Results
Kate Saenko1,2 and Trevor Darrell2
MIT CSAIL Cambridge MA
UC Berkeley EECS and ICSI Berkeley CA
saenko@csail.mit.edu trevor@eecs.berkeley.edu
Abstract
We propose an unsupervised method that given a word automatically selects
non-abstract senses of that word from an online ontology and generates images
depicting the corresponding entities When faced with the task of learning a visual model based only on the name of an object a common approach is to find
images on the web that are associated with the object name and train a visual classifier from the search result As words are generally polysemous this approach
can lead to relatively noisy models if many examples due to outlier senses are
added to the model We argue that images associated with an abstract word sense
should be excluded when training a visual classifier to learn a model of a physical
object While image clustering can group together visually coherent sets of returned images it can be difficult to distinguish whether an image cluster relates to
a desired object or to an abstract sense of the word We propose a method that uses
both image features and the text associated with the images to relate latent topics
to particular senses Our model does not require any human supervision and
takes as input only the name of an object category We show results of retrieving
concrete-sense images in two available multimodal multi-sense databases as well
as experiment with object classifiers trained on concrete-sense images returned by
our method for a set of ten common office objects
Introduction
Many practical scenarios call for robots or agents which can learn a visual model on the fly given
only a spoken or textual definition of an object category A prominent example is the Semantic
Robot Vision Challenge SRVC)1 which provides robot entrants with a text-file list of categories to
be detected shortly before the competition begins More generally we would like a robot or agent
to be able to engage in situated dialog with a human user and to understand what objects the user
is refering to It is generally unreasonable to expect users to refer only to objects covered by static
manually annotated image databases We therefore need a way to find images for an arbitrary object
in an unsupervised manner
A common approach to learning a visual model based solely on the name of an object is to find
images on the web that co-occur with the object name by using popular web search services and
train a visual classifier from the search results As words are generally polysemous mouse
and are often used in different contexts mouse pad this approach can lead to relatively noisy
models Early methods used manual intervention to identify clusters corresponding to the desired
sense or grouped together visually coherent sets of images using automatic image clustering
However image clusters rarely exactly align with object senses because of the large
variation in appearance within most categories Also clutter from abstract senses of the word that
http://www.semantic-robot-vision-challenge.org
Input Word cup
Object Sense drink container
cup
WISDOM
Online Dictionary
cup
search
Object Sense trophy
cup?(a?small?open?container usually?used?for?drinking;?usually?has?a?handle)?"he
put?the?cup?back?in?the?saucer";?"the?handle?of?the?cup?was?missing
Abstract Sense sporting event
cup,?loving?cup a?large?metal?vessel?with?two?handles?that?is?awarded?as?a?trophy
to?the?winner?of?a?competition)?"the?school?kept?the?cups?is?a?special?glass?case
cup a?contest in?which?a?cup?is?awarded)??the?World?Cup?is?the
world's?most?widely?watched?sporting?event
Figure
WISDOM
separates the concrete physical senses from the abstract ones
are not associated with a physical object can further complicate matters mouse as in a timid
person
To address these issues we propose an unsupervised Web Image Sense DisambiguatiOn Model
WISDOM illustrated in Figure Given a word WISDOM automatically selects concrete senses
of that word from a semantic dictionary and generates images depicting the corresponding entities
first finding coherent topics in both text and image domains and then grounding the learned topics
using the selected word senses Images corresponding to different visual manifestations of a single
physical sense are linked together based on the likelihood of their image content and surrounding
text words in close proximity to the image link being associated with the given sense
We make use of a well-known semantic dictionary WordNet which has been previously used
together with a text-only latent topic model to construct a probabilistic model of individual word
senses for use with online images We build on this work by incorporating a visual term and
by using the Wordnet semantic hierarchy to automatically infer whether a particular sense describes
a physical entity or a non-physical concept We show results of detecting such concrete senses in
two available multimodal text and image multi-sense databases the MIT-ISD dataset and
the UIUC-ISD dataset We also experiment with object classification in novel images using
classifiers trained on the images collected via our method for a set of ten common objects
Related Work
Several approaches to building object models from image search results have been proposed Some
have relied on visual similarity either selecting a single inlier image cluster based on a small validation set or bootstrapping object classifiers from existing labeled images In a classifier
based on text features such as whether the keyword appears in the URL was used re-rank the images before bootstrapping the image model However the text ranker was category-independent and
thus unable to learn words predictive of a specific word sense An approach most similar to ours
discovered topics in the textual context of images using Latent Dirichlet Allocation however
manual intervention by the user was required to sort the topics into positive and negative for each
category Also the combination of image and text features is used in some web retrieval methods
however our work is focused not on instance-based image retrieval but on category-level
modeling
Two recent papers have specifically addressed polysemous words In Saenko and Darrell the
use of dictionary definitions to train an unsupervised visual sense model was proposed However the
user was required to manually select the definition for which to build the model Furthermore the
While the first few pages of image search results returned by modern search engines generally have very
few abstract examples possibly due to the sucess of reranking based on previous user?s click-through history
results from farther down the list are much less uniform as our experimental results show
sense model did not incorporate visual features but rather used the text contexts to re-rank images
after which an image classifier was built on the top-ranked results Loeff performed
spectral clustering in both the text and image domain and evaluated how well the clusters matched
different senses However as a pure clustering approach this method cannot assign sense labels
In the text domain Yarowsky proposed an unsupervised method for traditional word sense
disambiguation and suggested the use of dictionary definitions as an initial seed Also Boiy
determined which words are related to a visual domain using hypothesis testing on a target
visual corpus compared to a general non-visual corpus
A related problem is modeling word senses in images manually annotated with words such as the
caption sky airplane Models of annotated images assume that there is a correspondence
between each image region and a word in the caption Corr-LDA Such models predict
words which serve as category labels based on image content In contrast our model predicts a
category label based on all of the words in the web image?s text context where a particular word
does not necessarily have a corresponding image region and vice versa In work closely related to
Corr-LDA a People-LDA model is used to guide topic formation in news photos and captions
using a specialized face recognizer The caption data is less constrained than annotations including
non-category words but still far more constrained than generic webpage text
Sense-Grounding with a Dictionary Model
We wish to estimate the probability that an image search result embedded in a web page is one of
a concrete or abstract concept First we determine whether the web image is related to a particular
word sense as defined by a dictionary The dictionary model presented in provides an estimate
of word sense based on the text associated with the web image We will first describe this model
and then extend it to include both an image component and an adaptation step to better reflect word
senses present in images
The dictionary model uses LDA on a large collection of text related to the query word to learn
latent senses/uses of the word LDA discovers hidden topics distributions over discrete
observations such as words in the data Each document is modeled as a mixture of topics
K}. A given collection of documents each containing a bag of Nd words is assumed
to be generated by the following process First we sample the parameters of a multinomial
distribution over words from a Dirichlet prior with parameter for each topic K. For
each document we sample the parameters of a multinomial distribution over topics from a
Dirichlet prior with parameter Finally for each word token we choose a topic zi from the
multinomial and then choose a word from the multinomial zi
Since learning LDA topics directly from the images text contexts can lead to poor results due to
the low quantity and irregular quality of such data an additional dataset of text-only web pages
is created for learning using regular web search The dictionary model then uses the limited text
available in the WordNet entries to relate dictionary sense to latent text topics For example sense
of bass contains the definition the lowest part of the musical range as well as the hypernym
pitch and other semantic relations The bag-of-words extracted from such a semantic entry for
sense is denoted by the variable es e2 eEs where Es is the total number
of words The dictionary model assumes that the sense is independent of the words conditioned on
the distribution of topics in the document For a web image with an associated text document dt the
conditional probability of sense is given by
s|dt
j)P j|dt
where the distribution of latent topics in the text context z|dt is given by the dt variable
computed by generalizing the learned LDA model to the unseen text contexts The likelihood of
a sense given latent topic is defined as the normalized average likelihood of words in the
dictionary entry es
Es
ei
Es
Incorporating Image Features The dictionary model does not take into account the image part
of the image/text pair Here we extend it to include an image term which can potentially provide
complementary information First we estimate s|di or the probability of a sense given an image
di Similar to the text-only case we learn an LDA model consisting of latent topics
using the visual bag-of-words extracted from the unlabeled images The estimated variables give
v|di To compute the conditional probability of a sense given a visual topic we marginalize the
joint across all image and associated text documents di dt in the collection
s|dt k)P v|di
Note that the above assumes conditional independence of the sense and the visual topic given the
observations Intuitively this provides us with an estimate of the collocation of senses with visual
topic We can now compute the probability of dictionary sense for a novel image di as
j)P j|di
Finally the joint text and image model is defined as the combination of the text-space and imagespace models via the sum rule
s|di dt s|di s|dt
Our assumption in using the sum rule is that the combination can be modelled as a mixture of
experts where the features of one modality are independent of sense given the other modality
Adaptation Recall that we can estimate dt for the unseen web image contexts by generalizing the
web-text LDA model using Gibbs sampling However web topics can be a poor match to image
search data the genome research topic of mouse Our solution is to adapt the web topics to
the image search data We do this by fixing the assignments of the web documents and sampling
the z?s of the image contexts for a few iterations This procedure updates the topics to better reflect
the latent dimensions present in the image search data without the overfitting effect mentioned
earlier
Filtering out Abstract Senses
To our knowledge no previous work has considered the task of detecting concrete abstract senses
in general web images We can do so by virtue of the multimodal sense grounding method presented
in the previous section Given a set of senses for a paricular word our task is to classify each sense
as being abstract or concrete Fortunately WordNet contains relatively direct metadata related to
the physicality of a word sense In particular one of the main functions of WordNet is to put words
in semantic relation to each other using the concepts of hyponym and hypernym For example
scarlet and crimson are hyponyms of while color is a hypernym of One can
follow the chain of direct hypernyms all the way to the top of the tree entity Thus we can detect
a concrete sense by examining its hypernym tree to see if it contains one of the following nodes
article instrumentality?,?article of clothing animal or body part What?s more we can thus
restrict the model to specific types of physical entities living things artifacts clothing etc
In addition WordNet contains lexical file information for each sense marking it as a state or an animal etc For example the sense mouse computer mouse is marked artifact In this paper we
classify a WordNet sense as being due to a concrete object when the lexical tag is one of animal
artifact body plant and We exclude people and proper nouns in the experiments
in this paper as well as prune away infrequent senses
The average word likelihood was found to be a good indicator of how relevant a topic is to a sense The
total word likelihood could be used but it would allow senses with longer entries to dominate
Data
We evaluated the outlined algorithms on three datasets the five-word MIT-ISD dataset the
three-word UIUC-ISD dataset and OFFICE dataset of ten common office objects that we collected for the classification experiment All datasets had been collected automatically by issuing
queries to the Yahoo Image SearchTM engine and downloading the returned images and corresponding HTML web pages For the MIT-ISD dataset the query terms used were BASS FACE MOUSE
SPEAKER and WATCH For the UIUC-ISD dataset three basic query terms were used BASS
CRANE and SQUASH To increase corpus size the authors also used supplemental query terms
for each word The search terms selected were those related to the concrete senses construction cranes whooping crane etc Since these human-selected search terms require human
input while our method only requires a list of words we exclude them from our experiments The
OFFICE dataset queries were CELLPHONE FORK HAMMER KEYBOARD MUG PLIERS
SCISSORS STAPLER TELEPHONE WATCH
The images were labeled by a human annotator with all concrete senses for each word The annotator
saw only the images and not the surrounding text or any dictionary definitions For the MIT-ISD
dataset each concrete sense was labeled as core related and unrelated Images where the object
was too small or too occluded were labeled as related For the UIUC-ISD dataset the labels for
each concrete sense were similarly core related and unrelated In addition a people label was
used for unrelated images depicting faces or a crowd The OFFICE dataset was only labeled with
core and unrelated labels We evaluated our models on two retrieval tasks retrieval of only core
images of each sense and retrieval of both core and related images In the former case core labels
were used as positive labels for each sense with related unrelated and people images labeled as
negative In the latter case core and related images were labeled as positive and unrelated and
people as negative Note that the labels were only used in testing and not in training
To provide training data for the web text topic model we also collected an unlabeled corpus of textonly webpages for each word These additional webpages were collected via regular web search for
the single-word search term CRANE and were not labeled
Features
When extracting words from web pages all HTML tags are removed and the remaining text is
tokenized A standard stop-word list of common English words plus a few domain-specific words
like is applied followed by a Porter stemmer Words that appear only once and the
actual word used as the query are pruned To extract text context words for an image the image
link is located automatically in the corresponding HTML page All word tokens in a 100-token
window surrounding the location of the image link are extracted The text vocabulary size used for
the dictionary model ranges between words for the different search words
To extract image features all images are resized to pixels in width and converted to grayscale
Two types of local feature points are detected in the image edge features and scale-invariant
salient points To detect edge points we first perform Canny edge detection and then sample a fixed
number of points along the edges from a distribution proportional to edge strength The scales of the
local regions around points are sampled uniformly from the range of pixels To detect scaleinvariant salient points we use the Harris-Laplace detector with the lowest strength threshold
set to Altogether edge points and approximately the same number of Harris-Laplace points
are detected per image A 128-dimensional SIFT descriptor is used to describe the patch surrounding each interest point After extracting a bag of interest point descriptors for each image vector
quantization is performed A codebook of size is constructed by k-means clustering a randomly
chosen subset of the database images per keyword and all images are converted to bags of the
resulting visual words cluster centers of the codebook No spatial information is included in the
image representation rather it is treated as a bag-of-words
The MIT-ISD and OFFICE datasets are available at http://people.csail.mit.edu/saenko
The UIUC-ISD dataset and its complete description can be obtained at
http://visionpc.cs.uiuc.edu/isd/index.html
Text Model
Image Model
Figure The top images returned by the text and the image models for mouse-4 device
Retrieval Experiments
In this section we evaluate WISDOM on the task of retrieving concrete sense images from search
results Below are the actual concrete senses that were automatically selected from WordNet by our
model for each word in the datasets
MIT-ISD bass-7 instrument bass-8 fish face-1 human face face-13 surface mouse-1 rodent mouse-4 device speaker-2 loudspeaker watch-1 timepiece
UIUC-ISD bass-7 instrument bass-8 fish crane-4 machine crane-5 bird squash-1 plant
squash-3 game
OFFICE cellphone-1 mobile phone fork-1 utensil hammer-2 hand tool keyboard-1 any
keyboard mug-1 drinking vessel mug-1 drinking vessel pliers-1 tool scissors-1
cutting tool stapler-1 stapling device telephone-1 landline phone watch-1 timepiece
We train a separate web text LDA model and a separate image LDA model for each word in the
dataset The number of topics is a parameter to the model that represents the dimensionality of
the latent space used by the model We set for all LDA models in the following experiments
This was done so that the number of latent text topics is roughly equal to the number of senses In the
image domain it is less clear what the number of topics should be Ideally each topic would coincide
with a visually coherent class of images all belonging to the same sense In practice because images
of an object class on the web are extremely varied multiple visual clusters are needed to encompass
a single visual category Our experiments have shown that the model is relatively insensitive to
values of this parameter in the range of To perform inference in LDA we used the Gibbs
sampling approach of implemented in the Matlab Topic Modeling Toolbox We used
symmetric Dirichlet priors with scalar hyperparameters and which have the
effect of smoothing the empirical topic distribution and iterations of Gibbs sampling
Figure shows the images that were assigned the highest probability for mouse-4 computer device
sense by the text-only model s|dt Figure and by the image-only model s|di Figure
Both models return high-precision results but somewhat different and complementary images As we expected the image model?s results are more visually coherent while the text model?s
results are more visually varied
Next we evaluate retrieval of individual senses using the multimodal model with
and compare it to the Yahoo search engine baseline This is somewhat unfair to the baseline as here
we assume that our model knows which sense to retrieve we will remove this assumption later
The recall-precision curves RPCs are shown in Figure The figure shows the RPCs for each
word in the MIT-ISD top row and UIUC-ISD bottom row datasets computed by thresholding
s|di dt WISDOM?s RPCs are shown as the green curves The blue curves are the RPCs obtained
by the original Yahoo image search retrieval order For example the top leftmost plot shows retrieval
of bass-7 musical instrument These results demonstrate that we are able to greatly improve the
retrieval of each concrete sense compared to the search engine
MIT-ISD data
UIUC-ISD data
Figure Recall-precision of each concrete sense core labels using the multimodal dictionary
model green and the search engine blue evaluated on two datasets
Core Senses MIT-ISD
Core Senses UIUC-ISD
Core+Related Senses MIT-ISD
Core+Related Senses UIUC-ISD
Figure Recall-precision of all concrete senses using WISDOM green and the search engine blue
WISDOM does fail to retrieve one sense defined as a vertical surface This is a highly
ambiguous sense visually although it has an artifact lexical tag One possibility for the future
is to exclude senses that are descendents of surface as being too ambiguous Also preliminary
investigation indicates that weighting the text and image components of the model differently can
result in improved results model weighting is therefore an important topic for future work
Next we evaluate the ability of WISDOM to filter out abstract senses Here we no longer assume that
the correct senses are known Figure shows the result of filtering out the abstract senses which is
done by evaluating the probability of any of the concrete senses in a given search result The ground
truth labels used to compute these RPCs are positive if an image was labeled either with any core
sense or with any core or related sense and negative otherwise These
results demonstrate that our model improves the retrieval of images of concrete physical senses
of words without any user input except for the word itself Figure shows how the model filters out
certain images including illustrations by an artist named Crane from search results for CRANE
Classification Experiments
We have shown that our method can improve retrieval of concrete senses therefore providing higherprecision image training data for object recognition algorithms We have conjectured that this leads
to better classification results in this section we provide some initial experiments to support this
claim We collected a dataset of ten office objects and trained ten-way SVM classifiers using
the vocabulary-guided pyramid match kernel over bags of local SIFT features implemented in the
LIBPMK library The training data for the SVM was either the first images returned from
the search engine or the top images ranked by our model Since we?re interested in objects we
keep only the artifact senses that descend from instrumentality or article Figure shows
classification results on held-out test data averaged over runs on random subsets of the
Yahoo Image Search
WISDOM
Figure The top images returned by the search engine for CRANE compared to our model
Figure Classification accuracy of ten objects in the OFFICE dataset
data Our method improves accuracy for most of the objects in particular classification of mug
improves greatly due to the non-object senses being filtered out This is a very difficult task as
evidenced by the baseline performance the average baseline accuracy is Training with our
method achieves accuracy a relative improvement We believe that this relative improvement is due to the higher precision of the training images and will persist even if the overall accuracy
were improved due to a better classifier
Conclusion
We presented WISDOM an architecture for clustering image search results for polysemous words
based on image and text co-occurrences and grounding latent topics according to dictionary word
senses Our method distinguishes which senses are abstract from those that are concrete allowing
it to filter out the abstract senses when constructing a classifier for a particular object of interest to
a situated agent This can be of particular utility to a mobile robot faced with the task of learning
a visual model based only on the name of an object provided on a target list or spoken by a human
user Our method uses both image features and the text associated with the images to relate estimated
latent topics to particular senses in a semantic database WISDOM does not require any human
supervision and takes as input only an English noun It estimates the probability that a search result
is associated with an abstract word sense rather than a sense that is tied to a physical object We
have carried out experiments with image and text-based models to form estimates of abstract
concrete senses and have shown results detecting concrete-sense images in two multimodal multisense databases We also demonstrated a relative improvement in accuracy when classifiers are
trained with our method as opposed to the raw search results
Acknowledgments
This work was supported in part by DARPA Google and NSF grants and

----------------------------------------------------------------

