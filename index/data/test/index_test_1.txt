query sentence: weakest link in subnetworks
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 3491-measures-of-clustering-quality-a-working-set-of-axioms-for-clustering.pdf

Measures of Clustering Quality A Working Set of
Axioms for Clustering
Margareta Ackerman and Shai Ben-David
School of Computer Science
University of Waterloo Canada
Abstract
Aiming towards the development of a general clustering theory we discuss abstract axiomatization for clustering In this respect we follow up on the work of
Kleinberg that showed an impossibility result for such axiomatization We
argue that an impossibility result is not an inherent feature of clustering but rather
to a large extent it is an artifact of the specific formalism used in
As opposed to previous work focusing on clustering functions we propose to
address clustering quality measures as the object to be axiomatized We show that
principles like those formulated in Kleinberg?s axioms can be readily expressed in
the latter framework without leading to inconsistency
A clustering-quality measure CQM is a function that given a data set and its partition into clusters returns a non-negative real number representing how strong or
conclusive the clustering is We analyze what clustering-quality measures should
look like and introduce a set of requirements axioms for such measures Our
axioms capture the principles expressed by Kleinberg?s axioms while retaining
consistency
We propose several natural clustering quality measures all satisfying the proposed
axioms In addition we analyze the computational complexity of evaluating the
quality of a given clustering and show that for the proposed CQMs it can be
computed in polynomial time
Introduction
In his highly influential paper Kleinberg advocates the development of a theory of clustering that
will be independent of any particular algorithm objective function or generative data model As a
step in that direction Kleinberg sets up a set of axioms aimed to define what a clustering function
is Kleinberg suggests three axioms each sounding plausible and shows that these seemingly natural
axioms lead to a contradiction there exists no function that satisfies all three requirements
Kleinberg?s result is often interpreted as stating the impossibility of defining what clustering is or
even of developing a general theory of clustering We disagree with this view In this paper we show
that the impossibility result is to a large extent due to the specific formalism used by Kleinberg
rather than being an inherent feature of clustering
Rather than attempting to define what a clustering function is and demonstrating a failed attempt
as does we turn our attention to the closely related issue of evaluating the quality of a given
data clustering In this paper we develop a formalism and a consistent axiomatization of that latter
notion
As it turns out the clustering-quality framework is richer and more flexible than that of clustering
functions In particular it allows the postulation of axioms that capture the features that Kleinberg?s
axioms aim to express without leading to a contradiction
A clustering-quality measure is a function that maps pairs of the form dataset clustering to
some ordered set say the set of non-negative real numbers so that these values reflect how good
or cogent that clustering is
Measures for the quality of a clusterings are of interest not only as a vehicle for axiomatizing clustering The need to measure the quality of a given data clustering arises naturally in many clustering
issues The aim of clustering is to uncover meaningful groups in data However not any arbitrary
partitioning of a given data set reflects such a structure Upon obtaining a clustering usually via
some algorithm a user needs to determine whether this clustering is sufficiently meaningful to rely
upon for further data mining analysis or practical applications Clustering-quality measures CQMs
aim to answer that need by quantifying how good is any specific clustering
Clustering-quality measures may also be used to help in clustering model-selection by comparing
different clusterings over the same data set comparing the results of a given clustering paradigm
over different choices of clustering parameters such as the number of clusters
When posed with the problem of finding a clustering-quality measure a first attempt may be to
invoke the loss objective function used by a clustering algorithm such as k-means or k-median
as a clustering-quality measure However such measures have some shortcomings for the purpose
at hand Namely these measures are usually not scale-invariant and they cannot be used to compare
the quality of clusterings obtained by different algorithms aiming to minimize different clustering
costs k-means with different values of See Section for more details
Clustering quality has been previously discussed in the applied statistics literature where a variety
of techniques for evaluating cluster validity were proposed Many of these methods such as the
external criteria discussed in are based on assuming some predetermined data generative model
and as such do not answer our quest for a general theory of clustering In this work we are concerned
with quality measures regardless of any specific generative model for examples see the internal
criteria surveyed in
We formulate a theoretical basis for clustering-quality evaluations We propose a set of requirements axioms of clustering-quality measures We demonstrate the relevance and consistency of
these axioms by showing that several natural notions satisfy these requirements In particular we
introduce quality-measures that reflect the underlying intuition of center-based and linkage-based
clustering These notions all satisfy our axioms and given a data clustering their value on that
clustering can be computed in polynomial time
Paper outline we begin by presenting Kleinberg?s axioms for clustering functions and discuss their
failure In Section we show how these axioms can be translated into axioms pertaining clustering
quality measures and prove that the resulting set of axioms is consistent In Section we discuss
desired properties of an axiomatization and propose an accordingly revised set of axioms Next in
Section we present several clustering-quality measures and claim that they all satisfy our axioms
Finally in Section we show that the quality of a clustering can be computed in polynomial time
with respect to our proposed clustering-quality measures
Definitions and Notation
Let be some domain set usually finite A function is a distance function over
if d(xi for all for any d(xi if and only if and
d(xi d(xj otherwise Note that we do not require the triangle inequality
A k-clustering of is a k-partition C2 Ck That is Ci Cj for and
Ci A clustering of is a k-clustering of for some A clustering is trivial if
each of its clusters contains just one point or if it consists of just one cluster
For and clustering of we write whenever and are in the same cluster of
clustering and otherwise
A clustering function for some domain set is a function that takes a distance function over
and outputs a clustering of
A clustering-quality measure CQM is a function that is given a clustering over where
is a distance function over and returns a non-negative real number as well as satisfies some
additional requirements In this work we explore the question of what these requirements should be
Kleinberg?s Axioms
Kleinberg proposes the following three axioms for clustering functions These axioms are
intended to capture the meaning of clustering by determining which functions from a domain set
endowed with a distance function are worthy of being considered clustering functions and which
are not Kleinberg shows that the set is inconsistent there exist no functions that satisfies all three
axioms
The first two axioms require invariance of the clustering that defines under some changes of the
input distance function
Function Scale Invariance Scale invariance requires that the output of a clustering function be
invariant to uniform scaling of the input
A function is scale-invariant if for every distance function and positive where
is defined by setting for every pair of domain points
Function Consistency Consistency requires that if within-cluster distances are decreased and
between-cluster distances are increased then the output of a clustering function does not change
Formally
Given a clustering over a distance function d0 is a C-consistent variant of if
d0 for all and d0 for all
A function is consistent if whenever d0 is an d)-consistent variant of
Function Richness Richness requires that by modifying the distance function any partition of the
underlying data set can be obtained
A function is rich if for each partitioning of there exists a distance function over so
that C.
Theorem Kleinberg There exists no clustering function that simultaneously satisfies scale
invariance consistency and richness
Discussion The intuition behind these axioms is rather clear Let us consider for example the
Consistency requirement It seems reasonable that by pulling closer points that are in the same
cluster and pushing further apart points in different clusters our confidence in the given clustering
will only rise However while this intuition can be readily formulated in terms of clustering quality
namely changes as these should not decrease the quality of a clustering the formulation through
clustering functions says more It actually requires that such changes to the underlying distance
function should not create any new contenders for the best-clustering of the data
For example consider Figure where we illustrate a good 6-clustering On the right hand-side we
show a consistent change of this 6-clustering Notice that the resulting data has a 3-clustering that is
reasonably better than the original 6-clustering While one may argue that the quality of the original
6-clustering has not decreased as a result of the distance changes the quality of the 3-clustering has
improved beyond that of the 6-clustering This illustrates a significant weakness of the consistency
axiom for clustering functions
The implicit requirement that the original clustering remains the best clustering following a consistent change is at the heart of Kleinberg?s impossibility result As we shall see below once we relax
that extra requirement the axioms are no longer unsatisfiable
Axioms of Clustering-Quality Measures
In this section we change the primitive that is being defined by the axioms from clustering functions
to clustering-quality measures We reformulate the above three axioms in terms of CQMs
Figure A consistent change of a 6-clustering
and show that this revised formulation is not only consistent but is also satisfied by a number of
natural clustering quality measures In addition we extend the set of axioms by adding another
axiom of clustering-quality measures that is required to rule out some measures that should not be
counted as CQMs
Clustering-Quality Measure Analogues to Kleinberg?s Axioms
The translation of the Scale Invariance axiom to the CQM terminology is straightforward
Definition Scale Invariance A quality measure satisfies scale invariance if for every clustering of and every positive
The translation of the Consistency axiom is the place where the resulting CQM formulation is indeed
weaker than the original axiom for functions While it clearly captures the intuition that consistent
changes to should not hurt the quality of a given partition it allows the possibility that as a result
of such a change some partitions will improve more than others1
Definition Consistency A quality measure satisfies consistency if for every clustering over
whenever d0 is a consistent variant of then d0
Definition Richness A quality measure satisfies richness if for each non-trivial clustering
of there exists a distance function over such that Argmax{m(C
Theorem Consistency scale invariance and richness for clustering-quality measures form a consistent set of requirements
Proof To show that scale-invariance consistency and richness form a consistent set of axioms we
present a clustering quality measure that satisfies the three axioms This measure captures a quality
that is intuitive for center-based clusterings In Section we introduce more quality measures that
capture the goal of other types of clusterings All of these CQM?s satisfy the above three axioms
For each point in the data set consider the ratio of the distance from the point to its closest center to
the distance from the point to its second closest center Intuitively the smaller this ratio is the better
the clustering points are more confident about their cluster membership We use the average of
this ratio as a quality measure
Definition Relative Point Margin The K-Relative Point Margin of is K-RMX,d
d(x,cx
d(x,cx0 where cx is the closest center to cx is a second closest center to and
The following formalization assumes that larger values of indicate better clustering quality For some
quality measures smaller values indicate better clustering quality in which case we reverse the direction of
inequalities for consistency and use Argmin instead of Argmax for richness
A set is a representative set of a clustering if it consists of exactly one point from each cluster
of C.
Definition Representative Set A set is a representative set of clustering
C2 Ck if and for all Ci
Definition Relative Margin The Relative Margin of a clustering over is
RMX,d
min
is a representative set of
avgx?X\K K-RMX,d
Smaller values of Relative Margin indicate better clustering quality
Lemma Relative Margin is scale-invariant
proof Let be a clustering of Let d0 be a distance function so that d0
d0
for all and some Then for any points
d0 Note also
that scaling does not change the centers selected by Relative Margin Therefore RMX,d0
RMX,d
Lemma Relative Margin is consistent
proof Let be a clustering of distance function Let d0 be a consistent variant of Then
for d0 and for d0 Therefore RMX,d0
RMX,d
Lemma Relative Margin is rich
proof Given a non-trivial clustering over a data set consider the distance function where
for all and for all Then Argmin{m(C
It follows that scale-invariance consistency and richness are consistent axioms
Soundness and Completeness of Axioms
What should a set of axioms for clustering satisfy Usually when a set of axioms is proposed
for some semantic notion a class of objects say clustering functions the aim is to have both
soundness and completeness Soundness means that every element of the described class satisfies
all axioms in particular soundness implies consistency of the axioms and completeness means
that every property shared by all objects of the class is implied by the axioms Intuitively ignoring
logic subtleties a set of axioms is complete for a class of objects if any element outside that class
fails at least one of these axioms
In our context there is a major difficulty there exist no semantic definition of what clustering is
We wish to use the axioms as a definition of clustering functions but then what is the meaning of
soundness and completeness We have to settle for less While we do not have a clear definition of
what is clustering and what is not we do have some examples of functions that should be considered
clustering functions and we can come up with some examples of partitionings that are clearly not
worthy of being called clustering We replace soundness by the requirement that all of our axioms
are satisfied by all these examples of common clustering functions relaxed soundness and we want
that partitioning functions that are clearly not clusterings fail at least one of our axioms relaxed
completeness
In this respect the axioms of badly fail the relaxed version of soundness For each of these
axioms there are natural clustering functions that fail to satisfy it this is implied by Kleinberg?s
demonstration that any pair of axioms is satisfied by a natural clustering while the three together
never hold We argue that our scale invariance consistency and richness are sound for the class
of CQMs However they do not make a complete set of axioms even in our relaxed sense There
are functions that should not be considered reasonable clustering quality measures and yet they
satisfy these three axioms One type of non-clustering-functions are functions that make cluster
membership decisions based on the identity of domain points For example the function that returns
the Relative Margin of a data set whenever some specific pair of data points belong to the same
cluster and twice the Relative Margin of the data set otherwise We overcome this problem by
introducing a new axiom
Isomorphism Invariance
This axiom resembles the permutation invariance objective function axiom by Puzicha
modeling the requirement that clustering should be indifferent to the individual identity of clustered elements This axiom of clustering-quality measures does not have a corresponding Kleinberg
axiom
Definition Clustering Isomorphism Two clusterings and over the same domain
are isomorphic denoted if there exists a distance-preserving isomorphism
such that for all if and only if
Definition Isomorphism Invariance A quality measure is isomorphism invariant if for all
clusterings over where m(C
Theorem The set of axioms consisting of Isomorphism Invariance Scale Invariance Consistency
and Richness all in their CQM formulation is a consistent set of axioms
Proof Just note that the Relative Margin quality measure satisfies all four axioms
As mentioned in the above discussion to have a satisfactory axiom system for any notion one needs
to require more than just consistency To be worthy of being labeled axioms the requirements we
propose should be satisfied by any reasonable notion of CQM. Of course since we cannot define
what CQMs are reasonable we cannot turn this into a formal statement What we can do however
is demonstrate that a variety of natural CQMs do satisfy all our axioms This is done in the next
section
Examples of Clustering Quality Measures
In a survey of validity measures Milligan discusses examples of quality measures that satisfy
our axioms namely scale-invariance consistency richness and perturbation invariance We have
verified that the best performing internal criteria examined in satisfy all our axioms
In this section we introduce two novel QCMs a measure that reflects the underlying intuition of
linkage-based clustering and a measure for center-based clustering In addition to satisfying the
axioms given a clustering these measures can computed in polynomial time
Weakest Link
In linkage-based clustering whenever a pair of points share the same cluster they are connected via
a tight chain of points in that cluster The weakest link quality measure focuses on the longest link
in such a chain
Definition Weakest Link Between Points
C-W LX,d
min
Ci
max(d(x
where is a clustering over and Ci is a cluster in C.
The weakest link of is the maximal value of C-W LX,d over all pairs of points belonging to
the same cluster divided by the shortest between-cluster distance
Definition Weakest Link of The Weakest Link of a clustering over is
maxx?C C-W LX,d
minx6?C
The range of values of weakest link is
Additive Margin
In Section we introduced Relative Margin a quality measure for center-based clustering We
now introduce another quality measure for center-based clustering Instead of looking at ratios
Additive Margin evaluates differences
Definition Additive Point Margin The K-Additive Point Margin of is K-AMX,d
cx0 cx where cx is the closest center to cx0 is a second closest center to and
The Additive Margin of a clustering is the average Additive Point Margin divided by the average
within-cluster distance The normalization is necessary for scale invariance
Definition Additive Margin The Additive Margin of a center-based clustering over
is
x?X K-AMX,d
AMX,d
min
is a representative set of
x?C
Unlike Relative Margin Additive Margin gives higher values to better clusterings
Computational complexity
For a clustering-quality measure to be useful it is important to be able to quickly compute the quality
of a clustering using that measure The quality of a clustering using the measures presented in this
paper can be computed in polynomial time in terms of the number of points in the data set
Using relative or Additive Margin it takes operations to compute the clustering quality
of a data set which is exponential in If a set of centers is given the Relative Margin can be
computed in O(nk operations and the Additive Margin can be computed in operations The
weakest link of a clustering can be computed in operations
Variants of quality measures
Given a clustering-quality measure we can construct new quality measures with different characteristics by applying the quality measure on a subset of clusters It suffices to consider a quality
measure that is defined for clusterings consisting of clusters Given such measure we can
create new quality measures For example
mmin
min
measures the worst quality of a pair of clusters in C.
Alternately we can define mmax and mavg which evaluate the best or average
quality of a pair of clusters in C. A nice feature of these variations is that if satisfies the four
axioms of clustering-quality measures then so do mmin mmax and mavg
More generally if is defined for clusterings on an arbitrary number of clusters we can define a
quality measure as a function of over larger clusterings For example mmax subset
Many such variations which apply existing clustering-quality measures
on subsets of clusters satisfy the axioms of clustering-quality measures whenever the original quality measure satisfies the axioms
Dependence on Number of Clusters
The clustering-quality measures discussed in this paper up to now are independent of the number
of clusters which enables the comparison of clusterings with a different number of clusters In this
section we discuss an alternative type of clustering quality evaluation that depends on the number of
clusters Such quality measures arise naturally from common loss functions objective functions
that drive clustering algorithm such as k-means or k-median
These common loss functions fail to satisfy two of our axioms scale-invariance and richness One
can easily overcome the dependence on scaling by normalization As we will show the resulting
normalized loss functions make a different type of clustering-quality measures from the measures
we previously discussed due to their dependence on the number of clusters
A natural remedy to the failure of scale invariance is to normalize a loss function by dividing it by
the variance of the data or alternatively by the loss of the 1-clustering of the data
Definition 13 L-normalization The L-normalization of a clustering over is
L-normalize(C
L(Call
where Call denotes the 1-clustering of
Common loss functions even after normalization usually have a bias towards either more refined
or towards more coarse clusterings they assign lower cost that is higher quality to more refined
respectively coarse clusterings This prevents using them as a meaningful tool for comparing
the quality of clusterings with different number of clusters We formalize this feature of common
clustering loss functions through the notion of refinement preference
Definition Refinement and coarsening For a pair of clusterings of the same domain
we say is a refinement of equivalently that is a coarsening of if for every cluster Ci
of Ci is a union of clusters of
Definition Refinement/Coarsening Preference A measure is refinement-preferring if for
every clustering of if it has a non-trivial refinement then there exists such a refinement of
for which m(C Coarsening-preferring measures are defined analogously
Note that both refinement preferring and coarsening preferring measures fail to satisfy the Richness
axiom
It seems that there is a divide between two types of evaluations for clusterings those that satisfy
richness and those that satisfy either refinement or coarsening preference To evaluate the quality of
a clustering using a refinement and coarsening preferring measure it is essential to fix the number
of clusters Since the correct number of clusters is often unknown measures that are independent of
the number of clusters apply in a more general setting
Conclusions
We have investigated the possibility of providing a general axiomatic basis for clustering Our
starting point was the impossibility result of Kleinberg We argue that a natural way to overcome
these negative conclusions is by changing the primitive used to formulate the axioms from clustering
functions to clustering quality measures CQMs We demonstrate the merits of the latter framework
by providing a set of axioms for CQMs that captures the essence of all of Kleinberg?s axioms while
maintaining consistency We propose several CQMs that satisfy our proposed set of axioms We
hope that this work and our demonstration of a way to overcome the impossibility result will
stimulate further research towards a general theory of clustering

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4659-topic-partitioned-multinetwork-embeddings.pdf

Topic-Partitioned Multinetwork Embeddings
Peter Krafft
CSAIL
MIT
pkrafft@mit.edu
Juston Moore Bruce Desmarais Hanna Wallach
Department of Computer Science Department of Political Science
University of Massachusetts Amherst
jmoore wallach}@cs.umass.edu
desmarais@polsci.umass.edu
Abstract
We introduce a new Bayesian admixture model intended for exploratory analysis of communication networks?specifically the discovery and visualization of
topic-specific subnetworks in email data sets Our model produces principled visualizations of email networks visualizations that have precise mathematical
interpretations in terms of our model and its relationship to the observed data
We validate our modeling assumptions by demonstrating that our model achieves
better link prediction performance than three state-of-the-art network models and
exhibits topic coherence comparable to that of latent Dirichlet allocation We
showcase our model?s ability to discover and visualize topic-specific communication patterns using a new email data set the New Hanover County email network
We provide an extensive analysis of these communication patterns leading us to
recommend our model for any exploratory analysis of email networks or other
similarly-structured communication data Finally we advocate for principled visualization as a primary objective in the development of new network models
Introduction
The structures of organizational communication networks are critical to collaborative problem solving Although it is seldom possible for researchers to directly observe complete organizational
communication networks email data sets provide one means by which they can at least partially observe and reason about them As a result?and especially in light of their rich textual detail existing
infrastructure and widespread usage?email data sets hold the potential to answer many important scientific and practical questions within the organizational and social sciences While some
questions may be answered by studying the structure of an email network as a whole other more
nuanced questions can only be answered at finer levels of granularity?specifically by studying
topic-specific subnetworks For example breaks in communication duplicated communication
about particular topics may indicate a need for some form of organizational restructuring In order to
facilitate the study of these kinds of questions we present a new Bayesian admixture model intended
for discovering and summarizing topic-specific communication subnetworks in email data sets
There are a number of probabilistic models that incorporate both network and text data Although
some of these models are specifically for email networks McCallum al.?s author?recipient
topic model most are intended for networks of documents such as web pages and the links
between them or academic papers and their citations In contrast an email network is more
naturally viewed as a network of actors exchanging documents actors are associated with nodes
while documents are associated with edges In other words an email network defines a multinetwork
in which there may be multiple edges one per email between any pair of actors Perhaps more
importantly much of the recent work on modeling networks and text has focused on tasks such as
Work done at the University of Massachusetts Amherst
Figure Our model partitions an observed email network left into topic-specific subnetworks
right by associating each author?recipient edge in the observed network with a single topic
predicting links or detecting communities Instead we take a complementary approach and focus on
exploratory analysis Specifically our goal is to discover and visualize topic-specific subnetworks
Rather than taking a two-stage approach in which subnetworks are discovered using one model and
visualized using another we present a single probabilistic model that partitions an observed email
network into topic-specific subnetworks while simultaneously producing a visual representation of
each subnetwork If network modeling and visualization are undertaken separately the resultant visualizations may not directly reflect the model and its relationship to the observed data Rather these
visualizations provide a view of the model and the data seen through the lens of the visualization
algorithm and its associated assumptions so any conclusions drawn from such visualizations can be
biased by artifacts of the visualization algorithm Producing principled visualizations of networks
visualizations that have precise interpretations in terms of an associated network model and its
relationship to the observed data remains an open challenge in statistical network modeling
Addressing this open challenge was a primary objective in the development of our new model
In order to discover and visualize topic-specific subnetworks our model must associate each author
recipient edge in the observed email network with a topic as shown in Figure Our model draws
upon ideas from latent Dirichlet allocation LDA to identify a set of corpus-wide topics of
communication as well as the subset of topics that best describe each observed email We model
network structure using an approach similar to that of Hoff al.?s latent space model LSM so
as to facilitate visualization Given an observed network LSM associates each actor in the network
with a point in K-dimensional Euclidean space For any pair of actors the smaller the distance
between their points the more likely they are to interact If or these interaction
probabilities collectively known as a communication pattern can be directly visualized in or
3-dimensional space via the locations of the actor-specific points Our model extends this idea by
associating a K-dimensional Euclidean space with each topic Observed author?recipient edges are
explicitly associated with topics via the K-dimensional topic-specific communication patterns
In the next section we present the mathematical details of our new model and outline a corresponding inference algorithm We then introduce a new email data set the New Hanover County NHC
email network Although our model is intended for exploratory analysis we test our modeling assumptions via three validation tasks In Section we show that our model achieves better link
prediction performance than three state-of-the-art network models We also demonstrate that our
model is capable of inferring topics that are as coherent as those inferred using LDA. Together
these experiments indicate that our model is an appropriate model of network structure and that
modeling this structure does not compromise topic quality As a final validation experiment we
show that synthetic data generated using our model possesses similar network statistics to those of
the NHC email network In Section we showcase our model?s ability to discover and visualize
topic-specific communication patterns using the NHC network We give an extensive analysis of
these communication patterns and demonstrate that they provide accessible visualizations of emailbased collaboration while possessing precise meaningful interpretations within the mathematical
framework of our model These findings lead us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data Finally we advocate for
principled visualization as a primary objective in the development of new network models
Topic-Partitioned Multinetwork Embeddings
In this section we present our new probabilistic generative model and associated inference algorithm for communication networks For concreteness we frame our discussion of this model in
terms of email data although it is generally applicable to any similarly-structured communication
data The generative process and graphical model are provided in the supplementary materials
A single email indexed by is represented by a set of tokens wn
that comprise the
text of that email an integer A indicating the identity of that email?s author and a
set of binary variables yr A
indicating whether each of the A actors in the network is
a recipient of that email For simplicity we assume that authors do not send emails to themselves
yr if Given a real-world email data set
our
model permits inference of the topics expressed in the text of the emails a set of topic-specific
K-dimensional embeddings points in K-dimensional Euclidean space of the A actors in the
network and a partition of the full communication network into a set of topic-specific subnetworks
As in LDA a topic is characterized by a discrete distribution over word types with probability vector A symmetric Dirichlet prior with concentration parameter is placed over
To capture the relationship between the topics expressed in an email and
that email?s recipients each topic is also associated with a communication pattern an A A
matrix of probabilities Given an email about topic authored by actor a element par is the
probability of actor a including actor as a recipient of that email Inspired by LSM each communication pattern is represented implicitly via a set of A points in K-dimensional Euclidean
such that par pra ksa sr
space sa A
and a scalar bias term
with sa I and If or this representation enables
each topic-specific communication pattern to be visualized in or 3-dimensional space via the locations of the points associated with the A actors It is worth noting that the dimensions of each
K-dimensional space have no inherent meaning In isolation each point sa conveys no information however the distance between any two points has a precise and meaningful interpretation in the
generative process Specifically the recipients of any email associated with topic are more likely
to be those actors near to the email?s author in the Euclidean space corresponding to that topic
Each email indexed by has a discrete distribution over topics A symmetric Dirichlet prior
with concentration parameter is placed over Each token wn is associated
with a topic assignment zn such that zn and wn for zn Our model does
not include a distribution over authors the generative process is conditioned upon their identities
The email-specific binary variables yr A
indicate the recipients of email and thus the
presence absence of email-specific edges from author to each of the A other actors
Consequently there may be multiple edges one per email between any pair of actors and defines
a multinetwork over the entire set of actors We assume that the complete multinetwork comprises
topic-specific subnetworks In other words each yr is associated with some topic and therefore
with topic-specific communication pattern such that yr Bern(par for a A natural
way to associate each yr with a topic would be to draw a topic assignment from in a manner
analogous to the generation of zn however as outlined by Blei and Jordan this approach can
result in the undesirable scenario in which one subset of topics is associated with tokens while another disjoint subset is associated with edges Additionally models of annotated data that possess
this exchangeable structure tend to exhibit poor generalization A better approach advocated
by Blei and Jordan is to draw a topic assignment for each yr from the empirical distribution over
topics defined by By definition the set of topics associated with edges will therefore be a subset of the topics associated with tokens One way of simulating this generative process is to associate
each yr with a position max and therefore with the topic assignment zn at
that position2 by drawing a position assignment xr max for each yr This
indirect procedure ensures that yr Bern(par for a xr and zn as desired
The function is the logistic function while the function is the l2 norm
Emails that do not contain any text convey information about the frequencies of communication between their authors and recipients As a result we do not omit such emails from instead we
augment each one with a single dummy topic assignment z1 for which there is no associated token w1
Inference
For real-world data
authors A
the tokens
a and recipients are observed while
are unobserved Dirichlet?multinomial conjugacy allows
and
and to be marginalized out while typical values for the remaining unobserved variables can
be sampled from their joint posterior distribution using Markov chain Monte Carlo methods In
this section we outline a Metropolis-within-Gibbs sampling algorithm that operates by sequentially
resampling the value of each latent variable sa bt zn or xr from its conditional posterior
Since zn is a discrete random variable new values may be sampled directly using
wn(d W\d,n A Z\d,n
yr(d
N\d,n
r:xr
N\d,n
pa(d
for
otherwise
where subscript denotes a quantity excluding data from position in email Count is
the total number of tokens in assigned to topic by of which are of type and
belong to email New values for discrete random variable xr may be sampled directly using
A zn Z\d,n
yr(d
pa(d
New values for continuous random variables sa and cannot be sampled directly from their
conditional posteriors but may instead be obtained using the Metropolis?Hastings algorithm With
a non-informative prior over sa sa the conditional posterior over sa is
a A a
ar
ar
where count
PD
Counts
and are defined similarly Likewise with an improper noninformative prior over the conditional posterior over is
A
A
ar
ar
r:r<a
Data
Due to a variety of factors involving personal privacy concerns and the ownership of content by
email service providers academic researchers rarely have access to organizational email data For
example the Enron data set 10]?arguably the most widely studied email data set?was only released because of a court order The public record is an alternative source of organizational email
data Public record data sets are widely available and can be continually updated yet remain relatively untapped by the academic community We therefore introduce and analyze a new public
record email data set relevant to researchers in the organizational and social sciences as well as machine learning researchers This data set consists of emails between the managers of the departments
that constitute the executive arm of government at the county level for New Hanover County North
Carolina In this semi-autonomous local government county managers act as executives and the individual departments are synonymous with the individual departments and agencies in for instance
the U.S. federal government Therefore not only does this email data set offer a view into the communication patterns of the managers of New Hanover County but analyses of it also serve as case
studies in modeling inter-agency communications in the U.S. federal government administration
The function evaluates to one if its argument evaluates to true and evaluates to zero otherwise
our model
Erosheva
baseline
MMSB
baseline
LSM
Number of Topics
our model
Erosheva
baseline
MMSB
baseline
LSM
Number of Topics
our model
Erosheva
baseline
LDA
Average Topic Coherence
our model
Erosheva
baseline
LDA
Average Topic Coherence
Average F1 Score
Average F1 Score
Number of Topics
Number of Topics
Figure Average link prediction performance for the NHC email network and the Enron data
set For MMSB and LSM we only report results obtained using the best-performing hyperparameter
values Average topic coherence scores for the NHC email network and the Enron data set
The New Hanover County NHC email network comprises the complete inboxes and outboxes of
department managers from the month of February In total there are emails of
which were authored by managers Of these emails were sent to other managers
via the or fields excluding any emails sent from a manager to him or herself only
For our experiments we used these emails between actors To verify that our model is
applicable beyond the NHC email network we also performed two validation experiments using
the Enron email data set For this data set we treated each unique @enron email address
as an actor and used only those emails between the most active actors determined by the total
numbers of emails sent and received Emails that were not sent to at least one other active actor via
the or fields were discarded To avoid duplicate emails we retained only those emails
from sent mail sent or sent items folders These steps resulted in a total of emails
involving actors Both data sets were preprocessed to concatenate the text of subject lines and
message bodies and to remove any stop words URLs quoted text and where possible signatures
Experiments
Our model is primarily intended as an exploratory analysis tool for organizational communication
networks In this section we use the NHC email network to showcase our model?s ability to discover
and visualize topic-specific communication subnetworks First however we test our underlying
modeling assumptions via three quantitative validation tasks as recommended by Schrodt
Link Prediction
In order to gauge our model?s predictive performance we evaluated its ability to predict the recipients of test emails from either the NHC email network or the Enron data set conditioned on the
text of those emails and the identities of their authors For each test email the binary variables
indicating the recipients of that email yr A
were treated as unobserved Typical values
for these variables were sampled from their joint posterior distribution and compared to the true
values to yield an F1 score We formed a test split of each data set by randomly selecting emails
with probability For each data set we averaged the F1 scores over five random test splits
We compared our model?s performance with that of two baselines and three existing network models thereby situating it within the existing literature Given a test email authored by actor a our
simplest baseline na??vely predicts that actor a will include actor as a recipient of that email with
probability equal to the number of non-test emails sent from actor a to actor divided by the total number of non-test emails sent by actor a Our second baseline is a variant of our model in
which each topic-specific communication pattern is represented explicitly via A(A
probabilities drawn from a symmetric Beta prior with concentration parameter Comparing our
model to this variant enables us to validate our assumption that topic-specific communication patterns can indeed be accurately represented by a set of A points one per actor in K-dimensional
Euclidean space We also compared our model?s performance to that of three existing network models a variant of Erosheva al.?s model for analyzing scientific publications LSM and the
mixed-membership stochastic blockmodel MMSB Erosheva al.?s model can be viewed as
a variant of our model in which the topic assignment for each yr is drawn from instead of
the empirical distribution over topics defined by Like our second baseline each topic-specific
communication pattern is represented explicitly via probabilities drawn from a symmetric Beta prior
with concentration parameter however unlike this baseline each one is represented using A prob(t
abilities such that par pr LSM can be viewed as a network-only variant of our model in which
text is not modeled As a result there are no topics and a single communication pattern This pattern is represented implicitly via a set of A actor-specific points in K-dimensional Euclidean space
Finally MMSB is a widely-used model for mixed-membership community discovery in networks
For our model and all its variants typical values for yr A
can be sampled from their joint
posterior distribution using an appropriately-modified version of the Metropolis-within-Gibbs algorithm in Section In all our experiments we ran this algorithm for iterations On
iteration we defined each proposal distribution to be a Gaussian distribution centered on the value
from iteration with covariance matrix max I thereby resulting in larger covariances
for earlier iterations Beta?binomial conjugacy allows the elements of to be marginalized out
in both our second baseline and Erosheva al.?s model For MMSB typical values can be sampled using a modified version of Chang?s Gibbs sampling algorithm We ran this algorithm
for iterations For all models involving topics we set concentration parameter to for the
NHC network and for the Enron data set For both data sets we set concentration parameter to
We varied the number of topics from to In order to facilitate visualization we used
2-dimensional Euclidean spaces for our model For LSM however we varied the dimensionality of
the Euclidean space from to We report only those results obtained using the best-performing
dimensionality For our second baseline and Erosheva al.?s model we set concentration parameter
to For MMSB we performed a grid search over all hyperparameter values and the number
of blocks and as with LSM report only those results obtained using the best-performing values.5
F1 scores averaged over five random test splits of each data set are shown in Figure Although our
model is intended for exploratory analysis it achieves better link prediction performance than the
other models Furthermore the fact that our model outperforms our second baseline and Erosheva
al.?s model validates our assumption that topic-specific communication patterns can indeed be
accurately represented by a set of A actor-specific points in 2-dimensional Euclidean space
Topic Coherence
When evaluating unsupervised topic models topic coherence metrics are often used as a
proxy for subjective evaluation of semantic coherence In order to demonstrate that incorporating
network data does not impair our model?s ability to model text we compared the coherence of topics
inferred using our model with the coherence of topics inferred using LDA our second baseline and
Erosheva al.?s model For each model we varied the number of topics from to and drew five
samples from the joint posterior distribution over the unobserved random variables in that model We
evaluated the topics resulting from each sample using Mimno al.?s coherence metric Topic
coherence averaged over the five samples is shown in Figure Our model achieves coherence
comparable to that of LDA. This result when combined with the results in Section demonstrates
that our model can achieve state-of-the-art predictive performance while producing coherent topics
Posterior Predictive Checks
We used posterior predictive checking to assess the extent to which our model is a good fit for the
NHC email network Specifically we defined four network statistics four discrepancy
functions that summarize meaningful aspects of the NHC network generalized graph transitivity
the dyad intensity distribution the vertex degree distribution and the geodesic distance distribution.6
We then generated synthetic networks from the posterior predictive distribution implied by our
These values were obtained by slice sampling typical values for the concentration parameters in LDA.
They are consistent with the concentration parameter values used in previous work
These values correspond to a prior over block memberships a prior over
diagonal entries of the blockmodel a prior over off-diagonal entries and blocks
These statistics are defined in the supplementary materials
Transitivity
Simulated Quantile
Degree
Simulated Quantile
Frequency
Observed Quantile
Actor Sorted by Observed Degree
Observed Quantile
Figure Four posterior predictive checks of our model using the NHC email network and
topics a histogram of the graph transitivity of the synthetic networks with the graph transitivity
of the NHC email network indicated by a vertical line a quantile?quantile plot comparing the
distribution of dyadic intensities in the synthetic networks to that of the observed network a box
plot indicating the sampled degree of each manager in the synthetic networks with managers sorted
from highest to lowest observed degree and their observed degrees indicated by a line and a
quantile?quantile plot comparing the observed and synthetic geodesic distance distributions
model and the NHC network We applied each discrepancy function to each synthetic network to
yield four distributions over the values of the four network statistics If our model is a good fit
for the NHC network these distributions should be centered around the values of the corresponding
discrepancy functions when computed using the observed NHC network As shown in Figure
our model generates synthetic networks with dyad intensity vertex degree and geodesic distance
distributions that are very similar to those of the NHC network The distribution over synthetic graph
transitivity values is not centered around the observed graph transitivity but the observed transitivity
is not sufficiently far into the tail of the distribution to warrant reparameterization of our model
Exploratory Analysis
In order to demonstrate our model?s novel ability to discover and visualize topic-specific communication patterns we performed an exploratory analysis of four such patterns inferred from the NHC
email network using our model These patterns are visualized in Figure Each pattern is represented implicitly via a single set of A points in 2-dimensional Euclidean space drawn from their joint
posterior distribution The recipients of any email associated with topic are more likely to be those
actors near to the email?s author in the Euclidean space corresponding to that topic We selected the
patterns in Figure so as to highlight the types of insights that can be obtained using our model
Although many structural properties may be of interest we focus on modularity and assortativity
For each topic-specific communication pattern we examined whether there are active disconnected
components in that topic?s Euclidean space high modularity The presence of such components indicates that there are groups of actors who engage in within but not between-group communication about that topic We also used a combination of node proximity and node coloration
to determine whether there is more communication between departments that belong to the same
division in the New Hanover County government organizational chart than between departments
within different divisions assortativity In Figure we show one topic that exhibits strong
modularity and little assortativity the Public Signage topic one topic that exhibits strong assortativity and little modularity the Broadcast Messages topic and one topic that exhibits both
strong assortativity and strong modularity the Meeting Scheduling topic The Public Relations
topic which includes communication with news agencies is mostly dominated by a cluster involving many departments Finally the Meeting Scheduling topic displays hierarchical structure with
two assistant county managers located at the centers of groups that correspond to their divisions
Exploratory analysis of communication patterns is a powerful tool for understanding organizational
communication networks For example examining assortativity can reveal whether actual communication patterns resemble official organizational structures Similarly if a communication pattern
exhibits modularity each disconnected component may benefit from organizational efforts to facilitate inter-component communication Finally structural properties other than assortativity and
modularity may also yield scientific or practical insights depending on organizational needs
Public Signage
Broadcast Messages
change signs sign process
ordinance
17
fw fyi bulletin summary
week legislative
PM
CC
HL
AM
SF
PS
DS
EG
EG
LB
AM
DS
FN
PS
PI
TX
BG
LB
FS
HR
MS
CM
RD
AM
AM
EM PG
IT
CE
EM
RD
PG
CA
CC
SF
TX
FS
SS
EV
IT
MS
PM
RM
BG
EL
EV
CM
CA
FN
YS
PI
SS
RM
YS
VS
HL
HR
VS
EL
CE
Public Relations
Meeting Scheduling
31
city breakdown information
give
63
meeting march board
agenda week
EL
FS
SF
CA
PS
RM
RD
EG
RM
HR
MS
PI
DS
FS
CE
EV
CM
EL
PG
RD
TX
CC
LB
CE
EM
AM
HR
BG
FN
HL
PM
SS
LB
HL
EM
AM
IT
PG
BG
CA
MS
PS
CM
EG
SS
IT
TX
CC
AM
YS
AM
VS
DS
PI
FN
YS
PM
VS
SF
Assistant County Manager
Budget
Cooperative Extension
County Attorney
County Commissioners
County Manager
Development Services
Elections
Emergency Management
Engineering
Environmental Management
Finance
Fire Services
Health
Human Resources
Information Technology
Library
Museum
Parks and Gardens
Planning and Inspections
Pretrial Release Screening
Property Management
Register of Deeds
Risk Management
Sheriff
Social Services
Tax
Veteran Services
Youth Empowerment Services
AM
BG
CE
CA
CC
CM
DS
EL
EM
EG
EV
FN
FS
HL
HR
IT
LB
MS
PG
PI
PS
PM
RD
RM
SF
SS
TX
VS
YS
EV
Figure Four topic-specific communication patterns inferred from the NHC email network Each
pattern is labeled with a human-selected name for the corresponding topic along with that topic?s
most probable words in order of decreasing
probability The size of each manager?s acronym in
topic t?s pattern given by da maxa da where da is the degree of actor a in that
subnetwork indicates how often that manager communicates about that topic Managers acronyms
are colored according to their respective division in the New Hanover County organizational chart
The acronym appears twice in all plots because there are two assistant county managers
Conclusions
We introduced a new Bayesian admixture model for the discovery and visualization of topic-specific
communication subnetworks Although our model is intended for exploratory analysis the validation experiments described in Sections and demonstrate that our model can achieve stateof-the-art predictive performance while exhibiting topic coherence comparable to that of LDA. To
showcase our model?s ability to discover and visualize topic-specific communication patterns we
introduced a new data set the NHC email network and analyzed four such patterns inferred from
this data set using our model Via this analysis were are able to examine the extent to which actual
communication patterns resemble official organizational structures and identify groups of managers
who engage in within but not between-group communication about certain topics Together these
predictive and exploratory analyses lead us to recommend our model for any exploratory analysis of
email networks or other similarly-structured communication data Finally our model is capable of
producing principled visualizations of email networks visualizations that have precise mathematical interpretations in terms of this model and its relationship to the observed data We advocate
for principled visualization as a primary objective in the development of new network models
Acknowledgments
This work was supported in part by the Center for Intelligent Information Retrieval and in part by
the NSF GRFP under grant Any opinions findings and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reflect those of the sponsors

<<----------------------------------------------------------------------------------------------------------------------->>

title: 964-an-input-output-hmm-architecture.pdf

An Input Output HMM Architecture
Yoshua Bengio
Dept Informatique Recherche
Operationnelle
Universite de Montreal Qc
bengioyOIRO.UMontreal.CA
Paolo Frasconi
Dipartimento di Sistemi Informatica
Universita di Firenze Italy
paoloOmcculloch.ing.unifi.it
Abstract
We introduce a recurrent architecture having a modular structure
and we formulate a training procedure based on the EM algorithm
The resulting model has similarities to hidden Markov models but
supports recurrent networks processing style and allows to exploit
the supervised learning paradigm while using maximum likelihood
estimation
INTRODUCTION
Learning problems involving sequentially structured data cannot be effectively dealt
with static models such as feedforward networks Recurrent networks allow to model
complex dynamical systems and can store and retrieve contextual information in
a flexible way Up until the present time research efforts of supervised learning
for recurrent networks have almost exclusively focused on error minimization by
gradient descent methods Although effective for learning short term memories
practical difficulties have been reported in training recurrent neural networks to
perform tasks in which the temporal contingencies present in the input/output
sequences span long intervals Bengio Mozer
Previous work on alternative training algorithms Bengio could suggest
that the root of the problem lies in the essentially discrete nature of the process
of storing information for an indefinite amount of time Thus a potential solution
is to propagate backward in time targets in a discrete state space rather than
differential error information Extending previous work Bengio Frasconi
in this paper we propose a statistical approach to target propagation based on the
EM algorithm We consider a parametric dynamical system with discrete states and
we introduce a modular architecture with subnetworks associated to discrete states
The architecture can be interpreted as a statistical model and can be trained by the
EM or generalized EM GEM algorithms Dempster considering the
internal state trajectories as missing data In this way learning is decoupled into
also AT&T Bell Labs Holmdel
Yoshua Bengio Paolo Frasconi
a temporal credit assignment subproblem and a static learning subproblem that
consists of fitting parameters to the next-state and output mappings defined by the
estimated trajectories In order to iteratively tune parameters with the EM or GEM
algorithms the system propagates forward and backward a discrete distribution over
the states resulting in a procedure similar to the Baum-Welch algorithm used
to train standard hidden Markov models HMMs Levinson HMMs
however adjust their parameters using unsupervised learning whereas we use EM
in a supervised fashion Furthermore the model presented here could be called
Input/Output HMM or IOHMM because it can be used to learn to map input
sequences to output sequences unlike standard HMMs which learn the output
sequence distribution This model can also be seen as a recurrent version of the
Mixture of Experts architecture Jacobs related to the model already
proposed in Cacciatore and Nowlan Experiments on artificial tasks Bengio
Frasconi have shown that EM recurrent learning can deal with long
term dependencies more effectively than backpropa~ation through time and other
alternative algorithms However the model used in Bengio Frasconi has
very limited representational capabilities and can only map an input sequence to a
final discrete state In the present paper we describe an extended architecture that
allows to fully exploit both input and output portions of the data as required by
the supervised learning paradigm In this way general sequence processing tasks
such as production classification or prediction can be dealt with
THE PROPOSED ARCHITECTURE
We consider a discrete state dynamical system based on the following state space
f(x
description
Yt Ut
where Ut is the input vector at time Yt is the output vector and
Xt I is a discrete state These equations define a generalized Mealy
finite state machine in which inputs and outputs may take on continuous values In
this paper we consider a probabilistic version of these dynamics where the current
inputs and the current state distribution are used to estimate the state distribution
and the output distribution for the next time step Admissible state transitions will
be specified by a directed graph whose vertices correspond to the model states
and the set of successors for state is Sj
The system defined by equations can be modeled by the recurrent architecture
depicted in Figure The architecture is composed by a set of state etworks
and a set of output networks OJ Each one of the state
and output networks is uniquely associated to one of the states,and all networks
share the same input Ut Each state network has the task of predicting the next
state distribution based on the current input and given that Xt-l Similarly
each output network OJ predicts the output of the system given the current state
and input All the subnetworks are assumed to be static and they are defined by
means of smooth mappings Nj Ut and OJ where 9j and 1Jj are vectors
of adjustable parameters connection weights The ranges of the functions
may be constrained in order to account for the underlying transition graph
Each output Pij of the state subnetwork Nj at time is associated to one
of the successors of state Thus the last layer of has as many units as the
cardinality of Sj. For convenience of notation we suppose that Pij,t are defined for
each and we impose the condition Pij for each not belonging
to The softmax function is used in the last layer Pij a,j ILlEsj ea
Sj where aij are intermediate variables that can be thought of as the
An Input Output HMM Architecture
cu nt Input
EIYt
Xt-l
current pectod output
given PIlat Input Mquenc
current atilt dlatrtbutton
Ct Pl I Ul
lull
Xt
Xt+l
Yt-l
Yt
Yt+l
Xt-l
Xt+l
Yt
I
I
Ut-l
HMM
I
Ut
IOHMM
Ut+l
Figure The proposed IOHMM architecture Bottom Bayesian network
expressing conditional dependencies for an IOHMM top Bayesian network for a
standard HMM
activations of the output units of subnetwork In this way Pij Tij,t
The vector
represents the internal state of the model and it is computed as
a linear combination of the outputs of the state networks gated by the previously
computed internal state
t-IIPj,t
where IPj,t Output networks compete to predict the global
output of the system 1Jt
1Jt
jt1Jjt
where 1Jjt is the output of subnetwork OJ. At this level we do not need
to further specify the internal architecture of the state and output subnetworks
Depending on the task the designer may decide whether to include hidden layers
and what activation rule to use for the hidden units
This connectionist architecture can be also interpreted as a probability model Let
us assume a multinomial distribution for the state variable Xt and let us consider
the main variable of the temporal recurrence If we initialize the vector
to positive numbers summing to it can be interpreted as a vector of initial state
probabilities In general we obtain relation it P(Xt I
having denoted
with ui the subsequence of inputs from time to inclusively Equation then
has the following probabilistic interpretation
un
P(Xt
lui
P(Xt
I Xt-I ut}P(Xt-1
lui-I
j=l
the subnetworks compute transition probabilities conditioned on the input
sequence Ut
Xt I Xt-l
Pij,t
As in neural networks trained to minimize the output squared error the output
1Jt of this architecture can be interpreted as an expected position parameter
for the probability distribution of the output Yt. However in addition to being
conditional on an input Ut this expectation is also conditional on the state Xt
Yoshua Bengio Paolo Frasconi
E[Yt I Xt,Ut The actual form of the output density denoted Y(Yt will
be chosen according to the task For example a multinomial distribution is suitable
for sequence classification or for symbolic mutually exclusive outputs Instead a
Gaussian distribution is adequate for producing continuous outputs In the first
case we use a softmax function at the output of subnetworks OJ in the second case
we use linear output units for the subnetworks OJ.
In order to reduce the amount of computation we introduce an independency model
among the variables involved in the probabilistic interpretation of the architecture
We shall use a Bayesian network to characterize the probabilistic dependencies
among these variables Specifically we suppose that the directed acyclic graph
depicted at the bottom of Figure 1b is a Bayesian network for the dependency
One of the most evident consequences
model associated to the variables
of this independency model is that only the previous state and the current input are
relevant to determine the next-state This one-step memory property is analogue
to the Markov assumption in hidden Markov models In fact the Bayesian
network for HMMs can be obtained by simply removing the Ut nodes and arcs from
them top of Figure Ib
I YI.
A SUPERVISED LEARNING ALGORITHM
The learning algorithm for the proposed architecture is derived from the maximum
likelihood principle The training data are a set of pairs of input output sequences
of length P}. Let denote the vector of
parameters obtained by collecting all the parameters Jj and iJi of the architecture
The likelihood function is then given by
p(Yip(p I uip(p
p=l
The output values used here as targets may also be specified intermittently For
example in sequence classification tasks one may only be interested in the output YT at the end of each sequence The modification of the likelihood to account
for intermittent targets is straightforward According to the maximum likelihood
principle the optimal parameters are obtained by maximizing In order to
apply EM to our case we begin by noting that the state variables Xt are not observed Knowledge of the model's state trajectories would allow one to decompose
the temporal learning problem into 2n static learning subproblems Indeed if Xt
were known the probabilities it would be either or and it would be possible
to train each subnetwork separately without taking into account any temporal dependency This observation allows to link EM learning to the target propagation
approach discussed in the introduction Note that if we used a Viterbi-like approximation considering only the most likely path we would indeed have 2n static
learning problems at each epoch In order to we derive the learning equations let
us define the complete data as uiP(p),yiP(p),xiP(p));p P}. The
corresponding complete data l%-likelihood is
L...IOgP(YIP(P),ZlP(P
I
p=l
Since lc depends on the hidden state variables it cannot be maximized directly The MLE optimization is then solved by introducing the auxiliary function
and iterating the following two,steps for
Estimation
Compute
Maximization Update the parameters as arg max?J
An Input Output HMM Architecture
The expectation of can be expressed as
Tp
it!og P(Yt I Xt Ut hij,tlog<Pij
I
where hij EIZitzj,t-l uf yf denoting Zit for an indicator variable if
Xt and otherwise The hat in it and hij means that these variables are
computed using the old parameters In order to compute hij we introduce
the forward probabilities Qit P(YL Xt uD and the backward probabilities
f3it p(yf I Xt
that are updated as follows
un
f3it fY(Yt;l1it Lt Pti(Ut+df3l,t+l
Qit fY(Yt l1it Lt pa(ut Qt
f3it Qj t-l<Pij
QiT
Each iteration of the EM algorithm requires to maximize We first
consider a simplified case in which the inputs are quantized belonging
to a finite alphabet and the subnetworks behave like lookup tables addressed by the input symbols we interpret each parameter as
Wi P(Xt I Xt-l O"t For simplicity we restrict the analysis to classification tasks and we suppose that targets are specified as desired final states for
each sequence Furthermore no output subnetworks are used in this particular
application of the algorithm In this case we obtain the reestimation formulae
Wijk
ij
ESj wt Ut=k
In general however if the subnetworks have hidden sigmoidal units or use a softmax function to constrain their outputs to sum to one the maximum of cannot
be found analytically In these cases we can resort to a GEM algorithm that simply produces an increase in for example by gradient ascent In this case the
derivatives of with respect to the parameters can be easily computed as follows
Let Ojlt be a generic weight in the state subnetwork From equation
L:L:L:hij,t_l_8<pij
Pij,t
where the partial derivatives can be computed using backpropagation Similarly denoting with t'Jik a generic weight of the output subnetwork Oi we have
t~logfY(Yy;l1it
where are also computed using backpropagation Intuitively the parameters
are updated as if the estimation step of EM had provided targets for the outputs of
the 2n subnetworks for each time Although GEM algorithms are also guaranteed
to find a local maximum of the likelihood their convergence may be significantly
slower compared to EM. In several experiments we noticed that convergence can be
accelerated with stochastic gradient ascent
Yoshua Bengio Paolo Frasconi
COMPARISONS
It appears natural to find similarities between the recurrent architecture described
so far and standard HMMs Levinson The architecture proposed in this
paper differs from standard HMMs in two respects computing style and learning
With IOHMMs sequences are processed similarly to recurrent networks an
input sequence can be synchronously transformed into an output sequence This
computing style is real-time and predictions of the outputs are available as the input
sequence is being processed This architecture thus allows one to implement all three
fundamental sequence processing tasks production prediction and classification
Finally transition probabilities in standard HMMs are fixed states form a
homogeneous Markov chain In IOHMMs transition probabilities are conditional
on the input and thus depend on time resulting in an inhomogeneous Markov chain
Consequently the dynamics of the system specified by the transition probabilities
are not fixed but are adapted in time depending on the input sequence
The other fundamental difference is in the learning procedure While interesting
for their capabilities of modeling sequential phenomena a major weakness of standard HMMs is their poor discrimination power due to unsupervised learning An
approach that has been found useful to improve discrimination in HMMs is based
on maximum mutual information MMI training It has been pointed out that
supervised learning and discriminant learning criteria like MMI are actually strictly
related Bridle Although the parameter adjusting procedure we have defined
is based on MLE
is used as desired output in response to the input
resulting
in discriminant supervised learning Finally it is worth mentioning that a number
of hybrid approaches have been proposed to integrate connectionist approaches into
the HMM frame\'Vork For example in Bengio the observations used
by the HMM are generated by a feedforward neural network In Bourlard and
Wellekens a feedforward network is used to estimate state probabilities conditional to the acoustic sequence A common feature of these algorithms and the
one proposed in this paper is that neural networks are used to extract temporally
local information whereas a Markovian system integrates long-term constraints
yf
uf
We can also establish a link between IOHMMs and adaptive mixtures of experts
Jacobs Recently Cacciatore Nowlan have proposed a
recurrent extension to the ME architecture called mixture of controllers in
which the gating network has feedback connections thus allowing to take temporal
context into account Our IOHMM architecture can be interpreted as a special case
of the MC architecture in which the set of state subnetworks play the role of a
gating network having a modular structure and second order connections
REGULAR GRAMMAR INFERENCE
In this section we describe an application of our architecture to the problem of
grammatical inference In this task the learner is presented a set of labeled strings
and is requested to infer a set of rules that define a formal language It can be
considered as a prototype for more complex language processing problems However
even in the simplest case regular grammars the task can be proved to
be NP-complete Angluin and Smith We report experimental results on
a set of regular grammars introduced by Tomita and afterwards used by
other researchers to measure the accuracy of inference methods based on recurrent
networks Giles Pollack Watrous and Kuhn
We used a scalar output with supervision on the final output YT that was modeled
as a Bernoulli variable fy YT l-YT with YT if the string
is rejected and YT if it is accepted In tbis application we did not apply
An Input Output HMM Architecture
Table Summary of experimental results on the seven Tomita's grammars
Grammar
Sizes
FSA min
Convergence
Average
Accuracies
Worst
Best
W&K Best
external inputs to the output networks This corresponds to modeling a Moore
finite state machine Given the absence of prior knowledge about plausible state
paths we used an ergodic transition graph fully connected).In the experiments
we measured convergence and generalization performance using different sizes for
the recurrent architecture For each setting we ran trials with different seeds
for the initial weights We considered a trial successful if the trained network was
able to correctly label all the training strings The model size was chosen using a
cross-validation criterion based on performance on randomly generated strings
of length For comparison in Table we also report for each grammar
the number of states of the minimal recognizing FSA Tomita We tested
the trained networks on a corpus of binary strings of length The
final results are summarized in Table The column Convergence reports the
fraction of trials that succeeded to separate the training set The next three columns
report averages and order statistics worst and best trial of the fraction of correctly
classified strings measured on the successful trials For each grammar these results
refer to the model size selected by cross-validation Generalization was always
perfect on grammars and For each grammar the best trial also attained
perfect generalization These results compare very favorably to those obtained with
second-order networks trained by gradient descent when using the learning sets
proposed by Tomita For comparison in the last column of Table we reproduce
the results reported by Watrous Kuhn in the best of five trials In most
of the successful trials the model learned an actual FSA behavior with transition
probabilities asymptotically converging either to or to This renders trivial the
extraction of the corresponding FSA Indeed for grammars and we found
that the trained networks behave exactly like the minimal recognizing FSA
A potential training problem is the presence of local maxima in the likelihood function For example the number of converged trials for grammars and is quite
small and the difficulty of discovering the optimal solution might become a serious
restriction for tasks involving a large number of states In other experiments Bengio Frasconi we noticed that restricting the connectivity of the transition
graph can significantly help to remove problems of convergence Of course this approach can be effectively exploited only if some prior knowledge about the state
space is available For example applications of HMMs to speech recognition always
rely on structured topologies
CONCLUSIONS
There are still a number of open questions In particular the effectiveness of the
model on tasks involving large or very large state spaces needs to be carefully evaluated In Bengio Frasconi we show that learning long term dependencies
in these models becomes more difficult as we increase the connectivity of the state
Yoshua Bengio Paolo Frasconi
transition graph However because transition probabilities of IOHMMs change at
each they deal better with this problem of long-term dependencies than standard
HMMs Another interesting aspect to be investigated is the capability of the model
to successfully perform tasks of sequence production or prediction For example
interesting tasks that could also be approached are those related to time series
modeling and motor control learning

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2428-clustering-with-the-connectivity-kernel.pdf

Clustering with the Connectivity Kernel
Bernd Fischer Volker Roth and Joachim M. Buhmann
Institute of Computational Science
Swiss Federal Institute of Technology Zurich
Zurich Switzerland
bernd.fischer volker.roth,jbuhmann}@inf.ethz.ch
Abstract
Clustering aims at extracting hidden structure in dataset While the problem of finding compact clusters has been widely studied in the literature extracting arbitrarily formed elongated structures is considered a
much harder problem In this paper we present a novel clustering algorithm which tackles the problem by a two step procedure first the data
are transformed in such a way that elongated structures become compact
ones In a second step these new objects are clustered by optimizing a
compactness-based criterion The advantages of the method over related
approaches are threefold robustness properties of compactness-based
criteria naturally transfer to the problem of extracting elongated structures leading to a model which is highly robust against outlier objects
the transformed distances induce a Mercer kernel which allows us
to formulate a polynomial approximation scheme to the generally Phard clustering problem iii the new method does not contain free kernel
parameters in contrast to methods like spectral clustering or mean-shift
clustering
Introduction
Clustering or grouping data is an important topic in machine learning and pattern recognition research Among various possible grouping principles those methods which try to
find compact clusters have gained particular importance Presumingly the most prominent
method of this kind is the K-means clustering for vectorial data Despite the powerful
modeling capabilities of compactness-based clustering methods they mostly fail in finding
elongated structures The fast single linkage algorithm is the most often used algorithm
to search for elongated structures but it is known to be very sensitive to outliers in the
dataset Mean shift clustering another method of this class is capable of extracting
elongated clusters only if all modes of the underlying probability distribution have one single maximum Furthermore a suitable kernel bandwidth parameter has to be preselected
Spectral clustering shows good performance in many cases but the algorithm is
only analyzed for special input instances while a complete analysis of the algorithm is still
missing Concerning the preselection of a suitable kernel width spectral clustering suffers
from similar problems as mean shift clustering
In this paper we present an alternative method for clustering elongated structures Apart
from the number of clusters it is a completely parameter-free grouping principle We build
up on the work on path-based clustering For a slight modification of the original prob
lem we show that the defined path distance induces a kernel matrix fulfilling Mercers condition After the computation of the path-based distance the compactness-based pairwise
clustering principle is used to partition the data While for the general P-hard pairwise
clustering problem no approximation algorithms are known we present a polynomial time
approximation scheme PTAS for our special case with path-based distances The Mercer
property of these distances allows us to embed the data in a dimensional vector
space even for non-metric input graphs In this vector space pairwise clustering reduces to
minimizing the K-means cost function in dimensions For the latter problem
however there exists a PTAS
In addition to this theoretical result we also present an efficient practical algorithm resorting to a 2-approximation algorithm which is based on kernel PCA. Our experiments suggest that kernel PCA effectively reduces the noise in the data while preserving the coarse
cluster structure Our method is compared to spectral clustering and mean shift clustering
on selected artificial datasets In addition the performance is demonstrated on the USPS
handwritten digits dataset
Clustering by Connectivity
The main idea of our clustering criterion is to transform elongated structures into compact
ones in a preprocessing step Given the transformed data we then infer a clustering solution
by optimizing a compactness based criterion The advantage of circumventing the problem
of directly finding connected elongated regions in the data as in the spanning tree approach is the following while spanning tree algorithms are extremely sensitive to outliers
the two-step procedure may benefit from the statistical robustness of certain compactness
based methods Concerning the general case of datasets which are not given in a vector
space but only characterized by pairwise dissimilarities the pairwise clustering model has
been shown to be robust against outliers in the dataset It may thus be a natural
choice to formulate the second step as searching for the partition vector
that minimizes the pairwise clustering cost function
PK
PC i:ci j:cj dij
where denotes the number of clusters ci denotes the number of
objects in cluster and dij is the pairwise effective dissimilarity between objects and
as computed by a preprocessing step
The idea of this preprocessing step is to define distances between objects by considering
certain paths through the total object set The natural formalization of such path problems
is to represent the objects as a graph consider a connected graph d0 with
vertices the objects and symmetric nonnegative edge weights d0ij on the edge the
original dissimilarities Let us denote by Pij all paths from vertex to vertex In order
to make those objects more similar which are connected by bridges of other objects
we define for each path Pij the effective dissimilarity dpij between and connected
by as the maximum weight on this path the weakest link on this path The total
dissimilarity between vertices and is then defined as the minimum of all path-specific
effective dissimilarities dpij
dij min
max
p?Pij
Figure illustrates the definition of the effective dissimilarity If the objects are in the same
cluster their pairwise effective dissimilarities will be small fig If the two objects
belong to two different clusters however all paths contain at least one large dissimilarity
and the resulting effective dissimilarity will be large fig Note that single outliers
as in fig do not affect the basic structure in the path-based distances A problem
dij
dij
dij
Figure Effective dissimilarities If objects belong to the same high-density region ij is small
If they are in different regions dij is larger To regions connected by a bridge
can only occur if the point density along a bridge between the two clusters is as high as
the density on the backbone of the clusters see In such a case however the points
belonging to the bridge can hardly be considered as outliers The reader should notice
that the single linkage algorithm does not posses the robustness properties since it will
separate the three most distant outlier objects in example from the remaining data but
it will not detect the dominant structure
Summarizing the above model we formalize the path-based clustering problem as
INPUT A symmetric matrix d0ij of nonnegative pairwise dissimilarities between objects with zero diagonal elements
QUESTION Find clusters by minimizing PC where the matrix represents the
effective dissimilarities derived from by eq
The Connectivity Kernel
In this section we show that the effective dissimilarities induce a Mercer kernel on the
weighted graph G. The Mercer property will then allow us to derive several approximation
results for the P-hard pairwise clustering problem in section
Definition A metric is called ultra-metric if it satisfies the condition dij
max(dik dkj for all distinct
Theorem The dissimilarities defined by induce an ultra-metric on G.
Proof We have to check the axioms of a metric distance measure plus the restricted triangle inequality dij max(dik dkj dij since the weights are nonnegative
dij dji since we consider symmetric weights iii dii follows immediately from
definition The proof of the restricted triangle inequality follows by contradiction
suppose there exists a triple for which dij max(dik dkj This situation however
contradicts the above definition of dij in this case there exists a path from to over
the weakest link of which is shorter than dij Equation then implies that dij must be
smaller or equal to max(dik dkj
Definition A metric is embeddable if there exists a set of vectors
Rp such that for all pairs kxi dij
A proof for the following lemma has been given in
Lemma For every ultra-metric is embeddable
Now we are considered with a realization of such an embedding We introduce the notion
of a centralized matrix Let be an matrix and let In n1 en
where
en is a n-vector of ones and In the identity matrix We define the
centralized as QP Q.
The following lemma for a proof see characterizes embeddings
Lemma Given a metric is embeddable iff QDQ is negative
semi)definite
The combination of both lemmata yields the following theorem
Theorem For the distance matrix defined in the setting of theorem the matrix
Dc with QDQ is a Gram matrix or Mercer kernel It contains dot products
between a set of vectors with squared Euclidean distances kxi dij
Proof Since is ultra-metric is embeddable by lemma and is negative
semi)definite by lemma Thus Dc is positive semi)definite As any positive
semi)definite matrix defines a Gram matrix or Mercer kernel Since scij is a dotproduct between two vectors and the squaredEuclidean distance
between and
is defined by kxi scii scjj 2scij dcii dcjj 2dcij With the definition
of the centralized distances
it can be seen easily that all but one term namely the original
distance cancel out dcii dcjj 2dcij dij
Approximation Results
Pairwise clustering is known to be P-hard To our knowledge there is no polynomial
time approximation algorithm known for the general case of pairwise clustering For our
special case in which the data are transformed into effective dissimilarities however we
now present a polynomial time approximation scheme
A Polynomial Time Approximation Scheme Let us first consider the computation of the
effective dissimilarities D. Despite the fact that the path-based distance is a minimum over
all paths from to the whole distance matrix can be computed in polynomial time
Lemma The path-based dissimilarity matrix defined by equation can be computed
in running time log
Proof The computation of the connectivity kernel matrix is an extention of Kruskal?s minimum spanning tree algorithm We start with clusters each containing one single object In each iteration step the two clusters Ci and Cj are merged with minimal costs
dij minp?Ci q?Cj d0pq where d0pq is the edge weight on the input graph The link dij
gives the effective dissimilarity of all objects in Ci to all objects in Cj To proof this one
can consider the case where dij is not the effective dissimilarity between Ci and Cj Then
there exists a path over some other cluster Ck where all objects on this path have a smaller
weight implying the existence of another pair of clusters with smaller merging costs The
running time is log for the spanning tree algorithm on the the complete input graph
and additional for filling all elements in the matrix D.
Let us now discuss the clustering step Recall first the problem of K-means clustering
given vectors Rp the task is to partition the vectors in such a way
that the squared Euclidean distance to the cluster centroids is minimized The objective
function for K-means is given by
PK
KM i:ci
where
j:cj
Minimizing the K-means objective function for squared Euclidean distances is P-hard
if the dimension of the vectors is growing with
Lemma There exists a polynomial time approximation scheme PTAS for KM in arbitrary dimensions and for fixed K.
Proof In Ostrovsky and Rabani presented a PTAS for K-means
Using this approximation lemma we are able to proof the existence of a PTAS for pairwise
data clustering using the distance defined by
Theorem for distances defined by there exists a PTAS for PC
Proof By lemma the dissimilarity matrix can be computed in polynomial time By
theorem we can find vectors Rp with dij For
squared Euclidean distances however there is an algebraic identity between PC
and KM By lemma there exists a PTAS for KM and thus for PC
A 2-approximation by Kernel PCA. While the existence of a PTAS is an interesting
theoretical approximation result it does not automatically follow that a PTAS can be used
in a constructive way to derive practical algorithms Taking such a practical viewpoint
we now consider another weaker approximation result from which however an efficient
algorithm can be designed easily From the fact that we can define a connectivity kernel
matrix we can use kernel PCA to reduce the data dimension The vectors are projected
on the first principle components Diagonalization of the centered kernel matrix leads to
with an orthogonal matrix vn containing the eigenvectors of
and a diagonal matrix diag(?1 containing the corresponding eigenvalues
on its diagonal Assuming now that the eigenvalues are in descending
Pp order
the data are projected on the first eigenvectors x0i vji
Theorem Embedding the path-based distances into RK by kernel PCA and enumerating
over all possible Voronoi partitions yields an O(nK algorithm which approximates
path-based clustering within a constant factor of
Proof The solution of the K-means cost function induces a Voronoi partition on the
dataset If the dimension of the data is kept fix the number of different Voronoi partitions is at most O(nKp and they can be enumerated in O(nKp+1 time Further if
the embedding dimension is chosen as K-means in RK is a 2-approximation algorithm for K-means in Combining both results we arrive at a 2-approximation
algorithm with running time O(nK
Heuristics without approximation guarantees The running time of the 2-approximation
algorithm may still be too large for many applications therefore we will refer to two heuristic optimization methods without approximation guarantees Instead of enumerating all
possible Voronoi partitions one can simply partition the data with the fast classical Kmeans algorithm In one sweep it assigns each object to the nearest centroid while keeping
all other object assignments fixed Then the centroids are relocated according to the new
assignments Since the running time grows linear with the data dimension it is useful to
first embed the data in dimensions which leads us to a functional which optimal solution
is even in the worst case within a factor of two of the desired solution as we know from
the above approximation results In this reduced space the K-means heuristics is applied
with the hope that there exist only few local minima in the low-dimensional subspace
As a second heuristic one can apply Ward?s method which is an agglomerative optimization
of the K-means objective function.1 It starts with clusters each containing one object
and in each step the two clusters that minimize the K-means objective function are merged
Ward?s method produces a cluster hierarchy For applications of this method see figure
Experiments
We first compare our method with the classical single linkage algorithm on artificial data
consisting of three noisy spirals see figure Our main concern in these experiments is
the robustness against noise in the data Figure shows the dendrogram produced by
single linkage The leaves of the tree are the objects of figure For better visualization
of the tree structure the bar diagrams below the tree show the labels of the three cluster
It has been shown in that Ward?s method is an optimization heuristics for Due to the
equivalence of and KM in our special case this property carries over to K-means
Figure Comparison to other clustering methods Mean shift clustering Spectral Clustering
Connectivity kernel clustering Color images at http://www.inf.ethz.ch/?befische/nips03
Figure Hierarchical Clustering Solutions for example Single Linkage Ward?s method
with connectivity kernel applied to embedded objects in dimensions Ward?s method after
kernel PCA embedding in dimensions
solution as drawn in fig The height of the inner nodes depicts the merging costs for
two subtrees Each level of the hierarchy is one cluster solution It is obvious that the main
parts of the spiral arms are found but the objects drawn on the right side are separated
from the rest of the cluster The respective objects are the outliers that are separated in the
highest hierarchical levels of the algorithm We conclude that for small single linkage
has the tendency to separates single outlier objects from the data
By way of the connectivity kernel we can transform the original dyadic data to
dimensional vectorial data To show comparable results for the connectivity kernel we
apply Ward?s method to the embedded vectors Figure shows the cluster hierarchy
for Ward?s method in the full space of dimensions Opposed to the single linkage
results the main structure of the spiral arms has been successfully found in the hierarchy
corresponding to the three cluster solution Below the three cluster lever the tree appears
to be very noisy It should also be noticed that the costs of the three cluster solution are
not much larger as the costs of the four cluster solution indicating that the three cluster
solution does not form a distinctly separated hierarchical level
Figure demonstrates that more distinctly separated levels can be found after applying
kernel PCA and embedding the objects into a low-dimensional space here dimensions
Ward?s method is then applied to the embedded objects One can see that the coarse struc
ture of the tree has been preserved while the costs of cluster solutions for have been
shrunken towards zero We conclude that PCA has the effect of de-noising the hierarchical
tree leading to a more robust agglomerative algorithm
Now we compare our results to other recently published clustering techniques that have
been designed to extract elongated structures Mean shift clustering computes a trajectory of vectors towards the gradient of the underlying probability density The probability
distribution is estimated with a density estimation kernel a Gaussian kernel The trajectories starting at each point in the feature space converge at the local maxima of the
probability distribution Mean shift clustering is only applicable to finite dimensional vector spaces because it implicitly involves density estimation A potential shortcoming of
mean-shift clustering is the following if the modes of the distribution have multiple local
maxima as in the spiral arm example there does not exist any kernel bandwidth to
successfully separate the data according to the underlying structure In figure the best
result for mean shift clustering is drawn For smaller values of the spiral arms are further
subdivided into additional clusters and for a larger bandwidth values the result becomes
more and more similar to compactness-based criteria like K-means
Spectral methods have become quite popular in the last years Usually the Laplacian
matrix based on a Gaussian kernel is computed By way of PCA the data are embedded
in a low dimensional space The K-means algorithm on the embedded data then gives
the resulting partition It has also been proposed to project the data on the unit sphere
before applying K-means Spectral clustering with a Gaussian kernel is known to be able
to separate nested circles but we observed that it has severe problems to extract the noisy
spiral arms see In spectral clustering the kernel width is a free parameter which has
to be selected correctly If is too large spectral clustering becomes similar to standard
K-means and fails to extract elongated structures If on the other hand is too small the
algorithm becomes increasingly sensitive to outliers in the sense that it has the tendency to
separate single outlier objects
Our approach to clustering with the connectivity kernel however could successfully extract
the three spiral arms as can be seen in figure The reader should notice that this method
does not require the user to preselect any kernel parameter
Figure Example from the USPS dataset Training example of digits and embedded in two
dimensions Ground truth labels K-means labels and clustering with connectivity kernel
In a last experiment we show the advantages of our method compared to a parameter-free
compactness criterion K-means on the problem of clustering digits and from the
USPS digits dataset Figure shows the clustering result of our method using the connectivity kernel The digit gray-value images of the USPS dataset are interpreted as
vectors and projected on the two leading principle components In figure the ground
truth solution is drawn Figure shows the partition by directly applying K-means clustering and figure shows the result produced by our method Compared to the ground
truth solution path-based clustering succeeded in extracting the elongated structures resulting in a very small error of only mislabeled digits The compactness-based Kmeans method on the other hand produces clearly suboptimal clusters with an error rate
of
Conclusion
In this paper we presented a clustering approach that is based on path-based distances in the
input graph In a first step elongated structures are transformed into compact ones which
in the second step are partitioned by the compactness-based pairwise clustering method
We showed that the transformed distances induce a Mercer kernel which in turn allowed
us to derive a polynomial time approximation scheme for the generally P-hard pairwise
clustering problem Moreover Mercers property renders it possible to embed the data
into low-dimensional subspaces by Kernel PCA. These embeddings form the basis for an
efficient 2-approximation algorithm and also for de-noising the data to robustify fast
agglomerative optimization heuristics Compared to related methods like single linkage
mean shift clustering and spectral clustering our method has been shown to successfully
overcome the problem of sensitivity to outlier objects while being capable of extracting
nested elongated structures Our method does not involve any free kernel parameters which
we consider to be a particular advantage over both mean shift and spectral clustering

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3957-penalized-principal-component-regression-on-graphs-for-analysis-of-subnetworks.pdf

Penalized Principal Component Regression on
Graphs for Analysis of Subnetworks
George Michailidis
Department of Statistics and EECS
University of Michigan
Ann Arbor MI
gmichail@umich.edu
Ali Shojaie
Department of Statistics
University of Michigan
Ann Arbor MI
shojaie@umich.edu
Abstract
Network models are widely used to capture interactions among component of
complex systems such as social and biological To understand their behavior it
is often necessary to analyze functionally related components of the system corresponding to subsystems Therefore the analysis of subnetworks may provide
additional insight into the behavior of the system not evident from individual
components We propose a novel approach for incorporating available network
information into the analysis of arbitrary subnetworks The proposed method offers an efficient dimension reduction strategy using Laplacian eigenmaps with
Neumann boundary conditions and provides a flexible inference framework for
analysis of subnetworks based on a group-penalized principal component regression model on graphs Asymptotic properties of the proposed inference method
as well as the choice of the tuning parameter for control of the false positive rate
are discussed in high dimensional settings The performance of the proposed
methodology is illustrated using simulated and real data examples from biology
Introduction
Simultaneous analysis of groups of system components with similar functions or subsystems has
recently received considerable attention This problem is of particular interest in high dimensional
biological applications where changes in individual components may not reveal the underlying
biological phenomenon whereas the combined effect of functionally related components could improve the efficiency and interpretability of results This idea has motivated the method of gene set
enrichment analysis GSEA along with a number of related methods The main premise
of this method is that by assessing the significance of sets rather than individual components
genes interactions among them can be preserved and more efficient inference methods can be
developed A different class of models and

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4878-understanding-dropout.pdf

Understanding Dropout
Peter Sadowski
Department of Computer Science
University of California Irvine
Irvine CA
pjsadows@ics.uci.edu
Pierre Baldi
Department of Computer Science
University of California Irvine
Irvine CA
pfbaldi@uci.edu
Abstract
Dropout is a relatively new algorithm for training neural networks which relies
on stochastically dropping out neurons during training in order to avoid the
co-adaptation of feature detectors We introduce a general formalism for studying dropout on either units or connections with arbitrary probability values and
use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks For deep neural networks the averaging properties
of dropout are characterized by three recursive equations including the approximation of expectations by normalized weighted geometric means We provide
estimates and bounds for these approximations and corroborate the results with
simulations Among other results we also show how dropout performs stochastic
gradient descent on a regularized error function
Introduction
Dropout is an algorithm for training neural networks that was described at NIPS In its
most simple form during training at each example presentation feature detectors are deleted with
probability and the remaining weights are trained by backpropagation All weights
are shared across all example presentations During prediction the weights are divided by two
The main motivation behind the algorithm is to prevent the co-adaptation of feature detectors or
overfitting by forcing neurons to be robust and rely on population behavior rather than on the
activity of other specific units In dropout is reported to achieve state-of-the-art performance on
several benchmark datasets It is also noted that for a single logistic unit dropout performs a kind of
geometric averaging over the ensemble of possible subnetworks and conjectured that something
similar may occur also in multilayer networks leading to the view that dropout may be an economical
approximation to training and using a very large ensemble of networks
In spite of the impressive results that have been reported little is known about dropout from a
theoretical standpoint in particular about its averaging regularization and convergence properties
Likewise little is known about the importance of using whether different values of can
be used including different values for different layers or different units and whether dropout can be
applied to the connections rather than the units Here we address these questions
Dropout in Linear Networks
It is instructive to first look at some of the properties of dropout in linear networks since these can
be studied exactly in the most general setting of a multilayer feedforward network described by an
underlying acyclic graph The activity in unit of layer can be expressed as
Sih
XX
l<h
hl
wij
Sj
with Sj0 Ij
where the variables denote the weights and I the input vector Dropout applied to the units can be
expressed in the form
Sih
XX
l<h
hl
wij
Sj
with
Sj0 Ij
where jl is a gating Bernoulli variable with plj Throughout this paper we assume
that the variables jl are independent of each other independent of the weights and independent of
the activity of the units Similarly dropout applied to the connections leads to the random variables
Sih
XX
l<h
hl hl
wij Sj
with
Sj0 Ij
For brevity in the rest of this paper we focus exclusively on dropout applied to the units but all the
results remain true for the case of dropout applied to the connections with minor adjustments
For a fixed input vector the expectation of the activity of all the units taken over all possible realizations of the gating variables hence all possible subnetworks is given by
E(Sih
XX
l<h
hl
wij
pj E(Sjl
for
with E(Sj0 Ij in the input layer In short the ensemble average can easily be computed by
hl
hl
feedforward propagation in the original network simply replacing the weights wij
by wij
pj
Dropout in Neural Networks
Dropout in Shallow Neural Networks
Pn
Consider first a single logistic unit with inputs ce??S and wj Ij
To achieve the greatest level of generality we assume that the unit produces different outputs
OP
Om corresponding to different sums S1 Sm with different probabilities P1 Pm
Pm In the most relevant case these outputs and these sums are associated with the
2n possible subnetworks of the unit The probabilities P1 Pm could be generated for
instance by using Bernoulli gating variables although this isP
not necessary for this derivation It is
useful to define the following four quantities the mean
Pi Oi the mean of the complements
E0
Pi Oi the weighted geometric
mean
GM OiPi and the
weighted geometric mean of the complements G0 Oi Pi We also define the normalized
weighted geometric mean GM G0 We can now prove the key averaging theorem
for logistic functions
Pi
Pi
Pi
GM Om
To prove this result we write
GM Om
Oi
The logistic function satisfies the identity ce
and thus
GM Om
Pi Si
ce
ce
Thus in the case of Bernoulli gating variables we can compute the WP
GM over all possible
dropout configurations by simple forward propagation by GM wj pj Ij A similar
result is true also for normalized exponential transfer functions Finally one can also show that
the only class of functions that satisfy GM are the constant functions and the
logistic functions
Dropout in Deep Neural Networks
We can now deal with the most interesting case of deep feedforward networks of sigmoidal units
described by a set of equations of the form
Oih Sih
XX
hl
wij
Oj
with
Oj0 Ij
l<h
where Oih is the output of unit in layer Dropout on the units can be described by
Oih Sih
XX
l<h
hl
wij
Oj
with Oj0 Ij
using the Bernoulli selector variables jl For each sigmoidal unit
GM Oih
Oi
Oi
Oih
where ranges over all possible subnetworks Assume for now that the GM provides a
good approximation to the expectation this point will be analyzed in the next section Then the
averaging properties of dropout are described by the following three recursive equations First the
approximation of means by NWGMs
E(Oih GM Oih
Second using the result of the previous section the propagation of expectation symbols
GM Oih ih E(Sih
And third using the linearity of the expectation with respect to sums and to products of independent
random variables
E(Sih
XX
l<h
hl
wij
pj E(Ojl
Equations and 13 are the fundamental equations explaining the averaging properties of the
dropout procedure The only approximation is of course Equation which is analyzed in the next
section If the network contains linear units then Equation is not necessary for those units and
their average can be computed exactly In the case of regression with linear units in the top layers
this allows one to shave off one layer of approximations The same is true in binary classification
by requiring the output layer to compute directly the GM of the ensemble rather than the
expectation It can be shown that for any error function that is convex up the error of the mean
weighted geometric mean and normalized weighted geometric mean of an ensemble is always less
than the expected error of the models
Equation is exact if and only if the numbers Oih are identical over all possible subnetworks
Thus it is useful to
measure
the consistency C(Oi I of neuron in layer for input I by using
the variance ar Oih taken over all subnetworks and their distribution when the input I is
fixed The larger the variance is the less consistent the neuron is and the worse we can expect
the approximation in Equation to be Note that for a random variable in the variance
cannot exceed anyway This is because ar(O
This measure can also be averaged over a training set or a test set
Given the results of the previous sections the network can also include linear units or normalized exponential units
The Dropout Approximation
Given a set of numbers O1 Om between and with probabilities P1 PM corresponding
to the outputs of a sigmoidal neuron for a fixed input and different subnetworks we are primarily
interested in the approximation of by GM The GM provides a good approximation
because we show below that to a first order of approximation GM and G. Furthermore there are formulae in the literature for bounding the error in terms of the consistency
the Cartwright and Field inequality However one can suspect that the GM provides
even a better approximation to than the geometric mean For instance if the numbers Oi satisfy
Oi consistently low then
and therefore
G0
G0
This is proven by applying Jensen?s inequality to the function for It is
also known as the Ky Fan inequality
To get even better results one must consider a second order approximation For this we write
Oi with Thus we have and ar(O
Using a Taylor expansion
pi pi
pi
pi
4pi pj R3
i<j
where R3 is the remainder and
R3
pi
ui
where ui Expanding the product gives
pi
pi
By symmetry we have
G0
Oi pi ar(O R3
where R3 is the higher order remainder Neglecting the remainder and writing and
ar(O we have
E?V
G0
and
G+G
2V
G+G
2V
Thus to a second order the differences between the mean and the geometric mean and the normalized geometric means satisfy
E?G?V
and
G+G
2V
and
G0
G+G
2V
Finally it is easy to check that the factor 2V is always less or equal to In addition
we always have with equality achieved only for Bernoulli variables Thus
G0
and
G0
2V
2V
The first inequality is optimal in the sense that it is attained in the case of a Bernoulli variable
with expectation and intuitively the second inequality shows that the approximation error is
always small regardless of whether is close to or In short the NWGM provides a
very good approximation to better than the geometric mean G. The property is always true to
a second order of approximation and it is exact when the activities are consistently low or when
GM since the latter implies GM E. Several additional properties of the
dropout approximation including the extension to rectified linear units and other transfer functions
are studied in
Dropout Dynamics
Dropout performs gradient descent on-line with respect to both the training examples and the ensemble of all possible subnetworks As such and with the appropriately decreasing learning rates
it is almost surely convergent like other forms of stochastic gradient descent To further
understand the properties of dropout it is again instructive to look at the properties of the gradient
in the linear case
Single Linear Unit
In the case of a single linear unit consider the two error functions EEN and ED associated with
the ensemble of all possible subnetworks and the network with dropout For a single input I these
are defined by
EEN
OEN
pi Ii
OD
Ii
ED
We use a single training input I for notational simplicity otherwise the errors of each training
example can be combined additively The learning gradient is given by
EEN
OEN
OEN
OEN pi Ii
ED
OD
OD
OD Ii Ii Ii2
wj Ii Ij
The dropout gradient is a random variable and we can take its expectation A short calculation yields
ED
EEN
EEN
pi pi
Ii2 ar(?i
Thus remarkably in this case the expectation of the gradient with dropout is the gradient of the
regularized ensemble error
EEN
1X
I ar(?i
The regularization term is the usual weight decay or Gaussian prior term based on the square of the
weights to prevent overfitting Dropout provides immediately the magnitude of the regularization
term which is adaptively scaled by the inputs and by the variance of the dropout variables Note that
pi is the value that provides the highest level of regularization
Single Sigmoidal Unit
The previous result generalizes to a sigmoidal unit ce??S trained to minimize
the relative entropy error log log(1 O)). In this case
ED
Ii
The terms and Ii are not independent but using a Taylor expansion with the GM approximation gives
EEN
ED
SEN Ii2 ar(?i
with SEN wj pj Ij Thus as in the linear case the expectation of the dropout gradient is approximately the gradient of the ensemble network regularized by weight decay terms with the proper
adaptive coefficients A similar analysis can be carried also for a set of normalized exponential
units and for deeper networks
Learning Phases and Sparse Coding
During dropout learning we can expect three learning phases At the beginning of learning when
the weights are typically small and random the total input to each unit is close to for all the units
and the consistency is high the output of the units remains roughly constant across subnetworks
and equal to with As learning progresses activities tend to move towards or
and the consistency decreases for a given input the variance of the units across subnetworks
increases As the stochastic gradient learning procedure converges the consistency of the units
converges to a stable value
Finally for simplicity assume that dropout
is applied only in layer where the units have an output
hl
of the form Oih Sih and Sih l<h wij
Oj For a fixed input Ojl is a constant since dropout
is not applied to layer Thus
ar(Sih
hl
wij
Ojl plj plj
l<h
under the usual assumption that the selector variables jl are independent of each other Thus
ar(Sih depends on three factors Everything else being equal it is reduced by Small weights
which goes together with the regularizing effect of dropout Small activities which shows that
dropout is not symmetric with respect to small or large activities Overall dropout tends to favor
small activities and thus sparse coding and Small close to or large close to values of the
dropout probabilities plj Thus values plj maximize the regularization effect but may also lead
to slower convergence to the consistent state Additional results and simulations are given in
Simulation Results
We use Monte Carlo simulation to partially investigate the approximation framework embodied by
the three fundamental dropout equations and 13 the accuracy of the second-order approximation and bounds in Equations and 22 and the dynamics of dropout learning We experiment
with an MNIST classifier of four hidden layers that replicates the
results in using the Pylearn2 and Theano software libraries[12 The network is trained with
a dropout probability of in the input and in the four hidden layers For fixed weights and
a fixed input Monte Carlo simulations are used to estimate the distribution of activity
in each neuron Let be the activation under the deterministic setting with the weights scaled
appropriately
The left column of Figure confirms empirically that the second-order approximation in Equation
and the bound in Equation 22 are accurate The right column of Figure shows the difference between the true ensemble average and the prediction-time neuron activity This difference
grows very slowly in the higher layers and only for active neurons
Figure Left The difference GM it?s second-order approximation in Equation
and the bound from Equation 22 plotted for four hidden layers and a typical fixed input Right
The difference between the true ensemble average and the final neuron prediction
Next we examine the neuron consistency during dropout training Figure 2a shows the three phases
of learning for a typical neuron In Figure we observe that the consistency does not decline in
higher layers of the network
One clue into how this happens is the distribution of neuron activity As noted in and section
above dropout training results in sparse activity in the hidden layers Figure This increases the
consistency of neurons in the next layer
The three phases of learning For a particular input a typical active neuron red starts out
with low variance experiences a large increase in
variance during learning and eventually settles to
some steady constant value In contrast a typical
inactive neuron blue quickly learns to stay silent
Shown are the mean with and percentiles
Consistency does not noticeably decline in the upper layers Shown here are the mean Std(O for active
neurons after training in each layer along
with the and percentiles
Figure
Figure In every hidden layer of a dropout trained network the distribution of neuron activations
is sparse and not symmetric These histograms were totalled over a set of random inputs

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5059-compete-to-compute.pdf

Compete to Compute
Rupesh Kumar Srivastava Jonathan Masci Sohrob Kazerounian
Faustino Gomez J?rgen Schmidhuber
IDSIA USI-SUPSI
Manno?Lugano Switzerland
rupesh jonathan sohrob tino juergen}@idsia.ch
Abstract
Local competition among neighboring neurons is common in biological neural networks In this paper we apply the concept to gradient-based
backprop-trained artificial multilayer NNs. NNs with competing linear
units tend to outperform those with non-competing nonlinear units and
avoid catastrophic forgetting when training sets change over time
Introduction
Although it is often useful for machine learning methods to consider how nature has arrived
at a particular solution it is perhaps more instructive to first understand the functional
role of such biological constraints Indeed artificial neural networks which now represent
the state-of-the-art in many pattern recognition tasks not only resemble the brain in a
superficial sense but also draw on many of its computational and functional properties
One of the long-studied properties of biological neural circuits which has yet to fully impact
the machine learning community is the nature of local competition That is a common
finding across brain regions is that neurons exhibit on-center off-surround organization
and this organization has been argued to give rise to a number of interesting
properties across networks of neurons such as winner-take-all dynamics automatic gain
control and noise suppression
In this paper we propose a biologically inspired mechanism for artificial neural networks
that is based on local competition and ultimately relies on local winner-take-all LWTA
behavior We demonstrate the benefit of LWTA across a number of different networks and
pattern recognition tasks by showing that LWTA not only enables performance comparable
to the state-of-the-art but moreover helps to prevent catastrophic forgetting common
to artificial neural networks when they are first trained on a particular task then abruptly
trained on a new task This property is desirable in continual learning wherein learning
regimes are not clearly delineated Our experiments also show evidence that a type of
modularity emerges in LWTA networks trained in a supervised setting such that different
modules subnetworks respond to different inputs This is beneficial when learning from
multimodal data distributions as compared to learning a monolithic model
In the following we first discuss some of the relevant neuroscience background motivating
local competition then show how we incorporate it into artificial neural networks and
how LWTA as implemented here compares to alternative methods We then show how
LWTA networks perform on a variety of tasks and how it helps buffer against catastrophic
forgetting
Neuroscience Background
Competitive interactions between neurons and neural circuits have long played an important
role in biological models of brain processes This is largely due to early studies showing that
many cortical and sub-cortical hippocampal and cerebellar regions of the
brain exhibit a recurrent on-center off-surround anatomy where cells provide excitatory
feedback to nearby cells while scattering inhibitory signals over a broader range Biological
modeling has since tried to uncover the functional properties of this sort of organization
and its role in the behavioral success of animals
The earliest models to describe the emergence of winner-take-all WTA behavior from local
competition were based on Grossberg?s shunting short-term memory equations which
showed that a center-surround structure not only enables WTA dynamics but also contrast
enhancement and normalization Analysis of their dynamics showed that networks with
slower-than-linear signal functions uniformize input patterns linear signal functions preserve
and normalize input patterns and faster-than-linear signal functions enable WTA dynamics
Sigmoidal signal functions which contain slower-than-linear linear and faster-than-linear
regions enable the supression of noise in input patterns while contrast-enhancing normalizing and storing the relevant portions of an input pattern form of soft WTA The
functional properties of competitive interactions have been further studied to show among
other things the effects of distance-dependent kernels inhibitory time lags development of self-organizing maps and the role of WTA networks in attention
Biological models have also been extended to show how competitive interactions in spiking
neural networks give rise to soft WTA dynamics as well as how they may be efficiently
constructed in VLSI
Although competitive interactions and WTA dynamics have been studied extensively in the
biological literature it is only more recently that they have been considered from computational or machine learning perspectives For example Maas showed that feedforward
neural networks with WTA dynamics as the only non-linearity are as computationally powerful as networks with threshold or sigmoidal gates and networks employing only soft
WTA competition are universal function approximators Moreover these results hold even
when the network weights are strictly positive?a finding which has ramifications for our
understanding of biological neural circuits as well as the development of neural networks
for pattern recognition The large body of evidence supporting the advantages of locally
competitive interactions makes it noteworthy that this simple mechanism has not provoked
more study by the machine learning community Nonetheless networks employing local
competition have existed since the late and along with serve as a primary
inspiration for the present work More recently maxout networks have leveraged locally
competitive interactions in combination with a technique known as dropout to obtain
the best results on certain benchmark problems
Networks with local winner-take-all blocks
This section describes the general network architecture with locally competing neurons
The network consists of blocks which are organized into layers Figure Each block
bi contains computational units neurons and produces an output vector
determined by the local interactions between the individual neuron activations in the block
yij g(h1i h2i hni
where is the competition/interaction function encoding the effect of local interactions
in each block and hji is the activation of the j-th neuron in block computed by
hi wij
where is the input vector from neurons in the previous layer wij is the weight vector of
neuron in block and is a generally non-linear activation function The output
activations are passed as inputs to the next layer In this paper we use the winner-take-all
interaction function inspired by studies in computational neuroscience In particular we
use the hard winner-take-all function
hi if hji hki
yij
otherwise
In the case of multiple winners ties are broken by index precedence In order to investigate the capabilities of the hard winner-take-all interaction function in isolation
Figure A Local Winner-Take-All LWTA network with blocks of size two showing the
winning neuron in each block shaded for a given input example Activations flow forward
only through the winning neurons errors are backpropagated through the active neurons
Greyed out connections do not propagate activations The active neurons form a subnetwork
of the full network which changes depending on the inputs
identity is used for the activation function in equation The difference between this
Local Winner Take All LWTA network and a standard multilayer perceptron is that no
non-linear activation functions are used and during the forward propagation of inputs local
competition between the neurons in each block turns off the activation of all neurons except
the one with the highest activation During training the error signal is only backpropagated
through the winning neurons
In a LWTA layer there are as many neurons as there are blocks active at any one time for
a given input pattern1 We denote a layer with blocks of size as LWTA-n For each input
pattern presented to a network only a subgraph of the full network is active the highlighted neurons and synapses in figure Training on a dataset consists of simultaneously
training an exponential number of models that share parameters as well as learning which
model should be active for each pattern Unlike networks with sigmoidal units where all of
the free parameters need to be set properly for all input patterns only a subset is used for
any given input so that patterns coming from very different sub-distributions can potentially be modelled more efficiently through specialization This modular property is similar
to that of networks with rectified linear units ReLU which have recently been shown to
be very good at several learning tasks links with ReLU are discussed in section
Comparison with related methods
Max-pooling
Neural networks with max-pooling layers have been found to be very useful especially
for image classification tasks where they have achieved state-of-the-art performance
These layers are usually used in convolutional neural networks to subsample the representation obtained after convolving the input with a learned filter by dividing the representation
into pools and selecting the maximum in each one Max-pooling lowers the computational
burden by reducing the number of connections in subsequent convolutional layers and adds
translational/rotational invariance
However there is always the possibility that the winning neuron in a block has an activation
of exactly zero so that the block has no output
before
after
before
after
max-pooling
LWTA
Figure Max-pooling LWTA In max-pooling each group of neurons in a layer
has a single set of output weights that transmits the winning unit?s activation in this
case to the next layer the layer activations are subsampled In an LWTA block
there is no subsampling The activations flow into subsequent units via a different set of
connections depending on the winning unit
At first glance the max-pooling seems very similar to a WTA operation however the two
differ substantially there is no downsampling in a WTA operation and thus the number of
features is not reduced instead the representation is sparsified figure
Dropout
Dropout can be interpreted as a model-averaging technique that jointly trains several
models sharing subsets of parameters and input dimensions or as data augmentation when
applied to the input layer This is achieved by probabilistically omitting dropping units from a network for each example during training so that those neurons do not
participate in forward/backward propagation Consider hypothetically training an LWTA
network with blocks of size two and selecting the winner in each block at random This
is similar to training a neural network with a dropout probability of Nonetheless the
two are fundamentally different Dropout is a regularization technique while in LWTA the
interaction between neurons in a block replaces the per-neuron non-linear activation
Dropout is believed to improve generalization performance since it forces the units to learn
independent features without relying on other units being active During testing when
propagating an input through the network all units in a layer trained with dropout are
used with their output weights suitably scaled In an LWTA network no output scaling is
required A fraction of the units will be inactive for each input pattern depending on their
total inputs Viewed this way WTA is restrictive in that only a fraction of the parameters
are utilized for each input pattern However we hypothesize that the freedom to use different
subsets of parameters for different inputs allows the architecture to learn from multimodal
data distributions more accurately
Rectified Linear units
Rectified Linear Units ReLU are simply linear neurons that clamp negative activations to
zero if otherwise ReLU networks were shown to be useful for
Restricted Boltzmann Machines outperformed sigmoidal activation functions in deep
neural networks and have been used to obtain the best results on several benchmark
problems across multiple domains
Consider an LWTA block with two neurons compared to two ReLU neurons where and
are the weighted sum of the inputs to each neuron Table shows the outputs y1 and
y2 in all combinations of positive and negative and for ReLU and LWTA neurons
For both ReLU and LWTA neurons and are passed through as output in half of the
possible cases The difference is that in LWTA both neurons are never active or inactive at
the same time and the activations and errors flow through exactly one neuron in the block
For ReLU neurons being inactive saturation is a potential drawback since neurons that
Table Comparison of rectified linear activation and LWTA-2
Positive
Positive
Negative
Positive
Negative
Negative
Positive
Negative
Negative
Positive
Positive
Negative
ReLU neurons
y1
y2
LWTA neurons
y1
y2
do not get activated will not get trained leading to wasted capacity However previous
work suggests that there is no negative impact on optimization leading to the hypothesis
that such hard saturation helps in credit assignment and as long as errors flow through
certain paths optimization is not affected adversely Continued research along these
lines validates this hypothesis but it is expected that it is possible to train ReLU
networks better
While many of the above arguments for and against ReLU networks apply to LWTA networks there is a notable difference During training of an LWTA network inactive neurons
can become active due to training of the other neurons in the same block This suggests
that LWTA nets may be less sensitive to weight initialization and a greater portion of the
network?s capacity may be utilized
Experiments
In the following experiments LWTA networks were tested on various supervised learning
datasets demonstrating their ability to learn useful internal representations without utilizing
any other non-linearities In order to clearly assess the utility of local competition no special
strategies such as augmenting data with transformations noise or dropout were used We
also did not encourage sparse representations in the hidden layers by adding activation
penalties to the objective function a common technique also for ReLU units Thus our
objective is to evaluate the value of using LWTA rather than achieving the absolute best
testing scores Blocks of size two are used in all the experiments.2
All networks were trained using stochastic gradient descent with mini-batches learning rate
lt and momentum mt at epoch given by
if min
otherwise
min
Tt mf if
mt
pf
if
where is the learning rate annealing factor min is the lower learning rate limit and
momentum is scaled from mi to mf over epochs after which it remains constant at
mf weight decay was used for the convolutional network section and max-norm
normalization for other experiments This setup is similar to that of
Permutation Invariant MNIST
The MNIST handwritten digit recognition task consists of images
training test of the digits centered by their center of mass In the permutation
invariant setting of this task we attempted to classify the digits without utilizing the 2D
structure of the images every digit is a vector of pixels The last examples in the
training set were used for hyperparameter tuning The model with the best hyperparameter
setting was trained until convergence on the full training set Mini-batches of size were
To speed up our experiments the Gnumpy and CUDAMat libraries were used
Table Test set errors on the permutation invariant MNIST dataset for methods without
data augmentation or unsupervised pre-training
Activation
Sigmoid
ReLU
ReLU dropout in hidden layers
LWTA-2
Test Error
Table Test set errors on MNIST dataset for convolutional architectures with no data
augmentation Results marked with an asterisk use layer-wise unsupervised feature learning
to pre-train the network and global fine tuning
Architecture
2-layer CNN layer MLP
2-layer ReLU CNN layer LWTA-2
3-layer ReLU CNN
2-layer CNN layer MLP
3-layer ReLU CNN stochastic pooling
3-layer maxout dropout
Test Error
used the pixel values were rescaled to no further preprocessing The best model
obtained which gave a test set error of consisted of three LWTA layers of
blocks followed by a softmax layer To our knowledge this is the best reported
error without utilizing implicit/explicit model averaging for this setting which does not use
deformations/noise to enhance the dataset or unsupervised pretraining Table compares
our results with other methods which do not use unsupervised pre-training The performance
of LWTA is comparable to that of a ReLU network with dropout in the hidden layers Using
dropout in input layers as well lower error rates of using ReLU and using
maxout have been obtained
Convolutional Network on MNIST
For this experiment a convolutional network CNN was used consisting of filters in
the first layer followed by a second layer of with and 32 maps respectively and
ReLU activation Every convolutional layer is followed by a max-pooling operation
We then use two LWTA-2 layers each with 64 blocks and finally a softmax output
layer A weight decay of was found to be beneficial to improve generalization The
results are summarized in Table along with other state-of-the-art approaches which do not
use data augmentation for details of convolutional architectures see
Amazon Sentiment Analysis
LWTA networks were tested on the Amazon sentiment analysis dataset since ReLU units
have been shown to perform well in this domain We used the balanced subset of the
dataset consisting of reviews of four categories of products Books DVDs Electronics and
Kitchen appliances The task is to classify the reviews as positive or negative The dataset
consists of positive and negative reviews in each category The text of each review
was converted into a binary feature vector encoding the presence or absence of unigrams
and bigrams Following the most frequent vocabulary entries were retained as
features for classification We then divided the data into equal balanced folds and
tested our network with cross-validation reporting the mean test error over all folds ReLU
activation was used on this dataset in the context of unsupervised learning with denoising
autoencoders to obtain sparse feature representations which were used for classification We
trained an LWTA-2 network with three layers of blocks each in a supervised setting to
directly classify each review as positive or negative using a 2-way softmax output layer We
obtained mean accuracies of Books DVDs Electronics and Kitchen
giving a mean accuracy of compared to reported in for denoising
autoencoders using ReLU and unsupervised pre-training to find a good initialization
Table LWTA networks outperform sigmoid and ReLU activation in remembering dataset
P1 after training on dataset P2.
Testing error on P1
After training on P1
After training on P2
LWTA
Sigmoid
ReLU
Implicit long term memory
This section examines the effect of the LWTA architecture on catastrophic forgetting That
is does the fact that the network implements multiple models allow it to retain information
about dataset A even after being trained on a different dataset To test for this implicit
long term memory the MNIST training and test sets were each divided into two parts P1
containing only digits and P2 consisting of the remaining digits
Three different network architectures were compared three LWTA layers each with
blocks of size three layers each with sigmoidal neurons and three layers each
of ReLU neurons All networks have a 5-way softmax output layer representing the
probability of an example belonging to each of the five classes All networks were initialized
with the same parameters and trained with a fixed learning rate and momentum
Each network was first trained to reach a log-likelihood error on the P1 training set
This value was chosen heuristically to produce low test set errors in reasonable time for
all three network types The weights for the output layer corresponding to the softmax
classifier were then stored and the network was trained further starting with new initial
random output layer weights to reach the same log-likelihood value on P2. Finally the
output layer weights saved from P1 were restored and the network was evaluated on the
P1 test set The experiment was repeated for different initializations
Table shows that the LWTA network remembers what was learned from P1 much better
than sigmoid and ReLU networks though it is notable that the sigmoid network performs
much worse than both LWTA and ReLU While the test error values depend on the learning
rate and momentum used LWTA networks tended to remember better than the ReLU
network by about a factor of two in most cases and sigmoid networks always performed
much worse Although standard network architectures are known to suffer from catastrophic
forgetting we not only show here for the first time that ReLU networks are actually quite
good in this regard and moreover that they are outperformed by LWTA We expect this
behavior to manifest itself in competitive models in general and to become more pronounced
with increasingly complex datasets The neurons encoding specific features in one dataset
are not affected much during training on another dataset whereas neurons encoding common
features can be reused Thus LWTA may be a step forward towards models that do not
forget easily
Analysis of subnetworks
A network with a single LWTA-m of blocks consists of mN subnetworks which can be
selected and trained for individual examples while training over a dataset After training
we expect the subnetworks consisting of active neurons for examples from the same class to
have more neurons in common compared to subnetworks being activated for different classes
In the case of relatively simple datasets like MNIST it is possible to examine the number
of common neurons between mean subnetworks which are used for each class To do this
which neurons were active in the layer for each example in a subset of examples were
recorded For each class the subnetwork consisting of neurons active for at least of the
examples was designated the representative mean subnetwork which was then compared to
all other class subnetworks by counting the number of neurons in common
Figure 3a shows the fraction of neurons in common between the mean subnetworks of each
pair of digits Digits that are morphologically similar such as and have subnetworks
with more neurons in common than the subnetworks for digits and or and
which are intuitively less similar To verify that this subnetwork specialization is a result
of training we looked at the fraction of common neurons between all pairs of digits for the
untrained
trained
Digits
Fraction of neurons in common
Digits
MNIST digit pairs
Figure Each entry in the matrix denotes the fraction of neurons that a pair of MNIST
digits has in common on average in the subnetworks that are most active for each of the
two digit classes The fraction of neurons in common in the subnetworks of each of the
55 possible digit pairs before and after training
same examples both before and after training Figure Clearly the subnetworks
were much more similar prior to training and the full network has learned to partition its
parameters to reflect the structure of the data
Conclusion and future research directions
Our LWTA networks automatically self-modularize into multiple parameter-sharing subnetworks responding to different input representations Without significant degradation of
state-of-the-art results on digit recognition and sentiment analysis LWTA networks also
avoid catastrophic forgetting thus retaining useful representations of one set of inputs even
after being trained to classify another This has implications for continual learning agents
that should not forget representations of parts of their environment when being exposed to
other parts We hope to explore many promising applications of these ideas in the future
Acknowledgments
This research was funded by EU projects WAY NeuralDynamics and NASCENCE additional funding from ArcelorMittal

<<----------------------------------------------------------------------------------------------------------------------->>

title: 298-language-induction-by-phase-transition-in-dynamical-recognizers.pdf

Language Induction by Phase Transition
in Dynamical Recognizers
Jordan B. Pollack
Laboratory for AI Research
The Ohio State University
Columbus,OH
pollack@cis.ohio-state.edu
Abstract
A higher order recurrent neural network architecture learns to recognize and
generate languages after being trained on categorized exemplars Studying
these networks from the perspective of dynamical systems yields two
interesting discoveries First a longitudinal examination of the learning
process illustrates a new form of mechanical inference Induction by phase
transition A small weight adjustment causes a bifurcation in the limit
behavior of the network This phase transition corresponds to the onset of the
network's capacity for generalizing to arbitrary-length strings Second a
study of the automata resulting from the acquisition of previously published
languages indicates that while the architecture is NOT guaranteed to find a
minimal finite automata consistent with the given exemplars which is an
NP-Hard problem the architecture does appear capable of generating nonregular languages by exploiting fractal and chaotic dynamics I end the paper
with a hypothesis relating linguistic generative capacity to the behavioral
regimes of non-linear dynamical systems
Introduction
I expose a recurrent high-order back-propagation network to both positive and negative
examples of boolean strings and report that although the network does not find the
minimal-description finite state automata for the languages which is NP-Hard Angluin
it does induction in a novel and interesting fashion and searches through a
hypothesis space which theoretically is not constrained to machines of finite state These
results are of import to many related neural models currently under development
Elman Giles Servan-Schreiber and relates ultimately to
the question of how linguistic capacity can arise in nature
Although the transitions among states in a finite-state automata are usually thought of as
being fully specified by a table a transition function can also be specified as a
mathematical function of the current state and the input It is known from McCulloch
Pitts that even the most elementary modeling assumptions yield finite-state
Pollack
control and it is worth reiterating that any network with the capacity to compute arbitrary
boolean functions say as logical sums of products lapedes farber how nets white
homik can be used recurrently to implement arbitrary finite state machines
From a different point of view a recurrent network with a state evolving across units
can be considered a k-dimensional discrete-time continuous-space dynamical tystem
with a precise initial condition and a state space in Z. a subspace of The
governing function F. is parameterized by a set of weights W. and merely computes the
next state from the current state and input a finite sequence of patterns
representing tokens from some alphabet
Zk(t FW(Zk(t).YjCt
If we view one of the dimensions of this system say Za. as an acceptance dimension
we can define the language accepted by such a Dynamical Recognizer as all strings of
input tokens evolved from the precise initial state for which the accepting dimension of
the state is above a certain threshold In network terms one output unit would be
subjected to a threshold test after processing a sequence of input patterns
The first question to ask is how can such a dynamical system be constructed or taught to
accept a particular language The weights in the network individually do not correspond
directly to graph transitions or to phrase structure rules The second question to ask is
what sort of generative power can be achieved by such systems
The Model
To begin to answer the question of learning I now present and elaborate upon my earlier
work on Cascaded Networks pollack which were used in a recurrent fashion to
learn parity depth-limited parenthesis balancing and to map between word sequences
and proposition representations pollack A Cascaded Network is a wellcontrolled higher-order connectionist architecture to which the back-propagation
technique of weight adjustment Rumelhart can be applied Basically it
consists of two subnetworks The function network is a standard feed-forward network
with or without hidden layers However the weights are dynamically computed by the
linear context network whose outputs are mapped in a fashion to the weights of the
function net Thus the input pattern to the context network is used to multiplex the the
function computed which can result in simpler learning tasks
When the outputs of the function network are used as inputs to context network a system
can be built which learns to produce specific outputs for variable-length sequences of
inputs Because of the multiplicative connections each input is in effect processed by a
different function Given an initial context and a sequence of inputs
the network computes a sequence of state vectors by
dynamically changing the set of weights Wij(t Without hidden units the forward pass
computation is
Wij(t
Wijk
Zi(t
geL Wij(t Yj(t
Language Induction by Phase ll'ansition in Dynamical Recognizers
where is the usual sigmoid function used in back-propagation system
In previous work I assumed that a teacher could supply a consistent and generalizable
desired-state for each member of a large set of strings which was a significant
overconstraint In learning a two-state machine like parity this did not matter as the I-bit
state fully determines the output However for the case of a higher-dimensional system
we know what the final output of a system should be but we don't care what its state
should be along the way
Jordan showed how recurrent back-propagation networks could be trained with
don't care conditions If there is no specific preference for the value of an output unit
for a particular training example simply consider the error term for that unit to be
This will work as long as that same unit receives feedback from other examples When
the don't-cares line up the weights to those units will never change My solution to this
problem involves a backspace unrolling the loop only once After propagating the errors
determined on only a subset of the weights from the acceptance unit Za
aE
da za(n za(n Yj(n
aE
The error on the remainder of the weights aaE
w"k
from the penuIOrnate
Orne step
aE
azk(n-l
a
a is calculated
using values
aE
aWajk awa/n
aE
aE
aWij(n-l aZi(n-l Yj(n-l
aE
aE
aWijk
aWij(n-l
This is done in batch epoch style for a set of examples of varying lengths
Induction as Phase Transition
In initial studies of learning the simple regular language of odd parity I expected the
recognizer to merely implement exclusive or with a feedback link It turns out that this
is not quite enough Because termination of back-propagation is usually defined as a
error logical is above recurrent use of this logic tends to a limit point In
other words mere separation of the exemplars is no guarantee that the network can
recognize parity in the limit Nevertheless this is indeed possible as illustrated by
illustrated below In order to test the limit behavior of a recognizer we can observe its
response to a very long characteristic string For odd parity the string requires an
alternation of responses
A small cascaded network composed of a function net and a context net
Pollack
requiring 18 weights was was trained on odd parity of a small set of strings up to length
At each epoch the weights in the network were saved in a file Subsequently each
configuration was tested in its response to the first characteristic strings In figure I
each vertical column corresponding to an epoch contains points between and
Initially all strings longer than length are not distinguished From cycle the
network is improving at separating finite strings At cycle 85 the network undergoes a
bifurcation where the small change in weights of a single epoch leads to a phase
transition from a limit point to a limit cycle This phase transition is so adaptive to the
classification task that the network rapidly exploits iL
iilU hli!iIi!ili
I
Figure
A bifurcation diagram showing the response of the parity-learner to the first
characteristic strings over epochs of training
I wish to stress that this is a new and very interesting form of mechanical induction and
reveals that with the proper perspective non-linear connectionist networks are capable of
much more complex behavior than hill-climbing Before the phase transition the
machine is in principle not capable of performing the serial parity task after the phase
transition it is The similarity of learning through a flash of insight to biological change
through a punctuated evolution is much more than coincidence
BenChmarking Results
Tomita performed elegant experiments in inducing finite automata from positive
and negative evidence using hillclim bing in the space of 9-state automata Each case was
defined by two sets of boolean strings accepted by and rejected by the regular languages
For the simple low dimensional dynamical systems usually studied the knob or cootrol parameter for
such a bifurcation diagram is a scalar variable here the control parameter is the entire vcc:tor of
weights in the network and bade-propagation turns the knobl
Language Induction by Phase ltansition in Dynamical Recognizers
listed below
no odd zero strings after odd strings
no triples of zeros
pairwise an even sum of 01 and lO's
number of number ofO's 3n
Rather than inventing my own training data or sampling these languages for a wellformed training set I ran all Tomita training environments as given on a sequential
cascaded network of a I-input 4-output function network with bias weights to set and
a 3-input 8-output context network with bias using a learning rate was of and a
momentum to Termination was when all accepted strings returned output bits above
and rejected strings below
Of Tomita's cases all but cases and converged without a problem in several
hundred epochs Case would not converge and kept treating a negative case as correct
because of the difficulty for my architecture to induce a trap state I had to modify the
training set by added reject strings and in order to overcome this problem
Case took several restarts and thousands of cycles to converge cause unknown The
complete experimental data is available in a longer report pollack
Because the states are in a box of low dimension,3 we can view these machines
graphically to gain some understanding of how the state space is being arranged Based
upon some intitial studies of parity my initial hypothesis was that a set of clusters would
be found organized in some geometric fashion an em bedding of a finite state
machine into a finite dimensional geometry such that each token'S transitions would
correspond to a simple transformation of space Graphs of the states visited by all
possible inputs up to length for the Tomita test cases are shown in figure Each
figure contains points but often they overlap
The images and are what were expected clumps of points which closely map to
states of equivalent FSA's Images and have limit ravine's which can each be
considered states as well
Discussion
However the state spaces and of the dynamical recognizers for Tomita cases
and are interesting because theoretically they are infinite state machines where
the states are not arbitrary or random requiring an infinite table of transitions but are
constrained in a powerful way by mathematical principle In other words the complexity
is in the dynamics not in the specifications weights
In thinking about such a principle consider other systems in which extreme observed
complexity emerges from algorithmic simplicity plus computational power It is
It can be argued that other FSA inducing methods get around this problem by presupposing rather than
learning trap states
One reason I have succeeded in such low dimensional induction is because my architecture is a Mealy
rather than Moore Machine Lee Giles Personal Communication
Pollack
A
Figure Images of the state-spaces
for the benchmark cases Each
image contains points
corresponding to the states of all
boolean strings up to length
Language Induction by Phase 1ransition in Dynamical Recognizers
interesting to note that by eliminating the sigmoid and commuting the Yj and Zk terms
the forward equation for higher order recurrent networks with is identical to the generator
of an Iterated Function System IFS Bamsley Thus my figures of statespaces which emerge from the projection of
into are of the same class of
mathematical object as Barnsley's fractal attractors the widely reproduced fern
Using the method of Grassberger Procaccia the correlation dimension of the
attractor in Figure was found to be about
The link between work in complex dynamical systems and neural networks is wellestablished both on the neurobiological level Skarda Freeman and on the
mathematical level Derrida Meir Huberman Hogg Kurten
Smolensky This paper expands a theme from an earlier proposal to link them at
the cognitive level pollack
There is an interesting formal question which has been brought out in the work of
Wolfram and others on the universality of cellular automata and more recently in
the work of Crutchfield Young on the descriptive complexity of bifurcating
systems What is the relationship between complex dynamics of neural systems and
traditional measures of computational complexity From this work and other supporting
evidence I venture the following hypothesis
is an Attractor
The state-space limit of a dynamical recognizer as
which is cut by a threshold similar decision function The complexity of
the language recognized is regular if the cut falls between disjoint limit
points or cycles context-free if it cuts a self-similar recursive region and
context-sensitive if it cuts a chaotic pseudo-random region
Acknowledgements
This research has been partially supported by the Office of Naval Research under
grant NOOO

<<----------------------------------------------------------------------------------------------------------------------->>

title: 162-mapping-classifier-systems-into-neural-networks.pdf

49
Mapping Classifier Systems
Into Neural Networks
Lawrence Davis
BBN Laboratories
BBN Systems and Technologies Corporation
Moulton Street
Cambridge MA
January
Abstract
Classifier systems are machine learning systems incotporating a genetic algorithm as the learning mechanism Although they respond to inputs that neural
networks can respond to their internal structure representation fonnalisms and
learning mechanisms differ marlcedly from those employed by neural network researchers in the same sorts of domains As a result one might conclude that these
two types of machine learning fonnalisms are intrinsically different This is one
of two papers that taken together prove instead that classifier systems and neural
networks are equivalent In this paper half of the equivalence is demonstrated
through the description of a transfonnation procedure that will map classifier
systems into neural networks that are isomotphic in behavior Several alterations
on the commonly-used paradigms employed by neural networlc researchers are
required in order to make the transfonnation worlc These alterations are noted
and their appropriateness is discussed The paper concludes with a discussion of
the practical import of these results and with comments on their extensibility
Introd uction
Classifier systems are machine learning systems that have been developed since the
by Holland and more recently by other members of the genetic algorithm
research community as well Classifier systems are varieties of genetic algorithms
algorithms for optimization and learning Genetic algorithms employ techniques
inspired by the process of biological evolution in order to evolve better and better
IThis paper has benefited from discussions with Wayne Mesard Rich Sutton Ron Williams Stewart
Wilson Craig Shaefer David Montana Gil Syswerda and other members of BARGAIN the Boston Area
Research Group in Genetic Algorithms and Inductive Networks
Davis
individuals that are taken to be solutions to problems such as optimizing a function
traversing a maze etc For an explanation of genetic algorithms the reader is
referred to Goldberg Classifier systems receive messages from an external
source as inputs and organize themselves using a genetic algorithm so that they will
learn to produce responses for internal use and for interaction with the external
source
This paper is one of two papers exploring the question of the fonnal relationship
between classifier systems and neural networks As normally employed the two sorts
of algorithms are probably distinct although a procedure for translating the operation
of neural networks into isomorphic classifier systems is given in Belew and Gherrity
The technique Belew and Gherrity use does not include the conversion of the
neural network learning procedure into the classifier system framework and it appears
that the technique will not support such a conversion Thus one might conjecture that
the two sorts of machine learning systems employ learning techniques that cannot be
reconciled although if there were a subsumption relationship Belew and Gherrity's
result suggests that the set of classifier systems might be a superset of the set of
neural networks
The reverse conclusion is suggested by consideration of the inputs that each sort
of learning algorithm processes When viewed as black boxes both mechanisms
for learning receive inputs carry out self-modifying procedures and produce outputs
The class of inputs that are traditionally processed by classifier systems the class
of bit strings of a fixed length is a subset of the class of inputs that have been
traditionally processed by neural networks Thus it appears that classifier systems
operate on a subset of the inputs that neural networks can process when viewed as
mechanisms that can modify their behavior
In fact both these impressions are correct One can translate classifier systems
into neural networks preserving their learning behavior and one can translate neural
networks into classifier systems again preserving learning behavior In order to do
so however some specializations of each sort of algorithm must be made This
paper deals with the translation from classifier systems to neural networks and with
those specializations of neural networks that are required in order for the translation
to take place The reverse translation uses quite different techniques and is treated
in Davis
The following sections contain a description of classifier systems a description of
the transformation operator discussions of the extensibility of the proof comments
on some issues raised in the course of the proof and conclusions
Classifier Systems
A classifier system operates in the context of an environment that sends messages to
the system and provides it with reinforcement based on the behavior it displays A
classifier system has two components a message list and a population of rule-like
entities called classifiers Each message on the message list is composed of bits and
Mapping Classifier Systems Into Neural Networks
each has a pointer to its source messages may be generated by the environment or
by a classifier Each classifier in the population of classifiers has three components
a match string made up of the characters and for don't care a message
made up of the characters and and a strength The top-level description of a
classifier system is that it contains a population of production rules that attempt to
match some condition on the message list thus classifying some input and post
their message to the message list thus potentially affecting the envirorunent or other
classifiers Reinforcement from the environment is used by the classifier system to
modify the strengths of its classifiers Periodically a genetic algorithm is invoked
to create new classifiers which replace certain members of the classifier set For
an explanation of classifier systems their potential as machine learning systems and
their formal properties the reader is referred to Holland al
Let us specify these processing stages more precisely A classifier system operates
by cycling through a fixed list of procedures In order these procedures are
Message List Processing Clear the message list Post the envirorunental
messages to the message list Post messages to the message list from classifiers
in the post set of the previous cycle Implement envirorunental reinforcement by
analyzing the messages on the message list and altering the strength of classifiers in
the post set of the previous cycle
Form the Bid Set. Determine which classifiers match a message in the
message list A classifier matches a message if each bit in its match field matches its
corresponding message bit A matches a a matches a I and a matches either
bit The set of all matching classifiers forms the current bid set Implement bid
taxes by subtracting a portion of the strength of each classifier in the bid set Add
the strength taken from to the strength of the classifier or classifiers that posted
messages matched by in the prior step
Form the Post Set. If the bid set is larger than the maximum post set size
choose classifiers stochastically to post from the bid set weighting them in proportion
to the magnitude of their bid taxes The set of classifiers chosen is the post set
Reproduction Reproduction generally does not occur on every cycle When it
does occur these steps are carried out Create children from parents Use
crossover and/or mutation chOOSing parents stochastically but favoring the strongest
ones Crossover and mutation are two of the operators used in genetic algorithms
Set the strength of each child to equal the average of the strength of that child's
parents Note this is one of many ways to set the strength of a new classifier
The transformation will work in analogous ways for each of them Remove
members of the classifier population and add the new children to the classifier
population
Mapping Classifiers Into Classifier Networks
The mapping operator that I shall describe maps each classifier into a classifier
network Each classifier network has links to environmental input units links to
51
52
Davis
other classifier networks and match post and message units The weights on the
links leading to a match node and leaving a post node are related to the fields in
the match and message lists in the classifier An additional link is added to provide
a bias term for the match node Note it is assumed here that the environment
posts at most one message per cycle Modifications to the transfonnation operator to
accommodate multiple environmental messages are described in the final comments
of this paper
Given a classifier system CS with classifiers each matching and sending messages of length we can construct an isomorphic neural network composed of
classifier networks in the following way For each classifier in CS we construct its
corresponding classifier network composed of match nodes I post node and
message nodes One match node the environmental match node has links to inputs
from the environment Each of the other match nodes is linked to the message and
post node of another classifier network The reader is referred to Figure for an
example of such a transformation
Each match node in a classifier network has incoming links The weights
on the first links are derived by applying the following transformation to the
elements of c's match field is associated with weight is associated with
weight and is associated with weight The weight of the final link is set to
where is the number of links with weight Thus a classifier with
match field would have an associated network with weights on the links
leading to its match nodes of and A classifier with match field
would have weights of and
The weights on the links to each message node in the classifier network are set
to equal the corresponding element of the classifier's message field Thus if the
message field of the classifier were the weights on the links leading to the
three message nodes in the corresponding classifier network would be I and
The weights on all other links in the classifier network are set to
Each node in a classifier network uses a threshold function to determine its activation level Match nodes have thresholds All other nodes have thresholds
If a node's threshold is exceeded the node's activation level is set to If not
it is set to
Each classifier network has an associated quantity called strength that may be
altered when the network is run during the processing cycle described below
A cycle of processing of a classifier system CS maps onto the following cycle of
processing in a set of classifier networks
Message List Processing Compute the activation level of each message
node in CS. If the environment supplies reinforcement on this cycle divide that
reinforcement by the number of post nodes that are currently active plus if the
environment posted a message on the preceding cycle and add the quotient to the
strength of each active post node's classifier network If there is a message on this
cycle from the environment map it onto the first environment nodes so that each
node associated with a is off and each node associated with a is on Tum the final
environmental node on If there is no environmental message turn all environmental
Mapping Classifier Systems Into Neural Networks
nodes off
Form the Bid Set. Compute the activation level of each match node in
each classifier network Compute the activation level of each bid node in each
classifier network the set of classifier networks with an active bid node is the bid
set Subtract a fixed proportion of the strength of each classifier network cn in
the bid set Add this amount to the strength of those networks connected to an active
match node in cn Strength given to the environment passes out of the system
Form the Post Set. If the bid set is larger than the maximum post set size
choose networks stochastically to post from the bid set weighting them in proportion
to the magnitude of their bid taxes The set of networks chosen is the post set This
might be viewed as a stochastic n-winners-take-all procedure
Reproduction If this is a cycle on which reproduction would occur in the
classifier system carry out its analog in the neural network in the following way
Create children from parents Use crossover and/or mutation choosing parents
stochastically but favoring the strongest ones The ternary alphabet composed of
I and is used instead of the classifier alphabet of and After each operator
is applied the final member of the match list is set to Write over the
weights on the match links and the message links of classifier networks to match
the weights in the children Choose networks to be re-weighted stochastically so that
the weakest ones are most likely to be chosen Set the strength of each re-weighted
classifier network to be the average of the strengths of its parents
It is simple to show that a classifier network match node will match a message
in just those cases in which its associated classifier matched a message There are
three cases to consider If the original match character was a then it matched any
message bit The corresponding link weight is set to so the state of the node it
comes from will not affect the activation of the match node it goes to If the original
match character was a then its message bit had to be a for the message to be
matched The corresponding link weight is set to and we see by inspection of the
weight on the final link the match node threshold and the fact that no other type
of link has a positive weight that every link with weight I must be connected to an
active node for the match node to be activated Finally the link weight corresponding
to a is set to If any of these links is connected to a node that is active then the
effect is that of turning off a node connected to a link with weight and we have
just seen that this will cause the match node to be inactive
Given this correspondence in matching behavior one can verify that a set of
classifier networks associated with a classifier system has the following properties
During each cycle of processing of the classifier system a classifier is in the bid set
in just those cases in which its associated networlc has an active bid node Assuming
that both systems use the same randomizing technique initialized in the same way
the classifier is in the post set in just those cases when the network is in the post
set Finally the parents that are chosen for reproduction are the transform as of those
chosen in the classifier system and the children produced are the transformations of
the classifier system parents The two systems are isomorphic in operation assuming
that they use the same random number generator
53
54
Davis
CLASSIFIER NETWORK
strength
CLASSIFIER NETWORK
strength
MESSAGE
NODES
TH
POST
NODES
TH
MATCH
NODES
TH
ENVIRONMENT
INPUT
NODES
Figure Result of mapping a classifier system
witH two classifiers into a neural network
Classifier has match field message field
and strength 49 Classifier has match field
message field and strength
Mapping Classifier Systems Into Neural Networks
Concluding Comments
The transfonnation procedure described above will map a classifier system into a
neural network that operates in the same way There are several points raised by the
techniques used to accomplish the mapping In closing let us consider four of them
First there is some excess complexity in the classifier networks as they are shown
here In fact one could eliminate all non-environmental match nodes and their
links since one can determine whenever a classifier network is reweigh ted whether it
matches the message of each other classifier network in the system If so one could
introduce a link directly from the post node of the other classifier networlc to the post
node of the new networlc The match nodes to the environment are necessary as
long as one cannot predict what messages the environment will post Message nodes
are necessary as long as messages must be sent out to the environment If not they
and their incoming links could be eliminated as well These simplifications have not
been introduced here because the extensions discussed next require the complexity
of the current architecture
Second on the genetic algorithm side the classifier system considered here is an
extremely simple one There are many extensions and refinements that have been
used by classifier system researchers I believe that such refinements can be handled
by expanded mapping procedures and by modifications of the architecture of the
classifier networks To give an indication of the way such modifications would go
let us consider two sample cases The first is the case of an environment that may
produce multiple messages on each cycle To handle multiple messages an additional
link must be added to each environmental match node with weight set to the match
node's threshold This link will latch the match node An additional match node
with links to the environment nodes must be added and a latched counting node
must be attached to it Given these two architectural modifications the cycle is
modified as follows During the message matching cycle a series of subcycles is
carried out one for each message posted by the environment In each subcycle an
environmental message is input and each environmental match node computes its
activation The environmental match nodes are latched so that each will be active
if it matched any environmental message The count nodes will record how many
were matched by each classifier network When bid strength'is paid from a classifier
network to the posters of messages that it matched the divisor is the number of
environmental messages matched as recorded by the count node plus the number
of other messages matched Finally when new weights are written onto a classifier
network's links they are written onto the match node connected to the count node
as well A second sort of complication is that of pass-through bits bits that
are passed from a message that is matched to the message that is posted This
sort of mechanism can be implemented in an obvious fashion by complicating the
structure of the classifier networlc Similar complications are produced by considering
multiple-message matching negation messages to effectors and so forth It is an
open question whether all such cases can be handled by modifying the architecture
and the mapping operator but I have not yet found one that cannot be so handled
55
56
Davis
Third the classifier networks do not use the sigmoid activation functions that support hill-c~bing techniques such as back-propagation Further they are recurrent
networks rather than strict feed-forwanl networks Thus one might wonder whether
the fact that one can carry out such transformations should affect the behavior of
researchers in the field This point is one that is taken up at greater length in the
companion paper My conclusion there is that several of the techniques imported into
the neural network domain by the mapping appear to improve the performance of neural networks These include tracking strength in order to guide the learning process
using genetic operators to modify the network makeup and using population-level
measurements in order to determine what aspects of a network to use in reproduction
The reader is referred to Montana and Davis for an example of the benefits
to be gained by employing these techniques
Finally one might wonder what the import of this proof is intended to be In
my view this proof and the companion proof suggest some exciting ways in which
one can hybridize the learning techniques of each field One such approach and its
successful application to a real-world problem is characterized in Montana and Davis

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3714-a-game-theoretic-approach-to-hypergraph-clustering.pdf

A Game-Theoretic Approach to
Hypergraph Clustering
Samuel Rota Bul`o
Marcello Pelillo
University of Venice Italy
srotabul,pelillo}@dsi.unive.it
Abstract
Hypergraph clustering refers to the process of extracting maximally coherent
groups from a set of objects using high-order rather than pairwise similarities
Traditional approaches to this problem are based on the idea of partitioning the
input data into a user-defined number of classes thereby obtaining the clusters as
a by-product of the partitioning process In this paper we provide a radically different perspective to the problem In contrast to the classical approach we attempt
to provide a meaningful formalization of the very notion of a cluster and we show
that game theory offers an attractive and unexplored perspective that serves well
our purpose Specifically we show that the hypergraph clustering problem can
be naturally cast into a non-cooperative multi-player clustering game whereby
the notion of a cluster is equivalent to a classical game-theoretic equilibrium concept From the computational viewpoint we show that the problem of finding the
equilibria of our clustering game is equivalent to locally optimizing a polynomial
function over the standard simplex and we provide a discrete-time dynamics to
perform this optimization Experiments are presented which show the superiority
of our approach over state-of-the-art hypergraph clustering techniques
Introduction
Clustering is the problem of organizing a set of objects into groups or clusters in a way as to have
similar objects grouped together and dissimilar ones assigned to different groups according to some
similarity measure Unfortunately there is no universally accepted formal definition of the notion
of a cluster but it is generally agreed that informally a cluster should correspond to a set of objects
satisfying two conditions an internal coherency condition which asks that the objects belonging to
the cluster have high mutual similarities and an external incoherency condition which states that
the overall cluster internal coherency decreases by adding to it any external object
Objects similarities are typically expressed as pairwise relations but in some applications higherorder relations are more appropriate and approximating them in terms of pairwise interactions can
lead to substantial loss of information Consider for instance the problem of clustering a given set of
d-dimensional Euclidean points into lines As every pair of data points trivially defines a line there
does not exist a meaningful pairwise measure of similarity for this problem However it makes
perfect sense to define similarity measures over triplets of points that indicate how close they are
to being collinear Clearly this example can be generalized to any problem of model-based point
pattern clustering where the deviation of a set of points from the model provides a measure of their
dissimilarity The problem of clustering objects using high-order similarities is usually referred to
as the hypergraph clustering problem
In the machine learning community there has been increasing interest around this problem Zien
and co-authors propose two approaches called clique expansion and star expansion respectively Both approaches transform the similarity hypergraph into an edge-weighted graph whose
edge-weights are a function of the hypergraph?s original weights This way they are able to tackle
the problem with standard pairwise clustering algorithms Bolla defines a Laplacian matrix for
an unweighted hypergraph and establishes a link between the spectral properties of this matrix and
the hypergraph?s minimum cut Rodr`?guez achieves similar results by transforming the hypergraph into a graph according to clique expansion and shows a relationship between the spectral
properties of a Laplacian of the resulting matrix and the cost of minimum partitions of the hypergraph Zhou and co-authors generalize their earlier work on regularization on graphs and
define a hypergraph normalized cut criterion for a k-partition of the vertices which can be achieved
by finding the second smallest eigenvector of a normalized Laplacian This approach generalizes
the well-known Normalized cut pairwise clustering algorithm Finally in we find another
work based on the idea of applying a spectral graph partitioning algorithm on an edge-weighted
graph which approximates the original edge-weighted hypergraph It is worth noting that the approaches mentioned above are devised for dealing with higher-order relations but can all be reduced
to standard pairwise clustering approaches A different formulation is introduced in where
the clustering problem with higher-order super-symmetric similarities is cast into a nonnegative
factorization of the closest hyper-stochastic version of the input affinity tensor
All the afore-mentioned approaches to hypergraph clustering are partition-based Indeed clusters
are not modeled and sought directly but they are obtained as a by-product of the partition of the input
data into a fixed number of classes This renders these approaches vulnerable to applications where
the number of classes is not known in advance or where data is affected by clutter elements which
do not belong to any cluster as in figure/ground separation problems Additionally by partitioning
clusters are necessarily disjoint sets although it is in many cases natural to have overlapping clusters
two intersecting lines have the point in the intersection belonging to both lines
In this paper following we offer a radically different perspective to the hypergraph clustering problem Instead of insisting on the idea of determining a partition of the input data and hence
obtaining the clusters as a by-product of the partitioning process we reverse the terms of the problem and attempt instead to derive a rigorous formulation of the very notion of a cluster This allows
one in principle to deal with more general problems where clusters may overlap and/or outliers
may get unassigned We found that game theory offers a very elegant and general mathematical
framework that serves well our purposes The basic idea behind our approach is that the hypergraph
clustering problem can be considered as a multi-player non-cooperative clustering game Within
this context the notion of a cluster turns out to be equivalent to a classical equilibrium concept from
evolutionary game theory as the latter reflects both the internal and external cluster conditions
alluded to before We also show that there exists a correspondence between these equilibria and
the local solutions of a polynomial linearly-constrained optimization problem and provide an algorithm for finding them Experiments on two standard hypergraph clustering problems show the
superiority of the proposed approach over state-of-the-art hypergraph clustering techniques
Basic notions from evolutionary game theory
Evolutionary game theory studies models of strategic interactions called games among large
numbers of anonymous agents A game can be formalized as a triplet where
is the set of players involved in the game is the set of pure
strategies the terminology of game-theory available to each player and is the payoff
function which assigns a payoff to each strategy profile the ordered set of pure strategies
played by the individuals The payoff function is assumed to be invariant to permutations of the
strategy profile It is worth noting that in general games each player may have its own set of strategies and own payoff function For a comprehensive introduction to evolutionary game theory we
refer to
By undertaking an evolutionary setting we assume to have a large population of non-rational agents
which are randomly matched to play a game Agents are considered non-rational because each of them initially chooses a strategy from which will be always played when selected
for the game An agent who selected strategy is called i-strategist Evolution in the population takes place because we assume that there exists a selection mechanism which by analogy with
a Darwinian process spreads the fittest strategies in the population to the detriment of the weakest
one which will in turn be driven to extinction We will see later in this work a formalization of such
a selection mechanism
The state of the population at a given time can be represented as a n-dimensional vector
where represents the fraction of i-strategists in the population at time The set of all possible
states describing a population is given by
x?R
and for all
i?S
which is called standard simplex In the sequel we will drop the time

<<----------------------------------------------------------------------------------------------------------------------->>

