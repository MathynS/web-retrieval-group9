query sentence: Sentiment of tweets on twitter
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 5275-global-belief-recursive-neural-networks.pdf

Global Belief Recursive Neural Networks
Romain Paulus Richard Socher
MetaMind
Palo Alto CA
romain,richard}@metamind.io
Christopher D. Manning
Stanford University
Serra Mall
Stanford CA
manning@stanford.edu
Abstract
Recursive Neural Networks have recently obtained state of the art performance on
several natural language processing tasks However because of their feedforward
architecture they cannot correctly predict phrase or word labels that are determined by context This is a problem in tasks such as aspect-specific sentiment
classification which tries to for instance predict that the word Android is positive
in the sentence Android beats iOS We introduce global belief recursive neural
networks GB-RNNs which are based on the idea of extending purely feedforward neural networks to include one feedbackward step during inference This
allows phrase level predictions and representations to give feedback to words We
show the effectiveness of this model on the task of contextual sentiment analysis We also show that dropout can improve RNN training and that a combination
of unsupervised and supervised word vector representations performs better than
either alone The feedbackward step improves F1 performance by over the
standard RNN on this task obtains state-of-the-art performance on the SemEval
challenge and can accurately predict the sentiment of specific entities
Introduction
Models of natural language need the ability to compose the meaning of words and phrases in order
to understand complex utterances such as facts multi-word entities sentences or stories There has
recently been a lot of work extending single word semantic vector spaces to compositional models of bigrams or phrases of arbitrary length 28 24 Work in this
area so far has focused on computing the meaning of longer phrases in purely feedforward types
of architectures in which the meaning of the shorter constituents that are being composed is not
altered However a full treatment of semantic interpretation cannot be achieved without taking into
consideration that the meaning of words and phrases can also change once the sentence context is
observed Take for instance the sentence in The Android?s screen is better than the iPhone?s
All current recursive deep learning sentiment models would attempt to classify the phrase The
Android?s screen or than the iPhone?s both of which are simply neutral The sentiment of the overall sentence is undefined it depends on which of the entities the user of the sentiment analysis cares
about Generally for many analyses of social media text users are indeed most interested in the
sentiment directed towards a specific entity or phrase
In order to solve the contextual classification problem in general and aspect-specific sentiment classification in particular we introduce global belief recursive neural networks GB-RNN These models
generalize purely feedforward recursive neural networks RNNs by including a feedbackward step
at inference time The backward computation uses the representations from both steps in its recursion and allows all phrases to update their prediction based on the global context of the sentence
Unlike recurrent neural networks or window-based methods the important context can be many
Part of this research was performed while the author was at Stanford University
Android
beats
iOS
Figure Illustration of the problem of sentiment classification that uses only the phrase to be labeled
and ignores the context The word Android is neutral in isolation but becomes positive in context
words away from the phrase that is to be labeled This will allow models to correctly classify that in
the sentence of Android is described with positive sentiment and iOS was not Neither was
possible to determine only from their respective phrases in isolation
In order to validate the GB-RNN?s ability to contextually disambiguate sentiment on real text we
use the Twitter dataset and annotations from Semeval Challenge Task The GB-RNN outperforms both the standard RNN and all other baselines as well the winner of the Sentiment competition of SemEval showing that it can successfully make use of surrounding context
Related Work
Neural word vectors One common way to represent words is to use distributional word vectors
learned via dimensionality reduction of large co-occurrence matrices over documents as in
latent semantic analysis local context windows or combinations of both Words
with similar meanings are close to each other in the vector space Since unsupervised word vectors computed from local context windows do not always encode task-specific information such
as sentiment word vectors can also be fine-tuned to such specific tasks We introduce a
hybrid approach where some dimensions are obtained from an unsupervised model and others are
learned for the supervised task We show that this performs better than both the purely supervised
and unsupervised semantic word vectors
Recursive Neural Networks The idea of recursive neural networks RNNs for natural language
processing NLP is to train a deep learning model that can be applied to inputs of any length
Unlike computer vision tasks where it is easy to resize an image to a fixed number of pixels natural sentences do not have a fixed size input However phrases and sentences have a grammatical
structure that can be parsed as a binary tree
Following this tree structure we can assign a fixed-length vector to each word at the leaves of
the tree and combine word and phrase pairs recursively to create intermediate node vectors of the
same length eventually having one final vector representing the whole sentence Multiple
recursive combination functions have been explored from linear transformation matrices to tensor
products In this work we use the simple single matrix RNN to combine node vectors at each
recursive step
Bidirectional-recurrent and bidirectional-recursive neural networks Recurrent neural networks
are a special case of recursive neural networks that operate on chains and not trees Unlike recursive
neural networks they don?t require a tree structure and are usually applied to time series In a recurrent neural network every node is combined with a summarized representation of the past nodes
and then the resulting combination will be forwarded to the next node Bidirectional recurrent neural network architectures have also been explored and usually compute representations
independently from both ends of a time series
Bidirectional recursive models developed in parallel with ours extend the definition of the
recursive neural netword by adding a backward propagation step where information also flows from
the tree root back to the leaves We compare our model to theirs theoretically in the model section
and empirically in the experiments
http://www.cs.york.ac.uk/semeval-2013/task2
Figure Propagation steps of the GB-RNN Step describes the standard RNN feedforward process showing that the vector representation of Android is independent of the rest of the document
Step computes additional vectors at each node red using information from the higher level
nodes in the tree blue allowing Android and iOS to have different representations given the
context
unfold the same autoencoder multiple times which gives it more representational power with
the same number of parameters Our model is different in that it takes into consideration more
information at each step and can eventually make better local predictions by using global context
Sentiment analysis Sentiment analysis has been the subject of research for some time
Most approaches in sentiment analysis use bag of words representations that do not take
the phrase structure into account but learn from word-level features We explore our model?s ability
to determine contextual sentiment on Twitter a social media platform
Global Belief Recursive Neural Networks
In this section we introduce a new model to compute context-dependent compositional vector representations of variable length phrases These vectors are trained to be useful as features to classify
each phrase and word shows an example phrase computation that we will describe in detail
below This section begins by motivating compositionality and context-dependence followed by a
definition of standard recursive neural networks Next we introduce our novel global belief model
and hybrid unsupervised-supervised word vectors
Context-Dependence as Motivation for Global Belief
A common simplifying assumption when mapping sentences into a feature vector is that word order
does not matter bag of words However this will prevent any detailed understanding of language
as exemplified in where the overall sentiment of the phrase Android beats iOS is unclear
Instead we need an understanding of each phrase which leads us to deep recursive models
The first step for mapping a sentence into a vector space is to parse them into a binary tree structure
that captures the grammatical relationships between words Such an input dependent binary tree then
determines the architecture of a recursive neural network which will compute the hidden vectors in a
bottom-up fashion starting with the word vectors The resulting phrase vectors are given as features
to a classifier This standard RDL architecture works well for classifying the inherent or contextindependent label of a phrase For instance it can correctly classify that a not so beautiful day is
negative in sentiment However not all phrases have an inherent sentiment as shown in
The GB-RNN addresses this issue by propagating information from the root node back to the
leaf nodes as described below There are other ways context can be incorporated such as with
bi-directional recurrent neural networks or with window-based methods Both of these methods
however cannot incorporate information from words further away from the phrase to be labeled
Standard Recursive Neural Networks
We first describe a simple recursive neural network that can be used for context-independent phraselevel classification It can also be seen as the first step of a GB-RNN
Assume for now that each word vector a Rn is obtained by sampling each element from a
uniform distribution All these vectors are columns of a large embedding
matrix Rn?|V where is the size of the vocabulary All word vectors are learned together
with the model
For the example word vector sequence abc of the RNN equations become
a
p1
p2
p1
where Rn?2n is the matrix governing the composition and the non-linear activation function Each node vector is the given as input to a softmax classifier for a classification task such as
sentiment analysis
GB-RNN Global Belief Recursive Neural Networks
Our goal is to include contextual information in the recursive node vector representations One
simple solution would be to just include the context words to the left and right of each pair as in
However this will only work if the necessary context is at most words away Furthermore
in order to capture more complex linguistic phenomena it may be necessary to allow for multiple
words to compose the contextual shift in meaning Instead we will use the feedforward nodes from
a standard RNN architecture and simply move back down the tree This can also be interpreted as
unfolding the tree and moving up its branches
Hence we keep the same for computing the forward node vectors but we introduce new
feedbackward vectors denoted with a down arrow at every level of the parse tree Unlike the
feedforward vectors which were computed with a bottom-up recursive function feedbackward vectors are computed with a top-down recursive function The backwards pass starts at the root node
and propagates all the way down to the single word vectors At the root note in our example the
node p2 we have
p2
where Vnd so that all node vectors are nd dimensional Starting from we recursively
get node vectors for every node as we go down the tree
p2
p1
a
p2
p1
where all vectors are nd dimensional and hence R(n+nd is a new de-composition
matrix Figure step illustrates this top-down recursive computation on our example Once we
have both feedforward and feedbackward vectors for a given node we concatenate them and employ
the standard softmax classifier
the final prediction For instance the classification for word
to make
a
a becomes ya softmax Wc
where we fold the bias into the C-class classifier weights
a
Wc
At the root node the equation for x?root could be replaced by simply copying x?root xroot But
there are two advantages of introducing a transform matrix First it helps clearly differentiating features computed during the forward step and the backward step in multiplication with
Second it allows to use a different dimension for the vectors which reduces the number of parameters in the and Wclass matrices and adds more flexibility to the model It also performs
better empirically
Hybrid Word Vector Representations
There are two ways to initialize the word vectors that are given as inputs to the RNN models The
simplest one is to initialize them to small random numbers as mentioned above and backpropagate
error signals into them in order to have them capture the necessary information for the task at hand
This has the advantage of not requiring any other pre-training method and the vectors are sure to
capture domain knowledge However the vectors are more likely to overfit and less likely to generalize well to words that have not been in the usually smaller labeled training set Another approach
Figure Hybrid unsupervised-supervised vector representations for the most frequent words
of the dataset For each horizontal vector the first dimensions are trained on unlabeled twitter
messages and the last dimensions are trained on labeled contextual sentiment examples
is to use unsupervised methods that learn semantic word vectors such as One then has the
option to backpropagate task specific errors into these vectors or keep them at their initialization
Backpropagating into them still has the potential disadvantage of hurting generalization apart from
slowing down training since it increases the number of parameters by a large amount there are usually many parameters in the embedding matrix L). Without propagating information
however one has to hope that the unsupervised method really captures all the necessary semantic
information which is often not the case for sentiment which suffers from the antonym problem
In this paper we propose to combine both ideas by representing each word as a concatenation of both
unsupervised vectors that are kept at their initialization during training and adding a small additional
vector into which we propagate the task specific error signal This vector representation applies only
to the feedforward word vectors and shold not be confused with the combination of the feedwordard
and feedbackward node vectors in the softmax
Figure shows the resulting word vectors trained on unlabeled documents on one part the first
dimensions and trained on labeled examples on the other part the remaining dimensions
Training
The GB-RNN is trained by using backpropagation through structure We train the parameters by
optimizing the regularized cross-entropy error for labeled node vectors with mini-batched AdaGrad
Since we don?t have labels for every node of the training trees we decided that unlabeled
nodes do not add an additional error during training For all models we use a development set to
cross-validate over regularization of the different weights word vector size mini-batch size dropout
probability and activation function rectified linear or logistic function
We also applied the dropout technique to improve training with high dimensional word vectors
Node vector units are randomly set to zero with a probability of at each training step Our
experiments show that applying dropout in this way helps differentiating word vector units and
hidden units and leads to better performance The high-dimensional hybrid word vectors that we
introduced previously have obtained a higher accuracy than other word vectors with the use of
dropout
Comparison to Other Models
The idea of unfolding of neural networks is commonly used in autoencoders as well as in a recursive
setting in this setting the unfolding is only used during training and not at inference time to
update the beliefs about the inputs
Irsoy and Cardie introduced a bidirectional RNN similar to ours It employs the same standard
feedforward RNN but a different computation for the backward vectors In practice their model is
defined by the same forward equations as ours However equation which computes the backward
vectors is instead
Wlb
Wrb
p1
Correct FUSION?s 5th General Meeting is tonight at in ICS Come out and carve pumpkins mid-quarter
with us
Correct I would rather eat my left foot then to be taking the SATs tomorrow
Correct Special THANKS to EVERYONE for coming out to Taboo Tuesday With DST tonight It was
FUN&educational @XiEtaDST
Correct Tough loss for @statebaseball today Good luck on Monday with selection Sunday
Correct I got the job at Claytons I start Monday doing Sheetrock MoneyMakin
Correct St Pattys is no big deal for me no fucks are given but Cinco De Mayo on the other hand thats my
2nd bday
Incorrect @Hannah Sunder The Walking Dead is just a great tv show its bad ass just started to watch the
2nd season to catch up with the 3rd
Figure Examples of predictions made by the GB-RNN for twitter documents In this example
red phrases are negative and blue phrases are positive On the last example the model predicted
incorrectly bad ass as negative
Where Wlb and Wrb
are two matrices with dimensions nd nd For a better comparison with our
model we rewrite and make explicit the blocks of
Wlf
Wlb
Wlf
p1 Wlb
Let
then
Wrf
Wrb
Wrf
p1 Wrb
p1
where the dimensions of Wlf
and Wrf
are nd and the dimensions of Wld
and Wrd
are nd nd
A closer comparison between Eqs. and reveals that both use a left and right forward transfor
mation Wlf
p1 and Wrf
p1 but the other parts of the sums differ In the bidirectional-RNN the
transformation of any children is defined by the forward parent and independent on its position left
or right node Whereas our GB-RNN makes uses of both the forward and backward parent node
The intuition behind our choice is that using both nodes helps to push the model to disentangled
the children from their backward parent vector We also note that our model does not use the forward node vector for computing the backward node vector but we find this not necessary since the
softmax function already combines the two vectors
Our model also has nd more parameters to compute the feedbackward vectors than the
bidirectional-RNN The matrix of our model has 2n nd parameters while the other
model has a total of nd parameters with the Wlf
Wrf
and matrices We show in the
next section that GB-RNN outperforms the bidirectional RNN in our experiments
Experiments
We present a qualitative and quantitative analysis of the GB-RNN on a contextual sentiment classification task The main dataset is provided by the SemEval Task competition We
outperform the winners of the challenge as well as several baseline and model ablations
Evaluation Dataset
The SemEval competition dataset is composed of tweets labeled for different sentiment classes
positive neutral and negative The tweets in this dataset were split into a train labeled phrases
development and development-test set The final test set is composed of examples shows example GB-RNN predictions on phrases marked for classification in this dataset
The development dataset consists only of tweets whereas the final evaluation dataset included also
short text messages SMS in the tables below
Tweets were parsed using the Stanford Parser which includes tokenizing of negations
don?t becomes two tokens do and We constrained the parser to keep each phrase labeled by the
dataset inside its own subtree so that each labeled example is represented by a single node and can
be classified easily
Classifier
SVM
SVM
SVM
GB-RNN
Feature Sets
stemming word cluster SentiWordNet
score negation
POS lexicon negations emoticons
elongated words scores syntactic dependency PMI
punctuation word n-grams emoticons
character n-grams elongated words
upper case stopwords phrase length
negation phrase position large sentiment lexicons microblogging features
parser unsupervised word vectors ensemble
Twitter
SMS
Table Comparison to the best Semeval Task systems their feature sets and F1 results on
each dataset for predicting sentiment of phrases in context The GB-RNN obtains state of the art
performance on both datasets
Model
Bigram Naive Bayes
Logistic Regression
SVM
RNN
Bidirectional-RNN Irsoy and Cardie
GB-RNN best single model
Twitter
SMS
Table Comparison with baselines F1 scores on the SemEval test datasets
Comparison with Competition Systems
The first comparison is with several highly tuned systems from the SemEval Task competition The competition was scored by an average of positive and negative class F1 scores Table
lists results for several methods together with the resources and features used by each method Most
systems used a considerable amount of hand-crafted features In contrast the GB-RNN only needs
a parser for the tree structure unsupervised word vectors and training data Since the competition
allowed for external data we outline below the additional training data we use Our best model is an
ensemble of the top GB-RNN models trained independently Their predictions were then averaged
to produce the final output
Comparison with Baselines
Next we compare our single best model to several baselines and model ablations We used the same
hybrid word vectors with dropout training for the RNN the bidirectional RNN and the GB-RNN
The best models were selected by cross-validating on the dev set for several hyper-parameters word
vectors dimension hidden node vector dimension number of training epochs regularization parameters activation function training batch size and dropout probability and we kept the models with
the highest cross-validation accuracy Table shows these results The most important comparison
is against the purely feedforward RNN which does not take backward sentence context into account
This model performs over worse than the GB-RNN
For the logistic regression and Bigram Naive Bayes classification each labeled phrase was taken
as a separate example removing the surrounding context Another set of baselines used a context
window for classification as well as the entire tweet as input to the classifier
Optimal performance for the single best GB-RNN was achieved by using vector sizes of dimensions pre-trained fixed word vectors and trained on sentiment data a mini-batch size of
dropout with and sigmoid non-linearity In table we show that the concatenation of
fixed unsupervised vectors with additional randomly initialized supervised vectors performs better
than both methods
Model Analysis Additional Training Data
Because the competition allowed the usage of arbitrary resources we included as training data labeled unigrams and bigrams extracted from the NRC-Canada system?s sentiment lexicon Adding
these additional training examples increased accuracy by Although this lexicon helps reduc7
Word vectors
supervised word vectors
semantic word vectors
hybrid word vectors
dimension
34
Twitter
SMS
Table F1 score comparison of word vectors on the SemEval Task test dataset
Chelski
want
this
Chelski
that it
so
bad
want
this
so
makes
me
even
that
it
bad
me
even
thinking
we
may
happier
beat
makes
thinking
we may
happier
twice
them
in
beat
days
at
SB
twice
them
in
days
at
SB
Figure Change in sentiment predictions in the tweet chelski want this so bad that it makes me even
happier thinking we may beat them twice in days at SB between the RNN left and the GB-RNN
right In particular we can see the change for the phrase want this so bad where it is correctly
predicted as positive with context
ing the number of unknown tokens it does not do a good job for training recursive composition
functions because each example is short
We also included our own dataset composed noisily labeled tweets using heuristics such as
smiley faces as well as the movie reviews dataset from In both datasets the labels only denote
the context-independent sentiment of a phrase or full sentence Hence we trained the final model in
two steps train the standard RNN then train the full GB-RNN model on the smaller context-specific
competition data Training the GB-RNN jointly in this fashion gave a accuracy improvement
Conclusion
We introduced global belief recursive neural networks applied to the task of contextual sentiment
analysis The idea of propagating beliefs through neural networks is a powerful and important piece
for interpreting natural language The applicability of this idea is more general than RNNs and can
be helpful for a variety of NLP tasks such as word-sense disambiguation
Acknowledgments
We thank the anonymous reviewers for their valuable comments

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5860-on-the-job-learning-with-bayesian-decision-theory.pdf

On-the-Job Learning with Bayesian Decision Theory
Keenon Werling
Department of Computer Science
Stanford University
keenon@cs.stanford.edu
Arun Chaganty
Department of Computer Science
Stanford University
chaganty@cs.stanford.edu
Percy Liang
Department of Computer Science
Stanford University
pliang@cs.stanford.edu
Christopher D. Manning
Department of Computer Science
Stanford University
manning@cs.stanford.edu
Abstract
Our goal is to deploy a high-accuracy system starting with zero training examples
We consider an on-the-job setting where as inputs arrive we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident As the model improves over time the reliance on crowdsourcing queries
decreases We cast our setting as a stochastic game based on Bayesian decision
theory which allows us to balance latency cost and accuracy objectives in a principled way Computing the optimal policy is intractable so we develop an approximation based on Monte Carlo Tree Search We tested our approach on three
datasets?named-entity recognition sentiment classification and image classification On the NER task we obtained more than an order of magnitude reduction
in cost compared to full human annotation while boosting performance relative to
the expert provided labels We also achieve a F1 improvement over having a
single human label the whole set and a F1 improvement over online learning
Poor is the pupil who does not surpass his master
Leonardo da Vinci
Introduction
There are two roads to an accurate AI system today gather a huge amount of labeled training
data and do supervised learning or use crowdsourcing to directly perform the task
However both solutions require non-trivial amounts of time and money In many situations one
wishes to build a new system to do Twitter information extraction to aid in disaster relief
efforts or monitor public opinion but one simply lacks the resources to follow either the pure ML
or pure crowdsourcing road
In this paper we propose a framework called on-the-job learning formalizing and extending ideas
first implemented in in which we produce high quality results from the start without requiring
a trained model When a new input arrives the system can choose to asynchronously query the
crowd on parts of the input it is uncertain about query about the label of a single token in a
sentence After collecting enough evidence the system makes a prediction The goal is to maintain
high accuracy by initially using the crowd as a crutch but gradually becoming more self-sufficient
as the model improves Online learning and online active learning are different in that
they do not actively seek new information prior to making a prediction and cannot maintain high
accuracy independent of the number of data instances seen so far Active classification like us
Get beliefs under
Decide to ask a crowd
learned model
George
worker in real time
PERSON
RESOURCE
NONE
LOCATION
http://www.crowd-workers.com
What is George here
Soup
on
George
str
Katrina
y1
y2
y3
y4
y5
x3
x4
x5
soup on george str
katrina
Incorporate feedback
return a prediction
RESOURCE
LOCATION
Soup
on
George
str
Katrina
y1
y2
y3
y4
y5
x3
x4
x5
location
person
resource
r1
none
Figure Named entity recognition on tweets in on-the-job learning
strategically seeks information by querying a subset of labels prior to prediction but it is based on
a static policy whereas we improve the model during test time based on observed data
To determine which queries to make we model on-the-job learning as a stochastic game based on
a CRF prediction model We use Bayesian decision theory to tradeoff latency cost and accuracy
in a principled manner Our framework naturally gives rise to intuitive strategies To achieve high
accuracy we should ask for redundant labels to offset the noisy responses To achieve low latency
we should issue queries in parallel whereas if latency is unimportant we should issue queries sequentially in order to be more adaptive Computing the optimal policy is intractable so we develop
an approximation based on Monte Carlo tree search and progressive widening to reason about
continuous time
We implemented and evaluated our system on three different tasks named-entity recognition sentiment classification and image classification On the NER task we obtained more than an order of
magnitude reduction in cost compared to full human annotation while boosting performance relative to the expert provided labels We also achieve a F1 improvement over having a single human
label the whole set and a F1 improvement over online learning An open-source implementation of our system dubbed LENSE for Learning from Expensive Noisy Slow Experts is available
at http://www.github.com/keenon/lense
Problem formulation
Consider a structured prediction problem from input to output yn
For example for named-entity recognition NER on tweets is a sequence of words in the tweet
on George and is the corresponding sequence of labels NONE LOCATION
LOCATION The full set of labels of PERSON LOCATION RESOURCE and NONE
In the on-the-job learning setting inputs arrive in a stream On each input we make zero or more
queries q1 q2 on the crowd to obtain labels potentially more than once for any positions in
The responses r1 r2 come back asynchronously which are incorporated into our current
prediction model Figure left shows one possible outcome We query positions q1
George and q2 The first query returns r1 LOCATION upon which we make
another query on the the same position q3 George and so on When we have sufficient
under the model Each
confidence about the entire output we return the most likely prediction
query qi is issued at time si and the response comes back at time ti Assume that each query costs
cents Our goal is to choose queries to maximize accuracy minimize latency and cost
on each input in
We make several remarks about this setting First we must make a prediction
the stream unlike in active learning where we are only interested in the pool or stream of examples
for the purposes of building a good model Second the responses are used to update the prediction
oGs
oGs
oGs
Legend
none
res
loc
per
r1 res
q1
r2 loc
q2
oGs
oGs
oGs
system
crowd
tnow
r1 loc
oGs
r1 loc
q1
r1 res
r2 per
q2
q4
r4 loc
Incorporating information from responses The bar graphs
represent the marginals over the labels for each token indicated
by the first character at different points in time The two timelines show how the system updates its confidence over labels
based on the crowd?s responses The system continues to issue
queries until it has sufficient confidence on its labels See the
paragraph on behavior in Section for more information
Game tree An example of a partial
game tree constructed by the system when
deciding which action to take in the state
the query
q1 has already been issued and the
system must decide whether to issue another query or wait for a response to q1
Figure Example behavior while running structure prediction on the tweet Soup on George str
We omit the RESOURCE from the game tree for visual clarity
model like in online learning This allows the number of queries needed and thus cost and latency
to decrease over time without compromising accuracy
Model
We model on-the-job learning as a stochastic game with two players the system and the crowd
The game starts with the system receiving input and ends when the system turns in a set of labels
yn During the system?s turn the system may choose a query action
to ask the crowd to label yq The system may also choose the wait action to wait for the
crowd to respond to a pending query or the return action to terminate the game and return
its prediction given responses received thus far The system can make as many queries in a row
simultaneously as it wants before deciding to wait or turn When the wait action is chosen
the turn switches to the crowd which provides a response to one pending query and advances
the game clock by the time taken for the crowd to respond The turn then immediately reverts back
to the system When the game ends the system chooses the return action the system evaluates a
utility that depends on the accuracy of its prediction the number of queries issued and the total time
taken The system should choose query and wait actions to maximize the utility of the prediction
eventually returned
In the rest of this section we describe the details of the game tree our choice of utility and specify
models for crowd responses followed by a brief exploration of behavior admitted by our model
Game tree Let us now formalize the game tree in terms of its states actions transitions and
rewards see Figure 2b for an example The game state tnow consists of the current
time tnow the actions that have been issued at times and the
responses that have been received at times Let rj and
tj iff qj is not a query action or its responses have not been received by time tnow
During the system?s turn when the system chooses an action qk the state is updated to
tnow q0 s0 r0 t0 where q0 qk s0 tnow r0 and
t0 If qk then the system chooses another action from the new state
If qk the crowd makes a stochastic move from Finally if qk the game ends
This rules out the possibility of launching a query midway through waiting for the next response However
we feel like this is a reasonable limitation that significantly simplifies the search space
and the system returns its best estimate of the labels using the responses it has received and obtains
a utility defined later
Let qj rj be the set of in-flight requests During the crowd?s
turn after the system chooses the next response from the crowd is chosen
arg minj?F t0j where t0j is sampled from the response-time model t0j pT sj t0j tnow for
each Finally a response is sampled using a response model rj0 p(rj0 and the state
is updated to tj r0 t0 where r0 rj0 rk and t0 t0j tk
Utility Under Bayesian decision theory the optimal choice for an action in state
tnow is the one that attains the maximum expected utility value for the game starting
at Recall that the system can return at any time at which point it receives a utility that trades
off two things The first is the accuracy of the MAP estimate according to the model?s best guess
of incorporating all responses received by time The second is the cost of making queries a
monetary cost wM per query made and penalty of wT per unit of time taken Formally we define
the utility to be
ExpAcc(p(y nQ wM tnow wT
ExpAcc(p Ep(y Accuracy(arg max
p(y
where nQ qj is the number of queries made p(y is a prediction
model that incorporates the crowd?s responses
The utility of wait and return actions is computed by taking expectations over subsequent trajectories
in the game tree This is intractable to compute exactly so we propose an approximate algorithm in
Section
Environment model The final component is a model of the environment crowd Given input
and queries qk issued at times sk we define a distribution over the
output responses rk and response times tk as follows
pR ri yqi pT ti si
The three components are as follows is the prediction model a standard linear-chain
CRF pR yq is the response model which describes the distribution of the crowd?s response
for a given a query when the true answer is yq and pT ti si specifies the latency of query
qi The CRF model is learned based on all actual responses not simulated ones using
AdaGrad To model annotation errors we set pR yq iff yq and distribute the
remaining probability for uniformly Given this full model we can compute simply
by marginalizing out and from Equation When conditioning on we ignore responses that
have not yet been received when rj for some
Behavior Let?s look at typical behavior that we expect the model and utility to capture Figure 2a
shows how the marginals over the labels change as the crowd provides responses for our running
example named entity recognition for the sentence Soup on George In the both timelines
the system issues queries on Soup and George because it is not confident about its predictions
for these tokens In the first timeline the crowd correctly responds that Soup is a resource and
that George is a location Integrating these responses the system is also more confident about
its prediction on and turns in the correct sequence of labels In the second timeline a crowd
worker makes an error and labels George to be a person The system still has uncertainty on
George and issues an additional query which receives a correct response following which the
system turns in the correct sequence of labels While the answer is still correct the system could
have taken less time to respond by making an additional query on George at the very beginning
We found the humans we hired were roughly accurate in our experiments
Game playing
In Section we modeled on-the-job learning as a stochastic game played between the system and
the crowd We now turn to the problem of actually finding a policy that maximizes the expected
utility which is of course intractable because of the large state space
Our algorithm Algorithm combines ideas from Monte Carlo tree search to systematically
explore the state space and progressive widening to deal with the challenge of continuous variables time Some intuition about the algorithm is provided below When simulating the system?s
turn the next state and hence action is chosen using the upper confidence tree UCT decision
rule that trades off maximizing the value of the next state exploitation with the number of visits
exploration The crowd?s turn is simulated based on transitions defined in Section To handle the
unbounded fanout during the crowd?s turn we use progressive widening that maintains a current set
of active or explored states which is gradually grown with time Let be the number of
times a state has been visited and be all successor states that the algorithm has sampled
Algorithm Approximating expected utility with MCTS and progressive widening
For all
function MONTE ARLOVALUE(state
increment
if system?s turn then
log
arg max?0
Initialize visits utility sum and children
Choose next state using UCT
MONTE ARLOVALUE
Record observed utility
return
else if crowd?spturn then
Restrict continuous samples using PW
if then
is sampled from set of already visited based on
else
is drawn based on
end if
return MONTE ARLOVALUE
else if game terminated then
return utility of according to
end if
end function
Experiments
In this section we empirically evaluate our approach on three tasks While the on-the-job setting we
propose is targeted at scenarios where there is no data to begin with we use existing labeled datasets
Table to have a gold standard
Baselines We evaluated the following four methods on each dataset
Human n-query The majority vote of human crowd workers was used as a prediction
Online learning Uses a classifier that trains on the gold output for all examples seen so
far and then returns the MLE as a prediction This is the best possible offline system it
sees perfect information about all the data seen so far but can not query the crowd while
making a prediction
Threshold baseline Uses the following heuristic For each label we ask for queries
such that Instead of computing the expected marginals over
the responses to queries in flight we simply count the in-flight requests for a given variable
and reduces the uncertainty on that variable by a factor of The system continues
launching requests until the threshold adjusted by number of queries in flight is crossed
Dataset Examples
NER
Task and notes
We evaluate on the
NER task3 a sequence labeling
problem over English sentences
We only consider the four tags corresponding to persons locations
organizations or none4
Sentiment
We evaluate on a subset of the
IMDB sentiment dataset that
consists of polar movie reviews the goal is binary classification of documents into classes POS
and NEG.
We evaluate on a celebrity face
classification task Each image must be labeled as one of the
following four choices Andersen
Cooper Daniel Craig Scarlet Johansson or Miley Cyrus
Face
Features
We used standard features the
current word current lemma previous and next lemmas lemmas in
a window of size three to the left
and right word shape and word
prefix and suffixes as well as word
embeddings
We used two feature sets the
first UNIGRAMS containing only
word unigrams and the second
RNN that also contains sentence
vector embeddings from
We used the last layer of a 11layer AlexNet trained on ImageNet as input feature embeddings
though we leave back-propagating
into the net to future work
Table Datasets used in this paper and number of examples we evaluate on
System
1-vote
3-vote
5-vote
Online
Threshold
LENSE
Delay/tok
ms
ms
ms
n/a
ms
ms
Named Entity Recognition
Qs/tok PER F1 LOC F1
n/a
ORG F1
F1
Face Identification
Latency Qs/ex Acc.
ms
ms
ms
n/a
n/a
ms
ms
Table Results on NER and Face tasks comparing latencies queries per token Qs/tok and performance metrics for NER and accuracy for Face
Predictions are made using MLE on the model given responses The baseline does not
reason about time and makes all its queries at the very beginning
LENSE Our full system as described in Section
Implementation and crowdsourcing setup We implemented the retainer model of on Amazon Mechanical Turk to create a pool of crowd workers that could respond to queries in real-time
The workers were given a short tutorial on each task before joining the pool to minimize systematic
errors caused by misunderstanding the task We paid workers to join the retainer pool and
an additional per query for NER since response times were much faster we paid
per query Worker response times were generally in the range of seconds for NER
seconds for Sentiment and seconds for Faces
When running experiments we found that the results varied based on the current worker quality To
control for variance in worker quality across our evaluations of the different methods we collected
worker responses and their delays on each label ahead of time5 During simulation we sample the
worker responses and delays without replacement from this frozen pool of worker responses
Summary of results Table and Table summarize the performance of the methods on the three
tasks On all three datasets we found that on-the-job learning outperforms machine and human-only
http://www.cnts.ua.ac.be/conll2003/ner
The original also includes a fifth tag for miscellaneous however the definition for miscellaneos is complex
making it very difficult for non-expert crowd workers to provide accurate labels
These datasets are available in the code repository for this paper
System
1-vote
3-vote
5-vote
Unigrams
Unigrams RNN embeddings
Queries per example
Qs/ex
Acc.
n/a
n/a
n/a
n/a
UNIGRAMS
Online
Threshold
LENSE
RNN
Latency
Online
Threshold
LENSE
Time
Figure Queries per example for LENSE on
Sentiment With simple UNIGRAM features the
model quickly learns it does not have the capacity to answer confidently and must query the
crowd With more complex RNN features the
model learns to be more confident and queries
the crowd less over time
Table Results on the Sentiment task comparing latency queries per example and accuracy
Queries per token
F1
LENSE
vote baseline
LENSE
online learning
Time
Time
Figure Comparing F1 and queries per token on the NER task over time The left graph compares
LENSE to online learning which cannot query humans at test time This highlights that LENSE
maintains high F1 scores even with very small training set sizes by falling back the crowd when it
is unsure The right graph compares query rate over time to 1-vote This clearly shows that as the
model learns it needs to query the crowd less
comparisons on both quality and cost On NER we achieve an F1 of at more than an order of
magnitude reduction on the cost of achieving comporable quality result using the 5-vote approach
On Sentiment and Faces we reduce costs for a comparable accuracy by a factor of around For the
latter two tasks both on-the-job learning methods perform less well than in NER. We suspect this
is due to the presence of a dominant class in NER that the model can very quickly learn to
expend almost no effort on LENSE outperforms the threshold baseline supporting the importance
of Bayesian decision theory
Figure tracks the performance and cost of LENSE over time on the NER task LENSE is not only
able to consistently outperform other baselines but the cost of the system steadily reduces over time
On the NER task we find that LENSE is able to trade off time to produce more accurate results than
the 1-vote baseline with fewer queries by waiting for responses before making another query
While on-the-job learning allows us to deploy quickly and ensure good results we would like to
eventually operate without crowd supervision Figure we show the number of queries per example
on Sentiment with two different features sets UNIGRAMS and RNN as described in Table With
simpler features UNIGRAMS the model saturates early and we will continue to need to query to
the crowd to achieve our accuracy target as specified by the loss function On the other hand
using richer features RNN the model is able to learn from the crowd and the amount of supervision
needed reduces over time Note that even when the model capacity is limited LENSE is able to
guarantee a consistent high level of performance
Reproducibility All code data and experiments for this paper are available on CodaLab at
https://www.codalab.org/worksheets/0x2ae89944846444539c2d08a0b7ff3f6f
Related Work
On-the-job learning draws ideas from many areas online learning active learning active classification crowdsourcing and structured prediction
Online learning The fundamental premise of online learning is that algorithms should improve
with time and there is a rich body of work in this area In our setting algorithms not only
improve over time but maintain high accuracy from the beginning whereas regret bounds only
achieve this asymptotically
Active learning Active learning for a survey algorithms strategically select most informative examples to build a classifier Online active learning performs active learning
in the online setting Several authors have also considered using crowd workers as a noisy oracle
It differs from our setup in that it assumes that labels can only be observed after
classification which makes it nearly impossible to maintain high accuracy in the beginning
Active classification Active classification 23 asks what are the most informative features
to measure at test time Existing active classification algorithms rely on having a fully labeled
dataset which is used to learn a static policy for when certain features should be queried which does
not change at test time On-the-job learning differs from active classification in two respects true
labels are never observed and our system improves itself at test time by learning a stronger model
A notable exception is Legion:AR which like us operates in on-the-job learning setting to for
real-time activity classification However they do not explore the machine learning foundations
associated with operating in this setting which is the aim of this paper
Crowdsourcing A burgenoning subset of the crowdsourcing community overlaps with machine
learning One example is Flock which first crowdsources the identification of features for an
image classification task and then asks the crowd to annotate these features so it can learn a decision
tree In another line of work TurKontrol models individual crowd worker reliability to optimize
the number of human votes needed to achieve confident consensus using a POMDP
Structured prediction An important aspect our prediction tasks is that the output is structured
which leads to a much richer setting for one-the-job learning Since tags are correlated the importance of a coherent framework for optimizing querying resources is increased Making active partial
observations on structures and has been explored in the measurements framework of and in the
distant supervision setting
Conclusion
We have introduced a new framework that learns from noisy crowds on-the-job to maintain high
accuracy and reducing cost significantly over time The technical core of our approach is modeling
the on-the-job setting as a stochastic game and using ideas from game playing to approximate the
optimal policy We have built a system LENSE which obtains significant cost reductions over a
pure crowd approach and significant accuracy improvements over a pure ML approach
Acknowledgments
We are grateful to Kelvin Guu and Volodymyr Kuleshov for useful feedback regarding the calibration of our models and Amy Bearman for providing the image embeddings for the face classification
experiments We would also like to thank our anonymous reviewers for their helpful feedback Finally our work was sponsored by a Sloan Fellowship to the third author

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6139-supervised-word-movers-distance.pdf

Supervised Word Mover?s Distance
Gao Huang Chuan Guo
Cornell University
gh349,cg563}@cornell.edu
Yu Sun Kilian Q. Weinberger
Cornell University
ys646,kqw4}@cornell.edu
Matt J. Kusner
Alan Turing Institute University of Warwick
mkusner@turing.ac.uk
Fei Sha
University of California Los Angeles
feisha@cs.ucla.edu
Abstract
Recently a new document metric called the word mover?s distance WMD has
been proposed with unprecedented results on kNN-based document classification
The WMD elevates high-quality word embeddings to a document metric by formulating the distance between two documents as an optimal transport problem
between the embedded words However the document distances are entirely unsupervised and lack a mechanism to incorporate supervision when available In
this paper we propose an efficient technique to learn a supervised metric which
we call the Supervised-WMD S-WMD metric The supervised training minimizes the stochastic leave-one-out nearest neighbor classification error on a perdocument level by updating an affine transformation of the underlying word embedding space and a word-imporance weight vector As the gradient of the original WMD distance would result in an inefficient nested optimization problem we
provide an arbitrarily close approximation that results in a practical and efficient
update rule We evaluate S-WMD on eight real-world text classification tasks on
which it consistently outperforms almost all of our 26 competitive baselines
Introduction
Document distances are a key component of many text retrieval tasks such as web-search ranking
book recommendation and news categorization Because of the variety of potential applications there has been a wealth of work towards developing accurate document distances
In large part prior work focused on extracting meaningful document representations
starting with the classical bag of words BOW and term frequency-inverse document frequency
TF-IDF representations These sparse high-dimensional representations are frequently nearly
orthogonal and a pair of similar documents may therefore have nearly the same distance as a
pair that are very different It is possible to design more meaningful representations through eigendecomposing the BOW space with Latent Semantic Indexing LSI or learning a probabilistic
clustering of BOW vectors with Latent Dirichlet Allocation LDA Other work generalizes LDA
or uses denoising autoencoders to learn a suitable document representation
Recently Kusner proposed the Word Mover?s Distance a new distance for text
documents that leverages word embeddings Given these high-quality embeddings the WMD
defines the distances between two documents as the optimal transport cost of moving all words from
one document to another within the word embedding space This approach was shown to lead to
state-of-the-art error rates in k-nearest neighbor kNN document classification
Authors contributing equally
This work was done while the author was a student at Washington University in St. Louis
Conference on Neural Information Processing Systems NIPS Barcelona Spain
Importantly these prior works are entirely unsupervised and not learned explicitly for any particular
task For example text documents could be classified by topic or by author which would lead to
very different measures of dissimilarity Lately there has been a vast amount of work on metric
learning 36 most of which focuses on learning a generalized linear Euclidean metric
These methods often scale quadratically with the input dimensionality and can only be applied to
high-dimensional text documents after dimensionality reduction techniques such as PCA
In this paper we propose an algorithm for learning a metric to improve the Word Mover?s Distance
WMD stands out from prior work in that it computes distances between documents without ever
learning a new document representation Instead it leverages low-dimensional word representations
for example word2vec to compute distances This allows us to transform the word embedding
instead of the documents and remain in a low-dimensional space throughout At the same time we
propose to learn word-specific importance weights to emphasize the usefulness of certain words
for distinguishing the document class
At first glance incorporating supervision into the WMD appears computationally prohibitive as
each individual WMD computation scales cubically with respect to the sparse dimensionality of
the documents However we devise an efficient technique that exploits a relaxed version of the
underlying optimal transport problem called the Sinkhorn distance This combined with a
probabilistic filtering of the training set reduces the computation time significantly
Our metric learning algorithm Supervised Word Mover?s Distance directly minimizes a
stochastic version of the leave-one-out classification error under the WMD metric Different from
classic metric learning we learn a linear transformation of the word representations while also learning re-weighted word frequencies These transformations are learned to make the WMD distances
match the semantic meaning of similarity encoded in the labels We show across datasets and 26
baseline methods the superiority of our method
Background
Here we describe the word embedding technique we use word2vec and the recently introduced
Word Mover?s Distance We then detail the setting of linear metric learning and the solution proposed by Neighborhood Components Analysis NCA which inspires our method
word2vec may be the most popular technique for learning a word embedding over billions of words
and was introduced by Mikolov Each word in the training corpus is associated with
an initial word vector which is then optimized so that if two words w1 and w2 frequently occur
together they have high conditional probability This probability is the hierarchical softmax of the word vectors vw1 and vw2 an easily-computed quantity which allows a simplified
neural language model the word2vec model to be trained efficiently on desktop computers Training an embedding over billions of words allows word2vec to capture surprisingly accurate word
relationships Word embeddings can learn hundreds of millions of parameters and are typically
by design unsupervised allowing them to be trained on large unlabeled text corpora ahead of time
Throughout this paper we use word2vec although many word embeddings could be used
Word Mover?s Distance Leveraging the compelling word vector relationships of word embeddings Kusner introduced the Word Mover?s Distance WMD as a distance between text
documents At a high level the WMD is the minimum distance required to transport the words
from one document to another We assume that we are given a word embedding matrix Rd?n
for a vocabulary of words Let Rd be the representation of the ith word as defined by this
embedding Additionally let da db be the n-dimensional normalized bag-of-words BOW vectors
for two documents where dai is the number of times word occurs in da normalized over all words
in da The WMD introduces an auxiliary transport matrix Rn?n such that Tij describes
how much of dai should be transported to dbj Formally the WMD learns to minimize
D(xi min
Tij kxi kp2 subject to
Tij dai
Tij dbj
where is usually set to or In this way documents that share many words even related ones
should have smaller distances than documents with very dissimilar words It was noted in Kusner
that the WMD is a special case of the Earth Mover?s Distance EMD also known
more generally as the Wasserstein distance The authors also introduce the word centroid distance which uses a fast approximation first described by Rubner kXd Xd0
It can be shown that the WCD always lower bounds the WMD. Intuitively the WCD represents each
document by the weighted average word vector where the weights are the normalized BOW counts
The time complexity of solving the WMD optimization problem is O(q log where is the
maximum number of unique words in either or d0 The WCD scales asymptotically by
Regularized Transport Problem To alleviate the cubic time complexity of the Wasserstein distance computation Cuturi formulated a smoothed version of the underlying transport problem by
adding an entropy regularizer to the transport objective This makes the objective function strictly
convex and efficient
Pn algorithms can be adopted to solve it In particular given a transport matrix
let Tij log(Tij be the entropy of T. For any the regularized primal
transport problem is defined as
min
Tij kxi kp2 subject to
Tij dai
Tij dbj
The larger is the closer this relaxation is to the original Wasserstein distance Cuturi propose
an efficient algorithm to solve for the optimal transport using a clever matrix-scaling algorithm
Specifically we may define the matrix Kij exp(??kxi and solve for the scaling vectors
to a fixed-point by computing da db in an alternating fashion
These yield the relaxed transport diag(u)K diag(v This algorithm can be shown to have
empirical time complexity O(q which is significantly faster than solving the WMD problem
exactly
Linear Metric Learning Assume that we have access to a training set Rd arranged as columns in matrix Rd?n and corresponding labels yn where
contains some finite number of classes Linear metric learning learns a matrix A Rr?d
where and defines the generalized Euclidean distance between two documents and as
dA kA(xi Popular linear metric learning algorithms are NCA LMNN
and ITML amongst others These methods learn a matrix A to minimize a loss function
that is often an approximation of the leave-one-out LOO classification error of the kNN classifier
Neighborhood Components Analysis NCA was introduced by Goldberger to learn
a generalized Euclidean metric Here the authors approximate the non-continuous leave-one-out
kNN error by defining a stochastic neighborhood process An input is assigned input as its
nearest neighbor with probability
exp(?d2A
pij
exp xk
where we define pii Under this stochastic neighborhood assignment an input with label
is classified correctly if its nearest neighbor isP
any from the same class yj The
probability of this event can be stated as pi
learns A by maximizing the
j:yj pij NCA
expected LOO accuracy pi or equivalently by minimizing log(pi the KL-divergence
from a perfect classification distribution pi for all
Learning a Word Embedding Metric
In this section we propose a method for learning a supervised document distance by way of learning a generalized Euclidean metric within the word embedding space and a word importance vector We will refer to the learned document distance as the Supervised Word Mover?s Distance SWMD To learn such a metric we assume we have a training dataset consisting of documents
dm where is the n?1)-dimensional simplex thus each document is represented as a normalized histogram over the words in the vocabulary of size For each document
we are given a label out of possible classes ym C}m Additionally
we are given a word embedding matrix Rd?n the word2vec embedding which defines a
d-dimensional word vector for each of the words in the vocabulary
Supervised WMD. As described in the previous section it is possible to define a distance between
any two documents da and db as the minimum cumulative word distance of moving da to db in
word embedding space as is done in the WMD. Given a labeled training set we would like to
improve the distance so that documents that share the same labels are close and those with different
labels are far apart We capture this notion of similarity in two ways First we transform the word
embedding which captures a latent representation of words We adapt this representation with a
linear transformation Axi where represents the embedding of the ith word Second as
different classification tasks and data sets may value words differently we also introduce a histogram
importance vector that re-weighs the word histogram values to reflect the importance of words
for distinguishing the classes
a da da
where denotes the element-wise Hadamard product After applying the vector and the linear
mapping A the WMD distance between documents da and db becomes
DA,w da db min
Tij kA(xi
Tij d?ai and
Tij d?bj
Loss Function Our goal is to learn the matrix A and vector to make the distance DA,w reflect
the semantic definition of similarity encoded in the labeled data Similar to prior work on metric
learning we achieve this by minimizing the kNN-LOO error with the distance DA,w
in the document space As the LOO error is non-differentiable we use the stochastic neighborhood
relaxation proposed by Hinton Roweis which is also used for NCA. Similar to prior work
we use the squared Euclidean word distance in We use the KL-divergence loss proposed in
NCA alongside the definition of neighborhood probability in which yields
exp(?D
A,w
a
log
exp da dc
b:yb ya
Gradient We can compute the gradient of the loss with respect to A and as follows
pab
DA,w da db
pa
a
where ab if and only if ya yb and ab otherwise
Fast computation of DA,w da db
Notice that the remaining gradient term above DA,w da db contains the nested linear
program defined in In fact computing this gradient just for a single pair of documents will
require time complexity O(q log where is the largest set of unique words in either document
This quickly becomes prohibitively slow as the document size becomes large and the number
of documents increase Further the gradient is not always guaranteed to exist instead we
must resort to subgradient descent Motivated by the recent works on fast Wasserstein distance
computation we propose to relax the modified linear program in eq using the entropy
as in eq As described in Section this allows us to approximately solve eq in O(q time
via diag(u)K diag(v We will use this approximate solution in the following gradients
Gradient A. It can be shown that
DA,w da db 2A
Tab
ij
A
where Tab is the optimizer of so long as it is unique otherwise it is a subgradient We
replace Tab by which is always unique as the relaxed transport is strongly convex
Gradient To obtain the gradient with respect to we need the optimal solution to the
dual transport problem
a
kA(xi
DA,w
da db max
a and
are functions of we have
Given that both
a DA,w
da
a)da db
b)db
DA,w
DA,w da db
a
a
Instead of solving the dual directly we obtain the relaxed optimal dual variables via the
vectors that were used to derive our relaxed transport Specifically we can solve for the
log(u
and log(v
log(v
where is the
dual variables as such log(u
p-dimensional all ones vector In general we can observe from eq that the above approximation
process becomes more accurate as grows However setting too large can make the algorithm
converges slower In our experiments we use which leads to a nice trade-off between speed
and approximation accuracy
Optimization
Alongside the fast gradient computation process in Algorithm S-WMD
troduced above we can further speed up the training with a clever initialization and batch gradient de Input word1 embedding
dataset y1 dm ym
scent
ca Xda a
Initialization The loss function in eq is non4 A NCA((c1 y1 cm ym
convex and is thus highly dependent on the initial
setting of A and A good initialization also dras while loop until convergence do
tically reduces the number of gradient steps required
Randomly select
For we initialize all its entries to all words
Compute gradients using eq
are assigned with the same weights at the begin
A A A gA
ning For A we propose to learn an initial projection
gw
within the word centroid distance defined end while
as D0 da db kXda Xdb described in Section The WCD should be a reasonable approximation to the WMD. Kusner point out
that the WCD is a lower bound on the WMD which holds true after the transformation with A.
We obtain our initialization by applying NCA in word embedding space using the WCD distance
between documents This is to say that we can construct the WCD dataset cm Rd
representing each text document as its word centroid and apply NCA in the usual way as described
in Section We call this learned word distance Supervised Word Centroid Distance
Batch Gradient Descent Once the initial matrix A is obtained we minimize the loss in
with batch gradient descent At each iteration instead of optimizing over the full training set
we randomly pick a batch of documents from the training set and compute the gradient for these
documents We can further speed up training by observing that the vast majority of NCA probabilities pab near zero This is because most documents are far away from any given document Thus
for a document da we can use the WCD to get a cheap neighbor ordering and only compute the
NCA probabilities for the closest set of documents Na based on the WCD. When we compute the
gradient for each of the selected documents we only use the document?s nearest neighbor documents defined by WCD distance to compute the NCA neighborhood probabilities In particular
the gradient is computed as follows
gA,w
pab pa pa
da db
a?B b?Na
where again Na is the set of nearest neighbors of document a With the gradient we update A and
with learning rates A and respectively Algorithm summarizes S-WMD in pseudo code
Complexity The empirical time complexity of solving the dual transport problem scales quadratically with Therefore the complexity of our algorithm is O(T BN d2 where
denotes the number of batch gradient descent iterations the batch size Na the
size of the nearest neighbor set and the maximum number of unique words in a document This
is because computing T?ij and using the alternating fixed point algorithm in Section
requires time while constructing the gradients from eqs and takes
time The approximated gradient eq requires this computation to be repeated BN times In
our experiments we set 32 and and computing the gradient at each iteration can be
done in seconds
Results
We evaluate S-WMD on different document corpora and compare the kNN error with unsupervised
WCD WMD and document representations In addition all document representation baselines
Table The document datasets and their descriptions used for visualization and evaluation
name
BBCSPORT
TWITTER
RECIPE
OHSUMED
CLASSIC
REUTERS
AMAZON
NEWS
twitter
recipe
ohsumed
classic
ne
reuters
BOW
dim
avg
words
72
amazon
20news
S-WMD
WMD
bbcsport
description
BBC sports articles labeled by sport
tweets categorized by sentiment
recipe procedures labeled by origin
medical abstracts class subsampled
academic papers labeled by publisher
news dataset train/test split
reviews labeled by product
canonical news article dataset
Figure t-SNE plots of WMD and S-WMD on all datasets
are used with and without leading supervised metric learning algorithms?resulting in an overall
total of 26 competitive baselines Our code is implemented in Matlab and is freely available at
https://github.com/gaohuang/S-WMD
Datasets and Baselines We evaluate all approaches on document datasets in the settings of
news categorization sentiment analysis and product identification among others Table describes
the classification tasks as well as the size and number of classes of each of the datasets We
evaluate against the following document representation/distance methods bag-of-words
a count of the number of word occurrences in a document the length of the vector is the number
of unique words in the corpus term frequency-inverse document frequency TF-IDF the BOW
vector normalized by the document frequency of each word across the corpus Okapi
a TF-IDF-like ranking function first used in search engines Latent Semantic Indexing LSI
projects the BOW vectors onto an orthogonal basis via singular value decomposition Latent Dirichlet Allocation LDA a generative probabilistic method that models documents as
mixtures of word topics We train LDA transductively on the combined collection of training
testing words and use the topic probabilities as the document representation Marginalized
Stacked Denoising Autoencoders mSDA a fast method for training stacked denoising autoencoders which have state-of-the-art error rates on sentiment analysis tasks For datasets larger
than RECIPE we use either a high-dimensional variant of mSDA or take of the features that
occur most often whichever has better performance Word Centroid Distance described
in Section Word Mover?s Distance described in Section For completeness we
also show results for the Supervised Word Centroid Distance S-WCD and the initialization of SWMD S-WMD init described in Section For methods that propose a document representation
as opposed to a distance we use the Euclidean distance between these vector representations for
visualization and kNN classification For the supervised metric learning results we first reduce the
dimensionality of each representation to dimensions if necessary with PCA and then run either NCA ITML or LMNN on the projected data We tune all free hyperparameters in all compared
methods with Bayesian optimization using the implementation of Gardner
kNN classification We show the kNN test error of all document representation and distance methods in Table For datasets that do not have a predefined train/test split BBCSPORT TWITTER
RECIPE CLASSIC and AMAZON we average results over five train/test splits and report standard errors For each dataset we highlight the best results in bold and those whose standard error
http://tinyurl.com/bayesopt
Table The kNN test error for all datasets and distances
DATASET
BBCSPORT
TWITTER
BOW
TF-IDF
KAPI
LSI
LDA
SDA
BOW
TF-IDF
KAPI
LSI
LDA
SDA
BOW
TF-IDF
KAPI
LSI
LDA
SDA
BOW
TF-IDF
KAPI
LSI
LDA
SDA
WCD
WMD
S-WCD
S-WMD INIT
S-WMD
RECIPE
OHSUMED
CLASSIC
REUTERS
NSUPERVISED
ITML
LMNN
NCA
ISTANCES IN THE ORD OVER FAMILY
AMAZON
NEWS
AVERAGE RANK
overlaps the mean of the best result On the right we also show the average rank across datasets
relative to unsupervised BOW bold indicates the best method We highlight the unsupervised
WMD in blue WMD and our new result in red Despite the very large number of competitive baselines S-WMD achieves the lowest kNN test error on datasets with the exception
of BBCSPORT CLASSIC and AMAZON On these datasets it achieves the 4th lowest on BBCSPORT
and CLASSIC and tied at 2nd on NEWS On average across all datasets it outperforms all other
26 methods Another observation is that S-WMD right after initialization S-WMD init performs
quite well However as training S-WMD is efficient shown in Table it is often well worth the
training time
For unsupervised baselines on datasets BBCSPORT
and OHSUMED where the previous state-of-the-art
MazdaTIFFBoone
fit motherboardhappening
WMD was beaten by LSI S-WMD reduces the eraskedautomotivehomosexuals
playoff motorcycles
ror of LSI relatively by and respectively
gay
dolphins computer animation
western
atheism
moon
In general supervision seems to help all methods
Keith
controller
tappedmotif graphics clippersecurity
on average One reason why NCA with a TF-IDF
orbitIsrael pro
lists Rutgershell
document representation may be performing better
talkNHL driver
hockeybiblical
firedbikerguns
auto saint
than S-WMD could be because of the long docu virtual
gunaltcircuit
autos
sell
cute
ment lengths in BBCSPORT and OHSUMED Havlabelride
riderrocket
Islamic
offerflight
ing denser BOW vectors may improve the inverse riding
keyDOD
cardrivers
document frequency weights which in turn may be IDEshipping
baseballcrypto
image
bikes
a good initialization for NCA to further fine-tune
monitor
story
Armenian card
polygon
Israeli
forsalefirearmsspace
On datasets with smaller documents such as TWITwarning
Turkishcopyencryption RISC
bus mouse
TER CLASSIC and REUTERS S-WMD outperforms
compatiblemotorcycle summarized
powerbookelectronics diamond
NCA with TF-IDF relatively by and
SCSI government chip
sun doctorNASA
respectively On CLASSIC WMD outperforms
DOS
S-WMD possibly because of a poor initialization
and that S-WMD uses the squared Euclidean dis Figure The words upweighted by
tance between word vectors which may be subop S-WMD on NEWS
timal for this dataset This however does not occur
for any other dataset
apple
bike
windows
sale
mac
Visualization Figure shows a 2D embedding of the test split of each dataset by WMD and
S-WMD using t-Stochastic Neighbor Embedding t-SNE The quality of a distance can be
visualized by how clustered points in the same class are Using this metric S-WMD noticeably
improves upon WMD on almost all the datasets Figure visualizes the top words with
largest weights learned by S-WMD on the NEWS dataset The size of each word is proportional
its learned weight We can observe that these upweighted words are indeed most representative for
the true classes of this dataset More detailed results and analysis can be found in the supplementary
Training time Table shows the training
times for S-WMD Note that the time to learn
the initial metric A is not included in time
shown in the second column Relative to the
initialization S-WMD is surprisingly fast This
is due to the fast gradient approximation and
the batch gradient descent introduced in Section and We note that these times are
comparable or even faster than the time it takes
to train a linear metric on the baseline methods
after PCA.
Table Distance computation times
ULL RAINING IMES
DATASET
BBCSPORT
TWITTER
RECIPE
OHSUMED
CLASSIC
REUTERS
AMAZON
NEWS
Related Work
METRICS
WCD WMD INIT
1M
1H
2H 7M
2H
WMD
4M
7M
1H
Metric learning is a vast field that includes both
supervised and unsupervised techniques
Yang Jin for a large survey Alongside NCA described in Section there are a number of popular methods for generalized Euclidean metric learning Large Margin Nearest Neighbors
LMNN learns a metric that encourages inputs with similar labels to be close in a local region
while encouraging inputs with different labels to be farther by a large margin Information-Theoretic
Metric Learning ITML learns a metric by minimizing a KL-divergence subject to generalized
Euclidean distance constraints Cuturi Avis was the first to consider learning the ground distance in the Earth Mover?s Distance In a similar work Wang Guibas learns a ground
distance that is not a metric with good performance in certain vision tasks Most similar to our
work Wang learn a metric within a generalized Euclidean EMD ground distance using
the framework of ITML for image classification They do not however consider re-weighting the
histograms which allows our method extra flexibility Until recently there has been relatively little
work towards learning supervised word embeddings as state-of-the-art results rely on making use
of large unlabeled text corpora Tang propose a neural language model that uses label
information from emoticons to learn sentiment-specific word embeddings
Conclusion
We proposed a powerful method to learn a supervised word mover?s distance and demonstrated
that it may well be the best performing distance metric for documents to date Similar to WMD
our S-WMD benefits from the large unsupervised corpus which was used to learn the word2vec
embedding The word embedding gives rise to a very good document distance which
is particularly forgiving when two documents use syntactically different but conceptually similar
words Two words may be similar in one sense but dissimilar in another depending on the articles in
which they are contained It is these differences that S-WMD manages to capture through supervised
training By learning a linear metric and histogram re-weighting through the optimal transport of
the word mover?s distance we are able to produce state-of-the-art classification results efficiently
Acknowledgments
The authors are supported in part by the grants from the
National Science Foundation and the Bill and Melinda Gates Foundation We also thank Dor Kedem
for many insightful discussions

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5754-coevolve-a-joint-point-process-model-for-information-diffusion-and-network-co-evolution.pdf

COEVOLVE A Joint Point Process Model for
Information Diffusion and Network Co-evolution
Mehrdad Farajtabar
Yichen Wang
Manuel Gomez-Rodriguez
Shuang Li
Hongyuan Zha
Le Song
Georgia Institute of Technology
MPI for Software Systems
mehrdad,yichen.wang,sli370}@gatech.edu
manuelgr@mpi-sws.org
zha,lsong}@cc.gatech.edu
Abstract
Information diffusion in online social networks is affected by the underlying network topology but it also has the power to change it Online users are constantly
creating new links when exposed to new information sources and in turn these
links are alternating the way information spreads However these two highly intertwined stochastic processes information diffusion and network evolution have
been predominantly studied separately ignoring their co-evolutionary dynamics
We propose a temporal point process model COEVOLVE for such joint dynamics allowing the intensity of one process to be modulated by that of the other
This model allows us to efficiently simulate interleaved diffusion and network
events and generate traces obeying common diffusion and network patterns observed in real-world networks Furthermore we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and
network evolution traces We experimented with both synthetic data and data gathered from Twitter and show that our model provides a good fit to the data as
well as more accurate predictions than alternatives
Introduction
Online social networks such as Twitter or Weibo have become large information networks where
people share discuss and search for information of personal interest as well as breaking news
In this context users often forward to their followers information they are exposed to via their
followees triggering the emergence of information cascades that travel through the network
and constantly create new links to information sources triggering changes in the network itself
over time Importantly recent empirical studies with Twitter data have shown that both information
diffusion and network evolution are coupled and network changes are often triggered by information
diffusion
While there have been many recent works on modeling information diffusion and network
evolution most of them treat these two stochastic processes independently and separately
ignoring the influence one may have on the other over time Thus to better understand information
diffusion and network evolution there is an urgent need for joint probabilistic models of the two
processes which are largely inexistent to date
In this paper we propose a probabilistic generative model COEVOLVE for the joint dynamics of
information diffusion and network evolution Our model is based on the framework of temporal
point processes which explicitly characterize the continuous time interval between events and it
consists of two interwoven and interdependent components refer to Appendix for an illustration
I. Information diffusion process We design an identity revealing multivariate Hawkes process to capture the mutual excitation behavior of retweeting events where the intensity of
such events in a user is boosted by previous events from her time-varying set of followees Al1
though Hawkes processes have been used for information diffusion before 17 18
the key innovation of our approach is to explicitly model the excitation due to a particular
source node hence revealing the identity of the source Such design reflects the reality that information sources are explicitly acknowledged and it also allows a particular information source to
acquire new links in a rate according to her informativeness
II. Network evolution process We model link creation as an information driven survival process
and couple the intensity of this process with retweeting events Although survival processes have
been used for link creation before the key innovation in our model is to incorporate retweeting events as the driving force for such processes Since our model has captured the source
identity of each retweeting event new links will be targeted toward the information sources with
an intensity proportional to their degree of excitation and each source?s influence
Our model is designed in such a way that it allows the two processes information diffusion and
network evolution unfold simultaneously in the same time scale and excise bidirectional influence
on each other allowing sophisticated coevolutionary dynamics to be generated see Figure
Importantly the flexibility of our model does not prevent us from efficiently simulating diffusion
and link events from the model and learning its parameters from real world data
Efficient simulation We design a scalable sampling procedure that exploits the sparsity of the
generated networks Its complexity is O(nd log where is the number of samples is the
number of nodes and is the maximum number of followees per user
Convex parameters learning We show that the model parameters that maximize the joint likelihood of observed diffusion and link creation events can be found via convex optimization
Finally we experimentally verify that our model can produce coevolutionary dynamics of information diffusion and network evolution and generate retweet and link events that obey common
information diffusion patterns cascade structure size and depth static network patterns
node degree and temporal network patterns shrinking diameter described in related literature Furthermore we show that by modeling the coevolutionary dynamics our model
provide significantly more accurate link and diffusion event predictions than alternatives in large
scale Twitter dataset
Backgrounds on Temporal Point Processes
A temporal point process is a random process whose realization consists of a list of discrete events
localized in time ti with ti and Many different types of data produced in online
social networks can be represented as temporal point processes such as the times of retweets and
link creations A temporal point process can be equivalently represented as a counting process
which records the number of events before time Let the history be the list of times of events
t2 tn up to but not including time Then the number of observed events in a small time
window dt between t+dt is dN ti dt and hence dN where
is a Dirac delta function More generally given a function we can define the convolution
with respect to dN as
dN
dN
ti
ti
The point process representation of temporal data is fundamentally different from the discrete time
representation typically used in social network analysis It directly models the time interval between
events as random variables and avoid the need to pick a time window to aggregate events It allows
temporal events to be modeled in a more fine grained fashion and has a remarkably rich theoretical
support
An important way to characterize temporal point processes is via the conditional intensity function
a stochastic model for the time of the next event given all the times of previous events Formally the conditional intensity function intensity for short is the conditional probability of
observing an event in a small window dt given the history
t)dt event in E[dN
where one typically assumes that only one event can happen in a small window of size dt
dN Then given a time we can also characterize the conditional probability that no event happens during and the conditional density that an event occurs at time
as exp and respectively Furthermore we can
express the log-likelihood of a list of events t2 tn in an observation window as
log ti
tn
This simple log-likelihood will later enable us to learn the parameters of our model from observed
data
Finally the functional form of the intensity is often designed to capture the phenomena of
interests Some useful functional forms we will use later are
Poisson process The intensity is assumed to be independent of the history but it can be
a time-varying function
Hawkes Process The intensity models a mutual excitation between events
dN
ti
ti
where exp(??t)I[t is an exponential triggering kernel is a baseline
intensity independent of the history Here the occurrence of each historical event increases the
intensity by a certain amount determined by the kernel and the weight making the intensity
history dependent and a stochastic process by itself We will focus on the exponential kernel in
this paper However other functional forms for the triggering kernel such as log-logistic function
are possible and our model does not depend on this particular choice and
iii Survival process There is only one event for an instantiation of the process
where becomes if an event already happened before
Generative Model of Information Diffusion and Network Co-evolution
In this section we use the above background on temporal point processes to formulate our probabilistic generative model for the joint dynamics of information diffusion and network evolution
Event Representation
We model the generation of two types of events tweet/retweet events er and link creation events
el Instead of just the time we record each event as a triplet
source
er or el
destination
time
For retweet event the triplet means that the destination node retweets at time a tweet originally
posted by source node Recording the source node reflects the real world scenario that information sources are explicitly acknowledged Note that the occurrence of event er does not mean
that is directly retweeting from or is connected to This event can happen when is retweeting
a message by another node where the original information source is acknowledged Node
will pass on the same source acknowledgement to its followers I agree @a @b @c
Original tweets posted by node are allowed in this notation In this case the event will simply be
er Given a list of retweet events up to but not including time the history Hus
of
retweets by due to source is Hus ei ui si ti and si The entire history
of retweet events is denoted as Hr Hus
For link creation event the triplet means that destination node creates at time a link to source
node from time on node starts following node To ease the exposition we restrict
ourselves to the case where links cannot be deleted and thus each directed link is created only
once However our model can be easily augmented to consider multiple link creations and deletions
per node pair as discussed in Section We denote the link creation history as Hl
Joint Model with Two Interwoven Components
Given users we use two sets of counting processes to record the generated events one for information diffusion and the other for network evolution More specifically
I. Retweet events are recorded using a matrix of size for each fixed time point The
s)-th entry in the matrix Nus counts the number of retweets of due to
source up to time These counting processes are identity revealing since they keep track of
the source node that triggers each retweet This matrix can be dense since Nus can be
nonzero even when node does not directly follow We also let dN dNus
II. Link events are recorded using an adjacency matrix of size for each fixed time point
The s)-th entry in the matrix Aus indicates whether is directly following
That is Aus means the directed link has been created before For simplicity of exposition
we do not allow self-links The matrix is typically sparse but the number of nonzero entries
can change over time We also define dA(t dAus
Then the interwoven information diffusion and network evolution processes can be characterized
using their respective intensities E[dN Hr Hl dt and E[dA(t Hr
Hl dt where us
and The sign
means that the intensity matrices will depend on the joint history Hr Hl and hence their
evolution will be coupled By this coupling we make the counting processes for link creation to
be information driven and the evolution of the linking structure to change the information diffusion process Refer to Appendix for an illustration of our joint model In the next two sections
we will specify the details of these two intensity matrices
Information Diffusion Process
We model the intensity for retweeting events using multivariate Hawkes process
us
I[u I[u
Auv dNvs
v?Fu
where is the indicator function and Fu Auv is the current set of followees of The term is the intensity of original
tweets by a user on his own initiative
becoming the source of a cascade and the term v?Fu Auv dNvs models the
propagation of peer influence over the network where the triggering kernel models the decay
of peer influence over time
Note that the retweet intensity matrix is by itself a stochastic process that depends on the timevarying network topology the non-zero entries in whose growth is controlled by the network
evolution process in Section Hence the model design captures the influence of the network
topology and each source?s influence on the information diffusion process More specifically
to compute us
one first finds the current set Fu of followees of and then aggregates
the retweets of these followees that are due to source Note that these followees may or may
not directly follow source Then the more frequently node is exposed to retweets of tweets
originated from source via her followees the more likely she will also retweet a tweet originated
from source Once node retweets due to source the corresponding Nus will be incremented
and this in turn will increase the likelihood of triggering retweets due to source among the followers
of Thus the source does not simply broadcast the message to nodes directly following her but
her influence propagates through the network even to those nodes that do not directly follow her
Finally this information diffusion model allows a node to repeatedly generate events in a cascade
and is very different from the independent cascade or linear threshold models which allow at
most one event per node per cascade
Network Evolution Process
We model the intensity for link creation using a combination of survival and Hawkes process
Aus dNus
where the term Aus effectively ensures a link is created only once and after that the corresponding intensity is set to zero The term denotes a baseline intensity which models when a
node decides to follow a source spontaneously at her own initiative The term t)?dNus
corresponds to the retweets of node due to tweets originally published by source where the triggering kernel models the decay of interests over time Here the higher the corresponding
retweet intensity the more likely will find information by source useful and will create a direct
link to
The link creation intensity is also a stochastic process by itself which depends on the retweet
events and is driven by the retweet count increments dNus It captures the influence of retweets
on the link creation and closes the loop of mutual influence between information diffusion and
network topology
Note that creating a link is more than just adding a path or allowing information sources to take
shortcuts during diffusion The network evolution makes fundamental changes to the diffusion
dynamics and stationary distribution of the diffusion process in Section As shown in
given a fixed network structure A the expected retweet intensity at time due to source
will depend of the network structure in a highly nonlinear fashion
I)t A I)t where Rm has a single nonzero entry
with value and I)t is the matrix exponential When the stationary intensity
I is also nonlinearly related to the network structure Thus given two network
structures and at two points in time which are different by a few edges the effect of
these edges on the information diffusion is not just simply an additive relation Depending on how
these newly created edges modify the eigen-structure of the sparse matrix their effect can be
drastic to the information diffusion
Remark In our model each user is exposed to information through a time-varying set of neighbors By doing so we couple information diffusion with the network evolution increasing the
practical application of our model to real-network datasets The particular definition of exposure
a retweet?s neighbor will depend on the type of historical information that is available Remarkably the flexibility of our model allows for different types of diffusion events which we can
broadly classify into two categories In a first category events corresponds to the times when an
information cascade hits a person for example through a retweet from one of her neighbors but
she does not explicitly like or forward the associated post In a second category the person decides
to explicitly like or forward the associated post and events corresponds to the times when she does
so Intuitively events in the latter category are more prone to trigger new connections but are also
less frequent Therefore it is mostly suitable to large event dataset for examples those ones generated synthetically In contrast the events in the former category are less likely to inspire new links
but found in abundance Therefore it is very suitable for real-world sparse data Consequently in
synthetic experiments we used the latter and in the real one we used the former It?s noteworthy that
is written based on the latter category but in appendix is drawn based on the former
Efficient Simulation of Coevolutionary Dynamics
We can simulate samples link creations tweets and retweets from our model by adapting Ogata?s
thinning algorithm originally designed for multidimensional Hawkes processes However a
naive implementation of Ogata?s algorithm would scale poorly for each sample we would
need to re-evaluate and thus to draw samples we would need to perform n2
operations where is the number of nodes
We designed a sampling procedure that is especially well-fitted for the structure of our model The
algorithm is based on the following key idea if we consider each intensity function in and
as a separate Hawkes process and draw a sample from each it is easy to show that the minimum among all these samples is a valid sample from the model However by drawing samples
from all intensities the computational complexity would not improve However when the network
is sparse whenever we sample a new node link event from the model only a small number
of intensity functions in the local neighborhood of the node the link will change As a consequence we can reuse most of the samples from the intensity functions for the next new sample
and find which intensity functions we need to change in O(log operations using a heap Finally we exploit the properties of the exponential function to update individual intensities for each
new sample in let ti and be two consecutive events then we can compute as
ti ti without the need to compare all previous events
The complete simulation algorithm is summarized in Algorithm in Appendix C. By using Algorithm we reduce the complexity from m2 to O(nd log where is the maximum number
of followees per node That means our algorithm scales logarithmically with the number of nodes
and linearly with the number of edges at any point in time during the simulation We also note that
the events for link creations tweets and retweets are generated in a temporally intertwined and inter5
Retweet
Intensity
Event occurrence time
Link
Cross covariance
Link
Spike trains
Retweet
Event occurrence time
Lag
Figure Coevolutionary dynamics for synthetic data Spike trains of link and retweet events
Link and retweet intensities Cross covariance of link and retweet intensities
Data
Power?law fit
Poisson fit
Power?law fit
Poisson fit
Data
Data
Poisson fit
Power?law fit
Poisson fit
Power?law fit
Data
Figure Degree distributions when network sparsity level reaches for fixed
leaving fashion by Algorithm This is because every new retweet event will modify the intensity
for link creation and after each link creation we also need to update the retweet intensities
Efficient Parameter Estimation from Coevolutionary Events
Given a collection of retweet events eri and link creation events A eli recorded within
a time window we can easily estimate the parameters needed in our model using maximum
likelihood estimation Here we compute the joint log-likelihood of
these events using
log ui si ti
us
log ui si ti
eri
tweet retweet
eli A
links
For the terms corresponding to retweets the log term only sums over the actual observed events
but the integral term actually sums over all possible combination of destination and source pairs
even if there is no event between a particular pair of destination and source For such pairs with
no observed events the corresponding counting processes have essentially survived the observation
window and the term us
simply corresponds to the log survival probability
Terms corresponding to links have a similar structure to those for retweet
Since us
and are linear in the parameters and respectively then log(?us
and log(?us are concave functions in these parameters Integration of us and us still results
in linear functions of the parameters Thus the overall objective in is concave and the global
optimum can be found by many algorithms In our experiments we adapt the efficient algorithm
developed in previous work Furthermore the optimization problem decomposes in
independent problems one per node and can be readily parallelized
Properties of Simulated Co-evolution Networks and Cascades
In this section we perform an empirical investigation of the properties of the networks and information cascades generated by our model In particular we show that our model can generate coevolutionary retweet and link dynamics and a wide spectrum of static and temporal network patterns
and information cascades Appendix contains additional simulation results and visualizations
Appendix contains an evaluation of our model estimation method in synthetic data
Retweet and link coevolution Figures visualize the retweet and link events aggregated
across different sources and the corresponding intensities for one node and one realization picked
at random Here it is already apparent that retweets and link creations are clustered in time and often
follow each other Further Figure shows the cross-covariance of the retweet and link creation
intensity computed across multiple realizations for the same node if t)"and are two
intensities the cross-covariance is a function of the time lag defined as dt
It can be seen that the cross-covariance has its peak around retweets and link creations are
sparsity
diameter
diameter
sparsity
Diameter Diameter
Figure Diameter for network sparsity Panels and show the diameter against sparsity
over time for fixed and for fixed respectively
percentage
percentage
Others
cascade size
others
others
cascade depth
Figure Distribution of cascade structure size and depth for different values and fixed
highly correlated and co-evolve over time For ease of exposition we illustrated co-evolution using
one node however we found consistent results across nodes
Degree distribution Empirical studies have shown that the degree distribution of online social
networks and microblogging sites follow a power law and argued that it is a consequence of
the rich get richer phenomena The degree distribution of a network is a power law if the expected
number of nodes md with degree is given by md where Intuitively the higher the
values of the parameters and the closer the resulting degree distribution follows a power-law
the lower their values the closer the distribution to an Erdos-Renyi random graph Figure
confirms this intuition by showing the degree distribution for different values of
Small shrinking diameter There is empirical evidence that the diameter of online social networks
and microblogging sites exhibit relatively small diameter and shrinks flattens as the network
grows Figures show the diameter on the largest connected component LCC
against the sparsity of the network over time for different values of and Although at the
beginning there is a short increase in the diameter due to the merge of small connected components
the diameter decreases as the network evolves Here nodes arrive to the network when they follow
are followed by a node in the largest connected component
Cascade patterns Our model can produce the most commonly occurring cascades structures as
well as heavy-tailed cascade size and depth distributions as observed in historical Twitter data
Figure summarizes the results The higher the value the shallower and wider the cascades
Experiments on Real Dataset
In this section we validate our model using a large Twitter dataset containing nearly tweet
retweet and link events from more than users We will show that our model can capture
the co-evolutionary dynamics and by doing so it predicts retweet and link creation events more
accurately than several alternatives Appendix contains detailed information about the dataset and
additional experiments
Retweet and link coevolution Figures visualize the retweet and link events aggregated
across different sources and the corresponding intensities given by our trained model for one node
picked at random Here it is already apparent that retweets and link creations are clustered in time
and often follow each other and our fitted model intensities successfully track such behavior Further Figure compares the cross-covariance between the empirical retweet and link creation
intensities and between the retweet and link creation intensities given by our trained model computed across multiple realizations for the same node The similarity between both cross-covariances
is striking and both has its peak around retweets and link creations are highly correlated and
co-evolve over time For ease of exposition as in Section we illustrated co-evolution using one
node however we found consistent results across nodes Appendix F).
Link prediction We use our model to predict the identity of the source for each test link event
given the historical link and retweet events before the time of the prediction and compare its
performance with two state of the art methods denoted as TRF and WENG TRF measures
Implementation codes are available at https://github.com/farajtabar/Coevolution
Intensity
Link
Event occurrence time
Retweet
Event occurrence time
Cross covariance
Link
Spike trains
Retweet
Estimated
Empirical
Lag
Figure Coevolutionary dynamics for real data Spike trains of link and retweet events
Estimated link and retweet intensities Empirical and estimated cross covariance of link and
retweet intensities
events
COEVOLVE
TRF
WENG
events
COEVOLVE
HAWKES
Top1
70
AvgRank
COEVOLVE
TRF
WENG
Top1
AvgRank
events
COEVOLVE
HAWKES
events
Links AR
Links Top-1
Activity AR
Activity Top-1
Figure Prediction performance in the Twitter dataset by means of average rank and success
probability that the true test events rank among the top-1 events
the probability of creating a link from a source at a given time by simply computing the proportion
of new links created from the source with respect to the total number of links created up to the given
time WENG considers different link creation strategies and makes a prediction by combining them
We evaluate the performance by computing the probability of all potential links using different
methods and then compute the average rank of all true test events AvgRank and the
success probability that the true test events rank among the top-1 potential events at each test
time We summarize the results in where we consider an increasing number of
training retweet/tweet events Our model outperforms TRF and WENG consistently For example
for training events our model achieves a SP times larger than TRF and WENG
Activity prediction We use our model to predict the identity of the node that is going to generate
each test diffusion event given the historical events before the time of the prediction and compare
its performance with a baseline consisting of a Hawkes process without network evolution For
the Hawkes baseline we take a snapshot of the network right before the prediction time and use
all historical retweeting events to fit the model Here we evaluate the performance the via the
same two measures as in the link prediction task and summarize the results in Figure against
an increasing number of training events The results show that by modeling the co-evolutionary
dynamics our model performs significantly better than the baseline
Discussion
We proposed a joint continuous-time model of information diffusion and network evolution which
can capture the coevolutionary dynamics mimics the most common static and temporal network
patterns observed in real-world networks and information diffusion data and predicts the network
evolution and information diffusion more accurately than previous state-of-the-arts Using point
processes to model intertwined events in information networks opens up many interesting future
modeling work Our current model is just a show-case of a rich set of possibilities offered by a point
process framework which have been rarely explored before in large scale social network modeling For example we can generalize our model to support link deletion by introducing an intensity
matrix modeling link deletions as survival processes gus
t)Aus
and then consider the counting process associated with the adjacency matrix to evolve as
E[dA(t)|Hr Hl dt dt We also can consider the number of nodes varying over time Furthermore a large and diverse range of point processes can also be used in the
framework without changing the efficiency of the simulation and the convexity of the parameter
estimation condition the intensity on additional external features such as node attributes
Acknowledge
The authors would like to thank Demetris Antoniades and Constantine Dovrolis for providing them
with the dataset The research was supported in part by NSF/NIH BIGDATA ONR
NSF NSF CAREER

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6193-learning-and-forecasting-opinion-dynamics-in-social-networks.pdf

Learning and Forecasting Opinion Dynamics in
Social Networks
Abir De
Isabel Valera
Niloy Ganguly
Sourangshu Bhattacharya
Manuel Gomez-Rodriguez
IIT Kharagpur
MPI for Software Systems
abir.de,niloy,sourangshu}@cse.iitkgp.ernet.in
ivalera,manuelgr}@mpi-sws.org
Abstract
Social media and social networking sites have become a global pinboard for exposition and discussion of news topics and ideas where social media users often
update their opinions about a particular topic by learning from the opinions shared
by their friends In this context can we learn a data-driven model of opinion dynamics that is able to accurately forecast users opinions In this paper we introduce SLANT a probabilistic modeling framework of opinion dynamics which
represents users opinions over time by means of marked jump diffusion stochastic differential equations and allows for efficient model simulation and parameter
estimation from historical fine grained event data We then leverage our framework to derive a set of efficient predictive formulas for opinion forecasting and
identify conditions under which opinions converge to a steady state Experiments
on data gathered from Twitter show that our model provides a good fit to the data
and our formulas achieve more accurate forecasting than alternatives
Introduction
Social media and social networking sites are increasingly used by people to express their opinions
give their hot takes on the latest breaking news political issues sports events and new products
As a consequence there has been an increasing interest on leveraging social media and social networking sites to sense and forecast opinions as well as understand opinion dynamics For example
political parties routinely use social media to sense people?s opinion about their political discourse1
quantitative investment firms measure investor sentiment and trade using social media and
corporations leverage brand sentiment estimated from users posts likes and shares in social media
and social networking sites to design their marketing campaigns2 In this context multiple methods
for sensing opinions typically based on sentiment analysis have been proposed in recent years
However methods for accurately forecasting opinions are still scarce despite the extensive
literature on theoretical models of opinion dynamics
In this paper we develop a novel modeling framework of opinion dynamics in social media and social networking sites SLANT3 which allows for accurate forecasting of individual users opinions
The proposed framework is based on two simple intuitive ideas users opinions are hidden until
they decide to share it with their friends neighbors and ii users may update their opinions
about a particular topic by learning from the opinions shared by their friends While the latter is one
of the main underlying premises used by many well-known theoretical models of opinion dynamics the former has been ignored by models of opinion dynamics despite its relevance on
closely related processes such as information diffusion
http://www.nytimes.com/2012/10/08/technology/campaigns-use-social-media-to-lure-younger-voters.html
http://www.nytimes.com/2012/07/31/technology/facebook-twitter-and-foursquare-as-corporate-focus-groups.html
Slant is a particular point of view from which something is seen or presented
Conference on Neural Information Processing Systems NIPS Barcelona Spain
More in detail our proposed model represents users latent opinions as continuous-time stochastic
processes driven by a set of marked jump stochastic differential equations SDEs Such construction allows each user?s latent opinion to be modulated over time by the opinions asynchronously
expressed by her neighbors as sentiment messages Here every time a user expresses an opinion by
posting a sentiment message she reveals a noisy estimate of her current latent opinion Then we
exploit a key property of our model the Markov property to develop
I. An efficient estimation procedure to find the parameters that maximize the likelihood of a
set of millions of sentiment messages via convex programming
II. A scalable simulation procedure to sample millions of sentiment messages from the proposed model in a matter of minutes
III. A set of novel predictive formulas for efficient and accurate opinion forecasting which
can also be used to identify conditions under which opinions converge to a steady state of
consensus or polarization
Finally we experiment on both synthetic and real data gathered from Twitter and show that our
model provides a good fit to the data and our predictive formulas achieve more accurate opinion
forecasting than several alternatives
Related work There is an extensive line of work on theoretical models of opinion dynamics and
opinion formation 17 However previous models typically share the following
limitations they do not distinguish between latent opinion and sentiment expressed opinion which is a noisy observation of the opinion thumbs up/down text sentiment they
consider users opinions to be updated synchronously in discrete time however opinions may be
updated asynchronously following complex temporal patterns iii the model parameters are
difficult to learn from real fine-grained data and instead are set arbitrarily as a consequence they
provide inaccurate fine-grained predictions and they focus on analyzing only the steady state
of the users opinions neglecting the transient behavior of real opinion dynamics which allows for
opinion forecasting methods More recently there have been some efforts on designing models that
overcome some of the above limitations and provide more accurate predictions However
they do not distinguish between opinion and sentiment and still consider opinions to be updated
synchronously in discrete time Our modeling framework addresses the above limitations and by
doing so achieves more accurate opinion forecasting than alternatives
Proposed model
In this section we first formulate our model of opinion dynamics starting from the data it is designed
for and then introduce efficient methods for model parameter estimation and model simulation
Opinions data Given a directed social network we record each message as
where the triplet means that the user posted a message with sentiment at time
Given a collection of messages m1 t1 en un mn tn the history Hu
gathers all messages posted by user up to but not including time
Hu ei ui mi ti and ti
and Hu denotes the entire history of messages up to but not including time
Generative process We represent users latent opinions as a multidimensional stochastic process
in which the u-th entry x?u represents the opinion of user at time and the sign
means that it may depend on the history Then every time a user posts a message at time
we draw its sentiment from a sentiment distribution p(m|x?u Here we can also think of the
sentiment of each message as samples from a noisy stochastic process mu p(mu
Further we represent the message times by a set of counting processes In particular we denote
the set of counting processes as a vector in which the u-th entry Nu counts
the number of sentiment messages user posted up to but not including time Then we can
characterize the message rate of the users using their corresponding conditional intensities as
E[dN dt
where dN dNu denotes the number of messages per user in the window dt
and denotes the associated user intensities which may depend on the history
We denote the set of user that follows by Next we specify the the intensity functions
the dynamics of the users opinions and the sentiment distribution p(m|x?u
Intensity for messages There is a wide variety of message intensity functions one can choose from
to model the users intensity In this work we consider two of the most popular functional
forms used in the growing literature on social activity modeling using point processes 24
I. Poisson process The intensity is assumed to be independent of the history and
constant
II. Multivariate Hawkes processes The intensity captures a mutual excitation phenomena between message events and depends on the whole history of message events
Hv before
bvu
ti
bvu dNv
v?u?N
ei Hv
v?u?N
where the first term models the publication of messages by user on her own
initiative and the second term with bvu models the publication of additional messages
by user due to the influence that previous messages posted by the users she follows have
on her intensity Here is an exponential triggering kernel modeling the decay
of influence of the past events over time and denotes the convolution operation
In both cases the couple is a Markov process future states of the process conditional on past and present states depends only upon the present state and we can express the users
intensity more compactly using the following jump stochastic differential equation
BdN
where the initial condition is The Markov property will become important later
Stochastic process for opinion The opinion x?u of a user at time adopts the following form
x?u
avu
mi g(t ti
avu mv t)dNv
v?N
ei Hv
v?N
where the first term models the original opinion a user starts with the second term
with avu models updates in user u?s opinion due to the influence that previous messages with
opinions mi posted by the users that follows has on her opinion Here where
denotes an exponential triggering kernel which models the decay of influence over time
The greater the value of the greater the user?s tendency to retain her own opinion Under this
form the resulting opinion dynamics are Markovian and can be compactly represented by a set of
coupled marked jumped stochastic differential equations proven in Appendix
Proposition The tuple is a Markov process whose dynamics are defined by
the following marked jumped stochastic differential equations
dx dN
dN
where the initial conditions are and the marks are the sentiment messages
mu with mu p(m|x?u and the sign denotes pointwise product
The above mentioned Markov property will be the key to the design of efficient model parameter
estimation and model simulation algorithms
Sentiment distribution The particular choice of sentiment distribution p(m|x?u depends on the
recorded marks For example one may consider
I. Gaussian Distribution The sentiment is assumed to be a real random variable
p(m|xu xu This fits well scenarios in which sentiment is extracted from
text using sentiment analysis
II. Logistic The sentiment is assumed to be a binary random variable
p(m|xu exp(?m xu This fits well scenarios in which sentiment is
measured by means of up votes down votes or likes
Our model estimation method can be easily adapted to any log-concave sentiment distribution However in the remainder of the paper we consider the Gaussian distribution since in our experiments
sentiment is extracted from text using sentiment analysis
Model parameter estimation
Given a collection of messages H(T mi ti recorded during a time period in
a social network we can find the optimal parameters A and by solving a
maximum likelihood estimation MLE problem4 To do so it is easy to show that the log-likelihood
of the messages is given by
XZ
A
log p(mi x?ui ti
log ti
ei
message sentiments
ei
maximize
A B).
u?V
message times
Then we can find the optimal parameters A using MLE as
Note that as long as the sentiment distributions are log-concave the MLE problem above is concave and thus can be solved efficiently Moreover the problem decomposes in independent
subproblems two per user since the first term in only depends on A whereas the last
two terms only depend on and thus can be readily parallelized Then we find using spectral projected gradient descent which works well in practice and achieves accuracy in
iterations and find A analytically since for Gaussian sentiment distributions
the problem reduces to a least-square problem Fortunately in each subproblem we can use the
Markov property from Proposition to precompute the sums and integrals in in linear time
O(|Hu Hv Appendix summarizes the overall estimation algorithm
Model simulation
We leverage the efficient sampling algorithm for multivariate Hawkes introduced by Farajtabar
to design a scalable algorithm to sample opinions from our model The two key ideas that
allow us to adapt the procedure by Farajtabar to our model of opinion dynamics while keeping
its efficiency are as follows the opinion dynamics defined by Eqs. and are Markovian and
thus we can update individual intensities and opinions in let ti and be two consecutive
events then we can compute as ti ti and as
ti ti respectively and social networks are typically sparse
and thus both A and are also sparse then whenever a node expresses its opinion only a small
number of opinions and intensity functions in its local neighborhood will change As a consequence
we can reuse the majority of samples from the intensity functions and sentiment distributions for the
next new sample Appendix I summarizes the overall simulation algorithm
Opinion forecasting
Our goal here is developing efficient methods that leverage our model to forecast a user u?s
opinion xu at time given the history up to time t0 In the context of our probabilistic model we will forecast this opinion by efficiently computing the conditional expectation
where denotes the average across histories from t0 to
while conditioning on the history up to
To this aim we will develop analytical and sampling based methods to compute the above conditional expectation Moreover we will use the former to identify under which conditions users
average opinion converges to a steady state and if so find the steady state opinion In this section
we write Ht to lighten the notation and denote the eigenvalues of a matrix by
Analytical forecasting
In this section we derive a set of formulas to compute the conditional expectation for both Poisson
and Hawkes messages intensities However since the derivation of such formulas for general multivariate Hawkes is difficult we focus here on the case when bvu for all and
rely on the efficient sampling based method for the general case
I. Poisson intensity Consider each user?s messages follow a Poisson process with rate Then
the conditional average opinion is given by proven in Appendix
Here if one decides to model the message intensities with a Poisson process
Theorem Given a collection of messages Ht0 recorded during a time period t0 and
for all then
EHt I
where diag and
v?N
auv
ti Hv
ti
mv ti
Remarkably we can efficiently compute both terms in by using the iterative algorithm by AlMohy for the matrix exponentials and the well-known GMRES method for the matrix
inversion Given this predictive formula we can easily study the stability condition and for stable
systems find the steady state conditional average opinion proven in Appendix
Theorem Given the conditions of Theorem if then
lim EHt I
The above results indicate that the conditional average opinions are nonlinearly related to the parameter matrix A which depends on the network structure and the message rates which in this case
are assumed to be constant and independent on the network structure Figure provides empirical
evidence of these results
II. Multivariate Hawkes Process Consider each user?s messages follow a multivariate Hawkes
process given by and bvu for all Then the conditional average opinion
is given by proven in Appendix
Theorem
Given a collection of messages Ht0 recorded during a time period t0 and
buu ei Hu for all then the conditional average satisfies the following
differential equation
dEHt
A?(t)]EHt
dt
where
diag EHt
EHt I t0
buv
ti
v?N
ti Hv
diag
Here we can compute the conditional average by solving numerically the differential equation
above which is not stochastic where we can efficiently compute the vector EHt by using
again the algorithm by Al-Mohy and the GMRES method
In this case the stability condition and the steady state conditional average opinion are given by
proven in Appendix
Theorem Given the conditions of Theorem if the transition matrix associated to the timevarying linear system described by satisfies that e?ct where
then
lim EHt I
where diag I
The above results indicate that the conditional average opinions are nonlinearly related to the parameter matrices A and B. This suggests that the effect of the temporal influence on the opinion
evolution by means of the parameter matrix of the multivariate Hawkes process is non trivial
We illustrate this result empirically in Figure
Theoretical
E[xu
u?V
Opinion-Trajectory
Opinion-Trajectory
Network G1
Theoretical
Experimental
E[xu
u?V
Network G2
Hawkes
Hawkes
u?V E[xu
Time
Temporal evolution
Time
Time
u?V E[xu
Time
Time
Temporal evolution
Time
Time
Hawkes
Node-ID
Hawkes
Experimental
Node-ID
Node-ID
Node-ID
Opinion-Trajectory
Opinion-Trajectory
Temporal evolution
Time
Temporal evolution
Figure Opinion dynamics on two 50-node networks G1 top and G2 bottom for Poisson
and Hawkes message intensities The first column visualizes the two networks and opinion of
each node at positive/negative opinions in red/blue The second column shows the temporal
evolution of the theoretical and empirical average opinion for Poisson intensities The third column
shows the temporal evolution of the empirical average opinion for Hawkes intensities where we
compute the average separately for positive and negative opinions in the steady state The
fourth and fifth columns shows the polarity of average opinion per user over time
Simulation based forecasting
Given the efficient simulation procedure described in Section we can readily derive a general
simulation based formula for opinion forecasting
1X
EHt
xl
where is the number of times that we simulate the opinion dynamics and x?l gathers the users
opinion at time for the l-th simulation Moreover we have the following theoretical guarantee
proven in Appendix
Theorem Simulate the opinion dynamics up to time t0 the following number of times
4xmax
where max
maxu?G
is the maximum variance of the users opinions which
we analyze in Appendix and xmax xu is an upper bound on the users absolute
opinions Then for each user the error between her true and estimated average opinion
satisfies that
x?u EHt with probability at least
Experiments
Experiments on synthetic data
We first provide empirical evidence that our model is able to produce different types of opinion
dynamics which may or may not converge to a steady state of consensus or polarization Then we
show that our model estimation and simulation algorithms as well as our predictive formulas scale
to networks with millions of users and events Appendix contains an evaluation of the accuracy of
our model parameter estimation method
Different types of opinion dynamics We first simulate our model on two different small networks
using Poisson intensities and then simulate our model on the
same networks while using Hawkes intensities with bvu on of the nodes chosen at
random and the original Poisson intensities on the remaining nodes Figure summarizes the results which show that our model is able to produce opinion dynamics that converge to consensus
second column and polarization third column the opinion forecasting formulas described in
Section closely match an simulation based estimation second column and iii the evolution of
Nodes
Poisson
Hawkes
Nodes
Time
Time
Time(s
Time
Informational
Temporal
Nodes
Estimation vs nodes Simulation vs nodes Forecast vs nodes
Poisson
Hawkes
02
Forecast-Time[T(hr
Forecast vs
Figure Panels and show running time of our estimation and simulation procedures against
number of nodes where the average number of events per node is Panels and show the
running time needed to compute our analytical formulas against number of nodes and time horizon
t0 where the number of nodes is In Panel hours For all panels the average
degree per node is The experiments are carried out in a single machine with 24 cores and 64 GB
of main memory
the average opinion and whether opinions converge to a steady state of consensus or polarization
depend on the functional form of message intensity5
Scalability Figure shows that our model estimation and simulation algorithms described in Sections and and our analytical predictive formulas described in Section scale to networks
with millions of users and events For example our algorithm takes minutes to estimate the
model parameters from million events generated by one million nodes using a single machine
with 24 cores and 64 GB RAM.
Experiments on real data
We use real data gathered from Twitter to show that our model can forecast users opinions more
accurately than six state of the art methods 19 Appendix L).
Experimental Setup We experimented with five Twitter datasets about current real-world events
Politics Movie Fight Bollywood and in which for each recorded message we compute its
sentiment value mi using a popular sentiment analysis toolbox specially designed for Twitter
Here the sentiment takes values and we consider the sentiment polarity to be simply
sign(m Appendix contains further details and statistics about these datasets
Opinion forecasting We first evaluate the performance of our model at predicting sentiment expressed opinion at a message level To do so for each dataset we first estimate the parameters of
our model SLANT using messages from a training set containing the chronologically first
of the messages Here we set the decay parameters of the exponential triggering kernels and
by cross-validation Then we evaluate the predictive performance of our opinion forecasting
formulas using the last of the messages6 More specifically we predict the sentiment value
for each message posted by user in the test set given the history up to hours before the time
of the message as
EHt Ht?T We compare the performance of our model with
the asynchronous linear model AsLM DeGroot?s model the voter model the biased
voter model the flocking model and the sentiment prediction method based on collaborative filtering by Kim in terms of the mean squared error between the true and the
estimated
sentiment value for all messages in the held-out set
and the
failure rate defined as the probability that the true and the estimated polarity do not coincide
P(sign(m sign(m
For the baselines algorithms which work in discrete time we simulate NT
rounds in where NT is the number of posts in time Figure summarizes the results
which show that our opinion forecasting formulas consistently outperform others both in terms
of MSE often by an order of magnitude and failure rate;7 its forecasting performance degrades
gracefully with respect to in contrast competing methods often fail catastrophically and iii it
achieves an additional mileage by using Hawkes processes instead of Poisson processes To some
extent we believe SLANT?s superior performance is due to its ability to leverage historical data to
learn its model parameters and then simulate realistic temporal patterns
Finally we look at the forecasting results at a network level and show that our forecasting formulas
can also predict the evolution of opinions macroscopically terms of the average opinion across
users Figure summarizes the results for two real world datasets which show that the forecasted
For these particular networks Poisson intensities lead to consensus while Hawkes intensities lead to polarization however we did find
other examples in which Poisson intensities lead to polarization and Hawkes intensities lead to consensus
Here we do not distinguish between analytical and sampling based forecasting since in practice they closely match each other
The failure rate is very close to zero for those datasets in which most users post messages with the same polarity
Flocking
Collab-Filter
BiasedVoter
DeGroot
Linear
SLANT
SLANT
Voter
MSE
Failure-Rate
hours
hours
hours
hours
hours
hours
Politics
hours
Movie
hours
Fight
hours
Bollywood
hours
US
1h
3h
5h
28 April
May
Time
May
1h
3h
5h
28 April
May
Time
May
Tw Movie Hawkes Tw Movie Poisson
1h
3h
5h
April
April
Time
13 April
Tw US Hawkes
Average Opinion
Average Opinion
Average Opinion
Average Opinion
Figure Sentiment prediction performance using a held-out set for each real-world dataset
Performance is measured in terms of mean squared error MSE on the sentiment value
and failure rate on the sentiment polarity P(sign(m sign(m
For each message in the
held-out set we predict the sentiment value given the history up to hours before the time of
the message for different values of Nowcasting corresponds to and forecasting to
The sentiment value and the sentiment polarity sign
1h
3h
5h
April
April
Time
13 April
Tw US Poisson
Figure Macroscopic sentiment prediction given by our model for two real-world datasets The
panels show the observed sentiment
blue running average inferred opinion
on the
training set red and forecasted opinion EHt Ht?T xu for and hours on
the test set black green and gray respectively where the symbol denotes average across users
opinions become less accurate as the time becomes larger since the average is computed on longer
time periods As expected our model is more accurate when the message intensities are modeled
using multivariate Hawkes We found qualitatively similar results for the remaining datasets
Conclusions
We proposed a modeling framework of opinion dynamics whose key innovation is modeling users
latent opinions as continuous-time stochastic processes driven by a set of marked jump stochastic
differential equations SDEs Such construction allows each user?s latent opinion to be modulated over time by the opinions asynchronously expressed by her neighbors as sentiment messages
We then exploited a key property of our model the Markov property to design efficient parameter
estimation and simulation algorithms which scale to networks with millions of nodes Moreover we
derived a set of novel predictive formulas for efficient and accurate opinion forecasting and identified
conditions under which opinions converge to a steady state of consensus or polarization Finally we
experimented with real data gathered from Twitter and showed that our framework achieves more
accurate opinion forecasting than state-of-the-arts
Our model opens up many interesting venues for future work For example in our model
assumes a linear dependence between users opinions however in some scenarios this may be a
coarse approximation A natural follow-up to improve the opinion forecasting accuracy would be
considering nonlinear dependences between opinions It would be interesting to augment our model
to jointly consider correlations between different topics One could leverage our modeling framework to design opinion shaping algorithms based on stochastic optimal control Finally one
of the key modeling ideas is realizing that users expressed opinions be it in the form of thumbs
up/down or text sentiment can be viewed as noisy discrete samples of the users latent opinion localized in time It would be very interesting to generalize this idea to any type of event data and
derive sampling theorems and conditions under which an underlying general continuous signal of
interest be it user?s opinion or expertise can be recovered from event data with provable guarantees
Acknowledgement Abir De is partially supported by Google India under the Google India PhD Fellowship
Award and Isabel Valera is supported by a Humboldt post-doctoral fellowship

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5793-the-population-posterior-and-bayesian-modeling-on-streams.pdf

The Population Posterior
and Bayesian Modeling on Streams
James McInerney
Columbia University
james@cs.columbia.edu
Rajesh Ranganath
Princeton University
rajeshr@cs.princeton.edu
David Blei
Columbia University
david.blei@columbia.edu
Abstract
Many modern data analysis problems involve inferences from streaming data However streaming data is not easily amenable to the standard probabilistic modeling
approaches which require conditioning on finite data We develop population
variational Bayes a new approach for using Bayesian modeling to analyze streams
of data It approximates a new type of distribution the population posterior which
combines the notion of a population distribution of the data with Bayesian inference in a probabilistic model We develop the population posterior for latent
Dirichlet allocation and Dirichlet process mixtures We study our method with
several large-scale data sets
Introduction
Probabilistic modeling has emerged as a powerful tool for data analysis It is an intuitive language
for describing assumptions about data and provides efficient algorithms for analyzing real data under
those assumptions The main idea comes from Bayesian statistics We encode our assumptions about
the data in a structured probability model of hidden and observed variables we condition on a data
set to reveal the posterior distribution of the hidden variables and we use the resulting posterior as
needed for example to form predictions through the posterior predictive distribution or to explore the
data through the posterior expectations of the hidden variables
Many modern data analysis problems involve inferences from streaming data Examples include
exploring the content of massive social media streams Twitter Facebook analyzing live video
streams estimating the preferences of users on an online platform for recommending new items and
predicting human mobility patterns for anticipatory computing Such problems however cannot
easily take advantage of the standard approach to probabilistic modeling which requires that we
condition on a finite data set
This might be surprising to some readers after all one of the tenets of the Bayesian paradigm is that
we can update our posterior when given new information Yesterday?s posterior is today?s prior
But there are two problems with using Bayesian updating on data streams The first problem is that
Bayesian inference computes posterior uncertainty under the assumption that the model is correct
In theory this is sensible but only in the impossible scenario where the data truly came from the
proposed model In practice all models provide approximations to the data-generating distribution
and when the model is incorrect the uncertainty that maximizes predictive likelihood may be larger or
smaller than the Bayesian posterior variance This problem is exacerbated in potentially never-ending
streams after seeing only a few data points uncertainty is high but eventually the model becomes
overconfident
The second problem is that the data stream might change over time This is an issue because
frequently our goal in applying probabilistic models to streams is not to characterize how they
change but rather to accommodate it That is we would like for our current estimate of the latent
variables to be accurate to the current state of the stream and to adapt to how the stream might slowly
change This is in contrast for example to time series modeling Traditional Bayesian updating
cannot handle this Either we explicitly model the time series and pay a heavy inferential cost or we
tacitly assume that the data are exchangeable that the underlying distribution does not change
In this paper we develop new ideas for analyzing data streams with probabilistic models Our
approach combines the frequentist notion of the population distribution with probabilistic models and
Bayesian inference
Main idea The population posterior Consider a latent variable model of data points This
is unconventional notation we will describe why we use it below Following we define the
model to have two kinds of hidden variables global hidden variables contain latent structure that
potentially governs any data point local hidden variables zi contain latent structure that only governs
the ith data point Such models are defined by the joint
p(xi zi
where and Traditional Bayesian statistics conditions on a fixed data set to obtain
the posterior distribution of the hidden variables As we discussed this framework cannot
accommodate data streams We need a different way to use the model
We define a new distribution the population posterior which enables us to consider Bayesian
modeling of streams Suppose we observe data points independently from the underlying population
distribution This induces a posterior which is a function of the random data
The population posterior is the expected value of this distribution
EF EF
Notice that this distribution is not a function of observed data it is a function of the population
distribution and the data size The data size is a hyperparameter that can be set it effectively
controls the variance of the population posterior How to best set it depends on how close the model
is to the true data distribution
We have defined a new problem Given an endless stream of data points coming from and a value
for our goal is to approximate the corresponding population posterior In this paper we will
approximate it through an algorithm based on variational inference and stochastic optimization As
we will show our algorithm justifies applying a variant of stochastic variational inference to
a data stream We used our method to analyze several data streams with two modern probabilistic
models latent Dirichlet allocation and Dirichlet process mixtures With held-out likelihood
as a measure of model fitness we found our method to give better models of the data than approaches
based on full Bayesian inference or Bayesian updating
Related work Researchers have proposed several methods for inference on streams of data
Refs propose extending Markov chain Monte Carlo methods for streaming data However
sampling-based approaches do not scale to massive datasets the variational approximation enables
more scalable inference In variational inference Ref. propose online variational inference by
exponentially forgetting the variational parameters associated with old data Stochastic variational
inference SVI also decay parameters derived from old data but interprets this in the context of
stochastic optimization Neither of these methods applies to streaming data both implicitly rely on
the data being of known size even when subsampling data to obtain noisy gradients
To apply the variational approximation to streaming data Ref. and Ref. both propose
Bayesian updating of the approximating family Ref. adapts this framework to nonparametric
mixture models Here we take a different approach changing the variational objective to incorporate
a population distribution and then following stochastic gradients of this new objective In Section
we show that this generally performs better than Bayesian updating
Independently Ref. applied SVI to streaming data by accumulating new data points into a
growing window and then uniformly sampling from this window to update the variational parameters
Our method justifies that approach Further they propose updating parameters along a trust region
instead of following natural gradients as a way of mitigating local optima This innovation can be
incorporated into our method
Variational Inference for the Population Posterior
We develop population variational Bayes a method for approximating the population posterior in
Our method is based on variational inference and stochastic optimization
The F-ELBO The idea behind variational inference is to approximate difficult-to-compute distributions through optimization We introduce an approximating family of distributions over the
latent variables and try to find the member of that minimizes the Kullback-Leibler
divergence to the target distribution
Population variational Bayes uses variational inference to approximate the population posterior
in It aims to minimize the KL divergence from an approximating family
arg min
As for the population posterior this objective is a function of the population distribution of data
points Notice the difference to classical VB. In classical VB we optimize the KL divergence
between and a posterior its objective is a function of a fixed data set
In contrast the objective in is a function of the population distribution
We will use the mean-field variational family where each latent variable is independent and governed
by a free parameter
q(zi
The free variational parameters are the global parameters and local parameters Though we
focus on the mean-field family extensions could consider structured families where there is
dependence between variables
In classical VB where we approximate the usual posterior we cannot compute the KL. Thus we
optimize a proxy objective called the ELBO evidence lower bound that is equal to the negative KL
up to an additive constant Maximizing the ELBO is equivalent to minimizing the KL divergence to
the posterior
In population VB we also optimize a proxy objective the F-ELBO The F-ELBO is an expectation of
the ELBO under the population distribution of the data
EF Eq log log log p(Xi Zi log q(Zi
The F-ELBO is a lower bound on the population evidence log EF and a lower bound on the
negative KL to the population posterior See Appendix The inner expectation is over the latent
variables and and is a function of the variational distribution The outer expectation is over
the random data points and is a function of the population distribution The F-ELBO is
thus a function of both the variational distribution and the population distribution
As we mentioned classical VB maximizes the classical ELBO which is equivalent to minimizing
the KL. The F-ELBO in contrast is only a bound on the negative KL to the population posterior
Thus maximizing the F-ELBO is suggestive but is not guaranteed to minimize the KL. That said our
studies show that this is a good quantity to optimize and in Appendix A we show that the F-ELBO
does minimize EF the population KL.
Conditionally conjugate models In the next section we will develop a stochastic optimization
algorithm to maximize First we describe the class of models that we will work with
Following we focus on conditionally conjugate models A conditionally conjugate model is one
where each complete conditional?the conditional distribution of a latent variable given all the other
latent variables and the observations?is in the exponential family This class includes many models
in modern machine learning such as mixture models topic models many Bayesian nonparametric
models and some hierarchical regression models Using conditionally conjugate models simplifies
many calculations in variational inference
Under the joint in we can write a conditionally conjugate model with two exponential families
p(zi h(zi exp t(zi
exp
We overload notation for base measures sufficient statistics and log normalizers Note
that is the hyperparameter and that
In conditionally conjugate models each complete conditional is in an exponential family and we
use these families as the factors in the variational distribution in Thus indexes the same
family as and indexes the same family as p(zi For example in latent Dirichlet
allocation the complete conditional of the topics is a Dirichlet the complete conditional of
the per-document topic mixture is a Dirichlet and the complete conditional of the per-word topic
assignment is a categorical See for details
Population variational Bayes We have described the ingredients of our problem We are given a
conditionally conjugate model described in Eqs. and a parameterized variational family in
and a stream of data from an unknown population distribution F. Our goal is to optimize the F-ELBO
in with respect to the variational parameters
The F-ELBO is a function of the population distribution which is an unknown quantity To overcome
this hurdle we will use the stream of data from to form noisy gradients of the F-ELBO we then
update the variational parameters with stochastic optimization technique to find a local optimum
by following noisy unbiased gradients
Before describing the algorithm however we acknowledge one technical detail Mirroring we
optimize an F-ELBO that is only a function of the global variational parameters The one-parameter
population VI objective is LF max LF This implicitly optimizes the local parameter
as a function of the global parameter and allows us to convert the potentially infinite-dimensional
optimization problem in to a finite one The resulting objective is identical to but with
replaced by Details are in Appendix B).
The next step is to form a noisy gradient of the F-ELBO so that we can use stochastic optimization
to maximize it Stochastic optimization maximizes an objective by following noisy and unbiased
gradients We will write the gradient of the F-ELBO as an expectation with respect to and
then use Monte Carlo estimates to form noisy gradients
We compute the gradient of the F-ELBO by bringing the gradient operator inside the expectations of
This results in a population expectation of the classical VB gradient with data points
We take the natural gradient which has a simple form in completely conjugate models
Specifically the natural gradient of the F-ELBO is
EF E?i t(xi Zi
We approximate this expression using Monte Carlo to compute noisy unbiased natural gradients at
To form the Monte Carlo estimate we collect data points from for each we compute the optimal
local parameters which is a function of the sampled data point and variational parameters we
then compute the quantity inside the brackets in Averaging these results gives the Monte Carlo
estimate of the natural gradient We follow the noisy natural gradient and repeat
The algorithm is summarized in Algorithm Because is a Monte Carlo estimate we are free to
draw data points from where and rescale the sufficient statistics by This makes
the natural gradient estimate noisier but faster to calculate As highlighted in this strategy is
more computationally efficient because early iterations of the algorithm have inaccurate values of
It is wasteful to pass through a lot of data before making updates to
Discussion Thus far we have defined the population posterior and showed how to approximate
it with population variational inference Our derivation justifies using an algorithm like stochastic
variational inference SVI on a stream of data It is nearly identical to SVI but includes an
additional parameter the number of data points in the population posterior
For
most models of interest this is justified by the dominated convergence theorem
Algorithm Population Variational Bayes
Randomly initialize global variational parameter
Set iteration
repeat
Draw data minibatch
Optimize local variational parameters
see
Calculate natural gradient
Update global variational parameter with learning rate
Update iteration count
until forever
Note we can recover the original SVI algorithm as an instance of population VI thus reinterpreting it
as minimizing the KL divergence to the population posterior We recover SVI by setting equal to
the number of data points in the data set and replacing the stream of data with F?x the empirical
distribution of the observations The stream in this case comes from sampling with replacement
from F?x which results in precisely the original SVI algorithm.2
We focused on the conditionally conjugate family for convenience the simple gradient in
We emphasize however that by using recent tools for nonconjugate inference 18 we
can adapt the new ideas described above?the population posterior and the F-ELBO?outside of
conditionally conjugate models
Finally we analyze the population posterior distribution under the assumption the only way
the stream affects the model is through the data Formally this means the unobserved variables in the model and the stream are independent given the data The population posterior without the local latent variables
which can be marginalized out is EF X)].
Expanding the expectation gives X)p(X showing that the population posterior distribution can be written as This can be depicted as a graphical model
This means first that the population posterior is well defined even when the model does not specify
the marginal distribution of the data and second rather than the classical Bayesian setting where the
posterior is conditioned on a finite fixed dataset the population posterior is a distributional posterior
conditioned on the stream
Empirical Evaluation
We study the performance of population variational Bayes population VB against SVI and streaming
variational Bayes SVB With large real-world data we study two models latent Dirichlet
allocation and Bayesian nonparametric mixture models comparing the held-out predictive
performance of the algorithms All three methods share the same local variational update which
is the dominating computational cost We study the data coming in a true ordered stream and in a
permuted stream to better match the assumptions of SVI Across data and models population VB
usually outperforms the existing approaches
Models We study two models The first is latent Dirichlet allocation LDA LDA is a
mixed-membership model of text collections and is frequently used to find its latent topics LDA
assumes that there are topics each of which is a multinomial distribution over a fixed
vocabulary Documents are drawn by first choosing a distribution over topics and then
This derivation of SVI is an application of Efron?s plug-in principle applied to inference of the
population posterior The plug-in principle says that we can replace the population with the empirical
distribution of the data to make population inferences In our empirical study however we found that
population VI often outperforms stochastic VI. Treating the data in a true stream and setting the number of data
points different to the true number can improve predictive accuracy
held out log likelihood
Time-ordered stream
New York Times
Twitter
18
Population-VB
Streaming-VB
Science
70
number of documents seen
held out log likelihood
Random time-permuted stream
New York Times
Science
18
Population-VB
Streaming-VB
SVI
Twitter
70
number of documents seen
Figure Held out predictive log likelihood for LDA on large-scale streamed text corpora PopulationVB outperforms existing methods for two out of the three settings We use the best settings of
drawing each word by choosing a topic assignment zdn Mult(?d and finally choosing a word from
the corresponding topic wdn zdn The joint distribution is
p(zdi p(wdi zdi
Fixing hyperparameters the inference problem is to estimate the conditional distribution of the topics
given a large collection of documents
The second model is a Dirichlet process mixture Loosely DP mixtures are mixture models
with a potentially infinite number of components thus choosing the number of components is part
of the posterior inference problem When using variational inference for DP mixtures we take
advantage of the stick breaking representation to construct a truncated variational approximation
The variables are mixture proportions Stick mixture components for infinite
mixture assignments zi and observations G(?zi The joint is
p(zi
The likelihood and prior on the components are general to the observations at hand In our study
of real-valued data we use normal priors and normal likelihoods in our study of text data we use
Dirichlet priors and multinomial likelihoods
For both models we vary usually fixed to the number of data points in traditional analysis
Datasets With LDA we analyze three large-scale streamed corpora articles from the New
York Times spanning years Science articles written over years and tweets
collected from Twitter on Feb We processed them all in a similar way choosing a
vocabulary based on the most frequent words in the corpus with stop words removed for the
New York Times for Science and for Twitter On Twitter each tweet is a document
and we removed duplicate tweets and tweets that did not contain at least words in the vocabulary
For each data stream all algorithms took a few hours to process all the examples we collected
With DP mixtures we analyze human location behavior data These data allow us to build periodic
models of human population mobility with applications to disaster response and urban planning
Such models account for periodicity by including the hour of the week as one of the dimensions of the
Time-ordered stream
held out log likelihood
Ivory Coast Locations
Geolife Locations
New York Times
Population-VB best
Streaming-VB
18
number of data points seen
held out log likelihood
Random time-permuted stream
Ivory Coast Locations
Geolife Locations
Population-VB best
Streaming-VB
SVI
New York Times
18
number of data points seen
Figure Held out predictive log likelihood for Dirichlet process mixture models on large-scale
streamed location and text data sets Note that we apply Gaussian likelihoods in the Geolife dataset
so the reported predictive performance is measured by probability density We chose the best for
each population-VB curve
held out log likelihood
Population-VB sensitivity to for LDA
New York Times
Science
Twitter
Population-VB true
logarithm base of
held out log likelihood
Population-VB sensitivity to for DP-Mixture
Ivory Coast Locations
Geolife Locations
New York Times
Population-VB true
logarithm base of
Figure We show the sensitivity of population-VB to hyperparameter based on final log
likelihoods in the time-ordered stream and find that the best setting of often differs from the true
number of data points which may not be known in any case in practice
data to be modeled The Ivory Coast location data contains discrete cell tower locations for
users recorded over months The Microsoft Geolife dataset contains latitude-longitude
GPS locations for users over years For both data sets our observations reflect down-sampling
the data to ensure that each individual is seen no more than once every minutes
Results We compare population VB with SVI and SVB for LDA and DP mixtures
SVB updates the variational approximation of the global parameter using density filtering with
exponential families The complexity of the approximation remains fixed as the expected sufficient
statistics from minibatches observed in a stream are combined with those of the current approximation
Here we give the final results We include details of how we set and fit hyperparameters below
We measure model fitness by evaluating the average predictive log likelihood on held-out data This
involves splitting held-out observations that were not involved in the posterior approximation of
into two equal halves inferring the local component distribution based on the first half and testing
with the second half For DP-mixtures we condition on the observed hour of the week and
predict the geographic location of the held-out data point
In standard offline studies the held-out set is randomly selected from the data With streams however
we test on the next documents for New York Times Science tweets for Twitter or
locations on Geo data This is a valid held-out set because the data ahead of the current position in
the stream have not yet been seen by the inference algorithms
Figure shows the performance for LDA. We looked at two types of streams one in which the data
appear in order and the other in which they have been permuted an exchangeable stream The
time permuted stream reveals performance when each data minibatch is safely assumed to be an
sample from this results in smoother improvements to predictive likelihood On our data we
found that population VB outperformed SVI and SVB on two of the data sets and outperformed SVI
on all of the data SVB performed better than population VB on Twitter
Figure shows a similar study for DP mixtures We analyzed the human mobility data and the
New York Times Ref also analyzed the New York Times On these data population VB
outperformed SVB and SVI in all settings.3
Hyperparameters Unlike traditional Bayesian methods the data set size is a hyperparameter to
population VB. It helps control the posterior variance of the population posterior Figure reports
sensitivity to for all studies for the time-ordered stream These plots indicate that the optimal
setting of is often different from the true number of data points the best performing population
posterior variance is not necessarily the one implied by the data The other hyperparameters to our
experiments are reported in Appendix C.
Conclusions and Future Work
We introduced the population posterior a distribution over latent variables that combines traditional
Bayesian inference with the frequentist idea of the population distribution With this idea we derived
population variational Bayes an efficient algorithm for probabilistic inference on streams On two
complex Bayesian models and several large data sets we found that population variational Bayes
usually performs better than existing approaches to streaming inference
In this paper we made no assumptions about the structure of the population distribution Making
assumptions such as the ability to obtain streams conditional on queries can lead to variants of
our algorithm that learn which data points to see next during inference Finally understanding the
theoretical properties of the population posterior is also an avenue of interest
Acknowledgments We thank Allison Chaney John Cunningham Alp Kucukelbir Stephan Mandt
Peter Orbanz Theo Weber Frank Wood and the anonymous reviewers for their comments This work
is supported by NSF ONR DARPA
NDSEG Facebook Adobe Amazon and the Siebel Scholar
and John Templeton Foundations
Though our purpose is to compare algorithms we make one note about a specific data set The predictive
accuracy for the Ivory Coast data set plummets after data points This is because of the data collection
policy For privacy reasons the data set provides the cell tower locations of a randomly selected cohort of
users every weeks The new cohort at data points behaves differently to previous cohorts in a way that
affects predictive performance However both algorithms steadily improve after this shock

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5237-learning-with-fredholm-kernels.pdf

Learning with Fredholm Kernels
Qichao Que Mikhail Belkin Yusu Wang
Department of Computer Science and Engineering
The Ohio State University
Columbus OH
que,mbelkin,yusu}@cse.ohio-state.edu
Abstract
In this paper we propose a framework for supervised and semi-supervised learning
based on reformulating the learning problem as a regularized Fredholm integral
equation Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels which we call Fredholm
kernels We proceed to discuss the noise assumption for semi-supervised learning and provide both theoretical and experimental evidence that Fredholm kernels
can effectively utilize unlabeled data under the noise assumption We demonstrate
that methods based on Fredholm learning show very competitive performance in
the standard semi-supervised learning setting
Introduction
Kernel methods and methods based on integral operators have become one of the central areas of
machine learning and learning theory These methods combine rich mathematical foundations with
strong empirical performance In this paper we propose a framework for supervised and unsupervised learning as an inverse problem based on solving the integral equation known as the Fredholm
problem of the first kind We develop regularization based algorithms for solving these systems
leading to what we call Fredholm kernels
In the basic setting of supervised learning we are given the data set where R.
We would like to construct a function such that and is nice enough
to generalize to new data points This is typically done by choosing from a class of functions
Reproducing Kernel Hilbert Space RKHS corresponding to a positive definite kernel for the kernel
methods and optimizing a certain loss function such as the square loss or hinge loss
In this paper we formulate a new framework for learning based on interpreting the learning problem
as a Fredholm integral equation This formulation shares some similarities with the usual kernel
learning framework but unlike the standard methods also allows for easy incorporation of unlabeled
data We also show how to interpret the resulting algorithm as a standard kernel method with a
non-standard data-dependent kernel somewhat resembling the approach taken in
We discuss reasons why incorporation of unlabeled data may be desirable concentrating in particular on what may be termed the noise assumption for semi-supervised learning which is related
but distint from manifold and cluster assumption popular in the semi-supervised learning literature
We provide both theoretical and empirical results showing that the Fredholm formulation allows for
efficient denoising of classifiers
To summarize the main contributions of the paper are as follows
We formulate a new framework based on solving a regularized Fredholm equation The framework naturally combines labeled and unlabeled data We show how this framework can be expressed
as a kernel method with a non-standard data-dependent kernel
We discuss the noise assumption in semi-supervised learning and provide some theoretical evidence that Fredholm kernels are able to improve performance of classifiers under this assumption
More specifically we analyze the behavior of several versions of Fredholm kernels based on combining linear and Gaussian kernels We demonstrate that for some models of the noise assumption
Fredholm kernel provides better estimators than the traditional data-independent kernel and thus
unlabeled data provably improves inference
We show that Fredholm kernels perform well on synthetic examples designed to illustrate the
noise assumption as well as on a number of real-world datasets
Related work Kernel and integral methods in machine learning have a large and diverse literature
The work most directly related to our approach is where Fredholm integral
equations were introduced to address the problem of density ratio estimation and covariate shift In
that work the problem of density ratio estimation was expressed as a Fredholm integral equation and
solved using regularization in RKHS This setting also relates to a line of work on on kernel mean
embedding where data points are embedded in Reproducing Kernel Hilbert Spaces using integral
operators with applications to density ratio estimation and other tasks A very interesting
recent work explores a shrinkage estimator for estimating means in RKHS following the SteinJames estimator originally used for estimating the mean in an Euclidean space The results obtained
in show how such estimators can reduce variance There is some similarity between that work
and our theoretical results presented in Section which also show variance reduction for certain
estimators of the kernel although in a different setting Another line of related work is the class
of semi-supervised learning techniques for a comprehensive overview related to manifold regularization where an additional graph Laplacian regularizer is added to take advantage
of the geometric/manifold structure of the data Our reformulation of Fredholm learning as a kernel addressing what we called noise assumptions parallels data-dependent kernels for manifold
regularization proposed in
Fredholm Kernels
We start by formulating learning framework proposed in this paper Suppose we are given labeled
pairs y1 xl yl from the data distribution defined on and unlabeled
points xl+u from the marginal distribution pX on For simplicity we will assume
that the feature space is a Euclidean space RD and the label set is either for binary
classification or the real line for regression Semi-supervised learning algorithms aim to construct
a predictor function by incorporating the information of unlabeled data distribution
To this end we introduce the integral operator KpX associated with a kernel function In our
setting does not have to be a positive semi-definite even symmetric kernel
KpX and KpX z)f z)pX
where is the space of square-integrable functions By the law of large numbers the above operator can be approximated using unlabeled data from pX as
l+u
Kp?X
This approximation provides a natural way of incorporating unlabeled data into algorithms In our
Fredholm learning framework we will use functions in KpX KpX where is
an appropriate Reproducing Kernel Hilbert Space RKHS as classification or regression functions
Note that unlike RKHS this space of functions KpX is density dependent
In particular this now allows us to formulate the following optimization problem for semi-supervised
classification/regression in a way similar to many supervised learning algorithms
The Fredholm learning framework solves the following optimization problem1
1X
kf k2H
arg min
We will be using the square loss to simplify the exposition Other loss functions can also be used in Eqn
The final classifier is Kp?X where Kp?X is the operator defined above Eqn is a
discretized and regularized version of the Fredholm integral equation KpX thus giving the
name of Fredholm learning framework
Even though at a first glance this setting looks similar to conventional kernel methods the extra
layer introduced by Kp?X makes significant difference in particular by allowing the integration
of information from unlabeled data distribution In contrast solutions to standard kernel methods
for most kernels linear polynomial or Gaussian kernels are completely independent of the
unlabeled data We note that our approach is closely related to where a Fredholm equation is
used to estimated the density ratio for two probability distributions
The Fredholm learning framework is a generalization of the standard kernel framework In fact if
the kernel is the function then our formulation above is equivalent to the Regularized Kernel
Pl
Least Squares equation arg minf 1l kf k2H We could also replace
the loss in Eqn by other loss functions such as hinge loss resulting in a SVM-like classifier
Finally even though Eqn is an optimization problem in a potentially infinite dimensional function
space a standard derivation using the Representer Theorem See full version for details yields
a computationally accessible solution as follows
l+u
kH vj Kl+u
Kl+u KH I
Kl+u
where Kl+u ij k(xi for and KH ij kH for
Note that Kl+u is a matrix
Fredholm kernels a convenient reformulation In fact we will see that Fredholm learning problem induces a new data-dependent kernel which we will refer to as Fredholm kernel2 To show this
connection we use the following identity which can be easily verified
Kl+u
Kl+u KH I
Kl+u Kl+u
Kl+u KH Kl+u
I
Define KF Kl+u KH Kl+u
to be the kernel matrix associated with a new kernel defined by
k?F
l+u
kH
and we consider the unlabeled data are fixed for computing this new kernel Using this new kernel
k?F the final classifying function from Eqn can be rewritten as
l+u
k?F xs KF
Because of Eqn we will sometimes refer to the kernels kH and as the inner and outer kernels
respectively It can be observed that this solution is equivalent to a standard kernel method but using
a new data dependent kernel k?F which we will call the Fredholm kernel since it is induced from
the Fredholm problem formulated in Eqn
Proposition The Fredholm kernel defined in Eqn is positive semi-definite as long as KH is
positive semi-definite for any set of data xl+u
The proof is given in the full version The outer kernel does not have to be either positive definite
or even symmetric When using Gaussian kernel for discrete approximation in Eqn might be
unstable when the kernel width is small so we also introduce the normalized Fredholm kernel
l+u
kH
k?FN
It is easy to check that the resulting Fredholm kernel k?FN is still symmetric positive semi-definite
Even though Fredholm kernel was derived using loss here it could also be derived when hinge
loss is used which will be explained in full version
We note that the term Fredholm Kernel has been used in mathematics page and also in a different
learning context Our usage represents a different object
The Noise Assumption and Semi-supervised Learning
In order for unlabeled data to be useful in classification tasks it is necessary for the marginal distribution of the unlabeled data to contain information about the conditional distribution of the labels
Several ways in which such information can be encoded has been proposed including the cluster
assumption and the manifold assumption The cluster assumption states that a cluster
a high density area contains only mostly points belonging to the same class That is if and
belong to the same cluster the corresponding labels y1 y2 should be the same The manifold
assumption assumes that the regression function is smooth with respect to the underlying manifold
structure of the data which can be interpreted as saying that the geodesic distance should be used
instead of the ambient distance for optimal classification The success of algorithms based on these
ideas indicates that these assumptions do capture certain characteristics of real data Still better
understanding of unlabeled data may still lead to progress in data analysis
The noise assumption We propose to formulate a new assumption the noise assumption which is that in the neighborhood of every point the directions with low variance for
the unlabeled data are uninformative with respect to the class labels and can be regarded as
noise While intuitive as far as we know it has
not been explicitly formulated in the context
of semi-supervised learning algorithms nor applied to theoretical analysis
Figure Left only labelled points and Right
with unlabelled points
Note that even if the noise variance is small along a single direction it could still significantly decrease the performance of a supervised learning algorithm if the noise is high-dimensional These
accumulated non-informative variations in particular increase the difficulty of learning a good classifier when the amount of labeled data is small The first figure on right illustrates the issue of noise
with two labeled points The seemingly optimal classification boundary the red line differs from
the correct one black due to the noisy variation along the axis for the two labeled points
Intuitively unlabeled data shown in the right panel of Figure can be helpful in this setting as low
variance directions can be estimated locally such that algorithms could suppress the influences of
the noisy variation when learning a classifier
Connection to cluster and manifold assumptions The noise assumption is compatible with the
manifold assumption within the manifold+noise model Specifically we can assume that the functions of interest vary along the manifold and are constant in the orthogonal direction Alternatively
we can think of directions with high variance as signal/manifold and directions with low variance as noise We note that the noise assumption does not require the data to conform to a
low-dimensional manifold in the strict mathematical sense of the word The noise assumption is
orthogonal to the cluster assumption For example Figure illustrates a situation where data has no
clusters but the noise assumption applies
Theoretical Results for Fredholm Kernels
Non-informative variation in data could degrade traditional supervised learning algorithms We
will now show that Fredholm kernels can be used to replace traditional kernels to inject them with
noise-suppression power with the help of unlabeled data In this section we will present two views
to illustrate how such noise suppression can be achieved Specifically in Section we show that
under certain setup linear Fredholm kernel suppresses principal components with small variance
In Section we prove that under certain conditions we are able to provide good approximations
to the true kernel on the hidden underlying space
To make our arguments more clear we assume that there are infinite amount of unlabelled data that
is we know the marginal distribution of data exactly We will then consider the following continuous
versions of the un-normalized andZnormalized
Fredholm kernels as in Eqn and
kFU
u)kH v)p(u)p(v)dudv
kFN
kH
p(u)p(v)dudv
w)p(w)dw
w)p(w)dw
Note in the above equations and in what follows we sometimes write instead of pX for the
marginal distribution when its choice is clear from context We will typically use kF to denote
appropriate normalized or unnormalized kernels depending on the context
Linear Fredholm kernels and inner products
For this section we consider the unormalized Fredholm kernel that is kF kFU If the outer
kernel is linear hu vi the resulting Fredholm kernel can be viewed as an
inner product Specifically the un-normalized Fredholm kernel from Eqn can be rewritten as
kF where
ukH v)v p(u)p(v)dudv
Thus kF is simply an inner product which depends on both the unlabeled data distribution
and the inner kernel kH This inner product re-weights the standard norm in feature space based
on variances along the principal directions of the matrix We show that for the model when unlabeled data is sampled from a normal distribution this kernel can be viewed as a soft thresholding
PCA suppressing the directions with low variance Specifically we have the following3
Theorem Let kH exp kx?zk
and assume the distribution pX for unlabeled data is
2t
a single multi-variate normal distribution We have
diag
Assuming that the data is mean-subtracted we see that xT re-scales the projections
along the principal components
when computing the inner product that is the rescaling factor for
the i-th principal direction is
Note that this rescaling factor
when On the other hand when we
have that 2i Hence can be considered as a soft threshold that eliminates the effects of
principal components with small variances When is small the rescaling factors are approximately
in which case is is proportional to the covariance matrix
proportional to
of the data XX
Kernel Approximation With Noise
We have seen that one special case of Fredholm kernel could achieve the effect of principal components re-scaling by using linear kernel as the outer kernel In this section we give a more general
interpretation of noise suppression by the Fredholm kernel
First we give a simple senario to provide some intuition behind the definition of Fredholm kernle Consider a standard supervised learning setting which uses the solution
Pl
arg minf 1l k2H as the classifier Let
target
kH
denote the ideal kernel that we intend to use on the clean
data which we call the target kernel from now on Now suppose what we have are two noisy labelled points xe and ze for
true data
and xe
ze The
target
evaluation of kH
xe ze can be quite different from the true
target
signal kH
leading to an suboptimal final classifier the
red line in Figure On the other hand now consider the
RR
Fredholm kernel from Eqn similarly from Eqn kF xe ze
k(xe kH
k(ze v)p(v)dudv and set the outer kernel to be the Gaussian kernel and the inner kernel kH to be
target
the same as target kernel kH
We can think of kF xe ze as an averaging of kH over all possible pairs of data weighted by k(xe and k(ze respectively Specifically points
The proof of this and other results can be found in the full version
that are close to xe resp ze with high density will receive larger weights Hence the weighted
averages will be biased towards
and respectively which presumably lie in high density regions
around xe and ze The value of kF xe ze tends to provide a more accurate estimate of kH
See the right figure for an illustration where the arrows indicate points with stronger influences in the
computation of kF xe ze than kH xe ze As a result the classifier obtained using the Fredholm
kernel will also be more resilient to noise and closer to the optimum
The Fredholm learning framework is rather flexible in terms of the choices of kernels and kH
In the remainder of this section we will consider a few specific scenarios and provide quantitative
analysis to show the noise robustness of the Fredholm kernel
Problem setup Assume that we have a ground-truth distribution over the subspace spanned by
the first dimension of the Euclidean space RD We will assume that this distribution is a single Gaussian Id Suppose this distribution is corrupted with Gaussian noise along the orthogonal subspace of dimension That is for any true point
drawn from Id
its observation xe is drawn from
ID?d Since the noise lies in a space orthogonal
to data distribution this means that any observed point labelled or unlabeled is sampled from
pX diag(?2 Id ID?d We will show that Fredholm kernel provides a better approximation to the original kernel given unlabeled data than simply computing the kernel of noisy points
We choose this basic setting to be able to state the theoretical results in a clean manner Even though
this is a Gaussian distribution over a linear subspace with noise this framework has more general
implications since local neighborhoods of manifolds are almost linear spaces
Note In this section we use normalized Fredholm kernel given in Eqn that is kF kFN for now
on Un-normalized Fredholm kernel displays similar behavior while the bounds are trickier
target
Linear Kernel First we consider the case where the target kernel kH
is the linear kernel
target
kH We will set kH in Fredholm kernel to also be linear and to be the Gaussian
ku?vk2
kernel 2t We will compare kF xe ze with the target kernel on the two observed
target
target
points that is with kH
xe ze The goal is to estimate kH
We will see that both
target
kF xe ze and appropriately scaled kH xe ze are unbiased estimators of kH
however
target
the variance of kF xe ze is smaller than that of kH xe ze making it a more precise estimator
Theorem Suppose the probability distribution for the unlabeled data pX
diag(?2 Id ID?d For Fredholm kernel defined in Eqn we have
target
Exe ze kH
xe ze Exe ze
kF xe ze
target
kF xe ze Varxe ze kH
xe ze
Moreover when Varxe ze
Remark Note that we have a normalization constant for the Fredholm kernel to make it an unbiased
estimator of
In practice choosing normalization is subsumed in selecting the regularization
parameter for kernel methods
Thus we can see the Fredholm kernel provides an approximation of the true linear kernel but with
smaller variance compared to the actual linear kernel on noisy data
Gaussian Kernel We now consider the case where the target kernel is the Gaussian kernel
target
kH
exp ku?vk
To approximate this kernel we will set both and kH to be Gaus2r
sian kernels To simplify the presentation of results we assume that and kH have the same kernel
width The resulting Fredholm kernel turns out to also be a Gaussian kernel whose kernel width
depends on the choice of
Our main result is the following Again similar to the case of linear kernel the Fredholm estimation
target
target
kF xe ze and kH
xe ze are both unbiased estimator for the target kH
up to a constant
but kF xe ze has a smaller variance
Theorem Suppose the probability distribution for the unlabeled
data pX
target
diag(?2 Id ID?d Given the target kernel kH
exp ku?vk
with
ker2r
nel width we can choose given by the equation
and two scaling
constants c1 c2 such that
target
target
Exe ze
kH xe ze Exe ze kF xe ze kH
target
and when we have Varxe ze
kH xe ze Varxe ze kF xe ze
Remark In practice when applying kernel methods for real world applications optimal kernel
width is usually unknown and chosen by cross-validation or other methods Similarly for our
Fredholm kernel one can also use cross-validation to choose the optimal for kF
Experiments
Using linear and Gaussian kernel for or kH respectively we will define three instances of the
Fredholm kernel as follows
FredLin1 xT and kH exp kx?zk
2r
kx?zk2
FredLin2 exp 2r
and kH
FredGauss kH exp kx?zk
2r
For the kernels in and that use the Gaussian kernel as outside
kernel we can also define their normalized version which we will
denote by by FredLin2(N and FredGauss(N respectively
Synthetic examples Noise and cluster assumptions
To isolate the ability of Fredholm kernels to deal with noise from
the cluster assumption we construct two synthetic examples that
violate the cluster assumption shown in Figure The figures show
first two dimensions with multi-variate Gaussian noise with variance in added The classification boundaries are
indicated by the color For each class we provide several labeled
points and large amount of unlabeled data Note that the classification boundary in the circle example is non-linear
We compare Fredholm kernel based classifier with RLSC Regularized Least Squares Classifier and two widely used semisupervised methods the transductive support vector machine and
Noise but not
LapRLSC Since the examples violate the cluster assumption the Figure
cluster
assumption
Gaussian
two existing semi-supervised learning algorithms Transductive
noise
in
is
added
Linear
SVM and LapRLSC should not gain much from the unlabeled data
For TSVM we use the primal TSVM proposed in and we will above and non-linear beuse the implementation of LapRLSC given in Different num low class boundaries
bers of labeled points are given for each class together with another
unlabeled points To choose the optimal parameters for each method we pick the parameters
based on their performance on the validation set while the final classification error is computed on
the held-out testing data set Results are reported in Table and in which Fredholm kernels show
clear improvement over other methods for synthetic examples in term of classification error
Real-world Data Sets Unlike artificial examples it is usually difficult to verify whether certain
assumptions are satisfied in real-world problems In this section we examine the performance of
Fredholm kernels on several real-world data sets and compare it with the baseline algorithms mentioned above
Linear Kernels Here we consider text categorization and sentiment analysis where linear methods
are known to perform well We use the following data represented by TF-IDF features
news group it has documents with classes and we select the first categories
for our experiment Webkb the original data set contains documents with unbalanced
classes and we pick the two largest classes with and instances respectively IMDB
movie review it has positive reviews and negative reviews of movie on IMDB.com
Twitter sentiment data from Sem-Eval it contains tweets with positive neural and negative sentiment We combine neutral and negative classes to set up a binary classification problem
Results are reported in Table In Table4 we use WebKB as an example to illustrate the change of
the performance as number of labeled points increases
Number
of Labeled
32
RLSC
TSVM
Methods(Linear
LapRLSC
FredLin1
FredLin2(N
Table Prediction error of different classifiers for the?two lines example
Number
of Labeled
32
64
K-RLSC
Methods(Gaussian
TSVM
LapRLSC
FredGauss(N
Table Prediction error of different classifiers for the circle example
Gaussian Kernel We test our methods on hand-written digit recognition The experiment use
subsets of two handwriting digits data sets MNIST and USPS the one from MNIST contains
digits in total with balanced examples for each class and the one for USPS is the original testing
set containing about 2k images The pixel values are normalized to as features Results are
reported in Table In Table we show that as we add additional Gaussian noise to MNIST data
Fredholm kernels start to show significant improvement
Data Set
Webkb
20news
IMDB
Twitter
RLSC
TSVM
Methods(Linear
FredLin1
FredLin2
FredLin2(N
Table The error of various methods on the text data sets labeled data per class are given with
rest of the data set as unlabeled points Optimal parameter for each method are used
Number
of Labeled
RLSC
TSVM
Methods(Linear
FredLin1
FredLin2
FredLin2(N
Table Prediction error on Webkb with different number of labeled points
Data Set
USPST
MNIST
K-RLSC
Methods(Gaussian
LapRLSC
FredGauss
FredGauss(N
Table Prediction error of nonlinear classifiers on the MNIST and USPS labeled data per class
are given with rest of the data set as unlabeled points Optimal parameter for each method are used
Number
of Labeled
K-RLSC
Methods(Gaussian
LapRLSC
FredGauss
FredGauss(N
Table The prediction error of nonlinear classifiers on MNIST corrupted with Gaussian noise with
standard deviation with different numbers of labeled points from to Optimal parameter
for each method are used
Acknowledgments The work was partially supported by NSF Grants and RI
We thank the anonymous NIPS reviewers for insightful comments

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5365-shaping-social-activity-by-incentivizing-users.pdf

Shaping Social Activity by Incentivizing Users
Mehrdad Farajtabar
Nan Du
Manuel Gomez-Rodriguez
Isabel Valera
Hongyuan Zha
Le Song
Georgia Institute of Technology
MPI for Software Systems
Univ Carlos III in Madrid
mehrdad,dunan}@gatech.edu
manuelgr@mpi-sws.org
zha,lsong}@cc.gatech.edu
ivalera@tsc.uc3m.es
Abstract
Events in an online social network can be categorized roughly into endogenous
events where users just respond to the actions of their neighbors within the network or exogenous events where users take actions due to drives external to the
network How much external drive should be provided to each user such that the
network activity can be steered towards a target state In this paper we model
social events using multivariate Hawkes processes which can capture both endogenous and exogenous event intensities and derive a time dependent linear relation between the intensity of exogenous events and the overall network activity
Exploiting this connection we develop a convex optimization framework for determining the required level of external drive in order for the network to reach a
desired activity level We experimented with event data gathered from Twitter
and show that our method can steer the activity of the network more accurately
than alternatives
Introduction
Online social platforms routinely track and record a large volume of event data which may correspond to the usage of a service url shortening service bit.ly These events can be categorized
roughly into endogenous events where users just respond to the actions of their neighbors within
the network or exogenous events where users take actions due to drives external to the network
For instance a user?s tweets may contain links provided by bit.ly either due to his forwarding of a
link from his friends or due to his own initiative to use the service to create a new link
Can we model and exploit these data to steer the online community to a desired activity level
Specifically can we drive the overall usage of a service to a certain level at least twice per
day per user by incentivizing a small number of users to take more initiatives What if the goal is
to make the usage level of a service more homogeneous across users What about maximizing the
overall service usage for a target group of users Furthermore these activity shaping problems need
to be addressed by taking into account budget constraints since incentives are usually provided in
the form of monetary or credit rewards
Activity shaping problems are significantly more challenging than traditional influence maximization problems which aim to identify a set of users who when convinced to adopt a product shall
influence others in the network and trigger a large cascade of adoptions First in influence
maximization the state of each user is often assumed to be binary either adopting a product or
not However such assumption does not capture the recurrent nature of product usage
where the frequency of the usage matters Second while influence maximization methods identify
a set of users to provide incentives they do not typically provide a quantitative prescription on how
much incentive should be provided to each user Third activity shaping concerns a larger variety of
target states such as minimum activity and homogeneity of activity not just activity maximization
In this paper we will address the activity shaping problems using multivariate Hawkes processes
which can model both endogenous and exogenous recurrent social events and were shown to be a
good fit for such data in a number of recent works More importantly
we will go beyond model fitting and derive a novel predictive formula for the overall network activity given the intensity of exogenous events in individual users using a connection between the
processes and branching processes Based on this relation we propose a convex
optimization framework to address a diverse range of activity shaping problems given budget constraints Compared to previous methods for influence maximization our framework can provide
more fine-grained control of network activity not only steering the network to a desired steady-state
activity level but also do so in a time-sensitive fashion For example our framework allows us to
answer complex time-sensitive queries such as which users should be incentivized and by how
much to steer a set of users to use a product twice per week after one month
In addition to the novel framework we also develop an efficient gradient based optimization algorithm where the matrix exponential needed for gradient computation is approximated using the
truncated Taylor series expansion This algorithm allows us to validate our framework in a
variety of activity shaping tasks and scale up to networks with tens of thousands of nodes We also
conducted experiments on a network of Twitter users and more than uses of a popular url shortening services Using held-out data we show that our algorithm can shape the network
behavior much more accurately than alternatives
Modeling Endogenous-Exogenous Recurrent Social Events
We model the events generated by users in a social network as a m-dimensional counting process
N2 Nm where Ni records the total number of events generated by
user up to time Furthermore we represent each event as a tuple ui ti where ui is the user identity and ti is the event timing Let the history of the process up to time be Ht ti ti
and Ht be the history until just before time Then the increment of the process dN in an infinitesimal window dt is parametrized by the intensity
E[dN dt
Intuitively the larger the intensity the greater the likelihood of observing an event in the time
window For instance a Poisson process in can be viewed as a special counting
process with a constant intensity function independent of time and history To model the presence
of both endogenous and exogenous events we will decompose the intensity into two terms
overall event intensity
exogenous event intensity
endogenous event intensity
where the exogenous event intensity models drive outside the network and the endogenous event
intensity models interactions within the network We assume that hosts of social platforms can
potentially drive up or down the exogenous events intensity by providing incentives to users while
endogenous events are generated due to users own interests or under the influence of network peers
and the hosts do not interfere with them directly The key questions in the activity shaping context
are how to model the endogenous event intensity which are realistic to recurrent social interactions
and how to link the exogenous event intensity to the endogenous event intensity We assume that the
exogenous event intensity is independent of the history and time
Multivariate Hawkes Process
Recurrent endogenous events often exhibit the characteristics of self-excitation where a user tends
to repeat what he has been doing recently and mutual-excitation where a user simply follows what
his neighbors are doing due to peer pressure These social phenomena have been made analogy to
the occurrence of earthquake and the spread of epidemics and can be well-captured by
multivariate Hawkes processes as shown in a number of recent works
More specifically a multivariate Hawkes process is a counting process who has a particular form
of intensity We assume that the strength of influence between users is parameterized by a sparse
nonnegative influence matrix A auu where auu means user directly excites
user We also allow A to have nonnegative diagonals to model self-excitation of a user Then the
intensity of the u-th dimension is
auui g(t ti
auu
g(t dNu
i:ti
where is a nonnegative kernel function such that for and ds
the second equality is obtained by grouping events according to users and use the fact that
t1
t2
t3
An example social network
Branching structure of events
Figure In Panel each directed edge indicates that the target node follows and can be influenced
by the source node The activity in this network is modeled using Hawkes processes which result in
branching structure of events shown in Panel Each exogenous event is the root node of a branch
top left most red circle at t1 and it occurs due to a user?s own initiative and each event can
trigger one or more endogenous events blue square at t2 The new endogenous events can create
the next generation of endogenous events green triangles at t3 and so forth The social network
will constrain the branching structure of events since an event produced by a user user can
only trigger endogenous events in the same user or one or more of her followers user or
g(t dNu
ui ti g(t ti Intuitively models the propagation of peer
influence over the network each event ui ti occurred in the neighbor of a user will boost her
intensity by a certain amount which itself decays over time Thus the more frequent the events
occur in the user?s neighbor the more likely she will be persuaded to generate a new event
For simplicity we will focus on an exponential kernel g(t ti ti in the reminder
of the paper However multivariate Hawkes processes and the branching processed explained in
next section is independent of the kernel choice and can be extended to other kernels such as powerlaw Rayleigh or any other long tailed distribution over nonnegative real domain Furthermore we
can rewrite equation in vectorial format
G(t dN
by defining a time-varying matrix auu Note that for multivariate
Hawkes processes the intensity itself is a random quantity which depends on the history Ht
We denote the expectation of the intensity with respect to history as
EHt
Connection to Branching Processes
A branching process is a Markov process that models a population in which each individual in
generation produces some random number of individuals in generation according some
distribution In this section we will conceptually assign both exogenous events and endogenous
events in the multivariate Hawkes process to levels generations and associate these events with
a branching structure which records the information on which event triggers which other events
Figure for an example Note that this genealogy of events should be interpreted in probabilistic
terms and may not be observed in actual data Such connection has been discussed in Hawkes
original paper on one dimensional Hawkes processes and it has recently been revisited in the
context of multivariate Hawkes processes by The branching structure will play a crucial role in
deriving a novel link between the intensity of the exogenous events and the overall network activity
More specifically we assign all exogenous events to the zero-th generation and record the number
of such events as These exogenous events will trigger the first generation of endogenous
events whose number will be recorded as Next these first generation of endogenous events
will further trigger a second generation of endogenous events and so on Then the total
number of events in the network is the sum of the numbers of events from all generations
Ht
Furthermore denote all events in generation as
Then independently for each event
ui ti Ht
in generation it triggers a Poisson process in its neighbor independently
with intensity auui g(t?ti Due to the superposition theorem of independent Poisson processes
the intensity of events at node and generation is simply the sum of conditional intensities
of the Poisson processes triggered by all its neighbors ui ti auui g(t
ti
Concatenate the intensity for all and use the
g(t dNu
time-varying matrix we have
G(t dN
where
is the intensity for counting process at k-th generation Again due to the superposition of independent Poisson processes we can decompose the
intensity of into a sum of conditional intensities from different generation
Next based on the above decomposition we will develop a closed form relation between the expected intensity EHt and the intensity of the exogenous events This relation will form the basis of our activity shaping framework
Linking Exogenous Event Intensity to Overall Network Activity
Our strategy is to first link the expected intensity EHt of events at the k-th
generation with and then derive a close form for the infinite series sum
Define a series of auto-convolution matrices one for each generation with I and
G(t ds
Then the expected intensity of events at the k-th generation is related to exogenous intensity by
Lemma
Next by summing together all auto-convolution matrices
I
we obtain a linear relation between the expected intensity of the network and the intensity of the
exogenous events The entries in the matrix roughly encode the influence between pairs of users More precisely the entry uv is the expected intensity of events
at node due to a unit level of exogenous intensity at(node We can also derive several other
useful quantities from For example uv can be thought of as the overall
influence user has on all users Surprisingly for exponential kernel the infinite sum of matrices
results in a closed form using matrix exponentials First let denote the Laplace transform of a
function and we have the following intermediate results on the Laplace transform of
Lemma
dt
Ak
With Lemma we are in a position
to prove our main theorem below
Theorem I
Theorem provides us a linear relation between exogenous event intensity and the expected overall
intensity at any point in time but not just stationary intensity The significance of this result is that
it allows us later to design a diverse range of convex programs to determine the intensity of the
exogenous event in order to achieve a target intensity
In fact we can recover the previous results in the stationary case as a special case of our general
result More specifically a multivariate Hawkes process is stationary if the spectral radius
A
dt
dt
auu
u,u
is strictly smaller than In this case the expected intensity is I independent
of the time We can obtain this relation from theorem if we let
Corollary I limt
Refer to Appendix A for all the proofs
Convex Activity Shaping Framework
Given the linear relation between exogenous event intensity and expected overall event intensity we
now propose a convex optimization framework for a variety of activity shaping tasks In all tasks
discussed below we will optimize the exogenous event intensity such that the expected overall
event intensity is maximized with respect to some concave utility in
maximize?(t),?(0
subject to
where cm is the cost per unit event for each user and is the total budget
Additional regularization can also be added to either to restrict the number of incentivized
users with norm or to promote a sparse solution with norm or to obtain a
smooth solution with regularization We next discuss several instances of the general
framework which achieve different goals their constraints remain the same and hence omitted
Capped Activity Maximization In real networks there is an upper bound a cap on the activity
each user can generate due to limited attention of a user For example a Twitter user typically posts
a limited number of shortened urls or retweets a limited number of tweets Suppose we know
the upper bound on a user?s activity how much activity each user is willing to generate
Then we can perform the following capped activity maximization task
maximize?(t),?(0
min
Minimax Activity Shaping Suppose our goal is instead maintaining the activity of each user in the
network above a certain minimum level or alternatively make the user with the minimum activity
as active as possible Then we can perform the following minimax activity shaping task
maximize?(t),?(0 minu
Least-Squares Activity Shaping Sometimes we want to achieve a pre-specified target activity
levels for users For example we may like to divide users into groups and desire a different level
of activity in each group Inspired by these examples we can perform the following least-squares
activity shaping task
maximize?(t),?(0
where encodes potentially additional constraints group partitions Besides Euclidean distance the family of Bregman divergences can be used to measure the difference between
and here That is given a function Rm convex in its argument we can use
as our objective function
Activity Homogenization Many other concave utility functions can be used For example we may
want to steer users activities to a more homogeneous profile If we measure homogeneity of activity
with Shannon entropy then we can perform the following activity homogenization task
maximize?(t),?(0
Scalable Algorithm
All the activity shaping problems defined above require an efficient evaluation of the instantaneous
average intensity at time which entails computing matrix exponentials to obtain In
small or medium networks we can rely on well-known numerical methods to compute matrix exponentials However in large networks the explicit computation of becomes intractable
Fortunately we can exploit the following key property of our convex activity shaping framework
the instantaneous average intensity only depends on through matrix-vector product operations
In particular we start by using Theorem to rewrite the multiplication of and a vector
as We then get a tractable solution by
first computing efficiently subtracting
from it and solving a sparse linear system of
equations A efficiently The steps are illustrated in Algorithm
Next we elaborate on two very efficient algorithms for computing the product of matrix exponential
with a vector and for solving a sparse linear system of equations
For the computation of the product of matrix exponential with a vector we rely on the iterative
algorithm by Al-Mohy which combines a scaling and squaring method with a truncated
Taylor series approximation to the matrix exponential For solving the sparse linear system of equa5
Algorithm Average Instantaneous Intensity
Algorithm PGD for Activity Shaping
input A
output
v1
v2 v2
v3 A v2
return v1
Initialize
repeat
Project into
Evaluate the gradient at
Update using the gradient
until convergence
tion we use the well-known GMRES method which is an Arnoldi process for constructing
an l2 orthogonal basis of Krylov subspaces The method solves the linear system by iteratively
minimizing the norm of the residual vector over a Krylov subspace
Perhaps surprisingly we will now show that it is possible to compute the gradient of the objective functions of all our activity shaping problems using the algorithm developed above for computing the average instantaneous intensity We only need to define the vector appropriately
for each problem as follows Activity maximization where is defined such that vj if and vj otherwise Minimax activity shaping
where is defined such that ej if min and
ej otherwise iii
Least-squares activity shaping Activity homogenization where on a vector is the element-wise
natural logarithm Since the activity maximization and the minimax activity shaping tasks require
only one evaluation of times a vector Algorithm can be used directly However computing
the gradient for least-squares activity shaping and activity homogenization is slightly more involved
and it requires to be careful with the order in which we perform the operations Refer to Appendix
for details Equipped with an efficient way to compute of gradients we solve the corresponding
convex optimization problem for each activity shaping problem by applying projected gradient descent PGD with the appropriate gradient1 Algorithm summarizes the key steps
Experimental Evaluation
We evaluate our framework using both simulated and real world held-out data and show that our
approach significantly outperforms several baselines The appendix contains additional experiments
Dataset description and network inference We use data gathered from Twitter as reported in
which comprises of all public tweets posted by users during a 8-month period from January
to September For every user we record the times she uses any of six popular url shortening services refer to Appendix for details We evaluate the performance of our framework on
a subset of active users linked by edges which we call 2K dataset and we evaluate its
scalability on the overall users linked by edges which we call dataset The
2K dataset accounts for url shortened service uses while the dataset accounts for
million uses Finally we treat each service as independent cascades of events
In the experiments we estimated the nonnegative influence matrix A and the exogenous intensity
using maximum log-likelihood as in previous work We used a temporal resolution
of one minute and selected the bandwidth by cross validation Loosely speaking
corresponds to loosing of the initial influence after minutes which may be explained by the
rapid rate at which each user news feed gets updated
Evaluation schemes We focus on three tasks capped activity maximization minimax activity
shaping and least square activity shaping We set the total budget to which corresponds
to supporting a total extra activity equal to actions per unit time and assume all users entail the
same cost In the capped activity maximization we set the upper limit of each user?s intensity
by adding a nonnegative random vector to their inferred initial intensity In the least-squares activity
shaping we set I and aim to create three user groups less-active moderate and super-active
We use three different evaluation schemes with an increasing resemblance to a real world scenario
Theoretical objective We compute the expected overall theoretical intensity by applying Theorem on the optimal exogenous event intensities to each of the three activity shaping tasks as well
as the learned A and We then compute and report the value of the objective functions
For nondifferential objectives subgradient algorithms can be used instead
logarithm of time
logarithm of time
GR
LS
LSGRD
OP
logarithm of time
PROP
PR
LSASH
AS
LSGRD
LS
PROP
rank correlation
LSASH
Euclidean distance
GR
GRD
LP
LP
I
MINMU
MU
UNI
MI
MMASH
UN
GRD
AS
LP
MM
MINMU
rank correlation
UNI
minimum activity
minimum activity
logarithm of time
logarithm of time
Euclidean distance
PR
PRK
MMASH
DEG
logarithm of time
WEI
DE
XMU
CAM
WE
I
PRK
XM
DEG
CA
WEI
rank correlation
XMU
sum of users activity
sum of users activity
CAM
Theoretical objective
Simulated objective
Held-out data
Figure Row Capped activity maximization Row Minimax activity shaping Row Leastsquares activity shaping means statistical significant at level of with paired t-test between
our method and the second best
Simulated objective We simulate cascades with Ogata?s thinning algorithm using the optimal exogenous event intensities to each of the three activity shaping tasks and the learned A and
We then estimate empirically the overall event intensity based on the simulated cascades by computing a running average over non-overlapping time windows and report the value of the objective
functions based on this estimated overall intensity Appendix provides a comparison between the
simulated and the theoretical objective
Held-out data The most interesting evaluation scheme would entail carrying out real interventions
in a social platform However since this is very challenging to do instead in this evaluation scheme
we use held-out data to simulate such process proceeding as follows We first partition the 8-month
data into five-day long contiguous intervals Then we use one interval for training and the
remaining 49 intervals for testing Suppose interval is used for training the procedure is as follows
We estimate A1 and using the events from interval Then we fix A1 and
and estimate for all other intervals 49
Given A1 and we find the optimal exogenous event intensities opt for each of the
three activity shaping task by solving the associated convex program We then sort the
estimated according to their similarity to opt using the Euclidean
distance opt
We estimate the overall event intensity for each of the 49 intervals as in the
simulated objective evaluation scheme and sort these intervals according to the value of
their corresponding objective function
Last we compute and report the rank correlation score between the two orderings obtained
in step and The larger the rank correlation the better the method
We repeat this procedure times choosing each different interval for training once and compute
and report the average rank correlations More details can be found in the appendix
rank correlation number of pairs with consistent ordering total number of pairs
Capped activity maximization We compare to a number of alternatives XMU heuristic
based on without optimization DEG and WEI heuristics based on the degree of the user
PRANK heuristic based on page rank refer to Appendix for further details The first row of
Figure summarizes the results for the three different evaluation schemes We find that our method
CAM consistently outperforms the alternatives For the theoretical objective CAM is better
than the second best DEG. The difference in overall users intensity from DEG is about which
roughly speaking leads to at least an increase of about 24 34 in the overall
number of events in a month In terms of simulated objective and held-out data the results are
similar and provide empirical evidence that compared to other heuristics degree is an appropriate
surrogate for influence while based on the poor performance of XMU it seems that high activity
does not necessarily entail being influential To elaborate on the interpretability of the real-world
experiment on held-out data consider for example the difference in rank correlation between CAM
and DEG which is almost Then roughly speaking this means that incentivizing users based
on our approach accommodates with the ordering of real activity patterns in
more pairs of realizations
Minimax activity shaping MMASH We compare to a number of alternatives UNI heuristic
based on equal allocation MINMU heuristic based on without optimization LP linear programming based heuristic GRD a greedy approach to leverage the activity Appendix for
more details The second row of Figure summarizes the results for the three different evaluation
schemes We find that our method MMASH consistently outperforms the alternatives For the theoretical objective it is about better than the second best LP. Importantly the difference between
MMASH and LP is not trifling and the least active user carries out more
actions in average over a month As one may have expected GRD and LP are the best among the
heuristics The poor performance of MINMU which is directly related to the objective of MMASH
may be because it assigns the budget to a low active user regardless of their influence However
our method by cleverly distributing the budget to the users whom actions trigger many other users
actions like those ones with low activity it benefits from the budget most In terms of simulated
objective and held-out data the algorithms performance become more similar
Least-squares activity shaping LSASH We compare to two alternatives PROP Assigning the
budget proportionally to the desired activity LSGRD greedily allocating budget according the difference between current and desired activity refer to Appendix for more details The third row of
Figure summarizes the results for the three different evaluation schemes We find that our method
LSASH consistently outperforms the alternatives Perhaps surprisingly PROP despite its simplicity seems to perform slightly better than LSGRD This is may be due to the way it allocates the
budget to users it does not aim to strictly fulfill users target activity but benefit more users by
assigning budget proportionally Refer to Appendix for additional experiments
Sparsity and Activity Shaping In some applications there is a limitation on the number of users we
can incentivize In our proposed framework we can handle this requirement by including a sparsity
constraint on the optimization problem In order to maintain the convexity of the optimization
problem we consider a l1 regularization term where a regularization parameter provides the
trade-off between sparsity and the activity shaping goal Refer to Appendix for more details and
experimental results for different values of
Scalability The most computationally demanding part of the proposed algorithm is the evaluation of
matrix exponentials which we scale up by utilizing techniques from matrix algebra such as GMRES
and Al-Mohy methods As a result we are able to run our methods in a reasonable amount of time
on the dataset specifically in comparison with a naive implementation of matrix exponential
evaluations Refer to Appendix for detailed experimental results on scalability
Appendix discusses the limitations of our framework and future work
Acknowledgement This project was supported in part by NSF NSF/NIH BIGDATA
NSF CAREER and Raytheon Faculty Fellowship to Le Song Isabel Valera acknowledge the support of Plan Regional-Programas I+D of Comunidad de Madrid
AGES-CM Ministerio de Ciencia Innovaci?on of Spain project DEIPRO
and program Consolider-Ingenio COMONSENS

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4532-learning-to-discover-social-circles-in-ego-networks.pdf

Learning to Discover Social Circles in Ego Networks
Jure Leskovec
Stanford USA
jure@cs.stanford.edu
Julian McAuley
Stanford USA
jmcauley@cs.stanford.edu
Abstract
Our personal social networks are big and cluttered and currently there is no good
way to organize them Social networking sites allow users to manually categorize
their friends into social circles circles on Google and lists on Facebook
and Twitter however they are laborious to construct and must be updated whenever a user?s network grows We define a novel machine learning task of identifying users social circles We pose the problem as a node clustering problem on
a user?s ego-network a network of connections between her friends We develop
a model for detecting circles that combines network structure as well as user profile information For each circle we learn its members and the circle-specific user
profile similarity metric Modeling node membership to multiple circles allows us
to detect overlapping as well as hierarchically nested circles Experiments show
that our model accurately identifies circles on a diverse set of data from Facebook
Google and Twitter for all of which we obtain hand-labeled ground-truth
Introduction
Online social networks allow users to follow streams of posts generated by hundreds of their friends
and acquaintances Users friends generate overwhelming volumes of information and to cope with
the information overload they need to organize their personal social networks One of the main
mechanisms for users of social networking sites to organize their networks and the content generated by them is to categorize their friends into what we refer to as social circles Practically all
major social networks provide such functionality for example circles on Google and lists on
Facebook and Twitter Once a user creates her circles they can be used for content filtering to
filter status updates posted by distant acquaintances for privacy to hide personal information
from coworkers and for sharing groups of users that others may wish to follow
Currently users in Facebook Google and Twitter identify their circles either manually or in a
na??ve fashion by identifying friends sharing a common attribute Neither approach is particularly
satisfactory the former is time consuming and does not update automatically as a user adds more
friends while the latter fails to capture individual aspects of users communities and may function
poorly when profile information is missing or withheld
In this paper we study the problem of automatically discovering users social circles In particular
given a single user with her personal social network our goal is to identify her circles each of which
is a subset of her friends Circles are user-specific as each user organizes her personal network of
friends independently of all other users to whom she is not connected This means that we can
formulate the problem of circle detection as a clustering problem on her ego-network the network
of friendships between her friends In Figure we are given a single user and we form a network
between her friends vi We refer to the user as the ego and to the nodes vi as alters The task then
is to identify the circles to which each alter vi belongs as in Figure In other words the goal is to
find nested as well as overlapping communities/clusters in u?s ego-network
Generally there are two useful sources of data that help with this task The first is the set of edges
of the ego-network We expect that circles are formed by densely-connected sets of alters
Figure An ego-network with labeled circles This network shows typical behavior that we observe in our data Approximately of our ground-truth circles from Facebook are contained
completely within another circle overlap with another circle and of the circles have no
members in common with any other circle The goal is to discover these circles given only the
network between the ego?s friends We aim to discover circle memberships and to find common
properties around which circles form
However different circles overlap heavily alters belong to multiple circles simultaneously
28 and many circles are hierarchically nested in larger ones Figure Thus it is important
to model an alter?s memberships to multiple circles Secondly we expect that each circle is not only
densely connected but its members also share common properties or traits Thus we need
to explicitly model different dimensions of user profiles along which each circle emerges
We model circle affiliations as latent variables and similarity between alters as a function of common profile information We propose an unsupervised method to learn which dimensions of profile
similarity lead to densely linked circles Our model has two innovations First in contrast to mixedmembership models we predict hard assignment of a node to multiple circles which proves
critical for good performance Second by proposing a parameterized definition of profile similarity we learn the dimensions of similarity along which links emerge This extends the notion of
homophily by allowing different circles to form along different social dimensions an idea related to the concept of Blau spaces We achieve this by allowing each circle to have a different
definition of profile similarity so that one circle might form around friends from the same school
and another around friends from the same location We learn the model by simultaneously choosing
node circle memberships and profile similarity functions so as to best explain the observed data
We introduce a dataset of ego-networks from Facebook Google and Twitter for which we
obtain hand-labeled ground-truth from different circles.1 Experimental results show that by
simultaneously considering social network structure as well as user profile information our method
performs significantly better than natural alternatives and the current state-of-the-art Besides being
more accurate our method also allows us to generate automatic explanations of why certain nodes
belong to common communities Our method is completely unsupervised and is able to automatically determine both the number of circles as well as the circles themselves
Further Related Work Topic-modeling techniques have been used to uncover mixedmemberships of nodes to multiple groups and extensions allow entities to be attributed with
text information 13 Classical algorithms tend to identify communities based on node
features or graph structure but rarely use both in concert Our work is related to in
the sense that it performs clustering on social-network data and which models memberships
to multiple communities Finally there are works that model network data similar to ours
though the underlying models do not form communities As we shall see our problem has unique
characteristics that require a new model An extended version of our article appears in
A Generative Model for Friendships in Social Circles
We desire a model of circle formation with the following properties Nodes within circles should
have common properties or aspects Different circles should be formed by different aspects
one circle might be formed by family members and another by students who attended the same
university Circles should be allowed to overlap and stronger circles should be allowed to form
within weaker ones a circle of friends from the same degree program may form within a circle
http://snap.stanford.edu/data
from the same university as in Figure We would like to leverage both profile information and
network structure in order to identify the circles Ideally we would like to be able to pinpoint which
aspects of a profile caused a circle to form so that the model is interpretable by the user
The input to our model is an ego-network along with profiles for each user
The center node of the ego-network the is not included in but rather consists only of
u?s friends the alters We define the ego-network in this way precisely because creators of circles
do not themselves appear in their own circles For each ego-network our goal is to predict a set of
circles CK Ck and associated parameter vectors that encode how each circle
emerged We encode user profiles into pairwise features that in some way capture what
properties the users and have in common We first describe our model which can be applied
using arbitrary feature vectors and in Section we describe several ways to construct feature
vectors that are suited to our particular application
We describe a model of social circles that treats circle memberships as latent variables Nodes within
a common circle are given an opportunity to form an edge which naturally leads to hierarchical and
overlapping circles We will then devise an unsupervised algorithm to jointly optimize the latent
variables and the profile similarity parameters so as to best explain the observed network data
Our model of social circles is defined as follows Given an ego-network and a set of circles
CK we model the probability that a pair of nodes form an edge as
exp
Ck
Ck
circles containing both nodes
all other circles
For each circle Ck is the profile similarity parameter that we will learn The idea is that
is high if both nodes belong to Ck and low if either of them do not trades-off
these two effects Since the feature vector encodes the similarity between the profiles of
two users and the parameter vector encodes what dimensions of profile similarity caused the
circle to form so that nodes within a circle Ck should look similar according to
Considering that edges are generated independently we can write the probability of as
p(e
p(e
e?E
where
is our set of model parameters Defining the shorthand notation
dk Ck
Ck
dk
Ck
allows us to write the log-likelihood of
log
e?V
e?E
Next we describe how to optimize node circle memberships as well as the parameters of the user
profile similarity functions given a graph and user profiles
Unsupervised Learning of Model Parameters
Treating circles as latent variables we aim to find
so as to maximize the regularized
log-likelihood of
argmax
We solve this problem using coordinate ascent on and
Ct
argmax l?t
argmax
Noting that is concave in we optimize through gradient ascent where partial derivatives are given by
de
e?V
dk
e?E
Ck
e?V
Ck
e?E
For fixed Ci we note that solving argmaxCi Ci can be expressed as pseudo-boolean
optimization in a pairwise graphical model it can be written as
Ck argmax
C)).
In words we want edges with high weight
P(under to appear in Ck and edges with low weight to
appear outside of Ck Defining ok Ck C\Ci dk the energy Ee of is
Ee Ee Ee
Ee
ok log(1 eok
log(1 eok
ok log(1 eok
log(1 eok
e?E
e?E
By expressing the problem in this form we can draw upon existing work on pseudo-boolean optimization We use the publicly-available QPBO software described in which is able to
accurately approximate problems of the form shown in We solve for each Ck in a
random order
The two optimization steps of and are repeated until convergence until
PK
We regularize using the norm
which leads to sparse and
readily interpretable parameters Since ego-networks are naturally relatively small our algorithm
can readily handle problems at the scale required In the case of Facebook the average ego-network
has around nodes while the largest network we encountered has nodes Note that
since the method is unsupervised inference is performed independently for each ego-network This
means that our method could be run on the full Facebook graph for example as circles are independently detected for each user and the ego-networks typically contain only hundreds of nodes
Hyperparameter estimation To choose the optimal number of circles we choose so as to
minimize an approximation to the Bayesian Information Criterion BIC
argmin BIC
where is the set of parameters predicted for a particular number of communities and
BIC log
The regularization parameter was determined using leave-one-out cross validation though in our experience did not significantly impact performance
Dataset Description
Our goal is to evaluate our unsupervised method on ground-truth data We expended significant time
effort and resources to obtain high quality hand-labeled data.2 We were able to obtain ego-networks
and ground-truth from three major social networking sites Facebook Google and Twitter
From Facebook we obtained profile and network data from ego-networks consisting of circles and users To do so we developed our own Facebook application and conducted a survey
of ten users who were asked to manually identify all the circles to which their friends belonged On
average users identified 19 circles in their ego-networks with an average circle size of 22 friends
Examples of such circles include students of common universities sports teams relatives etc
http://snap.stanford.edu/data
rst name
Alan
last name
Turing
position
company
name
work
type
name
education
type
rst name
Dilly
last name
Knox
position
company
position
work
education
company
name
type
Cryptanalyst
GC&CS
Cambridge
College
Princeton
Graduate School
Cryptanalyst
GC&CS
Cryptanalyst
Royal Navy
Cambridge
College
203first name Dilly
607last name Knox
607first name Alan
607last name Turing
617work position Cryptanalyst
617work location GC CS
607work location Royal Navy
617education name Cambridge
617education type College
education name Princeton
education type Graduate School
first name
607last name
617work position
617work location
415education name
education type
Figure Feature construction Profiles are tree-structured and we construct features by comparing paths in those trees Examples of trees for two users blue and pink are shown at
left Two schemes for constructing feature vectors from these profiles are shown at right top
right we construct binary indicators measuring the difference between leaves in the two trees
work?position?Cryptanalyst appears in both trees bottom right we sum over the leaf nodes
in the first scheme maintaining the fact that the two users worked at the same institution but discarding the identity of that institution
For the other two datasets we obtained publicly accessible data From Google we obtained data
from ego-networks consisting of circles and users The ego-networks represent all Google users who had shared at least two circles and whose network information
was publicly accessible at the time of our crawl The Google circles are quite different to those
from Facebook in the sense that their creators have chosen to release them publicly and because
Google is a directed network note that our model can very naturally be applied to both to directed
and undirected networks For example one circle contains candidates from the republican
primary who presumably do not follow their followers nor each other Finally from Twitter we
obtained data from ego-networks consisting of circles lists 19 27 and
users The ego-networks we obtained range in size from to nodes
Taken together our data contains different ego-networks circles and users
The size differences between these datasets simply reflects the availability of data from each of the
three sources Our Facebook data is fully labeled in the sense that we obtain every circle that a
user considers to be a cohesive community whereas our Google and Twitter data is only partially
labeled in the sense that we only have access to public circles We design our evaluation procedure
in Section so that partial labels cause no issues
Constructing Features from User Profiles
Profile information in all of our datasets can be represented as a tree where each level encodes
increasingly specific information Figure left From Google we collect data from six categories
gender last name job titles institutions universities and places lived From Facebook we collect
data from 26 categories including hometowns birthdays colleagues political affiliations etc For
Twitter many choices exist as proxies for user profiles we simply collect data from two categories
namely the set of hashtags and mentions used by each user during two-weeks worth of tweets
Categories correspond to parents of leaf nodes in a profile tree as shown in Figure
We first describe a difference vector to encode the relationship between two profiles A non-technical
description is given in Figure Suppose that users each have an associated profile tree Tv
and that Tv is a leaf in that tree We define the difference vector between two users and
as a binary indicator encoding the profile aspects where users and differ Figure top right
Tx Ty
Note that feature descriptors are defined per ego-network while many thousands of high schools
for example exist among all Facebook users only a small number appear among any particular
user?s friends
Although the above difference vector has the advantage that it encodes profile information at a fine
granularity it has the disadvantage that it is high-dimensional up to dimensions in the data
we considered One way to address this is to form difference vectors based on the parents of leaf
nodes this way we encode what profile categories two users have in common but disregard specific
values Figure bottom right For example we encode how many hashtags two users tweeted in
common but discard which hashtags they tweeted
l?children(p
This scheme has the advantage that it requires a constant number of dimensions regardless of the
size of the ego-network for Facebook for Google for Twitter as described above
Based on the difference vectors and
we now describe how to construct edge features
The first property we wish to model is that members of circles should have common relationships with each other
The second property we wish to model is that members of circles should have common relationships
to the ego of the ego-network In this case we consider the profile tree Tu from the ego user We
then define our features in terms of that user
is taken elementwise These two parameterizations allow us to assess which mechanism better captures users subjective definition of a circle In both cases we include a constant feature which controls the probability that edges form within circles or equivalently it measures
the extent to which circles are made up of friends Importantly this allows us to predict memberships
even for users who have no profile information simply due to their patterns of connectivity
we define
Similarly for the compressed difference vector
To summarize we have identified four ways of representing the compatibility between different
aspects of profiles for two users We considered two ways of constructing a difference vector
and two ways of capturing the compatibility of a pair of profiles
Experiments
Although our method is unsupervised we can evaluate it on ground-truth data by examining the
maximum-likelihood assignments of the latent circles CK after convergence Our
goal is that for a properly regularized model the latent variables will align closely with the human
labeled ground-truth circles
Evaluation metrics To measure the alignment between a predicted circle and a ground-truth
we compute the Balanced Error Rate BER between the two circles BER(C
circle
This measure assigns equal importance to false positives and false negatives
so that trivial or random predictions incur an error of on average Such a measure is preferable to
the loss for example which assigns extremely low error to trivial predictions We also report
the F1 score which we find produces qualitatively similar results
Aligning predicted and ground-truth circles Since we do not know the correspondence between
we compute the optimal match via linear assignment by maximizing
circles in and
max
BER(C
C?dom(f
That is if the number of predicted circles
where is a partial correspondence between and C.
then every circle must have a match
is less than the number of ground-truth circles
but if we do not incur a penalty for additional predictions that could have been circles
but were not included in the ground-truth We use established techniques to estimate the number of
nor can any
circles so that none of the baselines suffers a disadvantage by mispredicting
method predict the trivial solution of returning the powerset of all users We note that removing the
bijectivity requirement forcing all circles to be aligned by allowing multiple predicted circles
to match a single groundtruth circle or vice versa lead to qualitatively similar results
Accuracy BER
Accuracy score
Accuracy on detected communities Balanced Error Rate higher is better
multi-assignment clustering Streich Frank
low-rank embedding Yoshida
block-LDA Balasubramanyan and Cohen
84
77
72
72
70
70
our model friend-to-friend features eq
our model friend-to-user features eq
our model compressed features eq
our model compressed features eq
Facebook
Twitter
Google
Accuracy on detected communities score higher is better
multi-assignment clustering Streich Frank
low-rank embedding Yoshida
block-LDA Balasubramanyan and Cohen
59
38
38
34
34
our model friend-to-friend features eq
our model friend-to-user features eq
our model compressed features eq
our model compressed features eq
Facebook
Google
Twitter
Figure Performance on Facebook Google and Twitter in terms of the Balanced Error Rate
and the F1 score bottom Higher is better Error bars show standard error The improvement
of our best features compared to the nearest competitor are significant at the level or better
Baselines We considered a wide number of baseline methods including those that consider only
network structure those that consider only profile information and those that consider both First
we experimented with Mixed Membership Stochastic Block Models which consider only network information and variants that also consider text attributes For each node mixedmembership models predict a stochastic vector encoding partial circle memberships which we
threshold to generate hard assignments We also considered Block-LDA where we generate
documents by treating aspects of user profiles as words in a bag-of-words model
Secondly we experimented with classical clustering algorithms such as K-means and Hierarchical
Clustering that form clusters based only on node profiles but ignore the network Conversely we
considered Link Clustering and Clique Percolation which use network information but ignore profiles We also considered the Low-Rank Embedding approach of where node attributes
and edge information are projected into a feature space where classical clustering techniques can
be applied Finally we considered Multi-Assignment Clustering which is promising in that it
predicts hard assignments to multiple clusters though it does so without using the network
Of the eight baselines highlighted above we report the three whose overall performance was the best
namely Block-LDA which slightly outperformed mixed membership stochastic block models
Low-Rank Embedding and Multi-Assignment Clustering
Performance on Facebook Google and Twitter Data Figure shows results on our Facebook
Google and Twitter data Circles were aligned as described in with the number of circles
determined as described in Section For non-probabilistic baselines we chose
so as to
maximize the modularity as described in In terms of absolute performance our best model
achieves BER scores of on Facebook on Google and on Twitter scores are
and respectively The lower F1 scores on Google and Twitter are explained by the
fact that many circles have not been maintained since they were initially created we achieve high
recall we recover the friends in each circle but at low precision we recover additional friends who
appeared after the circle was created
Comparing our method to baselines we notice that we outperform all baselines on all datasets by a
statistically significant margin Compared to the nearest competitors our best performing features
improve on the BER by on Facebook on Google and on Twitter improvements
in terms of the F1 score are similar Regarding the performance of the baseline methods we
note that good performance seems to depend critically on predicting hard memberships to multiple
circles using a combination of node and edge information none of the baselines exhibit precisely
this combination a shortcoming our model addresses
Both of the features we propose friend-to-friend features and friend-to-user features perform
similarly revealing that both schemes ultimately encode similar information which is not surprising
studied the same degree
speak the same languages
feature index for
Americans
weight
feature index for
weight
weight
feature index for
Germans
who went to school in
studied the same degree
feature index for
same level of education
feature index for
college educated people
working at a particular institute
feature index for
feature index for
weight
living in S.F. or Stanford
weight
people with PhDs
weight
weight
weight
Figure Three detected circles on a small ego-network from Facebook compared to three groundtruth circles BER Blue nodes true positives Grey true negatives Red false positives
Yellow false negatives Our method correctly identifies the largest circle left a sub-circle contained within it center and a third circle that significantly overlaps with it right
worked for the same employer
at the same time
feature index for
Figure Parameter vectors of four communities for a particular Facebook user The top four plots
show complete features while the bottom four plots show compressed features both
cases BER For example the former features encode the fact that members of a particular
community tend to speak German while the latter features encode the fact that they speak the same
language Personally identifiable annotations have been suppressed
since users and their friends have similar profiles Using the compressed features and does
not significantly impact performance which is promising since they have far lower dimension than
the full features what this reveals is that it is sufficient to model categories of attributes that users
have in common same school same town rather than the attribute values themselves
We found that all algorithms perform significantly better on Facebook than on Google or Twitter
There are a few explanations Firstly our Facebook data is complete in the sense that survey participants manually labeled every circle in their ego-networks whereas in other datasets we only observe
publicly-visible circles which may not be up-to-date Secondly the 26 profile categories available
from Facebook are more informative than the categories from Google or the tweet-based profiles
we build from Twitter A more basic difference lies in the nature of the networks themselves edges
in Facebook encode mutual ties whereas edges in Google and Twitter encode follower relationships which changes the role that circles serve The latter two points explain why algorithms
that use either edge or profile information in isolation are unlikely to perform well on this data
Qualitative analysis Finally we examine the output of our model in greater detail Figure shows
results of our method on an example ego-network from Facebook Different colors indicate true
false positives and negatives Our method is correctly able to identify overlapping circles as well
as sub-circles circles within circles Figure shows parameter vectors learned for four circles for
a particular Facebook user Positive weights indicate properties that users in a particular circle have
in common Notice how the model naturally learns the social dimensions that lead to a social circle
Moreover the first parameter that corresponds to a constant feature has the highest weight this
reveals that membership to the same community provides the strongest signal that edges will form
while profile data provides a weaker but still relevant signal
Acknowledgements This research has been supported in part by NSF
DARPA XDATA DARPA GRAPHS Albert Yu Mary Bechmann Foundation Boeing Allyes Samsung Intel Alfred P. Sloan Fellowship and the Microsoft Faculty Fellowship

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5116-a-latent-source-model-for-nonparametric-time-series-classification.pdf

A Latent Source Model for
Nonparametric Time Series Classification
George H. Chen
MIT
georgehc@mit.edu
Stanislav Nikolov
Twitter
snikolov@twitter.com
Devavrat Shah
MIT
devavrat@mit.edu
Abstract
For classifying time series a nearest-neighbor approach is widely used in practice
with performance often competitive with or better than more elaborate methods
such as neural networks decision trees and support vector machines We develop
theoretical justification for the effectiveness of nearest-neighbor-like classification of time series Our guiding hypothesis is that in many applications such as
forecasting which topics will become trends on Twitter there aren?t actually that
many prototypical time series to begin with relative to the number of time series
we have access to topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data To operationalize
this hypothesis we propose a latent source model for time series which naturally
leads to a weighted majority voting classification rule that can be approximated
by a nearest-neighbor classifier We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under
our model accounting for how much of the time series we observe and the model
complexity Experimental results on synthetic data show weighted majority voting
achieving the same misclassification rate as nearest-neighbor classification while
observing less of the time series We then use weighted majority to forecast which
news topics on Twitter become trends where we are able to detect such trending
topics in advance of Twitter of the time with a mean early advantage of
hour and 26 minutes a true positive rate of and a false positive rate of
Introduction
Recent years have seen an explosion in the availability of time series data related to virtually every
human endeavor data that demands to be analyzed and turned into valuable insights A key
recurring task in mining this data is being able to classify a time series As a running example used
throughout this paper consider a time series that tracks how much activity there is for a particular
news topic on Twitter Given this time series up to present time we ask will this news topic go
viral Borrowing Twitter?s terminology we label the time series a trend and call its corresponding
news topic a trending topic if the news topic goes viral otherwise the time series has label not
trend We seek to forecast whether a news topic will become a trend before it is declared a trend
not by Twitter amounting to a binary classification problem Importantly we skirt the discussion
of what makes a topic considered trending as this is irrelevant to our mathematical development.1
Furthermore we remark that handling the case where a single time series can have different labels
at different times is beyond the scope of this paper
While it is not public knowledge how Twitter defines a topic to be a trending topic Twitter does provide
information for which topics are trending topics We take these labels to be ground truth effectively treating
how a topic goes viral to be a black box supplied by Twitter
Numerous standard classification methods have been tailored to classify time series yet a simple
nearest-neighbor approach is hard to beat in terms of classification performance on a variety of
datasets with results competitive to or better than various other more elaborate methods such
as neural networks decision trees and support vector machines More recently
researchers have examined which distance to use with nearest-neighbor classification or
how to boost classification performance by applying different transformations to the time series
before using nearest-neighbor classification These existing results are mostly experimental
lacking theoretical justification for both when nearest-neighbor-like time series classifiers should be
expected to perform well and how well
If we don?t confine ourselves to classifying time series then as the amount of data tends to infinity
nearest-neighbor classification has been shown to achieve a probability of error that is at worst
twice the Bayes error rate and when considering the nearest neighbors with allowed to grow
with the amount of data then the error rate approaches the Bayes error rate However rather
than examining the asymptotic case where the amount of data goes to infinity we instead pursue
nonasymptotic performance guarantees in terms of how large of a training dataset we have and how
much we observe of the time series to be classified To arrive at these nonasymptotic guarantees we
impose a low-complexity structure on time series
Our contributions We present a model for which nearest-neighbor-like classification performs well
by operationalizing the following hypothesis In many time series applications there are only a small
number of prototypical time series relative to the number of time series we can collect For example
posts on Twitter are generated by humans who are often behaviorally predictable in aggregate This
suggests that topics they post about only become trends on Twitter in a few distinct manners yet we
have at our disposal enormous volumes of Twitter data In this context we present a novel latent
source model time series are generated from a small collection of unknown latent sources each
having one of two labels say trend or not trend Our model?s maximum a posteriori MAP time
series classifier can be approximated by weighted majority voting which compares the time series
to be classified with each of the time series in the labeled training data Each training time series
casts a weighted vote in favor of its ground truth label with the weight depending on how similar
the time series being classified is to the training example The final classification is trend or not
trend depending on which label has the higher overall vote The voting is nonparametric in that it
does not learn parameters for a model and is driven entirely by the training data The unknown latent
sources are never estimated the training data serve as a proxy for these latent sources Weighted
majority voting itself can be approximated by a nearest-neighbor classifier which we also analyze
Under our model we show sufficient conditions so that if we have log time series in
our training data then weighted majority voting and nearest-neighbor classification correctly classify a new time series with probability at least
after observing its first log time steps As
our analysis accounts for how much of the time series we observe our results readily apply to the
online setting in which a time series is to be classified while it streams in as is the case for forecasting trending topics as well as the offline setting where we have access to the entire time series
Also while our analysis yields matching error upper bounds for the two classifiers experimental results on synthetic data suggests that weighted majority voting outperforms nearest-neighbor classification early on when we observe very little of the time series to be classified Meanwhile a specific
instantiation of our model leads to a spherical Gaussian mixture model where the latent sources are
Gaussian mixture components We show that existing performance guarantees on learning spherical
Gaussian mixture models require more stringent conditions than what our results need
suggesting that learning the latent sources is overkill if the goal is classification
Lastly we apply weighted majority voting to forecasting trending topics on Twitter We emphasize
that our goal is precognition of trends predicting whether a topic is going to be a trend before it
is actually declared to be a trend by Twitter or in theory any other third party that we can collect
ground truth labels from Existing work that identify trends on Twitter instead as part
of their trend detection define models for what trends are which we do not do nor do we assume
we have access to such definitions The same could be said of previous work on novel document
detection on Twitter In our experiments weighted majority voting is able to predict
whether a topic will be a trend in advance of Twitter of the time with a mean early advantage
of hour and 26 minutes a true positive rate of and a false positive rate of We empirically
find that the Twitter activity of a news topic that becomes a trend tends to follow one of a finite
number of patterns which could be thought of as latent sources
Outline Weighted majority voting and nearest-neighbor classification for time series are presented in Section We provide our latent source model and theoretical performance guarantees
of weighted majority voting and nearest-neighbor classification under this model in Section Experimental results for synthetic data and forecasting trending topics on Twitter are in Section
Weighted Majority Voting and Nearest-Neighbor Classification
Given a time-series2 we want to classify it as having either label trend or
not trend To do so we have access to labeled training data and which denote the sets
of all training time series with labels and respectively
Weighted majority voting Each positively-labeled example casts a weighted vote
for whether time series has label where d(T is some measure of similarity between the two time series and superscript indicates that we are only allowed to look
at the first time steps time steps of but we?re allowed to look outside of these
time steps for the training time series and constant
is a scaling parameter that determines
the sphere of influence of each example Similarly each negatively-labeled example in also
casts a weighted vote for whether time series has label
The similarity measure d(T could for example be squared Euclidean distance d(T
PT
kr sk2T However this similarity measure only looks at the first time
steps of training time series Since time series in our training data are known we need not restrict
our attention to their first time steps Thus we use the following similarity measure
d(T
min
max
max
min
max
max
kr
sk2T
where we minimize over integer time shifts with a pre-specified maximum allowed shift max
Here we have used to denote time series advanced by time steps
Finally we sum up all of the weighted votes and then all of the weighted votes The label
with the majority of overall weighted votes is declared as the label for
d(T
d(T
r2R
if
otherwise
Using a larger time window size corresponds to waiting longer before we make a prediction
We need to trade off how long we wait and how accurate we want our prediction Note that knearest-neighbor classification corresponds to only considering the nearest neighbors of among
all training time series all other votes are set to With we obtain the following classifier
Nearest-neighbor classifier Let rb arg minr2R d(T be the nearest neighbor of
Then we declare the label for to be
if rb
LN
if rb
A Latent Source Model and Theoretical Guarantees
We assume there to be unknown latent sources time series that generate observed time series
Let denote the set of all such latent sources each latent source in has a true label
or Let be the set of latent sources with label and be the set of those
with label The observed time series are generated from latent sources as follows
Sample latent source from uniformly at random.3 Let be the label of
We index time using for notationally convenience but will assume time series to start at time step
While we keep the sampling uniform for clarity of presentation our theoretical guarantees can easily be
extended to the case where the sampling is not uniform The only change is that the number of training data
needed will be larger by a factor of m?1min where min is the smallest probability of a particular latent source
occurring
activity
time
Figure Example of latent sources superimposed where each latent source is shifted vertically in
amplitude such that every other latent source has label and the rest have label
Sample integer time shift uniformly from max
Output time series to be latent source advanced by time steps followed
by adding noise signal The label associated
with the generated time series is the same as that of L. Entries of noise are
zero-mean sub-Gaussian with parameter which means that for any time index
E[exp exp
for all R.
The family of sub-Gaussian distributions includes a variety of distributions such as a zeromean Gaussian with standard deviation and a uniform distribution over
The above generative process defines our latent source model Importantly we make no assumptions
about the structure of the latent sources For instance the latent sources could be tiled as shown in
Figure where they are evenly separated vertically and alternate between the two different classes
and With a parametric model like a k-component Gaussian mixture model estimating
these latent sources could be problematic For example if we take any two adjacent latent sources
with label and cluster them then this cluster could be confused with the latent source having
label that is sandwiched in between Noise only complicates estimating the latent sources In
this example the k-component Gaussian mixture model needed for label would require to be
the exact number of latent sources with label which is unknown In general the number of
samples we need from a Gaussian mixture mixture model to estimate the mixture component means
is exponential in the number of mixture components As we discuss next for classification
we sidestep learning the latent sources altogether instead using training data as a proxy for latent
sources At the end of this section we compare our sample complexity for classification versus
some existing sample complexities for learning Gaussian mixture models
Classification If we knew the latent sources and if noise entries were across
then the maximum a posteriori MAP estimate for label given an observed time series is
if MAP
LMAP
otherwise
where
MAP
and
max
2V
exp
exp
kv
kv
sk2T
sk2T
However we do not know the latent sources nor do we know if the noise is Gaussian We
assume that we have access to training data as given in Section We make a further assumption
that the training data were sampled from the latent source model and that we have different training
time series Denote
max max Then we approximate the MAP classifier by
using training data as a proxy for the latent sources Specifically we take ratio replace the inner
sum by a minimum in the exponent replace and by and and replace by to
obtain the ratio
min 2D kr sk2T
2R exp
min 2D kr
sk2T
2R exp
Plugging in place of MAP in classification rule yields the weighted majority voting rule
Note that weighted majority voting could be interpreted as a smoothed nearest-neighbor approximation whereby we only consider the time-shifted version of each example time series that is closest
to the observed time series If we didn?t replace the summations over time shifts with minimums
in the exponent then we have a kernel density estimate in the numerator and in the denominator
Chapter where the kernel is Gaussian and our main theoretical result for weighted majority
voting to follow would still hold using the same proof.4
Lastly applications may call for trading off true and false positive rates We can do this by generalizing decision rule to declare the label of to be if and vary parameter
The resulting decision rule which we refer to as generalized weighted majority voting is thus
if
otherwise
where setting recovers the usual weighted majority voting This modification to the
classifier can be thought of as adjusting the priors on the relative sizes of the two classes Our
theoretical results to follow actually cover this more general case rather than only that of
Theoretical guarantees We now present the main theoretical results of this paper which identify
sufficient conditions under which generalized weighted majority voting and nearest-neighbor
classification can classify a time series correctly with high probability accounting for the size of
the training dataset and how much we observe of the time series to be classified First we define the
gap between and restricted to time length and with maximum time shift max as
G(T
max
min
2R
2D
kr
k2T
This quantity measures how far apart the two different classes are if we only look at length-T chunks
of each time series and allow all shifts of at most max time steps in either direction
Our first main result is stated below We defer proofs to the longer version of this paper
Theorem Performance guarantee for generalized weighted majority voting Let be
the number of latent sources with label and be the number of latent
sources with label For any under the latent source model with log time series
in the training data the probability of misclassifying time series with label using generalized
satisfies the bound
weighted majority voting
P(L
max
exp
max
An immediate consequence is that given error tolerance and with choice
then upper bound is at most by having each of the two terms on the right-hand side be
if log 2m log log and
G(T
log
log(2
max
log log
This means that if we have access to a large enough pool of labeled time series the pool has
log time series then we can subsample log of them to use as training data
Then with choice generalized weighted majority voting correctly classifies a new time
series with probability at least
if
G(T max log
log(2 max log
max
Thus the gap between sets and needs to grow logarithmic in the number of latent sources
in order for weighted majority voting to classify correctly with high probability Assuming that the
We use a minimum rather a summation over time shifts to make the method more similar to existing time
series classification work which minimize over time warpings rather than simple shifts
original unknown latent sources are separated otherwise there is no hope to distinguish between
the classes using any classifier and the gap in the training data grows as G(T max
otherwise the closest two training time series from opposite classes are within noise of
each other then observing the first log(2 max log time steps from
the time series is sufficient to classify it correctly with probability at least
A similar result holds for the nearest-neighbor classifier
Theorem Performance guarantee for nearest-neighbor classification For any under
the latent source model with log time series in the training data the probability of
satisfies the
misclassifying time series with label using the nearest-neighbor classifier
NN
bound
max exp
P(L
G(T max
NN
Our generalized weighted majority voting bound with corresponding to regular weighted
majority voting and matches our nearest-neighbor classification bound suggesting that
the two methods have similar behavior when the gap grows with In practice we find weighted
majority voting to outperform nearest-neighbor classification when is small and then as grows
large the two methods exhibit similar performance in agreement with our theoretical analysis For
small it could still be fairly likely that the nearest neighbor found has the wrong label dooming
the nearest-neighbor classifier to failure Weighted majority voting on the other hand can recover
from this situation as there may be enough correctly labeled training time series close by that contribute to a higher overall vote for the correct class This robustness of weighted majority voting
makes it favorable in the online setting where we want to make a prediction as early as possible
Sample complexity of learning the latent sources If we can estimate the latent sources accurately
then we could plug these estimates in place of the true latent sources in the MAP classifier and
achieve classification performance close to optimal If we restrict the noise to be Gaussian and
assume max then the latent source model corresponds to a spherical Gaussian mixture model
We could learn such a model using Dasgupta and Schulman?s modified EM algorithm Their
theoretical guarantee depends on the true separation between the closest two latent
sources namely
minv,v 2V kv which needs to satisfy
Then with
log G(T log
and
max log
max
their algorithm achieves with probability at least
an additive error Euclidean
distance close to optimal in estimating every latent source In contrast our result is in terms of gap
G(T max that depends not on the true separation between two latent sources but instead
on the minimum observed separation in the training data between two time series of opposite labels
In fact our gap in their setting grows as even when their gap G(T grows sublinear in
pT
In particular while their result cannot handle the regime where log
ours can using log training time series and observing the first log time
steps to classify a time series correctly with probability at least
see the longer version of this
paper for details
Vempala and Wang have a spectral method for learning Gaussian mixture models that can hane m2 training data
dle smaller G(T than Dasgupta and Schulman?s approach but requires
where we?ve hidden the dependence on
and other variables of interest for clarity of presentation
Hsu and Kakade have a moment-based estimator that doesn?t have a gap condition but under a
different non-degeneracy condition requires substantially more samples for our problem setup
to achieve an approximation of the mixture components These results
need substantially more training data than what we?ve shown is sufficient for classification
To fit a Gaussian mixture model to massive training datasets in practice using all the training data
could be prohibitively expensive In such scenarios one could instead non-uniformly subsample
O(T m3 time series from the training data using the procedure given in and then feed the
resulting smaller dataset referred to as an coreset to the EM algorithm for learning the latent
sources This procedure still requires more training time series than needed for classification and
lacks a guarantee that the estimated latent sources will be close to the true latent sources
Classification error rate on test data
Classification error rate on test data
Weighted majority voting
Nearest?neighbor classifier
Oracle MAP classifier
Weighted majority voting
Nearest?neighbor classifier
Oracle MAP classifier
activity
Figure Results on synthetic data Classification error rate number of initial time steps
used training set size log where Classification error rate at
All experiments were repeated times with newly generated latent sources training data and test
data each time Error bars denote one standard deviation above and below the mean value
time
Figure How news topics become trends on Twitter The top left shows some time series of activity
leading up to a news topic becoming trending These time series superimposed look like clutter but
we can separate them into different clusters as shown in the next five plots Each cluster represents
a way that a news topic becomes trending
Experimental Results
Synthetic data We generate latent sources where each latent source is constructed by
first sampling entries per time step and then applying a 1D Gaussian smoothing
filter with scale parameter Half of the latent sources are labeled and the other half Then
log training time series are sampled as per the latent source model where the noise added
is and max We similarly generate time series to use as test data We
set for weighted majority voting For we compare the classification error rates on
test data for weighted majority voting nearest-neighbor classification and the MAP classifier with
oracle access to the true latent sources as shown in Figure We see that weighted majority voting
outperforms nearest-neighbor classification but as grows large the two methods performances
converge to that of the MAP classifier Fixing we then compare the classification error
rates of the three methods using varying amounts of training data as shown in Figure the
oracle MAP classifier is also shown but does not actually depend on training data We see that as
increases both weighted majority voting and nearest-neighbor classification steadily improve in
performance
Forecasting trending topics on twitter We provide only an overview of our Twitter results here
deferring full details to the longer version of this paper We sampled examples of trends at
random from a list of June news trends and examples of non-trends based on phrases
appearing in user posts during the same month As we do not know how Twitter chooses what
phrases are considered as candidate phrases for trending topics it?s unclear what the size of the
Figure Results on Twitter data Weighted majority voting achieves a low error rate FPR
of TPR of and detects trending topics in advance of Twitter of the time with a mean
of hours when it does parameters Tsmooth Envelope of
all ROC curves shows the tradeoff between TPR and FPR. Distribution of detection times for
aggressive conservative bottom and in-between center parameter settings
non-trend category is in comparison to the size of the trend category Thus for simplicity we
intentionally control for the class sizes by setting them equal In practice one could still expressly
assemble the training data to have pre-specified class sizes and then tune for generalized weighted
majority voting In our experiments we use the usual weighted majority voting
to classify time series where max is set to the maximum possible we consider all shifts
Per topic we created its time series based on a pre-processed version of the raw rate of how often
the topic was shared its Tweet rate We empirically found that how news topics become trends
tends to follow a finite number of patterns a few examples of these patterns are shown in Figure
We randomly divided the set of trends and non-trends into into two halves one to use as training
data and one to use as test data We applied weighted majority voting sweeping over and
data pre-processing parameters As shown in Figure one choice of parameters allows us to
detect trending topics in advance of Twitter of the time and when we do we detect them an
average of hours earlier Furthermore we achieve a true positive rate TPR of and a false
positive rate FPR of Naturally there are tradeoffs between TPR FPR and how early we make
a prediction how small As shown in Figure an aggressive parameter setting yields
early detection and high TPR but high FPR and a conservative parameter setting yields low FPR
but late detection and low TPR. An in-between setting can strike the right balance
Acknowledgements This work was supported in part by the Army Research Office under MURI
Award GHC was supported by an NDSEG fellowship

<<----------------------------------------------------------------------------------------------------------------------->>

