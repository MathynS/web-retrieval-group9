query sentence: usage of telescope during weekends with daylight
---------------------------------------------------------------------
title: 3032-learning-time-intensity-profiles-of-human-activity-using-non-parametric-bayesian-models.pdf

Learning Time-Intensity Profiles of Human Activity
using Non-Parametric Bayesian Models
Alexander T. Ihler
Padhraic Smyth
Donald Bren School of Information and Computer Science
U.C. Irvine
ihler@ics.uci.edu
smyth@ics.uci.edu
Abstract
Data sets that characterize human activity over time through collections of timestamped events or counts are of increasing interest in application areas as humancomputer interaction video surveillance and Web data analysis We propose a
non-parametric Bayesian framework for modeling collections of such data In
particular we use a Dirichlet process framework for learning a set of intensity
functions corresponding to different categories which form a basis set for representing individual time-periods several days depending on which categories
the time-periods are assigned to This allows the model to learn in a data-driven
fashion what factors are generating the observations on a particular day including for example weekday versus weekend effects or day-specific effects corresponding to unique single-day occurrences of unusual behavior sharing information where appropriate to obtain improved estimates of the behavior associated
with each category Applications to real?world data sets of count data involving
both vehicles and people are used to illustrate the technique
Introduction
As sensor and storage technologies continue to improve in terms of both cost and performance
increasingly rich data sets are becoming available that characterize the rhythms of human activity
over time Examples include logs of radio frequency identification RFID tags freeway traffic
over time loop-sensor data crime statistics email and Web access logs and many more Such
data can be used to support a variety of different applications such as classification of human or
animal activities detection of unusual events or to support the broad understanding of behavior in
a particular context such as the temporal patterns of Web usage
To ground the discussion consider data consisting of a collection of individual or aggregated events
from a single sensor a time-stamped log recording every entry and exit from a building or the
timing and number of highway traffic accidents For example Figure shows several days worth of
data from a building log smoothed so that the similarities in patterns are more readily visible
Of interest is the modeling of the underlying intensity of the process generating the data where
intensity here refers to the rate at which events occur These processes are typically inhomogeneous
in time as in Figure as they arise from the aggregated behavior of individuals and thus exhibit
a temporal dependence linked to the rhythms of the underlying human activity The complexity
of this temporal dependence is application-dependent and generally unknown before observing the
data suggesting that non or semi-parametric methods methods whose complexity is capable of
growing as the number of observations increase may be particularly appropriate
Formulating the underlying event generation as an inhomogeneous Poisson process is a common first
step see as it allows the application of various classic density estimation techniques to
estimate the time-dependent intensity function normalized version of the rate function see Sec
tion Techniques used in this context include kernel density estimation wavelet analysis
discretization and nonparametric Bayesian models
Among these nonparametric Bayesian approaches have a number of appealing advan60
tages First they allow us to represent and reason about uncertainty in the intensity function
providing not just a single estimate but a distribution over functions Second the Bayesian
framework provides natural methods for model
selection allowing the data to be naturally
plained by a parsimonious set of intensity functions rather than using the most complex expla20
nation though similar effects may be achieved
using penalized likelihood functions
nally Bayesian methods generalize to multiple
or hierarchical models which allow informa0
18
24
tion to be shared among several related but differing sets of observations multiple days Figure Count data from a building entry log
of data This second point is crucial for many observed on ten Mondays each smoothed using a
problems as we rarely obtain many observa kernel function to enable visual comparison
tions of exactly the same process under exactly
the same conditions instead we observe multiple instances which are thought to be similar but may in fact represent any number of slightly
differing circumstances For example behavior may be dependent on not only time of day but also
day of week type of day weekend or weekday unobserved factors such as the weather or other
unusual circumstances Sharing information allows us to improve our model but we should only
do so where appropriate itself best indicated by similarity in the data By being Bayesian we can
remain agnostic of what data should be shared and reason over our uncertainty in this structure
In what follows we propose a non-parametric Bayesian framework for modeling intensity functions
for event data over time In particular we describe a Dirichlet process framework for learning the
unknown rate functions and learn a set of such functions corresponding to different categories Individual time-periods individual days are then represented as additive combinations of intensity
functions depending on which categories are assigned to each time-period This allows the model
to learn in a data-driven fashion what factors are generating the observations on a particular day
including for example weekday versus weekend effects as well as day-specific effects corresponding to unusual behavior present only on a single day Applications to two real?world data sets a
building access log and accident statistics are used to illustrate the technique
We will discuss in more detail in the sections that follow how our proposed approach is related to
prior work on similar topics Broadly speaking from the viewpoint of modeling of inhomogeneous
time-series of counts our work extends the work of to allow sharing of information among
multiple related processes different days Our approach can also be viewed as an alternative
to the hierarchical Dirichlet process HDP for problems where the patterns across different
groups are much more constrained than would be expected under an HDP model
Poisson processes
A common model for continuous-time event counting data is the Poisson process As the
discrete Poisson distribution is characterized by a rate parameter the Poisson process1 is characterized by a rate function it has the property that over any given
time interval the number of
events occurring within that time is Poisson with rate given by We shall use a Bayesian
semi-parametric model for described next
Let us suppose that we have a single collection of event times arising from a Poisson process
with rate function
Here we shall use the term Poisson process interchangeably with inhomogeneous Poisson process meaning that the rate is a non-constant function of time
where is defined on We may write where and
is the intensity function a normalized version of the rate function A Bayesian model places prior
distributions on these quantities by selecting a parametric prior for and a nonparametric prior for
we obtain a semi-parametric prior for Specifically we choose
where is the gamma distribution is a kernel function for example a Gaussian distribution
and DP is a Dirichlet process with parameter and base distribution G0 The Dirichlet process
provides a nonparametric prior for such thatP
with probability one has the form of a mixture
model with infinitely many components wj If desired we may also place prior
distributions on some or all of these quantities or the parameters of G0 as well
Dirichlet processes and their variations have gained recent attention for their ability to
provide representations consisting of arbitrarily large mixture models In particular they have been
the subject of recent work in modeling intensity functions for Poisson processes defined over time
and space?time
Monte Carlo Inference
For the Poisson process model just described the likelihood of the data at some
time is given by
exp
which as as we observe a complete data set becomes
The rightmost term term involving has the same form as the likelihood of the as samples
from the mixture model distribution defined by As in many mixture model applications it will be
helpful to create auxiliary assignment variables zi for each indicating with which of the mixture
components the sample is associated The complete data likelihood is then
zi
wzi zi
Inference is typically accomplished using Markov chain Monte Carlo MCMC sampling
Specifically although the posterior for has a simple closed form a
sampling from is more complicated Samples from can be drawn in a variety of ways One
of the most common methods is the so-called Chinese Restaurant Process CRP in which
the relative weights wj are marginalized out while drawing the assignment variables zi Such exact
sampling approaches work by exploiting the fact that only a finite number of the mixture components are occupied by the data by treating the unoccupied clusters as a single group the infinite
number of potential associations can be treated as a finite number The operations involved such as
sampling values for given a collection of associated event times are easier for certain choices
of and than others for example using a Gaussian kernel and normal-Wishart distribution the
necessary quantities have convenient closed forms
Another more brute-force way around the issue of having infinitely many mixture components is to
perform approximate sampling using a truncated Dirichlet process representation As described in for a given data set size and tolerance one can compute a maximum number
of components necessary to approximate the Dirichlet process with a Dirichlet distribution using
the relation
exp[?(M
and in this manner can work with finite numbers of mixture components This representation will
prove useful in Section
The truncated DP approximation is helpful primarily because it allows us to sample the complete
function as compared to only the occupied part in the CRP formulation Given a set of
assignments zi occupying arbitrarily numbered clusters we can sample the weights wj
in two steps.P
First we sample the occupied mixture weights wj and the total unoccupied
weight
wj by drawing independent Gamma-distributed random variables according to
and respectively and normalizing them to sum to one The values of weights wj in
the unoccupied clusters can then be sampled given
using the stick?breaking representation
of Sethuraman
Note that the truncated DP approximation highlights the importance of also sampling if we hope
for our representation to act non-parametric in the sense that it may grow more complex as the data
increase since for a fixed and the number of components is quite insensitive to For more
details on sampling such hyper-parameters see
Finite Time Domains
Our description of non-parametric Bayesian techniques for Poisson processes has so far made implicit use of the fact that the domain of is infinite When the domain of is finite for example
a few minor complications arise For example the kernel functions should properly
be defined as positive only on this interval One possible solution to this issue is to use an alternate
kernel function such as the Beta distribution However this means that posterior sampling of the
parameters is no longer possible in closed form Although methods such as Metropolis-Hastings
may be used they can be highly dependent on the choice of proposal density
Here we take a slightly different approach drawing truncated Gaussian kernels with parameters
sampled from a truncated Normal-Wishart distribution Specifically we define
dx
where is one on and zero elsewhere and is the normal-Wishart distribution Sampling in this model turns out to be relatively simple and efficient using rejection methods Given the
R1
restrictions imposed on and one can show that the normalizing quantity is
always greater than one-third Thus to sample from the posterior we simply draw from the original
closed form posterior distribution discarding and re-sampling if or with
probability
Categorical Models
As mentioned in the introduction we often have several collections of observations
with Nd arising from instances of the same or similar processes If these processes
are known to be identical and independent sharing information among them is relatively easy?we
obtain observations Nd with which to estimate and the di are collectively used to estimate
However if these processes are not necessarily identical sharing information becomes more
difficult
Yet it is just this situation which is most typical Again consider Figure which shows event data
from ten different Mondays Clearly there is a great deal of consistency in both size and shape
although not every day is exactly the same and one or two stand out as different Were we to
also look at for example Sundays or Tuesdays as we do in Section we would see that although
Sunday and Monday appear quite different and one suspects have little shared information Monday
and Tuesday appear relatively similar and this similarity can probably be used to improve our rate
estimates for both days
In this example we might reasonably assume that the category memberships are known for example whether a given day is a weekday or weekend or a Monday or Tuesday though we shall
relax this assumption in later sections Then given a structure of potential relationships what is a
reasonable model for sharing information among categories There are of course many possible
choices we use a simple additive model described in the next section
Additive Models
The intuition behind an additive model is that the data arises from the superposition of several
underlying causes present during the period of interest Again we initially assume that the category
memberships are known thus if a category is associated with a particular day the activity profile
associated with that category will be observed along with additional activity arising from each of
the other categories present
Let us associate a rate function fc with each category in our model We define the rate
function of a given day to be the sum of the rate functions of each category to which belongs
Denoting by sdc the binary-valued
membership indicator that category is present during day
we have that c:sdc
At first this model might seem quite restrictive However it matches our intuition of how the data
is generated stemming from the presence or absence of a particular behavioral pattern associated
with some underlying cause such as it being a work day In fact we do not want a model which
is too flexible such as a linear combination of patterns since it is not physically meaningful to say
for example that a day is only part Monday To learn the profiles associated with a given cause
things that happen every day versus only on weekdays or only on Mondays it makes sense
to take an all or nothing model where the pattern is either present or not This also suggests
that other methods of coupling Dirichlet processes such as the hierarchical Dirichlet process
may be too flexible The HDP couples the parameters of components across levels but only loosely
relates the actual shape of the profile since it allows components to be larger or smaller even
disappear completely In this is a desirable quality but in our application it is not Using an
additive model allows both a consistent size and shape to emerge for each category while associating
deviations from that profile to categories further down in the hierarchy
Inference in this system is not significantly more difficult than in the single rate function case Section We define the association as ydi zdi where ydi indicates which of the categories
generated
event di It is easy to sample ydi according to p(ydi c0
Sampling Membership
Of course it is frequently the case that the membership(s of each collection of data are not known
precisely In an extreme case we may have no idea which collections are similar and should be
grouped together and wish to find profiles in an unsupervised manner More commonly however
we have some prior knowledge and interpretation of the profiles but do not wish to strictly enforce
a known membership For example if we create categories with assigned meanings weekdays
weekends Sundays Mondays and so a day which is nominally a Monday but also happens
to be a holiday closure or other unusual circumstances may be completely different from other
Monday profiles Similarly a day with unusual extra activity receptions talks etc may see
behavior unique to its particular circumstances and warrant an additional category to represent it
We can accommodate both these possibilities by also sampling the values of the membership indicator variables sdc the binary indicator that day sees behavior from category To this end
let us assume we have some prior knowledge of these membership probabilities pdc sdc we may
then re-sample from their posterior distributions at each iteration of MCMC
This sampling step is difficult to do outside the truncated representation Although up until this point
we could easily have elected to use for example the CRP formulation for sampling the association
variables ydi zdi are tightly coupled with the memberships sdc since if any ydi we must have
that sdc Instead to sample the sdc we condition on the truncated rate functions with
truncation depth chosen to provide arbitrarily high precision The likelihood of the data under
these rate functions for any values of sdc can then be computed directly via where
sdc
sdc
and
In practice we propose changing the value of each membership variable sdc individually given
the others though more complex moves could also be applied This gives the following sequence of
MCMC sampling given a truncated representation of the sample membership variables
sdc given and sdc sample associations zdi given associations zdi sample
18
24
Sundays
18
Mondays
24
18
24
Tuesdays
Figure Posterior mean estimates of rate functions for building entry log data estimated individually for each day dotted and learned by sharing information among multiple days solid for
Sundays Mondays and Tuesdays Sharing information among similar days gives greatly
improved estimates of the rate functions resolving otherwise obscured features such as the decrease
during and increase subsequent to lunchtime
category magnitudes and a truncated representation of each fc consisting of weights wj
and parameters
Experiments
In this section we consider the application of our model to two data sets one mentioned previously
from the entry log of people entering a large campus building produced by optical sensors at the
front door and the other from a log of vehicular traffic accidents By design both data sets contain
about ten weeks worth of observations In both cases we have a plausible prior structure for and
interpretation of the categories that similar days will have similar profiles To this end we create
categories for all days weekends weekdays and Sundays through Saturdays Each of
these categories has a high probability pdc of membership for each eligible day To account
for the possibility of unusual increases in activity we also add categories unique to each day with
lower prior probability pdc of membership This allows but discourages each day to add a
new category if there is evidence of unusual activity
Building Entry Data
To see the improvement in the estimated rate functions when information is shared among similar
days Figure shows results from three different days of the week Sunday Monday Tuesday
Each panel shows the estimated profiles of each of the ten days estimated individually using only
that day?s observations under a Dirichlet process mixture model dotted lines Superimposed in
each panel is a single black curve corresponding to the total profile for that day of week estimated
using our categorical model so shows the sum of the rate functions for all days weekends
and Sundays while shows the sum of all days weekdays and Mondays We use the
same prior distributions for both the individual estimates and the shared estimate
Several features are worth noting First by sharing several days worth of observations the model can
produce a much more accurate estimate of the profiles In this case no single day contains enough
observations to be confident about the details of the rate function so each individually?estimated
rate function appears relatively smooth However when information from other days is included
the rate function begins to resolve into a clearly bi-modal shape for weekdays This bi-modal rate
behavior is quite real and corresponds to the arrival of occupants in the morning first mode a lull
during lunchtime and a larger narrower second peak as most occupants return from lunch
Second although Monday and Tuesday profiles appear similar they also have distinct behavior such
as increased activity late Tuesday morning This behavior too has some basis in reality corresponding to a regular weekly meeting held around lunchtime over most though not quite all of the weeks
in question The breakdown of a particular day the first Tuesday into its component categories is
shown in Figure As we might expect there is little consistency between weekdays and weekends
quite a bit of similarity among weekdays and among just Tuesdays and for this particular day very
little to set it apart from other Tuesdays
We can also check to see that the category memberships sdc are being used effectively One of
the Mondays in our data set fell on a holiday the individual profile very near zero If we average
the probabilities computed during MCMC to estimate the posterior probability of the sdc for that
18
24
All Days
18
24
Weekdays
18
24
Tuesdays
18
24
Unique
Figure Posterior mean estimates of the rate functions for each category to which the first Tuesday
data might belong For comparison the total rate sum of all categories is shown as the dotted
line The all days category is small indicating little consistency in the data between weekdays
and weekends the weekdays category is larger and contains a component which appears
to correspond to the occupants return from lunch the Tuesday category has modes in the
morning and afternoon perhaps capturing regular meetings or classes the unique category
category unique to this particular day shows little or no activity
18
24
18
24
18
24
Figure Profiles associated with individual-day categories in the entry log data for several days
with known events periods between dashed vertical lines The model successfully learns which
days have significant unusual activity and associates reasonable profiles with that activity note that
increases in entrance count rate typically occurs shortly before or at the beginning of the event time
particular day we find that it has near-zero probability of belonging to either the weekday or Monday
categories and uses only the all-day and unique categories
We can also examine days which have high probability of requiring their own category indicating unusual activity For this data set we also have partial ground truth consisting of a number
of dates and times when activities were scheduled to take place in the building Figure shows
three such days and the corresponding rate profiles associated with their single-day categories
Again all three days are estimated to have additional activity and the period of time for that activity
corresponds well with the actual start and end time shown in the schedule dashed vertical lines
Vehicular Accident Data
Our second data set consists of a database of vehicular accident times recorded by North Carolina police departments As we might expect
of driving patterns there is still less activity on
weekends but far more than was observed in
the campus building log
18
24
As before sharing information allows us to Figure Posterior mean and uncertainty for a
decrease our posterior uncertainty on the rate single day of accident data estimated individually
for any particular day Figure quantifies this red and with data sharing black Sharing data
idea by showing the posterior means and point considerably reduces the posterior uncertainty in
wise two-sigma confidence intervals for the the profile shape
rate function estimated for the same day the
first Monday in the data set using that day?s
data only red curves and using the category-based additive model black The additive model
leverages the additional data to produce much tighter estimates of the rate profile
As with the previous example the additional data also helps resolve detailed features of each day?s
profile as seen in Figure For example the weekday profiles show a tri-modal shape with one
mode corresponding to the morning commute a small mode around noon and another large mode
Sundays
18
24
18
Mondays
24
18
24
Fridays
Figure Posterior mean estimates of rate functions for vehicular accidents estimated individually
for each day dotted and with sharing among multiple days solid for Sundays Mondays
and Fridays As in Figure sharing information helps resolve features which the individual days
do not have enough data to reliably estimate
around the evening commute This also helps make the pattern of deviation on Friday clear showing
as we would expect increased activity at night
Conclusions
The increasing availability of logs of human activity data provides interesting opportunities for the
application of statistical learning techniques In this paper we proposed a non-parametric Bayesian
approach to learning time-intensity profiles for such activity data based on an inhomogeneous Poisson process framework The proposed approach allows collections of observations days to
be grouped together by category day of week weekday/weekend etc which in turn leverages
data across different collections to yield higher quality profile estimates When the categorization
of days is not a priori certain days that fall on a holiday or days with unusual non-recurring
additional activity the model can infer the appropriate categorization allowing for example automated detection of unusual events On two large real-world data sets the model was able to infer
interpretable activity profiles that correspond to real-world phenomena Directions for further work
in this area include richer models that allow for incorporation of observed covariates such as weather
and other exogenous phenomena as well as modeling of multiple spatially-correlated sensors
loop sensor data for freeway traffic

----------------------------------------------------------------

title: 1811-exact-solutions-to-time-dependent-mdps.pdf

Exact Solutions to Time-Dependent MDPs
Justin A. Boyan
ITA Software
Building
One Kendall Square
Cambridge MA
jab@itasoftware.com
Michael L. Littman
AT&T Labs-Research
and Duke University
Park Ave. Room
Florham Park NJ USA
mlittman@research.att com
Abstract
We describe an extension of the Markov decision process model in
which a continuous time dimension is included in the state space
This allows for the representation and exact solution of a wide
range of problems in which transitions or rewards vary over time
We examine problems based on route planning with public transportation and telescope observation scheduling
Introduction
Imagine trying to plan a route from home to work that minimizes expected time
One approach is to use a tool such as Mapquest which annotates maps with
information about estimated driving time then applies a standard graph-search
algorithm to produce a shortest route Even if driving times are stochastic the annotations can be expected times so this presents no additional challenge However
consider what happens if we would like to include public transportation in our route
planning Buses trains and subways vary in their expected travel time according to
the time of day buses and subways come more frequently during rush hour trains
leave on or close to scheduled departure times In fact even highway driving times
vary with time of day with heavier traffic and longer travel times during rush hour
To formalize this problem we require a model that includes both stochastic actions
as in a Markov decision process and actions with time-dependent stochastic
durations There are a number of models that include some of these attributes
Directed graphs with shortest path algorithms State transitions are deterministic action durations are time independent deterministic or stochastic
Stochastic Time Dependent Networks STDNS State transitions are deterministic action durations are stochastic and can be time dependent
Markov decision processes MDPS State transitions are stochastic action
durations are deterministic
Semi-Markov decision processes SMDPS State transitions are stochastic
action durations are stochastic but not time dependent
The work reported here was done while Boyan's affiliation was with NASA Ames
Research Center Computational Sciences Division
In this paper we introduce the Time-Dependent MDP TMDP model which generalizes all these models by including both stochastic state transitions and stochastic
time-dependent action durations At a high level a TMDP is a special continuousstate MDP consisting of states with both a discrete component and a real-valued
time component lR
With absolute time as part of the state space we can model a rich set of domain objectives including minimizing expected time maximizing the probability of making
a deadline or maximizing the dollar reward of a path subject to a time deadline
In fact using the time dimension to represent other one-dimensional quantities
TMDPS support planning with non-linear utilities risk-aversion or with a
continuous resource such as battery life or money
We define TMDPs and express their Bellman equations in a functional form that
gives at each state the one-step lookahead value at for all times in parallel
Section We use the term time-value function to denote a mapping from realvalued times to real-valued future reward With appropriate restrictions on the form
of the stochastic state-time transition function and reward function we guarantee
that the optimal time-value function at each state is a piecewise linear function of
time which can be represented exactly and computed by value iteration Section
We conclude with empirical results on two domains Section
General model
Missed the 8am
rdID
I
Caught the 8am tram
I I I I I I I ft I I I
REL
Pz
lSl
ABS
78
IQ
IIII L3 1At-lll
Dnve on backroad
HIghway off peak
I L4
Vz
III
23
REL
Highway rush hour
I I I I
Ls
I I I
Ps
REL
REL
Figure An illustrative route-planning example TMDP
Figure depicts a small route-planning example that illustrates several distinguishing features of the TMDP model The start state Xl corresponds to being at home
From here two actions are available taking the 8am train scheduled action
and driving to work via highway then backroads may be done at any time
Action a1 has two possible outcomes represented by III and Outcome III
Missed the 8am train is active after whereas outcome Caught the
train is active until this is governed by the likelihood functions L1 and
in the model These outcomes cause deterministic transitions to states Xl and
respectively but take varying amounts of time Time distributions in a TMDP may
be either relative REL or absolute In the case of catching the train
the distribution is absolute the arrival time shown in has mean
no matter what time before the action was initiated Boarding the train
earlier does not allow us to arrive at our destination earlier However missing the
train and returning to Xl has a relative distribution it deterministically takes
minutes from our starting time distribution P1 to return home
The outcomes for driving are Jl3 and Jl4. Outcome Jl3 Highway rush hour
is active with probability during the interval and with smaller probability outside that interval as shown by Outcome Jl4 Highway off peak
is complementary Duration distributions P3 and P4 both relative to the initiation
time show that driving times during rush hour are on average longer than those off
peak State X2 is reached in either case
From state only one action is available The corresponding outcome Jl5
Drive on backroad is insensitive to time of day and results in a deterministic
transition to state X3 with duration hour The reward function for arriving at
work is before and falls linearly to zero between and noon
The solution to a TMDP such as this is a policy mapping state-time pairs to
actions so as to maximize expected future reward As is standard in MDP methods
our approach finds this policy via the value function V*. We represent the value
function of a TMDP as a set of time-value functions one per state gives the
optimal expected future reward from state at time In our example of Figure
the time-value functions for X3 and X2 are shown as Va and V2. Because of the
deterministic one-hour delay of V2 is identical to V3 shifted back one hour This
wholesale shifting of time-value functions is exploited by our solution algorithm
The
model also allows a notion of dawdling in a state This means the
agent can remain in a state for as long as desired at a reward rate of
per unit time before choosing an action This makes it possible for example for an
agent to wait at home for rush hour to end before driving to work
TMDP
TMDP
Formally a
A
TMDP
consists of the following components
discrete state space
discrete action space
discrete set of outcomes each of the form Jl x~,TIt,PIt
EX the resulting state
Tit ABS REL specifies the type of the resulting time distribution
PIt(t if Tit ABS pdf over absolute arrival times of Jl
PIt if Tit REL pdf over durations of Jl
L(Jllx is the likelihood of outcome Jl given state time action a
R(Jl is the reward for outcome Jl at time with duration
is the reward rate for dawdling in state at time
We can define the optimal value function for a
with the following Bellman equations
TMDP
in terms of these quantities
sup
value function allowing dawdling
it
max
value function immediate action
expected value over outcomes
aEA
L(Jllx a U(Jl
itEM
U(Jl
PIt(t PIt(t
if Tit ABS
if Tit REL
These equations follow straightforwardly from viewing the TMDP as an undiscounted
continuous-time MDP Note that the calculations of U(Jl are convolutions of the
result-time pdf with the lookahead value V. In the next section we discuss
a concrete way of representing and manipulating the continuous quantities that
appear in these equations
Model with piecewise linear value functions
In the general model the time-value functions for each state can be arbitrarily
complex and therefore impossible to represent exactly In this section we show how
to restrict the model to allow value functions to be manipulated exactly
For each state we represent its time-value function Vi(t as a piecewise linear function of time Vi(t is thus represented by a data structure consisting of a set of
distinct times called breakpoints and for each pair of consecutive breakpoints the
equation of a line defined over the corresponding interval
Why are piecewise linear functions an appropriate representation Linear timevalue functions provide an exact representation for minimum-time problems Piecewise time-value functions provide closure under the max operator
Rewards must be constrained to be piecewise linear functions of start and arrival
times and action durations We write Rs(p Ra(P Rd(p
where Rs Ra and Rd are piecewise linear functions of start time arrival time
and duration respectively In addition the dawdling reward and the outcome
probability function must be piecewise constant
The most significant restriction needed for exact computation is that arrival and
duration pdfs be discrete This ensures closure under convolutions In contrast
convolving a piecewise constant pdf a uniform distribution with a piecewise
linear time-value function would in general produce a piecewise quadratic timevalue function further convolutions increase the degree with each iteration of value
iteration In Section below we discuss how to relax this restriction
Given the restrictions just mentioned all the operations used in the Bellman equations from Section namely addition multiplication integration supremum
maximization and convolution can be implemented exactly The running time
of each operation is linear in the representation size of the time-value functions
involved Seeding the process with an initial piecewise linear time-value function
we can carry out value iteration until convergence In general the running time
from one iteration to the next can increase as the number of linear pieces being
manipulated grows however the representations grow only as complex as necessary
to represent the value function exactly
Experimental domains
We present results on two domains transportation planning and telescope scheduling For comparison we also implemented the natural alternative to the piecewiselinear technique discretizing the time dimension and solving the problem as a standard MDP. To apply the MDP method three additional inputs must be specified
an earliest starting time latest finishing time and bin width Since this paper's
focus is on exact computations we chose a discretization level corresponding to the
resolution necessary for exact solution by the MDP at its grid points An advantage
of the MDP is that it is by construction acyclic so it can be solved by just one sweep
of standard value iteration working backwards in time The TMDP'S advantage is
that it directly manipulates entire linear segments of the time-value functions
Transportation planning
Figure illustrates an example TMDP for optimizing a commute from San Francisco
to NASA Ames The discrete states model both location and observed traffic
Figure The San Francisco to Ames commuting example
Q-functions at state 01 8ayshore heavy traffic
action drive to Ames
action drive to 8ayshore station
ij
ij
li
Optimal policy over time at state
action drive to Ames
action drive to 8ayshore station
nro
time
Figure The optimal Q-value functions and policy at state
conditions shaded and unshaded circles represent heavy and light traffic respectively Observed transition times and traffic conditions are stochastic and depend
on both the time and traffic conditions at the originating location At states
and the catch the train action induces an absolute arrival distribution
reflecting the train schedules
The domain objective is to arrive at Ames by We impose a linear penalty
for arriving between and noon and an infinite penalty for arriving after noon
There are also linear penalties on the number of minutes spent driving in light
traffic driving in heavy traffic and riding on the train the coefficients of these
penalties can be adjusted to reflect the commuter's tastes
Figure presents the optimal time-value functions and policy for state
US101&Bayshore heavy traffic There are two actions from this state corresponding to driving directly to Ames and driving to the train station to wait for
the next train Driving to the train station is preferred has higher Q-value at
times that are close but not too close to the departure times of the train
The full domain is solved in well under a second by both solvers Table The
optimal time-value functions in the solution comprise a total of linear segments
Telescope observation scheduling
Next we consider the problem of scheduling astronomical targets for a telescope to
maximize the scientific return of one night's viewing We are given possible
targets with associated coordinates scientific value and time window of visibility
Of course we can view only one target at a time We assume that the reward of
an observation is proportional to the duration of viewing the target Acquiring a
target requires two steps of stochastic duration moving the telescope taking time
roughly proportional to the distance traveled and calibrating it on the new target
Previous approaches have dealt with this stochasticity heuristically using a just-incase scheduling approach Here we model the stochasticity directly within the
TMDP framework The TMDP has states corresponding to the observations
and and actions per state corresponding to what to observe next The
Domain
Solver
SF-Commute
piecewise VI
exact grid VI
piecewise VI
exact grid VI
piecewise VI
exact grid VI
piecewise VI
exact grid VI
piecewise VI
exact grid VI
Telescope-IO
Telescope-25
Telescope-50
Telescope-100
Model
states
26
51
Value
sweeps
13
pieces
Runtime
secs
Table Summary of results The three rightmost columns measure solution complexity in terms of the number of sweeps of value iteration before convergence
the number of distinct pieces or values in the optimal value function and
the running time Running times are the median of five runs on an UltraSparc
CPU RAM
dawdling reward rate encodes the scientific value of observing at time
that value is at times when is not visible Relative duration distributions encode
the inter-target distances and stochastic calibration times on each transition
We generated random target lists of sizes and Visibility windows
were constrained to be within a 13-hour night specified with O.Ol-hour precision
Thus representing the exact solution with a grid required time bins per state
Table shows comparative results of the piecewise-linear and grid-based solvers
Conclusions
In sum we have presented a new stochastic model for time-dependent MDPS
TMDPS discussed applications and shown that dynamic programming with piecewise linear time-value functions can produce optimal policies efficiently In initial
comparisons with the alternative method of discretizing the time dimension the
TMDP approach was empirically faster used significantly less memory and solved
the problem exactly over continuous lR rather than just at grid points
In our exact computation model the requirement of discrete duration distributions
seems particularly restrictive We are currently investigating a way of using our
exact algorithm to generate upper and lower bounds on the optimal solution for
the case of arbitrary pdfs This may allow the system to produce an optimal or
provably near-optimal policy without having to identify all the twists and turns in
the optimal time-value functions Perhaps the most important advantage of the
piecewise linear representation will turn out to be its amenability to bounding and
approximation methods We hope that such advances will enable the solution of
city-sized route planning more realistic telescope scheduling and other practical
time-dependent stochastic problems
Acknowledgments
We thank Leslie Kaelbling Rich Washington and NSF CAREER grant

----------------------------------------------------------------

title: 4510-reducing-statistical-time-series-problems-to-binary-classification.pdf

Reducing statistical time-series problems to binary
classification
J?er?emie Mary
SequeL-INRIA/LIFL-CNRS
Universit?e de Lille France
Jeremie.Mary@inria.fr
Daniil Ryabko
SequeL-INRIA/LIFL-CNRS
Universit?e de Lille France
daniil@ryabko.net
Abstract
We show how binary classification methods developed to work on data can
be used for solving statistical problems that are seemingly unrelated to classification and concern highly-dependent time series Specifically the problems of
time-series clustering homogeneity testing and the three-sample problem are addressed The algorithms that we construct for solving these problems are based
on a new metric between time-series distributions which can be evaluated using
binary classification methods Universal consistency of the proposed algorithms
is proven under most general assumptions The theoretical results are illustrated
with experiments on synthetic and real-world data
Introduction
Binary classification is one of the most well-understood problems of machine learning and statistics
a wealth of efficient classification algorithms has been developed and applied to a wide range of
applications Perhaps one of the reasons for this is that binary classification is conceptually one of
the simplest statistical learning problems It is thus natural to try and use it as a building block for
solving other more complex newer or just different problems in other words one can try to obtain
efficient algorithms for different learning problems by reducing them to binary classification This
approach has been applied to many different problems starting with multi-class classification and
including regression and ranking to give just a few examples However all of these problems
are formulated in terms of independent and identically distributed samples This is also the
assumption underlying the theoretical analysis of most of the classification algorithms
In this work we consider learning problems that concern time-series data for which independence
assumptions do not hold The series can exhibit arbitrary long-range dependence and different timeseries samples may be interdependent as well Moreover the learning problems that we consider
the three-sample problem time-series clustering and homogeneity testing at first glance seem
completely unrelated to classification
We show how the considered problems can be reduced to binary classification methods The results
include asymptotically consistent algorithms as well as finite-sample analysis To establish the consistency of the suggested methods for clustering and the three-sample problem the only assumption
that we make on the data is that the distributions generating the samples are stationary ergodic this
is one of the weakest assumptions used in statistics For homogeneity testing we have to make some
mixing assumptions in order to obtain consistency results this is indeed unavoidable Mixing
conditions are also used to obtain finite-sample performance guarantees for the first two problems
The proposed approach is based on a new distance between time-series distributions that is between probability distributions on the space of infinite sequences which we call telescope distance
This distance can be evaluated using binary classification methods and its finite-sample estimates
are shown to be asymptotically consistent Three main building blocks are used to construct the tele1
scope distance The first one is a distance on finite-dimensional marginal distributions The distance
we use for this is the following dH suph?H EP EQ where are distributions
and is a set of functions This distance can be estimated using binary classification methods
and thus can be used to reduce various statistical problems to the classification problem This distance was previously applied to such statistical problems as homogeneity testing and change-point
estimation However these applications so far have only concerned data whereas we
want to work with highly-dependent time series Thus the second building block are the recent
results of that show that empirical estimates of dH are consistent under certain conditions
on for arbitrary stationary ergodic distributions This however is not enough evaluating dH
for stationary ergodic time-series distributions means measuring the distance between their finitedimensional marginals and not the distributions themselves Finally the third step to construct the
distance is what we call telescoping It consists in summing the distances for all the infinitely many
finite-dimensional marginals with decreasing weights
We show that the resulting distance telescope distance indeed can be consistently estimated based
on sampling for arbitrary stationary ergodic distributions Further we show how this fact can be
used to construct consistent algorithms for the considered problems on time series Thus we can
harness binary classification methods to solve statistical learning problems concerning time series
To illustrate the theoretical results in an experimental setting we chose the problem of time-series
clustering since it is a difficult unsupervised problem which seems most different from the problem of binary classification Experiments on both synthetic and real-world data are provided The
real-world setting concerns brain-computer interface BCI data which is a notoriously challenging
application and on which the presented algorithm demonstrates competitive performance
A related approach to address the problems considered here as well some related problems about
stationary ergodic time series is based on consistent empirical estimates of the distributional distance see and about the distributional distance The empirical distance is based on
counting frequencies of bins of decreasing sizes and telescoping A similar telescoping trick is
used in different problems sequence prediction Another related approach to time-series
analysis involves a different reduction namely that to data compression
Organisation Section is preliminary In Section we introduce and discuss the telescope distance Section explains how this distance can be calculated using binary classification methods
Sections and are devoted to the three-sample problem and clustering respectively In Section
under some mixing conditions we address the problems of homogeneity testing clustering with
unknown and finite-sample performance guarantees Section presents experimental evaluation
Some proofs are deferred to the supplementary material
Notation and definitions
Let FX be a measurable space the domain Time-series process distributions are probability measures on the space FN of one-way infinite sequences where FN is the Borel sigmaalgebra of We use the abbreviation for X1 Xk All sets and functions introduced
below particular the sets Hk and their elements are assumed measurable
A distribution is stationary if A A for all A FX
with FX being
the sigma-algebra of A stationary distribution is called stationary ergodic
if limn IXi..i+k A for every A FX N. This definition
which is more suited for the purposes of this work is equivalent to the usual one expressed in terms
of invariant sets see
A distance between time-series distributions
We start with a distance between distributions on and then we will extend it to distributions on
For two probability distributions and on and a set of measurable functions on
one can define the distance
dH sup EP EQ
h?H
Special cases of this distance are Kolmogorov-Smirnov Kantorovich-Rubinstein and
Fortet-Mourier metrics the general case has been studied since at least
We will be interested in the cases where dH implies Q. Note that in this case dH
is a metric the rest of the properties are easy to see For reasons that will become apparent shortly
Remark below we will be mainly interested in the sets that consist of indicator functions
In this case we can identify each with the set and by a slight abuse
of notation write dH suph?H It is easy to check that in this case dH is
a metric if and only if generates F. The latter property is often easy to verify directly First
of all it trivially holds for the case where is the set of halfspaces in a Euclidean It is also
easy to check that it holds if is the set of halfspaces in the feature space of most commonly used
kernels provided the feature space is of the same or higher dimension than the input space such as
polynomial and Gaussian kernels
Based on dH we can construct a distance between time-series probability distributions For two
time-series distributions we take the dH between k-dimensional marginal distributions of
and for each and sum them all up with decreasing weights
Definition telescope distance D). For two time series distributions and on the space
and a sequence of sets of functions H2 define the telescope distance
DH
wk sup Xk Yk
h?Hk
where wk is a sequence of positive summable real weights wk
Lemma DH is a metric if and only if dHk is a metric for every N.
Proof The statement follows from the fact that two process distributions are the same if and only if
all their finite-dimensional marginals coincide
For a pair of samples and define empirDefinition empirical telescope distance D).
ical telescope distance as
min{m,n
wk sup
h?Hk
All the methods presented in this work are based on the empirical telescope distance The key fact
is that it is an asymptotically consistent estimate of the telescope distance that is the latter can be
consistently estimated based on sampling
Theorem Let H2 Hk be a sequence of separable sets of indicator
functions of finite VC dimension such that Hk generates FX Then for every stationary ergodic
time series distributions and generating samples and we have
DH
lim
is a biased estimate of DH and
The proof is deferred to the supplementary material Note that
unlike in the case the bias may depend on the distributions however the bias is
Remark The condition that the sets Hk are sets of indicator function of finite VC dimension
comes from where it is shown that for any stationary ergodic distribution under these
conditions suph?Hk
is an asymptotically consistent estimate of
Xk This fact implies that dH can be consistently estimated from which the theorem is derived
using binary classification methods
Calculating
The main
The methods for solving various statistical problems that we suggest are all based on
can be calculated using binary classification methods Here we
appeal of this approach is that
explain how to do it
The definition of DH involves calculating summands where min{n that is
sup
h?Hk
for each Assuming that Hk are indicator functions calculating each of the summands
amounts to solving the following k-dimensional binary classification problem Consider
as class-1 examples and as class-0 examples The
supremum is attained on Hk that minimizes the empirical risk with examples wighted with
respect to the sample size Indeed then we can define the weighted empirical risk of any Hk as
which is obviously minimized by any Hk that attains
Thus as long as we have a way to find Hk that minimizes empirical risk we have a consistent
estimate of DH under the mild conditions on required by Theorem Since the dimension of the resulting classification problems grows with the length of the sequences one should
prefer methods that work in high dimensions such as soft-margin SVMs
A particularly remarkable feature is that the choice of Hk is much easier for the problems that we
consider than in the binary classification problem Specifically if for some fixed the classifier
that achieves the minimal Bayes error for the classification problem is not in Hk then obviously
the error of an empirical risk minimizer will not tend to zero no matter how much data we have In
and therefore in the learning
contrast all we need to achieve asymptotically error in estimating
problems considered below is that the sets Hk asymptotically generate FX and have a finite VC
dimension for each This is the case already for the set of hyperplanes in Rk Thus while the
choice of Hk say of the kernel to use in SVM is still important from the practical point of view
it is almost irrelevant for the theoretical consistency results Thus we have the following
and thus the error of the algorithms
Claim The approximation error DH
below can be much smaller than the error of classification algorithms used to calculate DH
Finally we remark that while in the number of summands is it can be replaced with any such
that without affecting any asymptotic consistency results A practically viable choice is
log in fact there is no reason to choose faster growing since the estimates for higher-order
summands will not have enough data to converge This is also the value we use in the experiments
The three-sample problem
We start with a conceptually simple problem known in statistics as the three-sample problem some
times also called time-series classification We are given three samples Xn
Ym and Zl It is known that and were generated by different time-series distributions whereas was generated by the same distribution as either or It
is required to find out which one is the case Both distributions are assumed to be stationary ergodic
but no further assumptions are made about them no independence mixing or memory assumptions The three sample-problem for dependent time series has been addressed in for Markov
processes and in for stationary ergodic time series The latter work uses an approach based on
the distributional distance
Indeed to solve this problem it suffices to have consistent estimates of some distance between time
series distributions Thus we can use the telescope distance The following statement is a simple
corollary of Theorem
Theorem Let the samples Xn Ym and Zl be
generated by stationary ergodic distributions and with and either
or Assume that the sets Hk are separable sets of indicator functions of
finite VC dimension such that Hk generates FX A test that declares if
and otherwise makes only finitely many errors with probability as
It is straightforward to extend this theorem to more than two classes in other words instead of
and one can have an arbitrary number of samples from different stationary ergodic distributions
Clustering time series
We are given samples XnNN generated by different stationary ergodic time-series distributions The number is known but the distributions are not It is required to group the samples into groups clusters that is to output
a partitioning of into sets While there may be many different approaches to define
what is a good clustering and in general deciding what is a good clustering is a difficult problem
for the problem of classifying time-series samples there is a natural choice proposed in those
samples should be put together that were generated by the same distribution Thus define target
clustering as the partitioning in which those and only those samples that were generated by the same
distribution are placed in the same cluster A clustering algorithm is called asymptotically consistent
if with probability there is an n0 such that the algorithm produces the target clustering whenever
maxi=1..N ni n0
Again to solve this problem it is enough to have a metric between time-series distributions that can
be consistently estimated Our approach here is based on the telescope distance and thus we use D.
The clustering problem is relatively simple if the target clustering has what is called the strict separation property every two points in the same target cluster are closer to each other than to any
point from a different target cluster The following statement is an easy corollary of Theorem
Theorem Assume that the sets Hk are separable sets of indicator functions of finite
VC dimension such that Hk generates FX If the distributions generating the samples
XnNN are stationary ergodic then with probability
from some maxi=1..N ni on the target clustering has the strict separation property with respect
H.
to
With the strict separation property at hand it is easy to find asymptotically consistent algorithms
We will give some simple examples but the theorem below can be extended to many other distancebased clustering algorithms
The average linkage algorithm works as follows The distance between clusters is defined as the
average distance between points in these clusters First put each point into a separate cluster Then
merge the two closest clusters repeat the last step until the total number of clusters is The farthest
point clustering works as follows Assign c1 to the first cluster For find the point
ct to the points already assigned
that maximizes the distance mint=1..i
to clusters and assign ci to the cluster Then assign each of the remaining points to the
nearest cluster The following statement is a corollary of Theorem
Theorem Under the conditions of Theorem average linkage and farthest point clusterings are
asymptotically consistent
Note that we do not require the samples to be independent the joint distributions of the samples may
be completely arbitrary as long as the marginal distribution of each sample is stationary ergodic
These results can be extended to the online setting in the spirit of
Speed of convergence
The results established so far are asymptotic out of necessity they are established under the assumption that the distributions involved are stationary ergodic which is too general to allow for
any meaningful finite-time performance guarantees Moreover some statistical problems such as
homogeneity testing or clustering when the number of clusters is unknown are provably impossible
to solve under this assumption
While it is interesting to be able to establish consistency results under such general assumptions it
is also interesting to see what results can be obtained under stronger assumptions Moreover since
it is usually not known in advance whether the data at hand satisfies given assumptions or not it
appears important to have methods that have both asymptotic consistency in the general setting and
finite-time performance guarantees under stronger assumptions
under certain mixing conditions and
In this section we will look at the speed of convergence of
use it to construct solutions for the problems of homogeneity and clustering with an unknown num5
ber of clusters as well as to establish finite-time performance guarantees for the methods presented
in the previous sections
A stationary distribution on the space of one-way infinite sequences FN can be uniquely
extended to a stationary distribution on the space of two-way infinite sequences FZ of the
form X0 X1
Definition mixing coefficients For a process distribution define the mixing coefficients
sup
where denotes the sigma-algebra of the random variables in brackets
When the process is called absolutely regular this condition is much stronger than
ergodicity but is much weaker than the assumption
Speed of convergence of
Assume that a sample is generated by a distribution that is uniformly mixing with coefficients Assume further that Hk is a set of indicator functions with a finite VC dimension dk
for each N.
The general tool that we use to obtain performance guarantees in this section is the following bound
that can be obtained from the results of
qn Hk
sup
h?Hk
tn 8tdnk e?ln
where tn are any integers in and n/tn The parameters tn should be set according to the
values of in order to optimize the bound
One can use similar bounds for classes of finite Pollard dimension or more general bounds
expressed in terms of covering numbers such as those given in Here we consider classes
of finite VC dimension only for the ease of the exposition and for the sake of continuity with the
previous section where it was necessary
Furthermore for the rest of this section we assume
geometric mixing distributions that is
for some Letting tn the bound becomes
qn Hk
n?k
8n(dk
Lemma Let two samples and be generated by stationary distributions and
whose mixing coefficients satisfy for some Let Hk be some sets of
indicator functions on whose VC dimension dk is finite and non-decreasing with Then
DH n0
where n0 min{n1 n2 the probability is with respect to and
log
log
Homogeneity testing
Given two samples and generated by distributions and respectively the problem
of homogeneity testing the two-sample problem consists in deciding whether A test
is called asymptotically consistent if its probability of error goes to zero as n0 min{m goes
to infinity In general for stationary ergodic time series distributions there is no asymptotically
consistent test for homogeneity so stronger assumptions are in order
Homogeneity testing is one of the classical problems of mathematical statistics and one of the most
studied ones Vast literature exits on homogeneity testing for data and for dependent processes
as well We do not attempt to survey this literature here Our contribution to this line of research is
to show that this problem can be reduced via the telescope distance to binary classification in the
case of strongly dependent processes satisfying some mixing conditions
It is easy to see that under the mixing conditions of Lemma a consistent test for homogeneity exists
and finite-sample performance guarantees can be obtained It is enough to find a sequence
such that Then the test can be constructed as follows say that the two se min{n,m
quences and were generated by the same distribution if
otherwise say that they were generated by different distributions The following statement is an immediate consequence of Lemma
Theorem Under the conditions of Lemma the probability of Type I error the distributions are
the same but the test says they are different of the described test is upper-bounded by n0
The probability of Type error the distributions are different but the test says they are the same is
upper-bounded by n0 where
The optimal choice of may depend on the speed at which dk the VC dimension of Hk increases
however for most natural cases recall that
are also parameters of the algorithm this growth is
polynomial so the main term to control is
For example if Hk is the set of halfspaces in Rk then dk and one can chose
The resulting probability of Type I error decreases as
Clustering with a known or unknown number of clusters
If the distributions generating the samples satisfy certain mixing conditions then we can augment
Theorems and with finite-sample performance guarantees
Theorem Let the distributions generating the samples
XnNN satisfy the conditions of Lemma Define
DH and mini=1..N ni Then with probability at least
the target clustering of the samples has the strict separation property In this case single linkage
and farthest point algorithms output the target clustering
Proof Note that a sufficient condition for the strict separation property to hold is that for every one
is within of the DH
out of pairs of samples the estimate
distance between the corresponding distributions It remains to apply Lemma to obtain the first
statement and the second statement is obvious Theorem
As with homogeneity testing while in the general case of stationary ergodic distributions it is impossible to have a consistent clustering algorithm when the number of clusters is unknown the
situation changes if the distributions satisfy certain mixing conditions In this case a consistent clustering algorithm can be obtained as follows Assign to the same cluster all samples that are at most
far from each other where the threshold is selected the same way as for homogeneity testing
and The optimal choice of this parameter depends on the choice of Hk
through the speed of growth of the VC dimension dk of these sets
Theorem Given samples generated by different stationary distributions unknown all satisfying the conditions of Lemma the probability of error misclustering at least
one sample of the described algorithm is upper-bounded by
2N
where DH and mini=1..N ni with ni being lengths of
the samples
Experiments
For experimental evaluation we chose the problem of time-series clustering Average-linkage clustering is used with the telescope distance between samples calculated using an SVM as described
in Section In all experiments SVM is used with radial basis kernel with default parameters of
libsvm
Synthetic data
For the artificial setting we have chosen highly-dependent time series distributions which have the
same single-dimensional marginals and which cannot be well approximated by finite or countablestate models The distributions are constructed as follows Select r0
uniformly at random then for each obtain ri by shifting by to the right and
removing the integer part The time series X2 is then obtained from ri by drawing a point
from a distribution law N1 if ri and from N2 otherwise N1 is a 3-dimensional Gaussian with
mean of and covariance matrix Id N2 is the same but with mean If is irrational1 then the
distribution is stationary ergodic but does not belong to any simpler natural distribution family
The single-dimensional marginal is the same for all values of The latter two properties
make all parametric and most non-parametric methods inapplicable to this problem
In our experiments we use two process distributions with
The dependence of error rate on the length of time series is shown on Figure One
clustering experiment on sequences of length takes about min on a standard laptop
Real data
To demonstrate the applicability of the proposed methods to realistic scenarios we chose the braincomputer interface data from BCI competition III The dataset consists of pre-processed
BCI recordings of mental imagery a person is thinking about one of three subjects left foot right
foot a random letter Originally each time series consisted of several consecutive sequences of
different classes and the problem was supervised three time series for training and one for testing
We split each of the original time series into classes and then used our clustering algorithm in a
completely unsupervised setting The original problem is 96-dimensional but we used only the first
dimensions using all 96 gives worse performance The typical sequence length is The
performance is reported in Table labeled TSSVM All the computation for this experiment takes
approximately minutes on a standard laptop
TSSVM
DTW
KCpA
SVM
Error rate
The following methods were used for comparison First we used dynamic time wrapping DTW
which is a popular base-line approach for time-series clustering The other two methods in
Table are from The comparison is not fully relevant since the results in are for different
settings the method KCpA was used in change-point estimation method different but also unsupervised setting and SVM was used in a supervised setting The latter is of particular interest
since the classification method we used in the telescope distance is also SVM but our setting is
unsupervised clustering
s1
s2
s3
Time of observation
Table Clustering accuracy in the BCI
dataset subjects columns methods
Figure Error of two-class clustering using
rows Our method is TSSVM
TSSVM time series in each target cluster averaged over runs
Acknowledgments This research was funded by the Ministry of Higher Education and Research Nord-Pasde-Calais Regional Council and FEDER Contrat de Projets Etat Region CPER ANR projects
EXPLO-RA Lampada and CoAdapt and by the European Community?s FP7 Program under grant agreements PASCAL2 and CompLACS
in the experiments simulated by a longdouble with a long mantissa

----------------------------------------------------------------

title: 2012-optimising-synchronisation-times-for-mobile-devices.pdf

Optimising Synchronisation Times for
Mobile Devices
Neil D. Lawrence
Department of Computer Science
Regent Court Portobello Road
Sheffield Sl U.K.
neil~dcs.shef ac.uk
Antony T. Rowstron Christopher Bishop Michael J. Taylor
Microsoft Research
J. J. Thomson Avenue
Cambridge CB3 OFB U.K.
antr,cmbishop,mitaylor}~microsoft.com
Abstract
With the increasing number of users of mobile computing devices
personal digital assistants and the advent of third generation
mobile phones wireless communications are becoming increasingly
important Many applications rely on the device maintaining a
replica of a data-structure which is stored on a server for example news databases calendars and e-mail ill this paper we explore
the question of the optimal strategy for synchronising such replicas
We utilise probabilistic models to represent how the data-structures
evolve and to model user behaviour We then formulate objective
functions which can be minimised with respect to the synchronisation timings We demonstrate using two real world data-sets that
a user can obtain more up-to-date information using our approach
Introduction
As the available bandwidth for wireless devices increases new challenges are presented in the utilisation of such bandwidth Given that always up connections are
generally considered infeasible an important area of research in mobile devices is
the development of intelligent strategies for communicating between mobile devices
and servers ill this paper we consider the scenario where we are interested in maintaining on a personal digital assistant PDA with wireless access an up-to-date
replica of some perhaps disparate data-structures which are evolving in time The
objective is to make sure our replica is not stale We will consider a limited number
of connections or synchronisations Each synchronisation involves a reconciliation
between the replica on the mobile device and the data-structures of interest on the
server Later in the paper we shall examine two typical examples of such an application,an internet news database and a user's e-mail messages Currently the typical
strategy for performing such reconciliations is to synchronise every minutes
lSee for example AvantGo http://vvv avantgo com
where is a constant we will call this strategy the uniformly-spaced strategy We
will make the timings of the synchronisations adaptable by developing a cost function that can be optimised with respect to the timings thereby improving system
performance
Cost Function
We wish to minimise the staleness of the replica where we define staleness as the
time between an update of a portion of the data-structure on the server and the time
of the synchronisation of that update with the PDA. For simplicity we shall assume
that each time the PDA synchronises all the outstanding updates are transferred
Thus after synchronisation the replica on the mobile device is consistent with the
master copy on the server Therefore if skis the time of the kth synchronisation
in a day and updates to the data-structure occur at times Uj then the average
staleness of the updates transferred during synchronisation Sk will be
As well as staleness we may be interested in optimising other criteria For example
mobile phone companies may seek to equalise demand across the network by introducing time varying costs for the synchronisations Additionally one could
argue that there is little point in keeping the replica fresh during periods when the
user is unlikely to check his PDA for example when he or she is sleeping We might
therefore want to minimise the time between the user's examination of the PDA
and the last synchronisation If the user looks at the PDA at times then we can
express this as
Given the timings Uj and the call cost schedule and synchronisations the
total cost function may now be written
aFk fJSk
k=l
where a and fJ are constants with units of which express the relative importance of the separate parts of the cost function Unfortunately of course whilst
we are likely to have knowledge of the call cost schedule we won't know the
true timings and and the cost function will be a priori incomputable If
though we have historic data2 relating to these times we can seek to make progress
by modelling these timings probabilistically Then rather than minimising the actual cost function we can look to minimise the expectation of the cost function
under these probabilistic models
Expected Cost
There are several different possibilities for our modelling strategy A sensible assumption is that there is independence between different parts of the data-structure
e-mail and business news can be modelled separately however there may
be dependencies between update times which occur within the same part The
2When modelling user ru:cess times if historic data is not available models could also
be constructed by querying the user about their likely ru:tivities
periodicity of the data may be something we can take advantage of but any nonstationarity in the data may cause problems There are various model classes we
could consider for this work however we restrict ourselves to stationary models
and ones in which updates arrive independently and in a periodic fashion
Let us take to be the largest period of oscillation in the data arrivals for a
particular portion of a data-structure We model this portion with a probability
distribution Naturally more than one update may occur in that interval
therefore our probability distribution really specifies a distribution over time given
one that one update user access has occurred To fully specify the model we
also are required to store the expected number of updates Ju accesses a that
occur in that interval
The expected value of Sk may now be written
where is an expectation under the distribution Au(t JuPu(t can be
viewed as the rate at which updates are occurring and So SK T.
We can model the user access times in a similar manner which leads us to the
expected value of the freshness Fk)Pa(t kk Aa(t)(t sk)dt where Aa(t
JaPa(t The overall expected cost which we will utilise as our objective function
may therefore be written
Fk)Pa
k=l
Probabilistic Models
We now have an objective function which is a function of the variables we wish to
optimise the synchronisation times but whilst we have mentioned some characteristics of the models Pu(t and Pa(t we have not yet fully specified their form
We have decreed that the models should be periodic and that they may consider
each datum to occur independently In effect we are modelling data which is mapped
to a circle Various options are available for handling such models for this work
we constrain our investigations to kernel density estimates
In order to maintain periodicity we must select a basis function for our KDE which
represents a distribution on a circle one simple way of achieving this aim is to wrap
a distribution that is defined along a line to the circle Mardia A traditional
density which represents a distribution on the line may be wrapped around
a circle of circumference to give us a distribution defined on the circle
where mod T. This means a basis function with its centre at that will
typically have probability mass when wraps around to maintain a continuous
density at T. The wrapped Gaussian distribution that we make use of takes the
form
The final kernel density estimate thus consists of mapping the data points tn
On
3In practice we must approximate the wrapped distribution by restricting the number
of terms in the sum
OJ
Co
OJ
Thu Fri Sat Sun
Thu Fri Sat Sun
Figure Left part of the KDE developed for the business category together with a
piecewise constant approximation Middle the same portion of the KDE for the FA Carling Premiership data Right percent decrease in staleness vs number of synchronisations
per day for e-mail data
and obtaining a distribution
n=l
where is the number of data-points and the width parameters can be set
through cross validation Models of this type may be made use of for both Pu(t
and Pa(t
Incorporating Prior Knowledge
The underlying component frequencies of the data will clearly be more complex than
simply a weekly or daily basis Ideally we should be looking to incorporate as much
of our prior knowledge about these component frequencies as possible IT we were
modelling financial market's news for example we would expect weekdays to have
similar characteristics to each other but differing characteristics from the weekend
For this work we considered four different scenarios of this type For the first
scenario we took day and placed no other constraints on the model For the
second we considered the longest period to be one week week and placed no
further constraints on the model For the remaining two though we also considered
to be one week but we implemented further assumptions about the nature of the
data Firstly we split the data into weekdays and weekends We then modelled these
two categories separately making sure that we maintained a continuous function
for the whole week by wrapping basis functions between weekdays and weekends
Secondly we split the datainto weekdays Saturdays and Sundays modelling each
category separately and again wrapping basis functions across the days
Model Selection
To select the basis function widths and to determine which periodicity assumption
best matched the data we utilised ten fold cross validation For each different
periodicity we used cross validation to first select the basis function width We
then compared the average likelihood across the ten validation sets selecting the
periodicity with the highest associated value
Optimising the Synchronisation Times
Given that our user model and our data model Pu(t will be a KDE based
on wrapped Gaussians we should be in a position to compute the required integrals
C/J
il
C/J
C/J
C/J
C/J
I
C/J
Figure Results from the news database tests Left
February March based models tested on April Middle
March April testing on May. Right April/May testing on
June The results are in the form of box plots The lower
line of the box represents the percentile of the data the
upper line the percentile and the central line the median The whiskers represent the maximum extent of the
data up to percentile percentile Data
which lies outside the whiskers is marked with crosses
C/J
24
xx
xx
xXx
in and evaluate our objective function and derivatives thereof
First though we must give some attention to the target application for the algorithm A known disadvantage of the standard kernel density estimate is the high
storage requirements of the end model The model requires that floating point
numbers must be stored where is the quantity of training data Secondly integrating across the cost function results in an objective function which is dependent
on a large number of evaluations of the cumulative Gaussian distribution Given
that we envisage that such optimisations could be occurring within a PDA or mobile
phone it would seem prudent to seek a simpler approach to the required minimisation
An alternative approach that we explored is to approximate the given distributions
with a functional form which is more amenable to the integration For example a
piecewise constant approximation to the KDE simplifies the integral considerably
It leads to a piecewise constant approximation for Aa(t and Integration
over which simply leads to a piecewise linear function which may be computed in a
straightforward manner Gradients may also be computed We chose to reduce the
optimisation to a series of one-dimensional line minimisations This can be achieved
in the following manner First note that the objective function as a function of a
particular synchronisation time Sk may be written
In other words each synchronisation is only dependent on that of its neighbours We
may therefore perform the optimisation by visiting each synchronisation time Sk in
a random order and optimising its position between its neighbours which involves a
one dimensional line minimisation of This process which is guaranteed to find
a local minimum in our objective function may be repeated until convergence
Results
In this section we mainly explore the effectiveness of modelling the data-structures
of interest We will briefly touch upon the utility of modelling the cost evolution
and user accesses in Section but we leave a more detailed exploration of this
area to later works
Modelling Data Structures
To determine the effectiveness of our approach we utilised two different sources of
data a news web-site and e-mail on a mail server
The news database data-set was collected from the BBC
News web site4 This site maintains a database of articles which are categorised according to subject for example UK News Business News Motorsport etc We
had six months of data from February to July for
24 categories of the database
We modelled the data by decomposing it into the different categories and modelling each separately This
allowed us to explore the periodicity of each category independently This is a sensible approach given that the
nature of the data varies considerably across the categories Two extreme examples are Business news and
FA Carling Premiership news Figure Business news
predominantly arrives during the week whereas FA Carling Premiership news arrives typically just after soccer
games finish on a Saturday Business news was best
modelled on a Weekday/Weekend basis and FA Carling Premiership news was best modelled on a Weekday Saturday Sunday basis To evaluate the feasibility
of our approach we selected three consecutive months
of data The inference step consisted of constructing our
models on data from the first two months To restrict
our investigations to the nature of the data evolution
only user access frequency was taken to be uniform and
cost of connection was considered to be constant For
the decision step we considered to 24 synchronisations
a day The synchronisation times were optimised for
each category separately they were initialised with a
uniformly-spaced strategy optimisation of the timings
then proceeded as described in Section The staleness associated with these timings was then computed
for the third month This value was compared with the
staleness resulting from the uniformly-spaced strategy
containing the same number of synchronisations The
percentage decrease in staleness is shown in figures
and in the form of box-plots
Xx
XX
xx
Figure
May/June
based models tested on
July
signifies the
FA Carling Premiership
Stream
Generally an improvement in performance is observed
however we note that in Figure the performance for several categories is extremely
4http://news.bbc.co.uk
5The FA Carling Premiership is England's premier division soccer
6The uniformly-spaced strategy's staleness varies with the timing of the first of the
synchronisations This figure was therefore an average of the staleness from all possible
starting points taken at five minute intervals
poor In particular the FA Carling Premiership stream in Figure The poor
performance is caused by the soccer season ending in May. As a result relatively
few articles are written in July most of them concerning player transfer speculation
and the timing of those articles is very different from those in May. In other words
the data evolves in a non-stationary manner which we have not modelled The other
poor performers are also sports related categories exhibiting non-stationarities
The e-mail data-set was collected by examining the logs of e-mail arrival times for
researchers from Microsoft's Cambridge research lab This data was collected for
January and February We utilised the January data to build the probabilistic models and the February data to evaluate the average reduction in staleness
Figure shows the results obtained
In practice a user is more likely to be interested in a combination of different categories of data Perhaps several different streams of news and his e-mail Therefore
to recreate a more realistic situation where a user has a combination of interests
we also collected e-mail arrivals for three users from February March and April
We randomly generated user profiles by sampling without replacement five
categories from the available twenty-seven rejecting samples where more than one
e-mail stream was selected We then modelled the users interests by constructing
an unweighted mixture of the five categories and proceeded to optimise the synchronisation times based on this model This was performed one hundred times The
average staleness for the different numbers of synchronisations per day is shown in
Figure
Note that the performance for the combined categories is worse than it is for each
individually This is to be expected as the entropy of the combined model will always
be greater than that of its constituents we therefore have less information about
arrival times and as a result there are less gains to be made over the uniformlyspaced strategy7
Affect of Cost and User Model
In the previous sections we focussed on modelling the evolution of the databases
Here we now briefly turn our attention to the other portions of the system user
behaviour and connection cost For this preliminary study it proved difficult to
obtain high quality data representing user access times We therefore artificially
generated a model which represents a user who accesses there device frequently
at breakfast lunchtime and during the evening and rarely at night Figure
simply shows the user model along with the result of optimising the cost
function for uniform data arrivals and fixed cost under this user model Note how
synchronisation times are suggested just before high periods of user activity are
about to occur Also in Figure is the effect of a varying cost under uniform
Pa(t and
Currently most mobile internet access providers appear to be charging a flat fee for
call costs typically in the U.K. about cents per minute However when demand
on their systems rise they may wish to incorporate a varying cost to flatten peak
demands This cost could be an actual cost for the user or alternatively a shadow
price specified by service provider for controlling demand Kelly We give a
simple example of such a call cost in Figure For this we considered user access
and data update rates to be constant Note how the times move away from periods
of high cost
7The uniformly-spaced strategy can be shown to be optimal when the entropy of the
underlying distribution is maximised uniform distribution across the interval
gj
Cl
7Lo
Cl
Cl
Figure Left change in synchronisation times for variable user access rates shows
the initialisation points the end points Middle change in synchronisation times for a
variable cost Right performance improvements for the combination of news and e-mail
Discussion
The optimisation strategy we suggest could be sensitive to local minima we did
not try a range of different initialisations to explore this phenomena However by
initialising with the uniformly-spaced strategy we ensured that we increased the
objective function relative to the standard strategy The month of July showed how
a non-stationarity in the data structure can dramatically affect our performance
We are currently exploring on-line Bayesian models which we hope will track such
non-stationarities
The system we have explored in this work assumed that the data replicated on the
mobile device was only modified on the server A more general problem is that of
mutable replicas where the data may be modified on the server or the client Typical
applications of such technology include mobile databases where sales personnel
modify portions of the database whilst on the road and a calendar application on
a PDA where the user adds appointments on the PDA.
Finally there are many other applications of this type of technology beyond mobile
devices Web crawlers need to estimate when pages are modified to maintain a
representative cache eho and Garcia-Molina Proxy servers could also
be made to intelligent maintain their caches of web-pages up-to-date Willis and
Mikhailov Wolman

----------------------------------------------------------------

title: 1461-refractoriness-and-neural-precision.pdf

Refractoriness and Neural Precision
Michael J. Berry and Markus Meister
Molecular and Cellular Biology Department
Harvard University
Cambridge MA
Abstract
The relationship between a neuron's refractory period and the precision of
its response to identical stimuli was investigated We constructed a model of
a spiking neuron that combines probabilistic firing with a refractory period
For realistic refractoriness the model closely reproduced both the average
firing rate and the response precision of a retinal ganglion cell The model is
based on a free firing rate which exists in the absence of refractoriness
This function may be a better description of a spiking neuron's response
than the peri-stimulus time histogram
INTRODUCTION
The response of neurons to repeated stimuli is intrinsically noisy In order to take this
trial-to-trial variability into account the response of a spiking neuron is often described
by an instantaneous probability for generating an action potential The response
variability of such a model is determined by Poisson counting statistics in particular the
variance in the spike count is equal to the mean spike count for any time bin Rieke
However recent experiments have found far greater precision in the vertebrate
retina Berry and the HI interneuron in the fly visual system de Ruyter In
both cases the neurons exhibited sharp transitions between silence and nearly maximal
firing When a neuron is firing near its maximum rate refractoriness causes spikes to
become more regularly spaced than for a Poisson process with the same firing rate Thus
we asked the question does the refractory period play an important role in a neuron's
response precision under these stimulus conditions
FIRING EVENTS IN RETINAL GANGLION CELLS
We addressed the role of refractoriness in the precision of light responses for retinal
ganglion cells
RECORDING AND STIMULATION
Experiments were performed on the larval tiger salamander The retina was isolated from
the eye and superfused with oxygenated Ringer's solution Action potentials from retinal
Refractoriness and Neural Precision
ganglion cells were recorded extracellularly with a multi-electrode array and their spike
times measured relative to the beginning of each stimulus repeat Meister
Spatially uniform white light was projected from a computer monitor onto the
photoreceptor layer The intensity was flickered by choosing a new value at random from
a Gaussian distribution mean standard deviation oJ every ms The mean light level
corresponded to photopic daylight vision Contrast is defined here
as the temporal standard deviation of the light intensity divided by the mean I.
Recordings extended over repeats of a segment of random flicker
The qualitative features of ganglion cell responses to random flicker stimulation at 35
contrast are seen in First spike trains had extensive periods in which no spikes
were seen in repeated trials Many spike trains were sparse in that the silent periods
covered a large fraction of the total stimulus time Second during periods of firing the
peri-stimulus time histogram PSTH rose from zero to the maximum firing rate
Hz on a time scale comparable to the time interval between spikes We
have argued that these responses are better viewed as a set of discrete firing events than
as a continuously varying firing rate Berry In general the firing events were
bursts containing more than one spike IB). Identifiable firing events were seen
across cell types similar results were also found in the rabbit retina Berry
A
If
I
I
I
I'r
I
I
ni
a
Time
Figure Response of a salamander ganglion cell to random flicker stimulation
Stimulus intensity in units of the mean for a segment spike rasters
from trials and the firing rate
FIRING EVENT PRECISION
Discrete episodes of ganglion cell firing were recognized from the PSTH as a contiguous
period of firing bounded by periods of complete silence To provide a consistent
demarcation of firing events we drew the boundaries of a firing event at minima in the
PSTH that were significantly lower than neighboring maxima PI and such that
PIP2 with 95 confidence Berry With these boundaries defined every
spike in each trial was assigned to exactly one firing event
Iv
M. Berry and M. Meister
Measurements of both timing and number precision can be obtained if the spike train is
parsed into such firing events For each firing event we accumulated the distribution of
spike times across trials and calculated several statistics the average time Tj of the first
spike in the event and its standard deviation OTj across trials which quantified the
temporal jitter of the first spike similarly the average number of spikes in the event
and its variance ONj across trials which quantified the precision of spike number In
trials that contained zero spikes for event no contribution was made to Tj or OTj while
a value of zero was included in the calculation of Nj and ONj
For the ganglion cell shown in the temporal jitter oT of the first spike in an event
was very small to Thus repeated trials of the same stimulus typically elicit
action potentials with a timing uncertainty of a few milliseconds The temporal jitter of
all firing events was distilled into a single number Tby taking the median o"er all events
The variance ON in the spike count was remarkably low as well it often approached the
lower bound imposed by the fact that individual trials necessarily produce integer spike
counts Because ON for all events ganglion cell spike trains cannot be completely
characterized by their firing rate Berry The spike number precision of a cell was
average variance over events and dividing by the average
assessed by comp.utin,fo
spike count This quantity also known as the Fano factor has a value
of one for a Poisson process with no refractoriness
tHe
PROBABILISTIC MODELS OF A SPIKE TRAIN
We start by reviewing one of the simplest probabilistic models of a spike train the
inhomogeneous Poisson model Here the measured spike times are used to estimate
the instantaneous rate of spike generation during a time Lit This can be written
fonnallyas
where is the number of repeated stimulus trials and is the Heaviside function
ex
x<O
We can randomly generate a sequence of spike trains from a set of random numbers
between zero and one aj with a If there is a spike at time tj then the next
spike time is found by numerically solving the equation
Jr(t)dt
INCLUDING AN ABSOLUTE REFRACTORY PERIOD
In order to add refractoriness to the Poisson spike-generator we expressed the firing rate
as the product of a free firing rate which obtains when the neuron is not
refractory and a recovery function which describes how the neuron recovers from
refractoriness Johnson Miller When the recovery function is zero spiking
is not possible and when it is one spiking is not affected The modified rule for
selecting spikes then becomes
lna
Jq(t)w(t-t;)dt
For an absolute refractory period of time the weight function is zero for times between
and J1 and one otherwise
Refractoriness and Neural Precision
B{t
Because the refractory period may exclude spiking in a given time bin the probability of
firing a spike when not prevented by the refractory period is higher than predicted by
This free firing rate can be estimated by excluding trials where the neuron is
unable to fire due to refractoriness
The sum is restricted to spike times ti nearest to the time bin on a given trial This
restriction follows from the assumption that the recovery function only depends on the
time since the last action potential Notice that this new probability obeys the inequality
and also that it depends upon the refractory period
I
a
OJ
ctS
ctS
Refractory Period
Figure Results for model spike trains with an absolute refractory period
Mean firing rate averaged over a segment circles Fano factor
a measure of spike number precision in an event triangles and
temporal jitter t'(diamonds plotted versus the absolute refractory period
Shown in dotted in each panel is the value for the real data
With this definition of the free firing rate we can now generate spike trains with the same
first order statistics the average firing rate for a range of values of the refractory
period For each value of we can then compare the second order statistics the
precision of the model spike trains to the real data To this end the free rate was
M. Berry and M. Meister
calculated for a segment of the response to random flicker of the salamander
ganglion cell shown in Then was used to generate spike trains Firing
events were identified in the set of model spike trains and their precision was calculated
Finally this procedure was repeated times for each value ofthe refractory period
Figure 2A plots the firing rate circles generated by the model averaged over the entire
segment of random flicker with error bars equal to the standard deviation of the rate
among the repeated sets The firing rate of the model matches the actual firing rate for
the real ganglion cell dashed up to refractory periods of J1 ms although the
deviation for larger refractory periods is still quite small For large enough values of the
absolute refractory period there will be inter-spike intervals in the real data that are
shorter than J1. In this case the free firing rate cannot be enhanced enough to match
the observed firing rate
While the mean firing rate is approximately constant for refractory periods up to ms the
precision changes dramatically Figure 2B shows that the Fano factor triangles has the
expected value of for no refractory period but drops to for the largest refractory
period In the temporal jitter diamonds also decreases as refractoriness is
added although the effect is not as large as for the precision of spike number The
sharpening of temporal precision is due to the fact that the probability rises more
steeply than so that the first spike occurs over a narrower range oftimes
The number precision of the model matches the real data for J1 to ms and the
timing precision matches for ms Therefore a probabilistic spike generator with an
absolute refractory period can match both the average firing rate and the precision of a
retinal ganglion cell's spike train with roughly the same value of one free parameter
USING A RELATIVE REFRACTORY PERIOD
Salamander ganglion cells typically have a relative refractory period that lasts beyond
their absolute refractory period This can be seen in 3A from the distribution of interspike intervals for the ganglion cell shown above the absolute refractory period
lasts for only ms while relative refractoriness extends to ms We can include the
effects of relative refractoriness by using weight values in that are between zero and
one Figure illustrates a parameter-free method for determining this weight function If
there were no refractoriness and a neuron had a constant firing rate then the inter-spike
interval distribution would drop exponentially This behavior is seen from the curve fit in
3A for intervals in the range to ms The recovery function can then be
found from the inter-spike interval distribution Berry
Notice in that the recovery function is zero out to ms rises almost
linearly between and ms and then reaches unity beyond ms
Using the weight function shown in the free firing rate was calculated and
sets of spike trains were generated The results summarized in Table give very
close agreement with the real data
Table Results for a Relative Refractory Period
QUANTITY
REAL DATA
MODEL
STD. DEV.
Firing Rate
Timing Precision
Number Precision
Hz
Hz
ms
Hz
Refractoriness and Neural Precision
Thus a Poisson spike generator with a relative refractory period reproduces the measured
precision A similar test performed over a population of ganglion cells also yielded close
agreement Berry
A
CT
OOO
Inter-Spike Interval
U.
Time
Figure Determination of the relative refractory period The interspike interval distribution diamonds is fit by an exponential curve solid
resulting in the recovery function
Not only is the average firing rate well-matched by the model but the firing rate in each
time bin is also very similar Figure 4A compares the firing rate for the real neuron to that
generated by the model The mean-squared error between the two is while the
counting noise estimated as the variance of the standard error divided by the variance of
is also Thus the agreement is limited by the finite number of repeated trials
Figure 4B compares the free firing rate to the observed rate firing is equal
to at the beginning of a firing event but becomes much larger after several spikes
have occurred In addition is generally smoother than because there is a
greater enhancement in at times following a peak in
In summary the free firing rate can be calculated from the raw spike train with no
more computational difficulty than and thus can be used for any spiking neuron
Furthermore has some advantages over in conjunction with a refractory
spike-generator it produces the correct response precision it does not saturate at high
firing rates so that it can continue to distinguish gradations in the neuron's response
Thus may prove useful for constructing models of the input-output relationship of a
spiking neuron Berry
Acknowledgments
We would like to thank Mike DeWeese for many useful conversations One of us MJB
acknowledges the support of the National Eye Institute The other MM acknowledges
the support of the National Science Foundation
Figure Illustration of the free fIring rate The observed fIring rate
for real data solid is compared to that from the model dotted The
free rate thick is shown on the same scale as thin All rates
used a time bin of ms and boxcar smoothing over bins

----------------------------------------------------------------

title: 602-unsmearing-visual-motion-development-of-long-range-horizontal-intrinsic-connections.pdf

Unsmearing Vistlal Motion
Development of Long-Range
Horizolltal Intrinsic Conllections
Kevin E. Martin
Jonathan A. Marshall
Department of Computer Science CB Sitterson Hall
University of North Carolina Chapel Hill NC
Abstract
Human VlSlon systems integrate information nonlocally across long
spatial ranges
For example a moving stimulus appears smeared
when viewed briefly yet sharp when viewed for a longer
exposure ms Burr This suggests that visual systems
combine information along a trajectory that matches the motion of
the stimulus Our self-organizing neural network model shows how
developmental exposure to moving stimuli can direct the formation of
horizontal trajectory-specific motion integration pathways that unsmear
representations of moving stimuli These results account for Burr's data
and can potentially also model ot.her phenomena such as visual inertia
INTRODUCTION
Nonlocal interactions strongly influence the processing of visual motion information
and the response characteristics of visual neurons Examples include attentional
modulation of receptive field shape modulation of neural response by stimuli beyond
the classical receptive field and neural response to large-field background motion
In this paper we present a model of the development of nonlocal neural mechanisms
for visual motion processing Our model Marshall is based on the
long-range excitatory horizontal intrinsic connections LEHICs that have been
identified in the visual cortex of a variety of animal species Blasdel Lund
Fitzpatrick Callaway Katz Gabbott Martin Whitteridge
Gilbert Wiesel Luhmann Martinez Millan Singer Lund
Michalski Gerstein Czarkowska Tarnecki Mitchison Crick
Nelson Frost Rockland Lund Rockland Lund
Humphrey Ts'o Gilbert vViesel
VISUAL UNSMEARING
Human visual systems summate signals over a period of approximately ms
in daylight Burr Ross Hogben This slow summation reinforces
Martin and Marshall
stationary stimuli but would tend to smear any moving object Nevertheless human
observers report perceiving both stationary and moving stimuli as sharp Anderson
Van Essen Gallant Burr Burr Ross Morrone Morgan
Benton Welch McKee Why do moving objects not appear smeared
Burr measured perceived smear of moving spots as a function of exposure
time He found that a moving visual spot appears smeared with a comet-like tail
when it is viewed for a brief exposure ms yet perfectly sharp when viewed
for a longer exposure ms Figure The ability to counteract smear at
longer exposures suggests that human visual systems combine integrate and
sharpen motion information from multiple locations along a specific spatiotemporal
trajectory that matches the motion of the stimulus Barlow Burr
Burr Ross in the domains of direction velocity position and time
This unsmearing phenomenon also suggests the existence of a memory-like effect
or persistence which would cause the behavior of processing mechanisms to differ
in the early smeared stages of a spot's motion and in the later unsmeared stages
NETWORK ARCHITECTURE
We built a biologically-modeled self-organizing neural network SONN containing
long-range excitatory horizontal intrinsic connections LEHICs that learns to
integrate visual motion information nonlocally The network laterally propagates
predictive moving stimulus information in a trajectory-specific manner to successive
image locations where a stimulus is likely to appear The network uses this
propagated information sharpen its representation of visual motion
LONG-RANGE EXCITATORY HORIZONTAL INTRINSIC
CONNECTIONS
The network LEHICs modeled several characteristics consistent with neurophysiological data
They are highly specific and anisotropic Callaway Katz
Motion
Oms
en
ms
Figure Motion unsmearing A spot presented for rns appears to have a comet-like
tail but a spot presented for lOS appears sharp and unsmeared Burr
Un smearing Visual Motion Development of Long-Range Horizontal Intrinsic Connections
They typically run between neurons with similar stimulus preferences
Callaway Katz
They can run for very long distances across the network space mm
horizontally across cortex Luhmann Martinez Millan Singer
They can be shaped adaptively through visual experience Callaway
Katz Luhmann Martinez Millan Singer
They may serve to predictively prime motion-sensitive neurons Gabbott
Martin Whitteridge
Some characteristics of our modeled LEHICs are also consistent with those of
the horizontal connections described by Hirsch Gilbert For instance
we predicted Marshall that horizontal excitatory input alone should not
cause suprathreshold activation but horizontal excitatory input should amplify
activation when local bottom-up excitation is present Hirsch Gilbert
directly observed these characteristics in area 17 pyramidal neurons in the cat
Since LEHICs are found in early vision processing areas like VI we hypothesize
that similar connections are likely to be found within higher cortical areas as
well like areas MT and STS. Our simulated networks may correspond to structures
in such higher areas Although our long-range lateral signals are modeled as
being excitatory Orban Gulyas Vogels they are also functionally
homologous to long-range trajectory-specific lateral inhibition of neurons tuned to
null-direction motion Ganz Felder Marlin Dougla Cynader
Mott.er Steinmetz Duffy Mountcastle
LEHICs constitute one possible means by which nonlocal communication can take
place in visual cortex Other means such as large bottom-up receptive fields can
also cause information to be transmitted nonlocally However the main difference
between LEHICs and bottom-up receptive fields is that LEHICs provide lateral
feedback information about the outcome of other processing within a given stage
This generates a form of memory or persistence Purely bottom-up net.works
without LEHICs or other feedba.ck would perform processing afresh at each
step so that the outcome of processing would be influenced only by the direct
feedforward inputs at each step
RESULTS OF NETWORK DEVELOPMENT
In our model developmental exposure to moving stimuli guides the formation
of motion-integration pathways that unsmear representations of moving stimuli
Our model network is repeat.edly exposed to training input sequences of smeared
motion patterns through bottom-up excitatory connections Smear is modeled as an
exponential decay and represents the responses of t.emporally integrating neurons
to moving visual stimuli The network contains a set of initially nonspecific LEHICs
with fixed signal transmission latencies The moving stimuli cause the pattern of
weights across the LEHICs to become refined eventually fonning chains that
correspond to trajectories in the visual environment
To model unsmearing fully we would need a retinotopically organized layer
of neurons tuned to different directions of motion and different velocities Each
trajectory in visual space would be represented by a set of like velocity and direction
sensitive neurons whose receptive fields are located along the trajectory These
neurons would be connected through a trajectory-specific chain of time-delayed
LEHICs Lat.eral inhibition between chains would be organized selectively to allow
representations of multiple stimuli to be simultaneously active Marshall
thereby letting most trajectory representations operate independently
Our simulation consists of a I-D subnet.work of the full network with 32
neurons sensitive to a single velocity and direction of motion Figure The
Martin and Marshall
lateral inhibitory connections are fixed in a Gaussian distribution but the LEHIC
weights can change according to a modified Hebbiall rule Grossberg
dt Zii
f(Xi)(-zii
where
represents the weight of the LEHIC from the jth neuron to the
ith neuron represents the value of the activation level of the ith neuron is
a slow learning ra.te h(xj max(O is a faster-than-linear signal function and
f(xd max(O is a faster-than-linear sampling function To model multiplestep trajectories we used LEHICs with three different signal transmission delays
Initially the LEHICs were all represented but their weights were zero
As stimuli move across the receptive fields of the neurons in the network many
neurons are coactive because the network is unable to resolve the smear By the
learning rule the weights of the LEHICs between these coactive neurons increase
This leads to a profusion of connection weights Figure analogous to the crude
clusters proposed by Callaway and Katz to describe the early postnatal
days structure of horizontal connections in cat area VI.
After sufficient exposure to moving stimuli the crude clusters in our simulation
become sharper Figure because of the faster-than-linear signal functions This
refinement of the pattern of connection weights into chains might correspond to the
later postnatal day development of refined clusters described by Callaway
and Katz
RESULTS OF NETWORK OPERATION
Before learning begins the network is incapable of unsmearing a stimulus moving
across the receptive fields of the neurons Figure As the stimulus moves from
one position to the next the pattern of neuron activations is no less smeared than
Lateral
Inhibitory
Connections
Excitatory Input
cI
LEHICs
Figure Three phases of modeled development Initial Lateral excitatory
connections were modifiable and had zero weight Lateral inhibition was fixed in a
Gaussian distribut.ion thickness of dotted arrows The neurons received sequences
of smeared rightward-moving excitatory input patterns Profusion During early
development lateral excitatory connections went through a phase of weight profusion The
output LERrC weights thickness of arrows from one neuron filled circle are shown
weights were biased toward rightward motion Refinement During later development
the pattern of weights settled into sets of regular anisotropic chains most of the early
profuse connections were eliminated No external parameters were manipulated during the
simulation to induce the initial-profusion-refinement phase transitions The simulation
contained three different signal transmission latencies but only one is shown here
Unsmearing Visual Motion Development of Long-Range Horizontal Intrinsic Connections
the moving input pattern No information is built up along the trajectory since the
LEHIC weights are still zero
After training the network is able to resolve the smear Figure in a manner
reminiscent of Burr results Figure As a stimulus moves it excites a sequence
of neurons whose receptive fields lie along its trajectory As each neuron receives
excitatory input in turn from the moving stimulus it becomes activated and emits
excitatory signals along its trajectory-specific LEHICs Subsequent neurons along
the trajectory then receive both direct stimulus-generated excitation and lateral
time-delayed excitation The combination causes these neurons to become even
more active thus activation accumulates along the chain toward an asymptote The
accumulating activation lets neurons farther along the trajectory more effectively
suppress via lateral inhibition the activation of the neurons carrying the trailing
smear The comet-like tail contracts progressively and the representation of the
Time:1
Time=2
Time=3
Time=1
ooOOOOOOO
I
Time=4
Time=5
Time:9
I
I"T
Time=5
Time=6
I
I
I I
Time=8
Time=9
OOOOOOO@.OOO
rT
r?r?r
Time=10
Time=11
Time=11
Time=12
Time=12
TT
r?T
I
e.OOOOOOOO
I
Time=10
I I
Time=8
Time=6
Time=7
I
Time=3
Time=2
Time=4
I
rT
Figure Results of unsmearing simulation A simulated spot moves rightward for
time steps along a I-D model retina Smeared input patterns are plotted as vertical lines
and relative output neuron activation patterns are plotted as shading intensity of circles
neurons Before learning left the network is unable to resolve the smear in the
input but after learning right the smear is resolved by time step The same test
input patterns are used both before and after learning
Martin and Marshall
moving stimulus becomes increasingly sharp
Each neuron's activation value changes according to a shunting differential
equation Grossberg
dtXi
Axi
XdEi xdIi
where the neuron's total excitatory input Ei Li combines bottomup input the smeared motion with summed lateral excitation input Li
I:j the neuron's inhibitory input is Ii zji
h(xj max(u and g(Xj max(O are faster-than-linear signal functions
and A and are constants
CONCLUSIONS AND FUTURE RESEARCH
One might wonder why visual systems allow smear to be represented during the
first ms of a stimulus motion since simple winner-take-all lateral inhibition
could easily eliminate t.he smear in t.he representation Our research lea.ds us
to suggest that representing smear lets human visual systems tolerate and even
exploit initial uncertainty in local motion measurements A system with winnertake-all sharpening could not generate a reliable trajectory prediction from an
initial inaccurate local motion measurement because the motion can be determined
accurately only after multiple measurements along the trajectory are combined
Figure The inaccurate trajectory predictions of such a network would impair
its ability to develop or maintain circuits for combining motion measurements
Marshall We conclude that motion perception systems need to represent
explicitly both initial smear and subsequent unsmearing
Figure 4a illustrates that when a moving object first appears the direction in which
it will move is uncertain As the object continues to move its successor positions
become increasingly predictable in general The initial smear in the representation
is necessary for communicating prior trajectory information to the representations
of many possible future trajectory positions
Faster-tha.n-linear signal functions Figure were used so that a neuron would
generate little lateral excitation and inhibition when it is uncertain about the
presence of the moving stimulus in its receptive field when a new stimulus appears
and so that a highly active neuron more certain about the presence of the stimulus
in its receptive field would generate strong lateral excitation and inhibition
Our results illustrate how visual systems may become able both to propagate motion
Uncertainty
Strong Certainty
Uncertain
Certain
Figure Visual motion system uncertainty When a moving object first appears the
direction in which it will move is uncertain top row circular shaded region As motion
proceeds second third and fourth rows the set of possible stimulus locatlOns becomes
increasingly predictable smaller shaded regions Faster-than-linear signal functions
maintain smear of uncertain data but sharpen more certain data
Unsmearing Visual Motion Development of Long-Range Horizontal Intrinsic Connections
information in a trajectory-specific manner and to use the propagated information
to unsmear representations of moving objects Regular anisotropic chain
patterns of time-delayed horizontal excitatory connections become established
through a learning procedure in response to exposure to ordinary moving visual
scenes Accumulation of propagated motion information along these chains
causes a sharpening that unsmears representations of moving visual stimuli
These results let us model the integration-along-trajectory revealed by Burr's
experiment within a developmental framework that corresponds to known
neurophysiological data they can potentially also let other nonlocal motion
phenomena such as visual inertia Anstis Ramachandran be modeled
ACKNOWLEDGEMENTS
This work was supported in part by the National Eye Institute by the
Office of Naval Research Cognitive and NeUl'ai Sciences and
by an Oak Ridge Associated Universit.ies Juuior Faculty Enhancement Award

----------------------------------------------------------------

title: 5872-efficient-and-robust-automated-machine-learning.pdf

Efficient and Robust Automated Machine Learning
Matthias Feurer
Aaron Klein
Katharina Eggensperger
Jost Tobias Springenberg
Manuel Blum
Frank Hutter
Department of Computer Science
University of Freiburg Germany
feurerm,kleinaa,eggenspk,springj,mblum,fh}@cs.uni-freiburg.de
Abstract
The success of machine learning in a broad range of applications has led to an
ever-growing demand for machine learning systems that can be used off the shelf
by non-experts To be effective in practice such systems need to automatically
choose a good algorithm and feature preprocessing steps for a new dataset at hand
and also set their respective hyperparameters Recent work has started to tackle this
automated machine learning AutoML problem with the help of efficient Bayesian
optimization methods Building on this we introduce a robust new AutoML system
based on scikit-learn using classifiers feature preprocessing methods and
data preprocessing methods giving rise to a structured hypothesis space with
hyperparameters This system which we dub AUTO SKLEARN improves on
existing AutoML methods by automatically taking into account past performance
on similar datasets and by constructing ensembles from the models evaluated
during the optimization Our system won the first phase of the ongoing ChaLearn
AutoML challenge and our comprehensive analysis on over diverse datasets
shows that it substantially outperforms the previous state of the art in AutoML We
also demonstrate the performance gains due to each of our contributions and derive
insights into the effectiveness of the individual components of AUTO SKLEARN
Introduction
Machine learning has recently made great strides in many application areas fueling a growing
demand for machine learning systems that can be used effectively by novices in machine learning
Correspondingly a growing number of commercial enterprises aim to satisfy this demand
BigML.com Wise.io SkyTree.com RapidMiner.com Dato.com Prediction.io DataRobot.com Microsoft?s Azure Machine
Learning Google?s Prediction API and Amazon Machine Learning At its core every effective machine learning
service needs to solve the fundamental problems of deciding which machine learning algorithm to
use on a given dataset whether and how to preprocess its features and how to set all hyperparameters
This is the problem we address in this work
More specifically we investigate automated machine learning AutoML the problem of automatically
without human input producing test set predictions for a new dataset within a fixed computational
budget Formally this AutoML problem can be stated as follows
Definition AutoML problem For let Rd denote a feature vector and
the corresponding target value Given a training dataset Dtrain y1 yn and
the feature vectors xn+m of a test dataset Dtest xn+m yn+m
drawn from the same underlying data distribution as well as a resource budget and a loss metric
the AutoML problem is to automatically produce test set predictions
Pm y?n+m The
loss of a solution y?n+m to the AutoML problem is given by
yn+j yn+j
In practice the budget would comprise computational resources such as CPU and/or wallclock time
and memory usage This problem definition reflects the setting of the ongoing ChaLearn AutoML
challenge The AutoML system we describe here won the first phase of that challenge
Here we follow and extend the AutoML approach first introduced by AUTO WEKA
http://automl.org At its core this approach combines a highly parametric machine learning
framework with a Bayesian optimization method for instantiating well for a given dataset
The contribution of this paper is to extend this AutoML approach in various ways that considerably
improve its efficiency and robustness based on principles that apply to a wide range of machine
learning frameworks such as those used by the machine learning service providers mentioned above
First following successful previous work for low dimensional optimization problems
we reason across datasets to identify instantiations of machine learning frameworks that perform
well on a new dataset and warmstart Bayesian optimization with them Section Second we
automatically construct ensembles of the models considered by Bayesian optimization Section
Third we carefully design a highly parameterized machine learning framework from high-performing
classifiers and preprocessors implemented in the popular machine learning framework scikit-learn
Section Finally we perform an extensive empirical analysis using a diverse collection of datasets
to demonstrate that the resulting AUTO SKLEARN system outperforms previous state-of-the-art
AutoML methods Section to show that each of our contributions leads to substantial performance
improvements Section and to gain insights into the performance of the individual classifiers and
preprocessors used in AUTO SKLEARN Section
AutoML as a CASH problem
We first review the formalization of AutoML as a Combined Algorithm Selection and Hyperparameter
optimization CASH problem used by AUTO WEKA?s AutoML approach Two important problems
in AutoML are that no single machine learning method performs best on all datasets and some
machine learning methods non-linear SVMs crucially rely on hyperparameter optimization
The latter problem has been successfully attacked using Bayesian optimization which nowadays
forms a core component of an AutoML system The former problem is intertwined with the latter since
the rankings of algorithms depend on whether their hyperparameters are tuned properly Fortunately
the two problems can efficiently be tackled as a single structured joint optimization problem
Definition CASH Let A be a set of algorithms and let the hyperparameters
of each algorithm have domain Further let Dtrain y1 yn be a train(1
ing set which is split into cross-validation folds Dvalid Dvalid and Dtrain Dtrain
such that Dtrain Dtrain Dvalid for K. Finally let Dtrain Dvalid denote the
loss that algorithm achieves on Dvalid when trained on Dtrain with hyperparameters Then
the Combined Algorithm Selection and Hyperparameter optimization CASH problem is to find the
joint algorithm and hyperparameter setting that minimizes this loss
A argmin
Dtrain Dvalid
This CASH problem was first tackled by Thornton in the AUTO WEKA system using the
machine learning framework WEKA and tree-based Bayesian optimization methods In
a nutshell Bayesian optimization fits a probabilistic model to capture the relationship between
hyperparameter settings and their measured performance it then uses this model to select the most
promising hyperparameter setting trading off exploration of new parts of the space exploitation
in known good regions evaluates that hyperparameter setting updates the model with the result
and iterates While Bayesian optimization based on Gaussian process models Snoek
performs best in low-dimensional problems with numerical hyperparameters tree-based models have
been shown to be more successful in high-dimensional structured and partly discrete problems
such as the CASH problem and are also used in the AutoML system YPEROPT SKLEARN
Among the tree-based Bayesian optimization methods Thornton found the random-forestbased SMAC to outperform the tree Parzen estimator TPE and we therefore use SMAC
to solve the CASH problem in this paper Next to its use of random forests SMAC?s main
distinguishing feature is that it allows fast cross-validation by evaluating one fold at a time and
discarding poorly-performing hyperparameter settings early
Bayesian optimizer
Xtrain Ytrain
Xtest
metalearning
data preprocessor
feature
classifier
preprocessor
ML framework
AutoML
system
build
ensemble
Y?test
Figure Our improved AutoML approach We add two components to Bayesian hyperparameter optimization
of an ML framework meta-learning for initializing the Bayesian optimizer and automated ensemble construction
from configurations evaluated during optimization
New methods for increasing efficiency and robustness of AutoML
We now discuss our two improvements of the AutoML approach First we include a meta-learning
step to warmstart the Bayesian optimization procedure which results in a considerable boost in
efficiency Second we include an automated ensemble construction step allowing us to use all
classifiers that were found by Bayesian optimization
Figure summarizes the overall AutoML workflow including both of our improvements We note
that we expect their effectiveness to be greater for flexible ML frameworks that offer many degrees of
freedom many algorithms hyperparameters and preprocessing methods
Meta-learning for finding good instantiations of machine learning frameworks
Domain experts derive knowledge from previous tasks They learn about the performance of machine
learning algorithms The area of meta-learning mimics this strategy by reasoning about the
performance of learning algorithms across datasets In this work we apply meta-learning to select
instantiations of our given machine learning framework that are likely to perform well on a new
dataset More specifically for a large number of datasets we collect both performance data and a set
of meta-features characteristics of the dataset that can be computed efficiently and that help to
determine which algorithm to use on a new dataset
This meta-learning approach is complementary to Bayesian optimization for optimizing an ML
framework Meta-learning can quickly suggest some instantiations of the ML framework that are
likely to perform quite well but it is unable to provide fine-grained information on performance
In contrast Bayesian optimization is slow to start for hyperparameter spaces as large as those of
entire ML frameworks but can fine-tune performance over time We exploit this complementarity by
selecting configurations based on meta-learning and use their result to seed Bayesian optimization
This approach of warmstarting optimization by meta-learning has already been successfully applied
before but never to an optimization problem as complex as that of searching the space
of instantiations of a full-fledged ML framework Likewise learning across datasets has also
been applied in collaborative Bayesian optimization methods while these approaches are
promising they are so far limited to very few meta-features and cannot yet cope with the highdimensional partially discrete configuration spaces faced in AutoML
More precisely our meta-learning approach works as follows In an offline phase for each machine
learning dataset in a dataset repository our case datasets from the OpenML repository
we evaluated a set of meta-features described below and used Bayesian optimization to determine
and store an instantiation of the given ML framework with strong empirical performance for that
dataset In detail we ran SMAC for 24 hours with 10-fold cross-validation on two thirds of the
data and stored the resulting ML framework instantiation which exhibited best performance on the
remaining third Then given a new dataset we compute its meta-features rank all datasets by
their L1 distance to in meta-feature space and select the stored ML framework instantiations for
the nearest datasets for evaluation before starting Bayesian optimization with their results
To characterize datasets we implemented a total of 38 meta-features from the literature including
simple information-theoretic and statistical meta-features such as statistics about the number
of data points features and classes as well as data skewness and the entropy of the targets All
meta-features are listed in Table of the supplementary material Notably we had to exclude the
prominent and effective category of landmarking meta-features which measure the performance
of simple base learners because they were computationally too expensive to be helpful in the online
evaluation phase We note that this meta-learning approach draws its power from the availability of
a repository of datasets due to recent initiatives such as OpenML we expect the number of
available datasets to grow ever larger over time increasing the importance of meta-learning
Automated ensemble construction of models evaluated during optimization
While Bayesian hyperparameter optimization is data-efficient in finding the best-performing hyperparameter setting we note that it is a very wasteful procedure when the goal is simply to make good
predictions all the models it trains during the course of the search are lost usually including some
that perform almost as well as the best Rather than discarding these models we propose to store them
and to use an efficient post-processing method which can be run in a second process on-the-fly to
construct an ensemble out of them This automatic ensemble construction avoids to commit itself to a
single hyperparameter setting and is thus more robust and less prone to overfitting than using the
point estimate that standard hyperparameter optimization yields To our best knowledge we are the
first to make this simple observation which can be applied to improve any Bayesian hyperparameter
optimization method
It is well known that ensembles often outperform individual models and that effective
ensembles can be created from a library of models Ensembles perform particularly well if
the models they are based on are individually strong and make uncorrelated errors Since
this is much more likely when the individual models are different in nature ensemble building is
particularly well suited for combining strong instantiations of a flexible ML framework
However simply building a uniformly weighted ensemble of the models found by Bayesian optimization does not work well Rather we found it crucial to adjust these weights using the predictions of
all individual models on a hold-out set We experimented with different approaches to optimize these
weights stacking gradient-free numerical optimization and the method ensemble selection
While we found both numerical optimization and stacking to overfit to the validation set and to be
computationally costly ensemble selection was fast and robust In a nutshell ensemble selection
introduced by Caruana is a greedy procedure that starts from an empty ensemble and then
iteratively adds the model that maximizes ensemble validation performance with uniform weight
but allowing for repetitions Procedure in the supplementary material describes it in detail We
used this technique in all our experiments building an ensemble of size
A practical automated machine learning system
To design a robust AutoML system as
feature
estimator
preprocessing
classifier
our underlying ML framework we chose
preprocessor
scikit-learn one of the best known
AdaBoost
RF
kNN
PCA
None fast ICA
and most widely used machine learning
learning
rate
estimators
max
depth
data
libraries It offers a wide range of well espreprocessor
tablished and efficiently-implemented ML
imputation balancing
rescaling
one hot enc
algorithms and is easy to use for both experts and beginners Since our AutoML
mean median
weighting
min/max standard
None
system closely resembles AUTO WEKA
but like YPEROPT SKLEARN is based Figure Structured configuration space Squared boxes
on scikit-learn we dub it AUTO SKLEARN denote parent hyperparameters whereas boxes with rounded
Figure depicts AUTO SKLEARN?s overall edges are leaf hyperparameters Grey colored boxes mark
components It comprises classification active hyperparameters which form an example configuration
and machine learning pipeline Each pipeline comprises one
algorithms preprocessing methods and feature preprocessor classifier and up to three data prepro4 data preprocessing methods We param cessor methods plus respective hyperparameters
eterized each of them which resulted in a
space of hyperparameters Most of these are conditional hyperparameters that are only active
if their respective component is selected We note that SMAC can handle this conditionality
natively
All classification algorithms in AUTO SKLEARN are listed in Table 1a and described in detail in
Section of the supplementary material They fall into different categories such as general linear
models algorithms support vector machines discriminant analysis nearest neighbors
na??ve Bayes decision trees and ensembles In contrast to AUTO WEKA we
name
name
cat cond
cont cond
AdaBoost
Bernoulli na??ve Bayes
decision tree
extreml rand trees
Gaussian na??ve Bayes
gradient boosting
kNN
LDA
linear SVM
kernel SVM
multinomial na??ve Bayes
passive aggressive
QDA
random forest
Linear Class SGD
classification algorithms
cat cond
cont cond
extreml rand trees prepr
fast ICA
feature agglomeration
kernel PCA
rand kitchen sinks
linear SVM prepr
no preprocessing
nystroem sampler
PCA
polynomial
random trees embed
select percentile
select rates
one-hot encoding
imputation
balancing
rescaling
preprocessing methods
Table Number of hyperparameters for each possible classifier left and feature preprocessing method
right for a binary classification dataset in dense representation Tables for sparse binary classification and
sparse/dense multiclass classification datasets can be found in the Section of the supplementary material
Tables 3b and We distinguish between categorical cat hyperparameters with discrete values
and continuous cont numerical hyperparameters Numbers in brackets are conditional hyperparameters which
are only relevant when another parameter has a certain value
focused our configuration space on base classifiers and excluded meta-models and ensembles that
are themselves parameterized by one or more base classifiers While such ensembles increased
AUTO WEKA?s number of hyperparameters by almost a factor of five to AUTO SKLEARN
only features hyperparameters We instead construct complex ensembles using our post-hoc
method from Section Compared to AUTO WEKA this is much more data-efficient in AUTO WEKA evaluating the performance of an ensemble with components requires the construction and
evaluation of models in contrast in AUTO SKLEARN ensembles come largely for free and it is
possible to mix and match models evaluated at arbitrary times during the optimization
The preprocessing methods for datasets in dense representation in AUTO SKLEARN are listed in
Table 1b and described in detail in Section of the supplementary material They comprise data
preprocessors which change the feature values and are always used when they apply and feature
preprocessors which change the actual set of features and only one of which or none is used Data
preprocessing includes rescaling of the inputs imputation of missing values one-hot encoding and
balancing of the target classes The possible feature preprocessing methods can be categorized into
feature selection kernel approximation matrix decomposition embeddings feature
clustering polynomial feature expansion and methods that use a classifier for feature selection
For example L1 regularized linear SVMs fitted to the data can be used for feature selection by
eliminating features corresponding to zero-valued model coefficients
As with every robust real-world system we had to handle many more important details in AUTO SKLEARN we describe these in Section of the supplementary material
Comparing AUTO SKLEARN to AUTO WEKA and YPEROPT SKLEARN
As a baseline experiment we compared the performance of vanilla AUTO SKLEARN without our
improvements to AUTO WEKA and YPEROPT SKLEARN reproducing the experimental setup
with datasets of the paper introducing AUTO WEKA We describe this setup in detail in
Section in the supplementary material
Table shows that AUTO SKLEARN performed statistically significantly better than AUTO WEKA
in cases tied it in cases and lost against it in For the three datasets where AUTO WEKA performed best we found that in more than of its runs the best classifier it chose is not
implemented in scikit-learn trees with a pruning component So far YPEROPT SKLEARN is more
of a proof-of-concept inviting the user to adapt the configuration space to her own needs than
a full AutoML system The current version crashes when presented with sparse data and missing
values It also crashes on Cifar-10 due to a memory limit which we set for all optimizers to enable a
Yeast
Waveform
Wine
Quality
Shuttle
Semeion
Secom
MRBI
Madelon
MNIST
Basic
KR-vs-KP
KDD09
Appetency
Gisette
German
Credit
Dexter
Convex
Cifar-10
Small
Dorothea
Cifar-10
Car
Amazon
Abalone
AS
AW
HS
Table Test set classification error of AUTO WEKA vanilla AUTO SKLEARN and YPEROPTSKLEARN as in the original evaluation of AUTO WEKA We show median percent error across
bootstrap samples based on runs simulating parallel runs Bold numbers indicate the best result
Underlined results are not statistically significantly different from the best according to a bootstrap test with
average rank
vanilla auto-sklearn
auto-sklearn ensemble
auto-sklearn meta-learning
auto-sklearn meta-learning ensemble
time sec
Figure Average rank of all four AUTO SKLEARN variants ranked by balanced test error rate across
datasets Note that ranks are a relative measure of performance here the rank of all methods has to add up
to and hence an improvement in BER of one method can worsen the rank of another The supplementary
material shows the same plot on a log-scale to show the time overhead of meta-feature and ensemble computation
fair comparison On the datasets on which it ran it statistically tied the best optimizer in cases
and lost against it in
Evaluation of the proposed AutoML improvements
In order to evaluate the robustness and general applicability of our proposed AutoML system on
a broad range of datasets we gathered binary and multiclass classification datasets from the
OpenML repository only selecting datasets with at least data points to allow robust
performance evaluations These datasets cover a diverse range of applications such as text classification digit and letter recognition gene sequence and RNA classification advertisement particle
classification for telescope data and cancer detection in tissue samples We list all datasets in Table
and in the supplementary material and provide their unique OpenML identifiers for reproducibility
Since the class distribution in many of these datasets is quite imbalanced we evaluated all AutoML
methods using a measure called balanced classification error rate We define balanced error
rate as the average of the proportion of wrong classifications in each class In comparison to standard
classification error the average overall error this measure the average of the class-wise error
assigns equal weight to all classes We note that balanced error or accuracy measures are often used
in machine learning competitions the AutoML challenge uses balanced accuracy
We performed runs of AUTO SKLEARN both with and without meta-learning and with and without
ensemble prediction on each of the datasets To study their performance under rigid time constraints
and also due to computational resource constraints we limited the CPU time for each run to hour we
also limited the runtime for a single model to a tenth of this minutes To not evaluate performance
on data sets already used for meta-learning we performed a leave-one-dataset-out validation when
evaluating on dataset we only used meta-information from the other datasets
Figure shows the average ranks over time of the four AUTO SKLEARN versions we tested We
observe that both of our new methods yielded substantial improvements over vanilla AUTO SKLEARN
The most striking result is that meta-learning yielded drastic improvements starting with the first
OpenML
dataset ID
AUTO
SKLEARN
AdaBoost
Bernoulli
na??ve Bayes
decision
tree
extreml
rand trees
Gaussian
na??ve Bayes
gradient
boosting
kNN
LDA
linear SVM
kernel SVM
multinomial
na??ve Bayes
passive
aggresive
QDA
random forest
Linear Class
SGD
38
46
Table Median balanced test error rate BER of optimizing AUTO SKLEARN subspaces for each classification
OpenML
dataset ID
AUTO
SKLEARN
densifier
extreml rand
trees prepr
fast ICA
feature
agglomeration
kernel PCA
rand
kitchen sinks
linear
SVM prepr
no
preproc
nystroem
sampler
PCA
polynomial
random
trees embed
select percentile
classification
select rates
truncatedSVD
method and all preprocessors as well as the whole configuration space of AUTO SKLEARN on 13 datasets
All optimization runs were allowed to run for 24 hours except for AUTO SKLEARN which ran for 48 hours
Bold numbers indicate the best result underlined results are not statistically significantly different from the best
according to a bootstrap test using the same setup as for Table
38
46
Table Like Table but instead optimizing subspaces for each preprocessing method and all classifiers
configuration it selected and lasting until the end of the experiment We note that the improvement
was most pronounced in the beginning and that over time vanilla AUTO SKLEARN also found good
solutions without meta-learning letting it catch up on some datasets thus improving its overall rank
Moreover both of our methods complement each other our automated ensemble construction
improved both vanilla AUTO SKLEARN and AUTO SKLEARN with meta-learning Interestingly the
ensemble?s influence on the performance started earlier for the meta-learning version We believe
that this is because meta-learning produces better machine learning models earlier which can be
directly combined into a strong ensemble but when run longer vanilla AUTO SKLEARN without
meta-learning also benefits from automated ensemble construction
Detailed analysis of AUTO SKLEARN components
We now study AUTO SKLEARN?s individual classifiers and preprocessors compared to jointly
optimizing all methods in order to obtain insights into their peak performance and robustness Ideally
we would have liked to study all combinations of a single classifier and a single preprocessor in
isolation but with classifiers and preprocessors this was infeasible rather when studying the
performance of a single classifier we still optimized over all preprocessors and vice versa To obtain
a more detailed analysis we focused on a subset of datasets but extended the configuration budget for
optimizing all methods from one hour to one day and to two days for AUTO SKLEARN Specifically
we clustered our datasets with g-means based on the dataset meta-features and used one
dataset from each of the resulting 13 clusters Table in the supplementary material for the list of
datasets We note that in total these extensive experiments required CPU years
Table compares the results of the various classification methods against AUTO SKLEARN Overall
as expected random forests extremely randomized trees AdaBoost and gradient boosting showed
auto-sklearn
gradient boosting
kernel SVM
random forest
auto-sklearn
gradient boosting
kernel SVM
random forest
45
Balanced Error Rate
Balanced Error Rate
35
time sec
MNIST OpenML dataset ID
time sec
Promise pc4 OpenML dataset ID
Figure Performance of a subset of classifiers compared to AUTO SKLEARN over time We show median test
error rate and the fifth and percentile over time for optimizing three classifiers separately with optimizing
the joint space A plot with all classifiers can be found in Figure in the supplementary material While
AUTO SKLEARN is inferior in the beginning in the end its performance is close to the best method
the most robust performance and SVMs showed strong peak performance for some datasets Besides
a variety of strong classifiers there are also several models which could not compete The decision
tree passive aggressive kNN Gaussian NB LDA and QDA were statistically significantly inferior
to the best classifier on most datasets Finally the table indicates that no single method was the best
choice for all datasets As shown in the table and also visualized for two example datasets in Figure
optimizing the joint configuration space of AUTO SKLEARN led to the most robust performance
A plot of ranks over time Figure and in the supplementary material quantifies this across all
13 datasets showing that AUTO SKLEARN starts with reasonable but not optimal performance and
effectively searches its more general configuration space to converge to the best overall performance
over time
Table compares the results of the various preprocessors against AUTO SKLEARN As for the
comparison of classifiers above AUTO SKLEARN showed the most robust performance It performed
best on three of the datasets and was not statistically significantly worse than the best preprocessor on
another of 13
Discussion and Conclusion
We demonstrated that our new AutoML system AUTO SKLEARN performs favorably against the
previous state of the art in AutoML and that our meta-learning and ensemble improvements for
AutoML yield further efficiency and robustness This finding is backed by the fact that AUTO SKLEARN won the auto-track in the first phase of ChaLearn?s ongoing AutoML challenge In this
paper we did not evaluate the use of AUTO SKLEARN for interactive machine learning with an expert
in the loop and weeks of CPU power but we note that that mode has also led to a third place in
the human track of the same challenge As such we believe that AUTO SKLEARN is a promising
system for use by both machine learning novices and experts The source code of AUTO SKLEARN is
available under an open source license at https://github.com/automl/auto-sklearn
Our system also has some shortcomings which we would like to remove in future work As one
example we have not yet tackled regression or semi-supervised problems Most importantly though
the focus on scikit-learn implied a focus on small to medium-sized datasets and an obvious direction
for future work will be to apply our methods to modern deep learning systems that yield state-ofthe-art performance on large datasets we expect that in that domain especially automated ensemble
construction will lead to tangible performance improvements over Bayesian optimization
Acknowledgments
This work was supported by the German Research Foundation under Priority Programme Autonomous
Learning SPP grant HU under Emmy Noether grant HU and under the BrainLinksBrainTools Cluster of Excellence grant number EXC

----------------------------------------------------------------

title: 1299-the-neurothermostat-predictive-optimal-control-of-residential-heating-systems.pdf

The Neurothermostat
Predictive Optimal Control of
Residential Heating Systems
Michael C. Mozer Lucky Vidmart Robert H. Dodiert
tDepartment of Computer Science
tDepartment of Civil Environmental and Architectural Engineering
University of Colorado Boulder CO
Abstract
The Neurothermostat is an adaptive controller that regulates indoor air temperature in a residence by switching a furnace on or
off The task is framed as an optimal control problem in which
both comfort and energy costs are considered as part of the control objective Because the consequences of control decisions are
delayed in time the Neurothermostat must anticipate heating demands with predictive models of occupancy patterns and the thermal response of the house and furnace Occupancy pattern prediction is achieved by a hybrid neural net look-up table The Neurothermostat searches at each discrete time step for a decision
sequence that minimizes the expected cost over a fixed planning
horizon The first decision in this sequence is taken and this process repeats Simulations of the Neurothermostat were conducted
using artificial occupancy data in which regularity was systematically varied as well as occupancy data from an actual residence
The Neurothermostat is compared against three conventional policies and achieves reliably lower costs This result is robust to the
relative weighting of comfort and energy costs and the degree of
variability in the occupancy patterns
For over a quarter century the home automation industry has promised to revolutionize our lifestyle with the so-called Smart House@ in which appliances lighting
stereo video and security systems are integrated under computer control However home automation has yet to make significant inroads at least in part because
software must be tailored to the home occupants
Instead of expecting the occupants to program their homes or to hire someone to
do so one would ideally like the home to essentially program itself by observing
the lifestyle of the occupants This is the goal of the Neural Network House Mozer
an actual residence that has been outfitted with over 75 sensorsincluding temperature light sound motion-and actua.tors to control air heating
water heating lighting and ventilation In this paper we describe one research
M. C. Mozer L. Vidmar and R. H. Dodier
project within the house the Neurothermostat that learns to regulate the indoor
air temperature automatically by observing and detecting patterns in the occupants
schedules and comfort preferences We focus on the problem of air heating with
a whole-house furnace but the same approach can be taken with alternative or
multiple heating devices and to the problems of cooling and ventilation
TEMPERATURE REGULATION AS AN OPTIMAL
CONTROL PROBLEM
Traditionally the control objective of air temperature regulation has been to minimize energy consumption while maintaining temperature within an acceptable comfort margin during certain times of the day and days of the week This is sensible
in commercial settings where occupancy patterns follow simple rules and where
energy considerations dominate individual preferences In a residence however the
desires and schedules of occupants need to be weighted equally with energy considerations Consequently we frame the task of air temperature regulation as a
problem of maximizing occupant comfort and minimizing energy costs
These two objectives clearly conflict but they can be integrated into a unified
framework via an optimal control aproach in which the goal is to heat the house
according to a policy that minimizes the cost
lim ut
where time is quantized into nonoverlapping intervals during which we assume
all environmental variables remain constant to is the interval ending at the current
time Ut is the control decision for interval turn the furnace is the
energy cost associated with decision Xt is the environmental state during interval
which includes the indoor temperature and the occupancy status of the home
and is the misery of the occupant given state To add misery and energy
costs a common currency is required Energy costs are readily expressed in dollars
We also determi'ne misery in dollars as we describe later
While we have been unable to locate any earlier work that combined energy and
comfort costs in an optimal control framework optimal control has been used in a
variety of building energy system control applications Henze Dodier
Khalid Omatu
THE NEUROTHERMOSTAT
Figure shows the system architecture of the Neurothermostat and its interaction
with the environment The heart of the Neurothermostat is a controller that at
time intervals of minutes can switch the house furnace on or off Because the consequences of control decisions are delayed in time the controller must be predictive
to anticipate heating demands The three boxes in the Figure depict components
that predict or model various aspects of the environment We explain their purpose
via a formal description of the controller operation
The controller considers sequences of decisions denoted and searches for the
sequence that minimizes the expected total cost over the planning horizon of
minutes
where the expectation is computed over future states of the environment conditional
on the decision sequence The energy cost in an interval depends only on the
control decision during that interval The misery cost depends on two components
The Neurothermostat
EnvlrcJni;imt
Figure The Neurothermostat and its interaction with the environment
of the state-the house occupancy status for empty for occupied and
the indoor air temperature
hu(t
hu(t hu(t
Because the true quantities hu and are unknown they must be estimated
The house thermal model of Figure provides and hu the occupant comfort cost
model provides and the home occupancy predictor provides
We follow a tradition of using neural networks for prediction in the context of building energy system control Curtiss Kreider Brandemuehl Ferrano
Wong Miller Seem although in our initial experiments we require
a network only for the occupancy prediction
PREDICTIVE OPTIMAL CONTROLLER
We propose a closed-loop controller that combines prediction with fixed-horizon
planning of the sort proposed by Clarke Mohtadi and lUffs At each
time step the controller considers all possible decision sequences over the pl~nning
horizon and selects the sequence that minimizes the expected total cost J. The
first decision in this sequence is then performed After minutes the planning and
execution process is repeated This approach assumes that beyond the planning
horizon all costs are independent of the first decision in the sequence
While dynamic programming is an efficient search algorithm it requires a discrete
state space Wishing to avoid quantizing the continuous variable of indoor temperature and the errors that might be introduced we performed performed exhaustive
search through the possible decision sequences which was tractable due to relatively short horizons and two additional domain constraints First because the
house occupancy status can reasonably be assumed to be independent of the indoor temperature need not be recalculated for every possible decision sequence
Second the current occupancy status depends on the recent occupancy history
Consequently one needs to predict occupancy patterns over the planning horizon
to compute However because most occupancy sequences are highly
improbable we find that considering only the most likely sequences-those containing at most two occupancy state transitions-produces the same decisions as doing
the search over the entire distribution reducing the cost from to
OCCUPANCY PREDICTOR
The basic task of the occupancy predictor is to estimate the probability that the
occupant will be home minutes in the future The occupancy predictor can be
run iteratively to estimate the probability of an occupancy pattern
If occupants follow a deterministic daily schedule a look up table indexed by time
of day and current occupancy state should capture occupancy patterns We thus
use a look up table to encode whatever structure possible and a neural network
M. C. Mozer L. Vidmar and R. H. Dodier
to encode residual structure The look up table divides time into fixed minute
bins The neural network consisted of the following inputs current time of day day
of the week average proportion of time home was occupied in the and
minutes from the present time of day on the previous three days and on the same
day of the week during the past four weeks and the proportion of time the home
was occupied during the past and minutes The network a standard
three-layer architecture was trained by back propagation The number of hidden
units was chosen by cross validation
THERMAL MODEL OF HOUSE AND FURNACE
A thermal model of the house and furnace predicts future indoor temperature(s
as a function of the outdoor temperature and the furnace operation While one
could perform system identification using neural networks a simple parameterized
resistance-capacitance model provides a reasonable first-order approximation
The RC model assumes that the inside of the house is at a uniform temperature
and likewise the outside a homogeneous flat wall separates the inside and outside
and this wall has a thermal resistance and thermal capacitance the entire
wall mass is at the inside temperature and the heat input to the inside is when
the furnace is running or zero otherwise Assuming that the outdoor temperature
denoted is constant the the indoor temperature at time is
hu(t hu(t exp RC
RQu(t
exp
where hu(to is the actual indoor temperature at the current time Rand were
determined by architectural properties of the Neural Network House to be
Kelvins/kilowatt and megajoules/Kelvin respectively The House furnace is
rated at Btu/hour and has fuel use efficiency resulting in an output
of kilowatts With natural gas at per CCF the cost of operating
the furnace is per hour
OCCUPANT COMFORT COST MODEL
In the Neural Network House the occupant expresses discomfort by adjusting a
set point temperature on a control panel However for simplicity we assume in
this work the setpoint is a constant A. When the home is occupied the misery
cost is a monotonic function of the deviation of the actual indoor temperature from
the set point When the home is empty the misery cost is zero regardless of the
temperature
There is a rich literature directed at measuring thermal comfort in a given environment dry-bulb temperature relative humidity air velocity c~othing insulation
etc for the average building occupant Fanger Gagge Stolwijk Nishi
Although the measurements indicate the fraction of the population which
is uncomfortable in a particular environment one might also interpret them as a
measure of an individual's level of discomfort As a function of dry-bulb temperature this curve is roughly parabolic We approximate it with a measure of misery
in a 6-minute period as follows
max
A
oa 24
The first term is a binary variable indicating the home occupancy state The
second term is a conversion factor from arbitrary misery units to dollars The
third term scales the misery cost from a full day to the basic update interval
The fourth term produces the parabolic relative misery function scaled so that a
temperature difference of produces one unit of misery with a deadband region
from A to A
We have chosen the conversion factor a using an economic perspective Consider
the lost productivity that results from trying to work in a home that is colder
The Neurothermostat
than desired for a 24 hour period Denote this loss measured in hours The cost
in dollars of this loss is then a where'Y is the individual's hourly salary With
this approch a can be set in a natural intuitive manner
SIMULATION METHODOLOGY
In all experiments we report below fJ minutes steps minute planning horizon and 28 dollars per hour The productivity
loss was varied from to hours
We report here results from the Neurothermostat operating in a simulated environment rather than in the actual Neural Network House The simulated environment incorporates the house/furnace thermal model described earlier and occupants
whose preferences follow the comfort cost model The outdoor temperature is assumed to remain a constant C. Thus the Neurothermostat has an accurate model
of its environment except for the occupancy patterns which it must predict based
on training data This allows us to evaluate the performance of the Neurothermostat and the occupancy model as occupancy patterns are varied uncontaminated
by the effect of inaccuracy in the other internal models
We have evaluated the Neurothermostat with both real and artificial occupancy
data The real data was collected from the Neural Network House with a single
resident over an eight month period using a simple algorithm based on motion
detector output and the opening and closing of outside doors The artificial data
was generated by a simulation of a single occupant The occupant would go to work
each day later on the weekends would sometimes come home for lunch sometimes
go out on weekend nights and sometimes go out of town for several days To
examine performance of the Neurothermostat as a function of the variability in
the occupant's schedule the simulation model included a parameter the variability
index An index of means that the schedule is entirely deterministic an index of
means that the schedule was very noisy but still contained statistical regularities
The index determined factors such as the likelihood and duration of out-of-town
trips and the variability in departure and return times
ALTERNATIVE HEATING POLICIES
In addition to the Neurothermostat we examined three nonadaptive control policies
These policies produce a setpoint at each time step and the furnace is switched on
if the temperature drops below the setpoint and off if the temperature rises above
the setpoint We need not be concerned about damage to the furnace by cycling
because control decisions are made ten minutes apart The constant-temperature
policy produces a fixed setpoint of C. The occupancy-triggered policy produces
a set point of when the house is empty when the house is occupied
The setback-thermostat policy lowers the setpoint from to half an
hour before the mean work departure time for that day of the week and raises it
back to half an hour before the mean work return time for that day of the
week The setback temperature for the occupancy-triggered and setback-thermostat
policies was determined empirically to minimize the total cost
RESULTS
OCCUPANCY PREDICTOR
Performance of three different predictors was evaluated using artificial data across
a range of values for the variability index For each condition we generated eight
training/test sets of artificial data each training and test set consisting of days
of data Table shows the normalized mean squared error MSE on the test set
averaged over the eight replications The normalization involved dividing the MSE
for each replication by the error obtained by predicting the future occupancy state
C. Mozer L. Vidmar and R. H. Dodier
Table Normalized MSE on Test Set for Occupancy Prediction-Artificial Data
lookup table
neural net
lookup table neural net
variability index
75
94 92
81
83 86
63
78 77
49
02
02
Productivity Loss hr
Productivity Loss hr
94
91
74
Variability Index
Variability Index
Figure Mean cost per day incurred by four control policies on artificial test data as
a function of the data's variability index for comfort lightly weighted left panel
and comfort heavily weighted right panel
is the same as the present state The main result here is that the combination of
neural network and look up table perform better than either component in isolation
ANOVA for combination table
for combination network indicating that the two components are capturing
different structure in the data
CONTROLLER WITH ARTIFICIAL OCCUPANCY DATA
Having trained eight occupancy predictors with different artificial data training
sets we computed misery and energy costs for the Neurothermostat on the corresponding test sets Figure shows the mean total cost per day as a function
of variability index control policy and relative comfort cost The robust result is
that the Neurothermostat outperforms the three nonadaptive control policies for all
levels of the variability index and for both a wide range of values of
Other patterns in the data are noteworthy Costs for the Neurothermostat tend to
rise with the variability index as one would expect because the occupant's schedule becomes less predictable The constant-temperature policy is worst if occupant
comfort is weighted lightly and begins to approach the Neurothermostat in performance as comfort costs are increased If comfort costs overwhelm energy costs
then the constant-temperature policy and the Neurothermostat converge
CONTROLLER WITH REAL OCCUPANCY DATA
Eight months of real occupancy data collected in the Neural Network House beginning in September was also used to generate occupancy models and test
controllers Three training/test splits were formed by training on five consecutive months and testing on the next month Table shows the mean daily cost
for the four controllers The Neurothermostat significantly outperforms the three
nonadaptive controllers as it did with the artificial data
DISCUSSION
The simulation studies reported here strongly suggest that adaptive control of residential heating and cooling systems is worthy of further investigation One is
The Neurothermostat
Table Mean Daily Cost Based on Real Occupancy Data
productivity loss
Neurothermostat
constant temperature
occupancy triggered
setback thermostat
tempted to trumpet the conclusion that adaptive control lowers heating costs but
before doing so one must be clear that the cost being lowered is a combination of
comfort and energy costs If one is merely interested in lowering energy costs then
simply shut off the furnace A central contribution of this work is thus the framing
of the task of air temperature regulation as an optimal control problem in which
both comfort and energy costs are considered as part of the control objective
A common reaction to this research project is My life is far too irregular to be
predicted I don return home from work at the same time every day An important finding of this work is that even a highly nondeterministic schedule contains
sufficient statistical regularity to be exploited by a predictive controller We found
this for both artificial data with a high variability index and real occupancy data
A final contribution of our work is to show that for periodic data such as occupancy
patterns that follow a weekly schedule the combination of a look up table to encode
the periodic structure and a neural network to encode the residual structure can
outperform either method in isolation
Acknowledgements
Support for this research has come from Lifestyle Technologies NSF award and a CRCW grant-in-aid from the University of Colorado This project
owes its existence to the dedication of many students particularly Marc Anderson
Josh Anderson Paul Kooros and Charles Myers Our thanks to Reid Hastie and
Gary McClelland for their suggestions on assessing occupant misery

----------------------------------------------------------------

title: 3185-learning-with-tree-averaged-densities-and-distributions.pdf

Learning with Tree-Averaged Densities and
Distributions
Sergey Kirshner
AICML and Dept of Computing Science
University of Alberta
Edmonton Alberta Canada T6G
sergey@cs.ualberta.ca
Abstract
We utilize the ensemble of trees framework a tractable mixture over superexponential number of tree-structured distributions to develop a new model
for multivariate density estimation The model is based on a construction of treestructured copulas multivariate distributions with uniform on marginals
By averaging over all possible tree structures the new model can approximate
distributions with complex variable dependencies We propose an EM algorithm
to estimate the parameters for these tree-averaged models for both the real-valued
and the categorical case Based on the tree-averaged framework we propose a
new model for joint precipitation amounts data on networks of rain stations
Introduction
Multivariate real-valued data appears in many real-world data sets and a lot of research is being
focused on the development of multivariate real-valued distributions One of the challenges in constructing such distributions is that univariate continuous distributions commonly do not have a clear
multivariate generalization The most studied exception is the multivariate Gaussian distribution owing to properties such as closed form density expression with a convenient generalization to higher
dimensions and closure over the set of linear projections However not all problems can be addressed fairly with Gaussians mixtures multimodal distributions heavy-tailed distributions
and new approaches are needed for such problems
While modeling multivariate distributions is in general difficult due to complicated functional forms
and the curse of dimensionality learning models for individual variables univariate marginals is
often straightforward Once the univariate marginals are known assumed known the rest can
be modeled using copulas multivariate distributions with all univariate marginals equal to uniform
distributions on A large portion of copula research concentrated on bivariate
copulas as extensions to higher dimensions are often difficult Thus if the desired distribution decomposes into its univariate marginals and only bivariate distributions the machinery of copulas can
be effectively utilized
Distributions with undirected tree-structured graphical models have exactly these properties as probability density functions over the variables with tree-structured conditional independence graphs can be written as a product involving univariate marginals and bivariate marginals
corresponding to the edges of the tree While tree-structured dependence is perhaps too restrictive
a richer variable dependence can be obtained by averaging over a small number of different tree
structures or all possible tree structures the latter can be done analytically for categorical-valued
distributions with an ensemble-of-trees model In this paper we extend this tree-averaged model
to continuous variables with the help of copulas and derive a learning algorithm to estimate the
parameters within the maximum likelihood framework with EM Within this framework the
parameter estimation for tree-structured and tree-averaged models requires optimization over only
univariate and bivariate densities potentially avoiding the curse of dimensionality a property not
shared by alternative models that relax the dependence restriction of trees vines
The main contributions of the paper are the new tree-averaged model for multivariate copulas a
parameter estimation algorithm for tree-averaged framework for both categorical and real-valued
complete data and a new model for multi-site daily precipitation amounts an important application
in hydrology In the process we introduce previously unexplored tree-structured copula density and
an algorithm for estimation of its structure and parameters The paper is organized as follows
First we describe copulas their densities and some of their useful properties Section We then
construct multivariate copulas with tree-structured dependence from bivariate copulas Section
and show how to estimate the parameters of the bivariate copulas and perform the edge selection
To allow more complex dependencies between the variables we describe a tree-averaged copula
a novel copula object constructed by averaging over all possible spanning trees for tree-structured
copulas and derive a learning algorithm for the estimation of the parameters from data for the treeaveraged copulas Section We apply our new method to a benchmark data set Section
we also develop a new model for multi-site precipitation amounts a problem involving both binary
rain/no rain and continuous how much rain variables Section
Copulas
Let Xd be a vector random variable with corresponding probability distribution
cdf defined on Rd We denote by the set of components variables of and refer
to individual variables as Xv for V. For simplicity we will refer to assignments to random variables by lower case letters Xv xv will be denoted by xv Let Fv xv
Xv xv Xu denote a univariate marginal of over the variable Xv
Let pv xv denote the probability density function pdf of Xv Let av Fv xv and let
a ad so a is a vector of quantiles of components of with respect to corresponding
univariate marginals Next we define copula a multivariate distribution over vectors of quantiles
Definition The copula associated with is a distribution function that
satisfies
Fd xd Rd
If is a continuous distribution
on Rd with univariate marginals F1 Fd then
F1 Fd ad is the unique choice for
Assuming that has d-th order partial derivatives the probability density function pdf can be
obtained from the distribution function via differentiation and expressed in terms of a derivative of
a copula
av
pv xv
xd
xd
ad
xv
v?V
where
v?V
is referred to as a copula density function
Suppose we are given a complete data set xN of d-component real-valued vectors xn1 xd1 under assumption A maximum likelihood estimate for the
parameters of from data can be obtained my maximizing the log-likelihood of
XX
v?V
pv xnv
Fd xnd
The first term of the log-likelihood corresponds to the total log-likelihood of all univariate marginals
of and the second term to the log-likelihood of its d-variate copula These terms are not independent as the second term in the sum is defined in terms of the probability expressions in the first
summand except for a few special cases a direct optimization of is prohibitively complicated
However a useful and asymptotically consistent heuristic is first to maximize the log-likelihood for
the marginals first term only and then to estimate the parameters for the copula given the solution
for the marginals The univariate marginals can be accurately estimated by either fitting the parameters for some appropriately chosen univariate distributions or by applying non-parametric methods1
as the marginals are estimated independent of each other and do not suffer from the curse of dimensionality Let p?v xv be the estimated pdf for component
and F?v be the corresponding cdf
Let A a aN where an and xnd be a set of estimated
quantiles Under the above heuristic ML estimate for copula density is computed by maximizing
PN
an
Exploiting Tree-Structured Dependence
Joint probability distributions are often modeled with probabilistic graphical models where the structure of the graph captures the conditional independence relations of the variables The joint distribution is then represented as a product of functions over subsets of variables We would like to
keep the number of variables for each of the functions small as the number of parameters and the
number of points needed for parameter estimation often grows exponentially with the number of
variables Thus we focus on copulas with tree dependence Trees play an important role in probabilistic graphical models as they allow for efficient exact inference as well as structure and
parameter learning They can also be placed in a fully Bayesian framework with decomposable
priors allowing to compute expected values over all possible spanning trees of product of functions
defined on the edges of the trees As we will see later in this section under the tree-structured dependence a copula density can be computed as products of bivariate copula densities over the edges
of the graph This property allows us to estimate the parameters for the edge copulas independently
Tree-Structured Copulas
We consider tree-structured Markov networks undirected graphs that do not have loops For a
distribution admitting tree-structured Markov networks referred from now on as tree-structured
distributions assuming that and for Rd the density for
can be rewritten as
puv xu xv
pv xv
pu xu pv xv
v?V
This formulation easily follows from the Hammersley-Clifford theorem Note that for
a copula density cuv au av for xu xv can be computed using Equation
puv xu xv
cuv au av
pu xu pv xv
Using Equations and cp for can be computed as
puv xu xv
cp
cp au av
pu xu pv xv
v?V pv xv
Equation states that a copula density for a tree-structured distribution decomposes as a product
of bivariate copulas over its edges The converse is true as well a tree-structured copula can be
constructed by specifying copulas for the edges of the tree
Theorem Given a tree or a forest and copula densities cuv au av for
cE
cuv au av
is a valid copula density
For a tree-structured density the copula log-likelihood can be rewritten as
cuv anu anv
These approaches for copula estimation are referred to as inference for the margins IFM and canonical
maximum likelihood CML for parametric and non-parametric forms for the marginals respectively
PN
and the parameters can be fitted by maximizing cuv anu anv independently for different
pairs E. The tree structure can be learned from the data as well as in the Chow-Liu
algorithm Full algorithm can be found in an extended version of the paper
Tree-Averaged Copulas
While the framework from Section is computationally efficient and convenient for implementation the imposed tree-structured dependence is too restrictive for real-world problems Vines
for example deal with this problem by allowing recursive refinements for the bivariate probabilities
over variables not connected by the tree edges However vines require estimation of additional characteristics of the distribution conditional rank correlations requiring estimation over large sets
of variables which is not advisable when the amount of available data is not large Our proposed
method would only require optimization of parameters of bivariate copulas from the corresponding
two components of weighted data vectors Using the Bayesian framework for spanning trees from
it is possible to construct an object constituting a convex combination over all possible spanning
trees allowing a much richer set of conditional independencies than a single tree
Meil?a and Jaakkola proposed a decomposable prior over all possible spanning tree structures
Let be a symmetric matrix of non-negative weights for all pairs of distinct variables and zeros on
the diagonal Let be a set of all possible spanning trees over V. The probability distribution over
all spanning tree structures over is defined as
uv where
uv
E?E
Even though the sum is over trees can be efficiently computed in closed form using
a weighted generalization of Kirchoff?s Matrix Tree Theorem
Theorem Let be a distribution over spanning tree structures defined by Then the
normalization constant is equal to the determinant with matrix representing the
first rows and columns of the matrix given by
uv
Luv Lvu
vw
w?V
is a generalization of an adjacency matrix and is a generalization of the Laplacian matrix
The decomposability property of the tree prior Equation allows us to compute the average of
the tree-structured distributions over all tree structures In such averaging was applied to
tree-structured distributions over categorical variables Similarly we define a tree-averaged copula
density as a convex combination of copula densities of the form
uv
cuv au av
E?E
E?E
where entry of matrix denotes uv cuv au av A finite convex combination of copulas
is a copula so is a copula density
Parameter Estimation
Given a set of estimated quantile values A a suitable parameter values edge weight matrix and
parameters for bivariate edge copulas can be found by maximizing the log-likelihood of A
an
an
However the parameter optimization of cannot be done analytically Instead noticing that
we are dealing with a mixture model granted one where the number of mixture components is
super-exponential we propose performing the parameter optimization with the EM algorithm
A possibility of EM algorithm for ensemble-of-trees with categorical data was mentioned but the idea
was abandoned due to the concern about the M-step
Algorithm REE AVERAGED OPULA ENSITY(D
Inputs A complete data set of d-component real-valued vectors a set of of bivariate parametric copula densities cuv
Estimate univariate margins F?v Xv for all components treating all components
independently
Replace with A consisting of vectors an F?d for each vector
in
Initialize and
Run until convergence as determined by change in log-likelihood Equation
E-step For all vectors an and pairs compute E|an
M-step
Update with gradient ascent
Update uv for all pairs by setting partial derivative with respect to parameters
of uv Equation to zero and solving corresponding equations
Output Denoting au xu and av xv
p?v xv
v?V
Figure Algorithm for estimation of a pdf with tree-averaged copulas
While there are possible mixture components spanning trees in the E-step we only need
to compute the posterior probabilities for edges Each step of EM consists
of find
ing parameters maximizing the expected joint log-likelihood given current
parameter values where
En an an
En
sn uv
cuv anu anv
sn
En a
E?E
E?E
cuv anu anv uv
an
The probability distribution En a is of the same form as the tree prior so to compute
sn one needs to compute the sum of probabilities of all trees containing edge
Theorem Let be a tree prior defined in Equation Let where
is obtained by removing row and column from L. Then
uv Quu Qvv 2Quv
uw Quu
wv Qvv
As a consequence of Theorem for each an all edge probabilities sn can
be computed
simultaneously with time complexity of a single matrix inversion
d3 Assuming a candidate bivariate copula cuv has one free parameter uv uv can be optimized
by setting
cuv anu anv uv
sn
uv
to See for more details The parameters of the tree prior can be updated by maximizing
sn uv
an expression concave in uv can be updated using a gradient ascent algorithm on
uv with time complexity d3 per iteration The outline of the EM algorithm is
shown in Figure Assuming the complexity
of each bivariate copula update is the time
complexity of each EM iteration is d3
The EM algorithm can be easily transferred to tree averaging for categorical data The E-step does
not change and in the M-step the parameters for the univariate marginals are updated ignoring
bivariate terms Then the parameters for the bivariate distributions for each edge are updated constrained on the new values of the parameters for the univariate distributions While the algorithm
does not guarantee a maximization of the expected log-likelihood it nonetheless worked well in our
experiments
Experiments
MAGIC Gamma Telescope Data Set
First we tested our tree-averaged density estimator on a MAGIC Gamma Telescope Data Set
from the UCI Machine Learning Repository We considered only the examples from
class gamma signal this set consists of vectors of real-valued components The univariate marginals are not Gaussian some are bounded some have multiple
modes shows an average log-likelihood of models trained on training sets with
and evaluated on 2000-example test sets averaged over
training and test sets The marginals were estimated using Gaussian kernel density estimators KDE with Rule-of-Thumb bandwidth selection All of the models except for full Gaussian
have the same marginals differ only in the multivariate dependence copula As expected from
the curse of dimensionality product KDE improves logarithmically with the amount of data Not
only the marginals are not Gaussian evidenced by a Gaussian copula with KDE marginals outperforming a Gaussian distribution the multivariate dependence is also not Gaussian evidenced by a
tree-structured Frank copula outperforming a tree-structured and a full Gaussian copula However
model averaging even with the wrong dependence model tree-averaged Gaussian copula yields
superior performance
Multi-Site Precipitation Modeling
We applied the tree-averaged framework to the problem of modeling daily rainfall amounts for a
regional spatial network of stations The task is to build a generative model capturing the spatial
and temporal properties of the data This model can be used in at least two ways first to sample
sequences from it and to use them as inputs for other models crop models and second as
a descriptive model of the data Hidden Markov models possible with non-homogeneous transitions are being frequently used for this task with the transition distribution responsible
for modeling of temporal dependence and the emission distributions capturing most of the spatial
dependence Additionally HMMs can be viewed as assigning rainfall daily patterns to weather
states corresponding emission components and both these states as described by either their
parameters or the statistics of the patterns associated with it and their temporal evolution often
offer useful synoptic insight We will use HMMs as the wrapper model with tree-averaged and
tree-structured distributions to model the emission components
The distribution of daily rainfall amounts for any given station can be viewed as a non-overlapping
mixture with one component corresponding to zero precipitation and the other component to positive precipitation For a station let rv be the precipitation amount be a probability of positive
precipitation and let fv rv be a probability density function for amounts given positive precipitation
rv
rv
fv rv rv
For a pair of stations let uv denote the probability of simultaneous positive amounts and
cuv Fu ru Fv rv uv denote the copula density for simultaneous positive amounts
then
uv
ru
uv fv rv
ru
ru rv uv
ru
uv fu ru
uv fu ru fv rv Fu ru Fv rv ru
rv
rv
rv
rv
We can now define a tree-structured and tree-averaged probability distributions pt and pta
respectively over the amounts
ru rv uv
uv
pt
rv
uv
ru rv
v?V
pt
rv
pta
E?E
v?V
rv
We employ univariate exponential distributions fv rv
cuv au av
and bivariate Gaussian copulas
au
uv
uv
We applied the models to a data set collected from stations from a region in Southeastern Australia April-October sequences 30-dimensional vectors each We
used a 5-state HMM with three different types of emission distributions tree-averaged pta treestructured pt and conditionally independent first term of pt and pta We will refer to these
models HMM-TA HMM-Tree and HMM-CI respectively For HMM-TA we reduced the number
of free parameters by only allowing edges for stations adjacent to each other as determined by the
the Delaunay triangulation We also did not learn the edge weights setting them to for
selected edges and to for the rest To make sure that the models do not overfit we computed their
out-of-sample log-likelihood with cross-validation leaving out one year at a time not shown
states were chosen because the leave-one-year out log-likelihood starts to flatten out for HMM-TA
at states The resulting log-likelihoods divided by the number of days and stations are
and for HMM-TA HMM-Tree and HMM-CI respectively To see how well
the models capture the properties of the data we trained each model on the whole data set with
restarts of and then simulated sequences of length We are particularly interested
in how well they measure pairwise dependence we concentrate on two measures log-odds ratio
for occurrence and Kendall?s measure of concordance for pairs when both stations had positive
amounts Both are shown in Both plots suggest that HMM-CI underestimates the pairwise
dependence for strongly dependent pairs as indicated by its trend to predict lower absolute values
for log-odds and concordance HMM-Tree estimating the dependence correctly mostly for strongly
dependent pairs as indicated by good prediction for high values but underestimating it for moderate dependence and HMM-TA performing the best for most pairs except for the ones with very
strong dependence
Acknowledgements
This work has been supported by the Alberta Ingenuity Fund through the AICML We thank Stephen
Charles CSIRO Australia for providing us with precipitation data

----------------------------------------------------------------

title: 5254-quantized-estimation-of-gaussian-sequence-models-in-euclidean-balls.pdf

Quantized Estimation of Gaussian Sequence Models
in Euclidean Balls
Yuancheng Zhu
John Lafferty
Department of Statistics
University of Chicago
Abstract
A central result in statistical theory is Pinsker?s theorem which characterizes the
minimax rate in the normal means model of nonparametric estimation In this
paper we present an extension to Pinsker?s theorem where estimation is carried
out under storage or communication constraints In particular we place limits
on the number of bits used to encode an estimator and analyze the excess risk
in terms of this constraint the signal size and the noise level We give sharp
upper and lower bounds for the case of a Euclidean ball which establishes the
Pareto-optimal minimax tradeoff between storage and risk in this setting
Introduction
Classical statistical theory studies the rate at which the error in an estimation problem decreases as
the sample size increases Methodology for a particular problem is developed to make estimation
efficient and lower bounds establish how quickly the error can decrease in principle Asymptotically
matching upper and lower bounds together yield the minimax rate of convergence
Rn inf sup R(fb
fb
This is the worst-case error in estimating an element of a model class where R(fb is the risk
or expected loss and fb is an estimator constructed on a data sample of size The corresponding
sample complexity of the estimation problem is min{n Rn
In the classical setting the infimum is over all estimators In contemporary settings it is increasingly
of interest to understand how error depends on computation For instance when the data are high
dimensional and the sample size is large constructing the estimator using standard methods may
be computationally prohibitive The use of heuristics and approximation algorithms may make
computation more efficient but it is important to understand the loss in statistical efficiency that this
incurs In the minimax framework this can be formulated by placing computational constraints on
the estimator
Rn Bn
inf
sup R(fb
fb:C(fb)?Bn
Here C(fb Bn indicates that the computation C(fb used to construct fb is required to fall within
a computational budget Bn Minimax lower bounds on the risk as a function of the computational budget thus determine a feasible region for computation-constrained estimation and a Paretooptimal tradeoff for error versus computation
One important measure of computation is the number of floating point operations or the running
time of an algorithm Chandrasekaran and Jordan have studied upper bounds for statistical
estimation with computational constraints of this form in the normal means model However useful
lower bounds are elusive This is due to the difficult nature of establishing tight lower bounds for
this model of computation in the polynomial hierarchy apart from any statistical concerns Another
important measure of computation is storage or the space used by a procedure In particular we
may wish to limit the number of bits used to represent our estimator fb The question then becomes
how does the excess risk depend on the budget Bn imposed on the number of bits C(fb used to
encode the estimator
This problem is naturally motivated by certain applications For instance the Kepler telescope
collects flux data for approximately stars The central statistical task is to estimate
the lightcurve of each star nonparametrically in order to denoise and detect planet transits If this
estimation is done on board the telescope the estimated function values may need to be sent back
to earth for further analysis To limit communication costs the estimates can be quantized The
fundamental question is what is lost in terms of statistical risk in quantizing the estimates Or in
a cloud computing environment such as Amazon a large number of nonparametric estimates
might be constructed over a cluster of compute nodes and then stored for example in Amazon
for later analysis To limit the storage costs which could dominate the compute costs in many
scenarios it is of interest to quantize the estimates How much is lost in terms of risk in principle
by using different levels of quantization
With such applications as motivation we address in this paper the problem of risk-storage tradeoffs
in the normal means model of nonparametric estimation The normal means model is a centerpiece
of nonparametric estimation It arises naturally when representing an estimator in terms of an orthogonal basis Our main result is a sharp characterization of the Pareto-optimal tradeoff
curve for quantized estimation of a normal means vector in the minimax sense We consider the
case of a Euclidean ball of unknown radius in Rn This case exhibits many of the key technical challenges that arise in nonparametric estimation over richer spaces including the Stein phenomenon
and the problem of adaptivity
As will be apparent to the reader the problem we consider is intimately related to classical rate
distortion theory Indeed our results require a marriage of minimax theory and rate distortion
ideas We thus build on the fundamental connection between function estimation and lossy source
coding that was elucidated in Donoho?s Wald Lectures This connection can also be used
to advantage for practical estimation schemes As we discuss further below recent advances on
computationally efficient near-optimal lossy compression using sparse regression algorithms
can perhaps be leveraged for quantized nonparametric estimation
In the following section we present relevant background and give a detailed statement of our results
In Section we sketch a proof of our main result on the excess risk for the Euclidean ball case
Section presents simulations to illustrate our theoretical analyses Section discusses related
work and outlines future directions that our results suggest
Background and problem formulation
In this section we briefly review the essential elements of rate-distortion theory and minimax theory
to establish notation We then state our main result which bridges these classical theories
In the rate-distortion setting we have a source that produces a sequence X2 Xn
each component of which is independent and identically distributed as The goal is to
transmit a realization from this sequence of random variables using a fixed number of bits in such
a way that results in the minimal expected distortion with respect to the original data Suppose
that we are allowed to use a total budget of nB bits so that the average number of bits per variable
is which is referred to as the rate To transmit or store the data the encoder describes the source
sequence by an index where
Rn 2nB
is the encoding function The nB-bit index is then transmitted or stored without loss A decoder
based on the index using a
when receiving or retrieving the data represents by an estimate
decoding function
2nB Rn
The image of the decoding function is called the codebook which is a discrete set in Rn with
cardinality no larger than 2nB The process is illustrated in Figure and variously referred to as
Xn
Encoder
Decoder
Xn
Encoder
Decoder
Figure Encoding and decoding process for lossy compression top and quantized estimation
bottom For quantized estimation the model mean vector is deterministic not random
source coding lossy compression or quantization We call the pair of encoding and decoding functions Qn an B)-rate distortion code We will also use Qn to denote the composition
of the two functions Qn
A distortion measure or a loss function is used to evaluate the performance of
the above coding and transmission process In this paper we will use the squared loss d(Xi
XP
The distortion between two sequences and is then defined by dn
the average of the per observation distortions We drop the subscript in when
it is clear from the context The distortion or risk for a B)-rate distortion code Qn is defined as
the expected loss Qn Denoting by Qn,B the set of all B)-rate distortion codes
the distortion rate function is defined as
lim inf
inf
Qn Qn,B
Qn
This distortion rate function depends on the rate as well as the source distribution For the
source according to the well-known rate distortion theorem
When is zero meaning no information gets encoded at all this bound becomes which is the
expected loss when each random variable is represented by its mean As approaches infinity the
distortion goes to zero
The previous discussion assumes the source random variables are independent and follow a common
distribution The goal is to minimize the expected distortion in the reconstruction of
after transmitting or storing the data under a communication constraint Now suppose that
ind
for
We assume the variance is known and the means are unknown Suppose fur we want to estimate
thermore that instead of trying to minimize the recovery distortion d(X
the means with a risk as small as possible but again using a budget of bits per index
Without the communication constraint this problem has been very well studied Let
bn bn denote an estimator of the true mean For a parameter space
Rn the minimax risk over is defined as
1X
bi
inf sup bn inf sup
bn
bn
For the ball of radius
1X
c2
Pinsker?s theorem gives the exact limiting form of the minimax risk
lim inf inf
sup
bn
bn
c2
c2
To impose a communication constraint we incorporate a variant of the source coding scheme described above into this minimax framework of estimation Define a B)-rate estimation code
Figure Our result establishes the Pareto-optimal
tradeoff in the nonparametric normal means problem for risk versus number of bits
Risk
c2
Curves for five signal sizes are shown c2
The noise level is With zero
bits the rate is c2 the highest point on the risk
curve The rate for large approaches the Pinsker
bound c2 c2
c2
c4
c2
Bits per symbol
Mn as a pair of encoding and decoding functions as before The encoding function
Rn 2nB is a mapping from observations to an index set The decoding
function is a mapping from indices to models Rn We write the composition of the encoder
and decoder as Mn which we call a quantized estimator Denoting by
Mn,B the set of all B)-rate estimation codes we then define the quantized minimax risk as
Rn
sup Mn
inf
Mn Mn,B
We will focus on the case where our parameter space is the ball defined in and write
Rn Rn
In this setting we let go to infinity and define the asymptotic quantized minimax risk as
lim inf Rn lim inf
inf
sup
Mn Mn,B
Mn
Qn Once again denoting
Note that we could estimate based on the quantized data
by Qn,B the set of all B)-rate distortion codes such an estimator is written Qn
Clearly if the decoding functions of Qn are injective then this formulation is equivalent The
quantized minimax risk is then expressed as
inf
Rn inf
sup
Qn Qn,B
The many normal means problem exhibits much of the complexity and subtlety of general nonparametric regression and density estimation problems It arises naturally in the estimation of a function
expressed in terms of an orthogonal function basis Our main result sharply characterizes the
excess risk that communication constraints impose on minimax estimation for
Main results
Our first result gives a lower bound on the exact quantized asymptotic risk in terms of and
Theorem For and the asymptotic minimax risk defined in satisfies
c2
c4
c2
This lower bound on the limiting minimax risk can be viewed as the usual minimax risk without
quantization plus an excess risk term due to quantization If we take to be zero the risk becomes
c2 which is obtained by estimating all of the means simply by zero On the other hand letting
we recover the minimax risk in Pinsker?s theorem This tradeoff is illustrated in Figure
The proof of the theorem is technical and we defer it to the supplementary material Here we sketch
the basic idea of the proof Suppose we are able to find a prior distribution on and a random
vector en such that for any B)-rate estimation code Mn the following holds
c4
c2
EX en
c2
c2
EX Mn
III
sup
EX Mn
Then taking an infimum over Mn Mn,B gives us the desired result In fact we can take
the prior on to be c2 In and the model becomes c2 and
Then according to Lemma inequality holds with en being the minimizer to the optimization
problem
min
p(?en
en
subject to I(X en nB
p(?en p(?en
The equality holds due to Lemma The inequality III can be shown by a limiting concentration
argument on the prior distribution which is included in the supplementary material
Lemma Suppose that X1 Xn are independent and generated by and
p(xi Suppose Mn is an B)-rate estimation code with risk Mn D. Then
the rate is lower bounded by the solution to the following problem
min
p(?en
subject to
I(X en
en
p(?en p(?en
The next lemma gives the solution to problem when we have c2 and
Lemma Suppose c2 and for For any random
vector en satisfying en and p(?en p(?en we have
c4
I(X en log
c2
c2
Combining the above two lemmas we obtain a lower bound of the risk assuming that follows the
prior distribution
Corollary Suppose Mn is a B)-rate estimation code for the source c2 and
then
Mn
c2
c4
c2
c2
An adaptive source coding method
We now present a source coding method which we will show attains the minimax lower bound
asymptotically with high probability
Suppose that the encoder is given a sequence of observations Xn and both the encoder
and the decoder know the radius of the ball in which the mean vector lies The steps of the
source coding method are outlined below
Step Generating codebooks The codebooks are distributed to both the encoder and the decoder
Generate codebook dc2 ne
Generate codebook which consists of 2nB random vectors from the uniform
distribution on the n-dimensional unit sphere
Step Encoding
Encode bb2
kXk
by arg min{|b2 bb2 b2 B}.
Encode by arg max{hX
by their corresponding indices using log c2 log nB bits
Step Transmit or store
Step Decoding
by the transmitted or stored indices
Recover
Estimate by
We make several remarks on this quantized estimation method
Remark The rate of this coding method is
log c2
log
2n
which is asymptotically bits
Remark The method is probabilistic the randomness comes from the construction of the codebook Denoting by the ensemble of such random quantizers there is then a natural onenB
to-one mapping between and
and we attach probability measure to
corresponding to the product uniform distribution on
nB
Remark The main idea behind this coding scheme is to encode the magnitude and the direction
of the observation vector separately in such a way that the procedure adapts to sources with different
norms of the mean vectors
Remark The computational complexity of this source coding method is exponential in Therefore like the Shannon random codebook this is a demonstration of the asymptotic achievability
of the lower bound rather than a practical scheme to be implemented We discuss possible
computationally efficient algorithms in Section
The following shows that with high probability this procedure will attain the desired lower bound
asymptotically
Theorem For a sequence of vectors
satisfying and as
b2
b4
log
Mn
b2
b2
for some constant that does not depend on but could possibly depend on and B). The
probability measure is with respect to both Mn and Rn
This theorem shows that the source coding method not only achieves the desired minimax lower
bound for the ball with high probability with respect to the random codebook and source distribution but also adapts to the true magnitude of the mean vector It agrees with the intuition that
the hardest mean vector to estimate lies on the boundary of the ball Based on Theorem we can
obtain a uniform high probability bound for mean vectors in the ball
Corollary For any sequence of vectors
satisfying and as
c2
c4
log
Mn
c2
c2
for some constant that does not depend on
We include the details of the proof of Theorem in the supplementary material which carefully
analyzes the three terms in the following decomposition of the loss function
Estimate
James?Stein
Index
Figure Comparison of the quantized estimates with different rates the James-Stein estimator and the true
mean vector The heights of the bars are the averaged estimates based on replicates Each large background
rectangle indicates the original mean component
bX
bX
bX
kb
bX
bX
A1
A2
A3
b2
b2
with bb2 kX Term A1 characterizes the quantization error Term
where
A2 does not involve the random codebook and is the loss of a type of James-Stein estimator The
cross term A3 vanishes as
Simulations
In this section we present a set of simulation results showing the empirical performance of the
proposed quantized estimation method Throughout the simulation we fix the noise level
while varying the other parameters and B.
First we show in Figure the effect of quantized estimation and compare it with the James-Stein
estimator Setting and we randomly generate a mean vector Rn with
c2 A random vector is then drawn from In and quantized estimates with rates
are calculated
for comparison we also compute the James-Stein estimator given
by JS kX We repeat this sampling and estimation procedure times and report
the averaged risk estimates in Figure We see that the quantized estimator essentially shrinks the
random vector towards zero With small rates the shrinkage is strong with all the estimates close
to zero Estimates with larger rates approach the James-Stein estimator
In our second set of simulations we choose from to reflect different signal-tonoise ratios and choose from For each combination of the values of and
we vary the dimension of the mean vector which is also the number of observations Given a set
of parameters and a mean vector is generated uniformly on the sphere k?n c2
and data are generated following the distribution In We quantize the data using the
source coding method and compute the mean squared error between the estimator and the true mean
vector The procedure is repeated times for each of the parameter combinations and the average
and standard deviation of the mean squared errors are recorded The results are shown in Figure
We see that as increases the average error decreases and approaches the theoretic lower bound in
Theorem Moreover the standard deviation of the mean squared errors also decreases confirming
the result of Theorem that the convergence is with high probability
Discussion and future work
In this paper we establish a sharp lower bound on the asymptotic minimax risk for quantized estimators of nonparametric normal means for the case of a Euclidean ball Similar techniques can be
MSE
Figure Mean squared errors and standard deviations of the quantized estimator versus for
different values of The horizontal dashed lines indicate the lower bounds
applied to the setting where the parameter space is an ellipsoid a2j c2 A
principal case of interest is the Sobolev ellipsoid of order where a2j as The
Sobolev ellipsoid arises naturally in nonparametric function estimation and is thus of great importance We leave this to future work
Donoho discusses the parallel between rate distortion theory and Pinsker?s work in his Wald Lectures Focusing on the case of the Sobolev space of order which we denote by Fm it
is shown that the Kolmogorov entropy Fm and the rate distortion function satisfy
Fm P(X Fm as This connects the worst-case minimax
analysis and least-favorable rate distortion function for the function class Another informationtheoretic formulation of minimax rates lies in the so-called le Cam equation
However both are different from the direction we pursue in this paper which is to impose communication constraints in minimax analysis
In other related work researchers in communications theory have studied estimation problems in
sensor networks under communication constraints Draper and Wornell obtain a result on the
so-called one-step problem for the quadratic-Gaussian case which is essentially the same as the
statement in our Corollary In fact they consider a similar setting but treat the mean vector as
random and generated independently from a known normal distribution In contrast we assume
a fixed but unknown mean vector and establish a minimax lower bound as well as an adaptive
source coding method that adapts to the fixed mean vector within the parameter space Zhang
also consider minimax bounds with communication constraints However the analysis in
is focused on distributed parametric estimation where the data are distributed between several
machines Information is shared between the machines in order to construct a parameter estimate
and constraints are placed on the amount of communication that is allowed
In addition to treating more general ellipsoids an important direction for future work is to design
computationally efficient quantized nonparametric estimators One possible method is to divide
the variables into smaller blocks and quantize them separately A more interesting and promising
approach is to adapt the recent work of Venkataramanan that uses sparse regression for
lossy compression We anticipate that with appropriate modifications this scheme can be applied to
quantized nonparametric estimation to yield practical algorithms trading off a worse error exponent
in the convergence rate to the optimal quantized minimax risk for reduced complexity encoders and
decoders
Acknowledgements
Research supported in part by NSF grant AFOSR grant ONR
grant and an Amazon AWS in Education Machine Learning Research grant The
authors thank Andrew Barron John Duchi and Alfred Hero for valuable comments on this work

----------------------------------------------------------------

