query sentence: usage of spectograms in astronomy 
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 2153-prediction-and-semantic-association.pdf

Prediction and Semantic Association
Thomas L. Griffiths Mark Steyvers
Department of Psychology
Stanford University Stanford CA
gruffydd,msteyver}@psych.stanford.edu
Abstract
We explore the consequences of viewing semantic association as
the result of attempting to predict the concepts likely to arise in a
particular context We argue that the success of existing accounts
of semantic representation comes as a result of indirectly addressing
this problem and show that a closer correspondence to human data
can be obtained by taking a probabilistic approach that explicitly
models the generative structure of language
Introduction
Many cognitive capacities such as memory and categorization can be analyzed as
systems for efficiently predicting aspects of an organism's environment Previously such analyses have been concerned with memory for facts or the properties
of objects where the prediction task involves identifying when those facts might
be needed again or what properties novel objects might possess However one of
the most challenging tasks people face is linguistic communication Engaging in
conversation or reading a passage of text requires retrieval of a variety of concepts
from memory in response to a stream of information This retrieval task can be
facilitated by predicting which concepts are likely to be needed from their context
having efficiently abstracted and stored the cues that support these predictions
In this paper we examine how understanding the problem of predicting words
from their context can provide insight into human semantic association exploring
the hypothesis that the association between words is at least partially affected
by their statistical relationships Several researchers have argued that semantic
association can be captured using high-dimensional spatial representations with
the most prominent such approach being Latent Semantic Analysis LSA We
will describe this procedure which indirectly addresses the prediction problem We
will then suggest an alternative approach which explicitly models the way language
is generated and show that this approach provides a better account of human word
association data than LSA although the two approaches are closely related The
great promise of this approach is that it illustrates how we might begin to relax some
of the strong assumptions about language made by many corpus-based methods
We will provide an example of this showing results from a generative model that
incorporates both sequential and contextual information
Latent Semantic Analysis
Latent Semantic Analysis addresses the prediction problem by capturing similarity
in word usage seeing a word suggests that we should expect to see other words
with similar usage patterns Given a corpus containing words and documents
the input to LSA is a word-document co-occurrence matrix in which fwd
corresponds to the frequency with which word occurred in document This
matrix is transformed to a matrix via some function involving the term frequency
fwd and its frequency across documents fw Many applications of LSA in cognitive
science use the transformation
gwd IOg{fwd
Hw
Wlog{W
d-l
logD
where Hw is the normalized entropy of the distribution over documents for each
word Singular value decomposition SVD is applied to to extract a lower
dimensional linear subspace that captures much of the variation in usage across
words The output of LSA is a vector for each word locating it in the derived
subspace The association between two words is typically assessed using the cosine of
the angle between their vectors a measure that appears to produce psychologically
accurate results on a variety of tasks For the tests presented in this paper
we ran LSA on a subset of the TASA corpus which contains excerpts from texts
encountered by children between first grade and the first year of college Our subset
used all documents and the words that occurred at least
ten times in the whole corpus with stop words removed From this we extracted a
dimensional representation which we will use throughout the paper
The topic model
Latent Semantic Analysis gives results that seem consistent with human judgments
and extracts information relevant to predicting words from their contexts although
it was not explicitly designed with prediction in mind This relationship suggests
that a closer correspondence to human data might be obtained by directly attempting to solve the prediction task In this section we outline an alternative approach
that involves learning a probabilistic model of the way language is generated One
generative model that has been used to outperform LSA on information retrieval
tasks views documents as being composed of sets of topics If we assume that
the words that occur in different documents are drawn from topics where each
topic is a probability distribution over words then we can model the distribution
over words in anyone document as a mixture of those topics
P(Wi
LP(Wil zi j)P(Zi
j=l
where Zi is a latent variable indicating the topic from which the ith word was drawn
and P(wilzi is the probability of the ith word under the jth topic The words
likely to be used in a new context can be determined by estimating the distribution
over topics for that context corresponding to
Intuitively P(wlz indicates which words are important to a topic while
is the prevalence of those topics within a document For example imagine a world
where the only topics of conversation are love and research We could then express
IThe dimensionality of the representation is an important parameter for both models in
this paper LSA performed best on the word association task with around dimensions
so we used the same dimensionality for the topic model
the probability distribution over words with two topics one relating to love and the
other to research The content of the topics would be reflected in P(wlz the
love topic would give high probability to words like JOY PLEASURE or HEART while
the research topic would give high probability to words like SCIENCE MATHEMATICS
or EXPERIMENT Whether a particular conversation concerns love research or
the love of research would depend upon its distribution over topics which
determines how these topics are mixed together in forming documents
Having defined a generative model learning topics becomes a statistical problem
The data consist of words where each Wi belongs to some document di as in a word-document co-occurrence matrix For each document we
have a multinomial distribution over the topics with parameters so for a
word in document P(Zi The jth topic is represented by a multinomial distribution over the words in the vocabulary with parameters so
P(wilzi To make predictions about new documents we need to assume
a prior distribution on the parameters Existing parameter estimation algorithms
make different assumptions about with varying results Here we present a
novel approach to inference in this model using Markov chain Monte Carlo with a
symmetric Dirichlet(a prior on for all documents and a symmetric Dirichlet(,B
prior on for all topics In this approach we do not need to explicitly represent
the model parameters we can integrate out and defining the model simply in
terms of the assignments of words to topics indicated by the Zi
Markov chain Monte Carlo is a procedure for obtaining samples from complicated
probability distributions allowing a Markov chain to converge to the taq~et distribution and then drawing samples from the states of that chain We
use Gibbs sampling where each state is an assignment of values to the variables
being sampled and the next state is reached by sequentially sampling all variables
from their distribution when conditioned on the current values of all other variables
and the data We will sample only the assignments of words to topics Zi. The
conditional posterior distribution for Zi is given by
Zi=)Zi wex
where
is the assignment of all
Zk
n(di a
n_i,j Ta
such that
and is the number
of words assigned to topic that are the same as n~L is the total number of
words assigned to topic n~J,j is the number of words from document assigned
to topic and is the total number of words in document all not counting
the assignment of the current word Wi. are free parameters that determine how
heavily these distributions are smoothed
We applied this algorithm to our subset of the TASA corpus which contains
word tokens Setting a we obtained samples of
topics with samples from each of runs with a burn-in of iterations and
a lag of iterations between samples Each sample consists of an assignment of
every word token to a topic giving a value to each Zi. A subset of the topics
found in a single sample are shown in Table For each sample we can compute
2Random numbers were generated with the Mersenne Twister which has an extremely
deep period For each run the initial state of the Markov chain was found using an
on-line version of Equation
FEEL
FEELINGS
FEELING
ANGRY
WAY
THINK
SHOW
FEELS
PEOPLE
FRIENDS
THINGS
MIGHT
HELP
HAPPY
FELT
LOVE
ANGER
BEING
WAYS
FEAR
MUSIC
BALL
GAME
TEAM
PLAY
DANCE
PLAYS
STAGE
PLAYED
BAND
AUDIENCE
MUSICAL
DANCING
RHYTHM
PLAYING
THEATER
DRUM
ACTORS
SHOW
BALLET
ACTOR
DRAMA
SONG
PLAY
BASEBALL
FOOTBALL
PLAYERS
GAMES
PLAYING
FIELD
PLAYED
PLAYER
COACH
BASKETBALL
SPORTS
HIT
BAT
TENNIS
TEAMS
SOCCER
SCIENCE
STUDY
SCIENTISTS
SCIENTIFIC
KNOWLEDGE
WORK
CHEMISTRY
RESEARCH
BIOLOGY
MATHEMATICS
LABORATORY
STUDYING
SCIENTIST
PHYSICS
FIELD
STUDIES
UNDERSTAND
STUDIED
SCIENCES
MANY
WORKERS
WORK
LABOR
JOBS
WORKING
WORKER
WAGES
FACTORY
JOB
WAGE
SKILLED
PAID
CONDITIONS
PAY
FORCE
MANY
HOURS
EMPLOYMENT
EMPLOYED
EMPLOYERS
FORCE
FORCES
MOT IO
BODY
GRAVITY
MASS
PULL
NEWTON
OBJECT
LAW
DIRECTION
MOVING
REST
FALL
ACTING
MOMENTUM
DISTANCE
GRAVITATIONAL
PUSH
VELOCITY
Table Each column shows the most probable words in one of the topics
obtained from a single sample The organization of the columns and use of boldface
displays the way in which polysemy is captured by the model
the posterior predictive distribution and posterior mean for
P(wl
P(wl
Iz
nj
Predicting word association
We used both LSA and the topic model to predict the association between pairs
of words comparing these results with human word association norms collected by
Nelson McEvoy and Schreiber These word association norms were established
by presenting a large number of participants with a cue word and asking them to
name an associated word in response A total of of the words in these norms
appear in the set of taken from the TASA corpus
Latent Semantic Analysis
In LSA the association between two words is usually measured using the cosine
of the angle between their vectors We ordered the associates of each word in the
norms by their frequencies making the first associate the word most commonly
given as a response to the cue For example the first associate of NEURON is BRAIN
We evaluated the cosine between each word and the other words in the norms
and then computed the rank of the cosine of each of the first ten associates or
all of the associates for words with less than ten The results are shown in Figure
Small ranks indicate better performance with a rank of one meaning that the
target word had the highest cosine The median rank of the first associate was 32
and LSA correctly predicted the first associate for of the words
The topic model
The probabilistic nature of the topic model makes it easy to predict the words likely
to occur in a particular context If we have seen word WI in a document then we
can determine the probability that word W2 occurs in that document by computing
w2IwI The generative model allows documents to contain multiple topics which
LSA cosine
LSA inner product
Topi model
l;r
lin
Associate number
Figure Performance of different methods of prediction on the word association
task Error bars show one standard error estimated with bootstrap samples
is extremely important to capturing the complexity of large collections of words
and computing the probability of complete documents However when comparing
individual words it is more effective to assume that they both come from a single
topic This assumption gives us
where we use Equation for P(wlz and is uniform consistent with the symmetric prior on and the subscript in Pi w2lwd indicates the restriction to a single
topic This estimate can be computed for each sample separately and an overall
estimate obtained by averaging over samples We computed Pi w2Iwi for the
words in the norms and then assessed the rank of the associates in the resulting
distribution using the same procedure as for LSA. The results are shown in Figure
The median rank for the first associate was 32 with of the first associates exactly correct The probabilistic model performed better than LSA with
the improved performance becoming more apparent for the later associates
Discussion
The central problem in modeling semantic association is capturing the interaction
between word frequency and similarity of word usage Word frequency is an important factor in a variety of cognitive tasks and one reason for its importance is its
predictive utility A higher observed frequency means that a word should be predicted to occur more often However this effect of frequency should be tempered by
the relationship between a word and its semantic context The success of the topic
model is a consequence of naturally combining frequency information with semantic
similarity when a word is very diagnostic of a small number of topics semantic
context is used in prediction Otherwise word frequency plays a larger role
The effect of word frequency in the topic model can be seen in the rank-order
correlation of the predicted ranks of the first associates with the ranks predicted
by word frequency alone which is In contrast the cosine is used in LSA
because it explicitly removes the effect of word frequency with the corresponding
correlation being The cosine is purely a measure of semantic similarity
which is useful in situations where word frequency is misleading such as in tests of
English fluency or other linguistic tasks but not necessarily consistent with human
performance This measure was based in the origins of LSA in information retrieval
but other measures that do incorporate word frequency have been used for modeling
psychological data We consider one such measure in the next section
Relating LSA and the topic model
The decomposition of a word-document co-occurrence matrix provided by the topic
model can be written in a matrix form similar to that of LSA. Given a worddocument co-occurrence matrix we can convert the columns into empirical estimates of the distribution over words in each document by dividing each column
by its sum Calling this matrix the topic model approximates it with the nonnegative matrix factorization where column of gives and column
of gives The inner product matrix ppT is proportional to the empirical estimate of the joint distribution over words P(WI We can write ppT OOT
corresponding to P(WI z"Z P(wIl zdP(W2Iz2)P(ZI,Z2 with OOT an empirical estimate of P(ZI The theoretical distribution for P(ZI is proportional to where I is the identity matrix so OOT should be close to diagonal
The single topic assumption removes the off-diagonal elements replacing OOT with
I to give PI Wl ex
By comparison LSA transforms to a matrix via Equation then the SVD
gives UDV for some low-rank diagonal D. The locations of the words along
the extracted dimensions are UD. If the column sums do not vary extensively
the empirical estimate of the joint distribution over words specified by the entries in
will be approximately ex GG The properties of the SVD guarantee
that XX the matrix of inner products among the word vectors is the best lowrank approximation to GG in terms of squared error The transformations in
Equation are intended to reduce the effects of word frequency in the resulting
representation making XX more similar to
We used the inner product between word vectors to predict the word association
norms exactly as for the cosine The results are shown in Figure The inner
product initially shows worse performance than the cosine with a median rank
of 34 for the first associate and exactly correct but performs better for later
associates The rank-order correlation with the predictions of word frequency for
the first associate was similar to that for the topic model The rankorder correlation between the ranks given by the inner product and the topic model
was while the cosine and the topic model correlate at The
inner product and PI w2lwd in the topic model seem to give quite similar results
despite being obtained by very different procedures This similarity is emphasized
by choosing to assess the models with separate ranks for each cue word since this
measure does not discriminate between joint and conditional probabilities While
the inner product is related to the joint probability of WI and PI w2lwd is a
conditional probability and thus allows reasonable comparisons of the probability
of W2 across choices of WI as well as having properties like asymmetry that are
exhibited by word association
syntax
HE
YOU
THEY
I
SHE
WE
IT
PEOPLE
EVERYONE
OTHERS
SCIENTISTS
SOMEONE
WHO
NOBODY
ONE
SOMETHING
ANYONE
EVERYBODY
SOME
THEN
ON
AT
INTO
FROM
WITH
THROUGH
OVER
AROUND
AGAINST
ACROSS
UPON
TOWARD
UNDER
ALONG
NEAR
BEHIND
OFF
ABOVE
DOWN
BEFORE
BE
MAKE
GET
HAVE
GO
TAKE
DO
FIND
USE
SEE
HELP
KEEP
GIVE
LOOK
COME
WORK
MOVE
LIVE
EAT
BECOME
semantics
SAID
ASKED
THOUGHT
TOLD
SAYS
MEANS
CALLED
CRIED
HOWS
ANSWERED
TELLS
REPLIED
SHOUTED
EXPLAINED
LAUGHED
MEANT
WROTE
SHOWED
BELIEVED
WHISPERED
MAP
NORTH
EARTH
SOUTH
POLE
MAPS
EQUATOR
WEST
LINES
EAST
AUSTRALIA
GLOBE
POLES
HEMISPHERE
LATITUDE
PLACES
LAND
WORLD
COMPASS
CONTINE NTS
DOCTOR
PATIENT
HEALTH
HOSPITAL
MEDICAL
CARE
PATIENTS
NURSE
DOCTORS
MEDICINE
NURSING
TREATMENT
NURSES
PHYSICIAN
HOSPITALS
DR
ICK
ASSISTANT
EMERGENCY
PRACTICE
Table Each column shows the most probable words in one of the 48 syntactic
states of the hidden Markov model four columns on the left or one of the
semantic topics two columns on the right obtained from a single sample
Exploring more complex generative models
The topic model which explicitly addresses the problem of predicting words from
their contexts seems to show a closer correspondence to human word association
than LSA. A major consequence of this analysis is the possibility that we may be
able to gain insight into some of the associative aspects of human semantic memory
by exploring statistical solutions to this prediction problem In particular it may
be possible to develop more sophisticated generative models of language that can
capture some of the important linguistic distinctions that influence our processing
of words The close relationship between LSA and the topic model makes the latter
a good starting point for an exploration of semantic association but perhaps the
greatest potential of the statistical approach is that it illustrates how we might go
about relaxing some of the strong assumptions made by both of these models
One such assumption is the treatment of a document as a bag of words in which
sequential information is irrelevant Semantic information is likely to influence only
a small subset of the words used in a particular context with the majority of the
words playing functional syntactic roles that are consistet across contexts Syntax is
just as important as semantics for predicting words and may be an effective means
of deciding if a word is context-dependent In a preliminary exploration of the
consequences of combining syntax and semantics in a generative model for language
we applied a simple model combining the syntactic structure of a hidden Markov
model HMM with the semantic structure of the topic model Specifically we used
a third-order HMM with states in which one state marked the start or end of
a sentence 48 states each emitted words from a different multinomial distribution
and one state emitted words from a document-dependent multinomial distribution
corresponding to the topic model with We estimated parameters for this
model using Gibbs sampling integrating out the parameters for both the HMM and
the topic model and sampling a state and a topic for each of the word
tokens in the corpus Some of the state and topic distributions from a single sample
after iterations are shown in Table The states of the HMM accurately picked
out many of the functional classes of English syntax while the state corresponding
to the topic model was used to capture the context-specific distributions over nouns
3This larger number is a result of including low frequency and stop words
Combining the topic model with the HMM seems to have advantages for both no
function words are absorbed into the topics and the HMM does not need to deal
with the context-specific variation in nouns The model also seems to do a good job
of generating topic-specific text we can clamp the distribution over topics to pick
out those of interest and then use the model to generate phrases For example we
can generate phrases on the topics of research the chief wicked selection of research
in the big months astronomy peered upon your scientist's door or anatomy
established with principles expected in biology language expressly wanted
that better vowel and the law but the crime had been severely polite and
confused or custody on enforcement rights is plentiful While these phrases
are somewhat nonsensical they are certainly topical
Conclusion
Viewing memory and categorization as systems involved in the efficient prediction
of an organism's environment can provide insight into these cognitive capacities
Likewise it is possible to learn about human semantic association by considering
the problem of predicting words from their contexts Latent Semantic Analysis
addresses this problem and provides a good account of human semantic association
Here we have shown that a closer correspondence to human data can be obtained
by taking a probabilistic approach that explicitly models the generative structure
of language consistent with the hypothesis that the association between words
reflects their probabilistic relationships The great promise of this approach is the
potential to explore how more sophisticated statistical models of language such as
those incorporating both syntax and semantics might help us understand cognition
Acknowledgments
This work was generously supported by the NTT Communications Sciences Laboratories
We used Mersenne Twister code written by Shawn Cokus and are grateful to Touchstone
Applied Science Associates for making available the TASA corpus and to Josh Tenenbaum
for extensive discussions on this topic

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5301-dynamic-rank-factor-model-for-text-streams.pdf

Dynamic Rank Factor Model for Text Streams
Shaobo Han Lin Esther Salazar and Lawrence Carin
Duke University Durham NC
shaobo.han lin.du esther.salazar lcarin}@duke.edu
Abstract
We propose a semi-parametric and dynamic rank factor model for topic modeling capable of discovering topic prevalence over time and learning contemporary multi-scale dependence structures providing topic and word correlations as a byproduct The high-dimensional and time-evolving ordinal/rank observations such as word counts after an arbitrary monotone transformation are
well accommodated through an underlying dynamic sparse factor model The
framework naturally admits heavy-tailed innovations capable of inferring abrupt
temporal jumps in the importance of topics Posterior inference is performed
through straightforward Gibbs sampling based on the forward-filtering backwardsampling algorithm Moreover an efficient data subsampling scheme is leveraged
to speed up inference on massive datasets The modeling framework is illustrated
on two real datasets the US State of the Union Address and the JSTOR collection
from Science
Introduction
Multivariate longitudinal ordinal/count data arise in many areas including economics opinion polls
text mining and social science research Due to the lack of discrete multivariate distributions supporting a rich enough correlation structure one popular choice in modeling correlated categorical
data employs the multivariate normal mixture of independent exponential family distributions after
appropriate transformations Examples include the logistic-normal model for compositional data
the Poisson log-normal model for correlated count data and the ordered probit model for
multivariate ordinal data Moreover a dynamic Bayesian extension of the generalized linear
model may be considered for capturing the temporal dependencies of non-Gaussian data such
as ordinal data In this general framework the observations are assumed to follow an exponential family distribution with natural parameter related to a conditionally Gaussian dynamic model
via a nonlinear transformation However these model specifications may still be too restrictive
in practice for the following reasons Observations are usually discrete non-negative and with
a massive number of zero values and unfortunately far from any standard parametric distributions
multinomial Poisson negative binomial and even their zero-inflated variants The number
of contemporaneous series can be large bringing difficulties in sharing/learning statistical strength
and in performing efficient computations iii The linear state evolution is not truly manifested after
a nonlinear transformation where positive shocks such as outliers and jumps are magnified and
negative shocks are suppressed hence handling temporal jumps up and down is a challenge for
the above models
We present a flexible semi-parametric Bayesian model termed dynamic rank factor model DRFM
that does not suffer these drawbacks We first reduce the effect of model misspecification by modeling the sampling distribution non-parametrically To do so we fit the observed data only after
some implicit monotone transformation learned automatically via the extended rank likelihood
Second instead of treating panels of time series as independent collections of variables we analyze
them jointly with the high-dimensional cross-sectional dependencies estimated via a latent factor
contributed equally
model Finally by avoiding nonlinear transformations both smooth transitions and sudden changes
jumps are better preserved in the state-space model using heavy-tailed innovations
The proposed model offers an alternative to both dynamic and correlated topic models
with additional modeling facility of word dependencies and improved ability to handle jumps It
also provides a semi-parametric Bayesian treatment of dynamic sparse factor model Further our
proposed framework is applicable in the analysis of multiple ordinal time series where the innovations follow either stationary Gaussian or heavy-tailed distributions
Dynamic Rank Factor Model
We perform analysis of multivariate ordinal time series In the most general sense such ordinal
variables indicate a ranking of responses in the sample space rather than a cardinal measure
Examples include real continuous variables discrete ordered variables with or without numerical
scales or more specially counts which can be viewed as discrete variables with integer numeric
scales Our goal is twofold discover the common trends that govern variations in observations
and extract interpretable patterns from the cross-sectional dependencies
Dependencies among multivariate non-normal variables may be induced through normally distributed latent variables Suppose we have ordinal-valued time series yp,t
The general framework contains three components
yp,t g(zp,t zp,t
where is the sampling distribution or marginal likelihood for the observations the latent variable zp,t is modeled by assumed to be Gaussian with underlying system parameters and
is the system equation representing Markovian dynamics for the time-evolving parameter
In order to gain more model flexibility and robustness against misspecification we propose a semiparametric Bayesian dynamic factor model for multiple ordinal time series analysis The model is
based on the extended rank likelihood allowing the transformation from the latent conditionally
Gaussian dynamic model to the multivariate observations treated non-parametrically
Extended rank likelihood There exist many approaches for dealing with ordinal data however they all have some restrictions For continuous variables the underlying normality assumption
could be easily violated without a carefully chosen deterministic transformation For discrete ordinal variables an ordered probit model with cut points becomes computationally expensive if the
number of categories is large For count variables a multinomial model requires finite support on
the integer values Poisson and negative binomial models lack flexibility from a practical viewpoint
and often lead to non-conjugacy when employing log-normal priors
Being aware of these issues a natural candidate for consideration is the ERL With appropriate
monotone transformations learned automatically from data it offers a unified framework for handling both continuous and discrete ordinal variables The ERL depends only on the ranks of the
observations zero values in observations are further restricted to have negative latent variables
zp,t D(Y p,t yp,t yp0 zp,t zp0 and zp,t if yp,t
In particular this offers a distribution-free approach with relaxed assumptions compared to parametric models such as Poisson log-normal It also avoids the burden of computing nuisance
parameters in the ordered probit model cut points The ERL has been utilized in Bayesian Gaussian
copula modeling to characterize the dependence of mixed data In a low-rank decomposition of the covariance matrix is further employed and efficient posterior sampling is developed in
The proposed work herein can be viewed as a dynamic extension of that framework
Latent sparse dynamic factor model
In the forthcoming text denotes a gamma distribution with shape parameter and rate
parameter TN(l,u denotes a univariate truncated normal distribution within the interval
and is the half-normal distribution that only has non-negative support
Assume where is usually a high-dimensional covariance matrix
To reduce the number of parameters we assume a low rank factor model decomposition of the
covariance matrix such that
st I
Common trends importance of topics are captured by a low-dimensional factor score parameter
st We assume autoregressive dynamics on sk,t with heavy-tailed innovations
sk,t TPBN(e
where follows the three-parameter beta mixture of normal TPBN(e distribution Parameter controls the peak around zero controls the heaviness on the tails and controls the
global sparsity with a half-Cauchy prior This prior encourages smooth transitions in general
while jumps are captured by the heavy tails The conjugate hierarchy may be equivalently represented as
h2
Truncated normal priors are employed on and assume
Note that the extended rank likelihood is scale-free therefore we do not need to include a redundant
intercept parameter in For the same reason we set I
Model identifiability issues Although
the covariance matrix is not identifiable the related
correlation matrix may be identified using the
parameter expansion technique Further the rank in the low-rank decomposition of is
also not unique For the purpose of brevity we do not explore this uncertainty here but the tools
developed in the Bayesian factor analysis literature 18 can be easily adopted
Identifiability is a key concern for factor analysis Conventionally for fixed a full-rank lowertriangular structure in ensures identifiability Unfortunately this assumption depends on the
ordering of variables As a solution we add nonnegative and sparseness constraints on the factor
loadings to alleviate the inherit ambiguity while also improving interpretability Also we add a
Procrustes post-processing step on the posterior samples to reduce this indeterminacy
The nonnegative and near sparseness constraints are imposed by the following hierarchy
lp,k lp,k up,k up,k
Integrating out lp,k and up,k we obtain a half-TPBN prior TPBN The columnwise shrinkage parameters enable factors to be of different sparsity levels We set hyperparameters a For weakly informative priors we set
Extension to handle multiple documents
nt
At each time point we may have a corpus of documents nt
nt where is a dimensional
observation vector and Nt denotes the number of documents at time The model presented in
Section is readily extended to handle this situation Specifically at each time point for each
nt
is
document nt the ERL representation for word count denoted by yp,t
nt
nt
yp,t
zp,t
nt Nt
where nt RP and is the vocabulary size We assume a latent factor model for nt such that
nt bnt nt nt I bnt st
where RP
is the topic-word loading matrix representing the topics as columns of
The factor score vector bnt RK is the topic usage for each document nt corresponding to locations in a low-dimensional RK space The other parts of the model remain unchanged The latent
trajectory represents the common trends for the topics Moreover through the forward filtering backward sampling FFBS algorithm we also obtain time-evolving topic correlation
matrices RK?K and word dependencies matrices RP offering a multi-scale graph
representation a useful tool for document visualization
Comparison with admixture topic models
Many topic models are unified in the admixture framework
wk,n
Base
Admix
where is the dimensional observation vector of word counts in the th document and denotes the vocabulary size Traditionally is generated from an admixture of base distributions wn
is the admixture weight topic proportion for document and is the canonical parameter word
distribution for topic which denotes the location of the kth topic on the dimensional simplex
For example latent Dirichlet allocation LDA assumes the base distribution to be multinomial
with wn Dir The correlated topic model CTM modifies the topic distribution with wn Logistic Normal The dynamic topic model DTM analyzes document collections in a known chronological order In order to incorporate the state space model both
the topic proportion and the word distribution are changed to logistic normal with isotropic covariance matrices wt Logistic Normal(wt?1 I and Logistic Normal(?k,t?1 vI
respectively To overcome the drawbacks of multinomial base spherical topic models assume
the von Mises-Fisher vMF distribution as its base distribution with lying on a
unit dimensional sphere Recently in the base and word distribution are both replaced with
Poisson Markov random fields MRFs which characterizes word dependencies
We present here a semi-parametric factor model formulation
P(y D(Y
sk,n
with defined as above RP
is a vector of nonnegative weights indicating the vocabulary usage in each individual topics and sn RK is the topic usage Note that the extended
rank likelihood does not depend on any assumptions about the data marginal distribution making it
appropriate for a broad class of ordinal-valued observations term frequency-inverse document
frequency tf-idf or rankings beyond word counts However the proposed model here is not an
admixture model as the topic usage is allowed to be either positive or negative
The DRFM framework has some appealing advantages It is more natural and convenient to incorporate with sparsity rank selection and state-space model it provides topic-correlations and
word-dependences as a byproduct and iii computationally this model is tractable and often leads
to locally conjugate posterior inference DRFM has limitations Since the marginal distributions
are of unspecified types objective criteria perplexity is not directly computable This makes
quantitative comparisons to other parametric baselines developed in the literature very difficult
Conjugate Posterior Inference
Let denote the set of parameters in basic model and let be
the augmented data from the ERL We use Gibbs sampling to approximate the joint posterior distribution R(Y The algorithm alternates between sampling R(Y and
R(Y reduced to The derivation of the Gibbs sampler is straightforward
and for brevity here we only highlight the sampling steps for and the forward filtering backward
sampling FFBS steps for the trajectory The Supplementary Material contains further details
for the inference
PK
Sampling zp,t p(zp,t R(Y TN[zp,t zp,t sk,t where zp,t
max{zp0 yp0 yp,t and zp,t min{zp0 yp0 yp,t
This conditional sampling scheme is widely used in In a novel Hamiltonian Monte
Carlo HMC approach has been developed recently for a Gaussian copula extended rank likelihood
model where ranking is only within each row of Z. This method simultaneously samples a column
vector of conditioned on other columns with higher computation but better mixing
Sampling st we have the state model st Qt and the observation model
st where A Qt diag I for
Forward Filtering beginning at with s0 I for all we
find the on-line posteriors at p(st mt where mt
and Qt AV AT
Amt?1
Backward Sampling starting from
mt Ve the backward smoothing density the
where
conditional distribution of given st is st
A Qt st A Qt A
There exist different variants of FFBS schemes for a detailed comparison the method we
choose here enjoys fast decay in autocorrelation and reduced computation time
For brevity we omit the dependencies on in notation
Time-evolving topic and word dependencies
We also have the backward recursion density at
Ve where
A Qt
ft and A Qt
We perform inference on the time-evolving topic dependences in using the posterior
covariances Ve with topic correlation matrices
and further obtain the time-evolving word dependencies capsuled in
with Ve I Essentially this can be viewed as a dynamic Gaussian copula model
where is a non-decreasing function of a univariate marginal
yp,t g(e
zp,t
likelihood and is the correlation matrix capturing the multivariate dependence
We obtain a posterior distribution for as a byproduct without having to estimate the nuisance
parameters in marginal likelihoods This decoupling strategy resembles the idea of copula
models
Accelerated MCMC via document subsampling
For large-scale datasets recent approaches efficiently reduce the computational load of Monte Carlo
Markov chain MCMC by data subsampling We borrow this idea of subsampling documents when considering a large corpora in our experiments we consider analysis of articles
in the magazine Science composed of articles from years to and a vocabulary
size In our model the augmented data nt nt Nt for each document is relatively
expensive to sample One simple method is random document sampling without replacement However by treating all likelihood contributions symmetrically this method leads to a highly inefficient
MCMC chain with poor mixing
Alternatively we adopt the probability proportional-to-size PSS sampling scheme in
sampling the documents with inclusion probability proportional to the likelihood contributions For
each MCMC iteration the sub-sampling procedure for documents at time is designed as follows
Step Given a small subset Vt Nt of chosen documents only sample dt for all
Vt and compute the augment log-likelihood contributions with integrated out Vt dt
where
I Note that only a K-dimensional matrix inversion is
I
required by using the Woodbury matrix inversion formula
Step Similar to we use a Gaussian process to predict the log-likelihood for
the remaining documents Vtc dt K(Vtc Vt K(Vt Vt Vt dt where is a Nt
Nt squared-exponential
kernel
which denotes the similarity of documents K(y
exp it jt Nt
ed wd d0 wd0
Step Calculate the inclusion probability wd exp dt Nt
Step Sampling the next subset Vt of pre-specified size Vt with inclusion probability
ed and
store it for the use of the next MCMC iteration
In practice this adaptive design allows MCMC to run more efficiently on a full dataset of large
scale often mitigating the need to do parallel MCMC implementation Future work could also consider nonparametric function estimation subject to monotonicity constraint Gaussian process
projections recently developed in
Experiments
Different from DTM the proposed model has the jumps directly at the level of the factor scores
no exponentiation or normalization needed and therefore it proved more effective in uncovering
jumps in factor scores over time Demonstrations of this phenomenon in a synthetic experiment are
detailed in the Supplementary Material In the following we present exploratory data analysis on
two real examples demonstrating the ability of the proposed model to infer temporal jumps in topic
importance and to infer correlations across topics and words
Case Study I State of the Union dataset
The State of the Union dataset contains the transcripts of US State of the Union addresses
from to We take each transcript as a document we have one document per year
After removing stop words and removing terms that occur fewer than times in one document and
less than times overall we have unique words The observation yp,t corresponds to
the frequency of word of the State of the Union transcript from year
We apply the proposed DRFM setting and learned topics To better understand the temporal
dynamic per topic six topics are selected and the posterior mean of their latent trajectories
are shown in Figure with also the top most probable words associated with each of the topics
A complete table with all learned topics and top words is provided in the Supplementary
Material The learned trajectory associated with every topic indicates different temporal patterns
across all the topics Clearly we can identify jumps associated with some key historical events For
instance for Topic we observe a positive jump in associated with the Mexican-American
war Topic 13 is related with the Spanish-American war of with a positive jump in that year
In Topic 24 we observe a positive jump in when the Panama Canal was officially opened
words Panana and canal are included In Topic 18 the positive jumps observed from to
seem to be associated with the creation of the State Children?s Health Insurance Program in
We note that the words for this topic are explicitly related with this issue Topic appears to
be related to banking the significant spike around appears to correspond to the Second Bank
of the United States which was allowed to go out of existence and end national banking that year
In Congress passed the National Banking Act which ended the free-banking period from
note the spike around in Topic
Topic
Topic 17
Topic 13
Topic 18
Topic 24
Topic#10
Mexico
Government
Texas
United
War
Mexican
Army
Territory
Country
Peace
Policy
Lands
Topic#13
Government
United
Islands
Commission
Island
Cuba
Spain
Act
General
Military
International
Officiers
Topic
Topic#24
United
Treaty
Isthmus
Public
Panama
Law
Territory
America
Canal
Service
Banks
Colombia
Topic#17
Jobs
Country
Tax
American
Economy
Deficit
Americans
Energy
Businesses
Health
Plan
Care
Topic#18
Children
America
Americans
Care
Tonight
Support
Century
Health
Working
Challenge
Security
Families
Topic#25
Government
Public
Banks
Bank
Currency
Money
United
Federal
American
National
Duty
Institutions
Figure State of the Union dataset Above Time evolving from to for six selected
topics The plotted values represent the posterior means Below Top most probable words
associated with the above topics
Our modeling framework is able to capture dynamic patterns of topics and word correlations To
illustrate this we select three years associated with some meaningful historical events and analyze
their corresponding topic and word correlations Figure first row shows graphs of the topic
correlation matrices in which the nodes represent topics and the edges indicate positive green and
negative red correlations we show correlations with absolute value larger than We notice
that Topics and 22 are positively correlated with those years Some of the most probable words
associated with each of them are increase united law and legislation for Topic and war
Mexico peace army enemy and military for Topic We also are interested in understanding
the time-varying correlation between words To do so and for the same years as before in Figure
second row we plot the dendrogram associated with the learned correlation matrix for words In
the plots different colors indicate highly correlated word clusters defined by cutting the branches off
the dendrogram Those figures reveal different sets of highly correlated words for different years By
Mexican-American War
T6
T5
T2
T5
T7
T9
T1
tre
ex cu asu
pe rr an
nd en
itu cys
re
dofisca
milllliars
illion
constitut
ion
union
president
tio om
nareeed
re ion
at rgy
nene
nt
lth
heeavelopmse
ogram
pr
ic
nom
eco
program
country
general
pow
er
auth
pub ority
gfoederlic
al
cict onv ernm
ize en
ns tio nt
ry
sp ct ritoy
ater at
tre ited
unationaistratio
dmin
a olicy
bject
su sent
pre
we ns tio
po itizneven
cco ain
sp
actrritor
teeaty
tr ited
unexican
texa
mi pea
for litar ce
ce
arm
ys
islacnuba
islandds
mexico
texas
mexican
treasury
bank
ncy
curreident
prespolicyn
a io al
istr tion on
min natitutinion
ad
co
pr
am subese
foer ricaject
eig
dep orrdaden
ar tm er
canant
cour
americat
americans
service
increase
nu
ju mber
ta ne
carx
job
ch
amoniigldre
am
erericat
ica
om
ed
frereetions
a rt
ou
ana tment
epa
er
ordde
tra ign
fore
american
country
general
autho
po
ri
pubwer ty
gfoederlic
al
cocitiz ernm
nv ens
nt
en
tio
ry
sp ctrritoy
ate at
tre ited
unx
taare
bs
jo ren
childght
toni
pm
enhea ent
erglth
pe a
milit acer
a
forcery
armys
islands
cuba
island
mexxicaos
te an
ic
mex gold
tes
noilvers
ndve
boser
re
secretary
attentpioort
re ne
ju er
se
num
rea aw
inc essor
sin ab a
bu
servicey
secrettiaron
atterneporet
jun er
mb
nu rearsamic
incrog omms
ra
og
ec ro
secretary
attentpio
rt
re
ent
pres ject
sub laws
es
sin bo al
bu lation ion
na na
de
ve
lo
om
ed
frereetions
a rt
ou
ana tment
epa
er
ordde
tra ign
fore
american
country
general
autho
pub rity
li
fe
de
ov ral
re ern
co nio side men
ns nt
tit
ut
io
T1
T9
nation
labor
bu
lawsiness
ta
jobare
tocnhilds
a
am ghren
er ric
ic an
a
T1
bil illio
wlio
mil eacearn
forcitary
armes
island
cubas
island
mexico
T2
go
silotesld
resbonder
trea erves
sury
anks
curren
cy
policy
administration
ent
pm
develo ra
og
pr
omic
ecoongramsh
pr earltgy
en turesal
nd fis llars
pe
do
ex
T9
T8
T7
T4
T2
T3
T4
T8
T3
T8
T3
T5
T6
T6
T4
Iraq War
service
billion
fis
ex cal
millipenditure
lla
es rs
si on erve
gonotlverds
ld
T7
Economic Depression
Figure State of the Union dataset First row Inferred correlations between topics for some
specific years associated with some meaningful historical events Green edges indicate positive
correlations and red edges indicate negative correlations Second row Learned dendrogram based
upon the correlation matrix between the top words associated with each topic we display
unique words in total
inspecting all the words correlation we noticed that the set of words government federal public
power authority general country are highly correlated across the whole period
Case Study Analysis of Science dataset
We analyze a collection of scientific documents from the JSTOR Science journal This dataset
contains a collection of documents from to with approximately
documents per year After removing terms that occurred fewer than times the total vocabulary
size is We learn topics from the inferred posterior distribution for brevity and
simplicity we only show of them We handle about documents per iteration subsampling
rate Table shows the selected topics and the top most probable words associated with
each of them By inspection we notice that those topics are related with specific fields in science
For instance Topic is more related to scientific research Topic to natural resources and
Topic to genetics Figure shows the time-varying trend for some specific words zbp,1:T which
reveals the importance of those words across time Finally Figure shows the correlation between
the selected topics For instance in and topic related to mouse cells human
transgenic and topic 17 related to virus rna tumor infection are highly correlated
DNA
RNA
Gene
Cancer
Patients
Nuclear
Astronomy
Psychology
Brain
Figure Science dataset the inferred latent trend for variable zbp,1:T associated with words
T8
T9
T3
T7
T2
T6
T5
T9
T4
T5
T3
T4
T8
T9
T1
T1
T6
T5
T2
T4
T2
T7
T3
T6
T8
T1
T7
Figure Science dataset Inferred correlations between topics for some specific years Green
edges indicate positive correlations and red edges indicate negative correlations
Table Selected topics associated with the analysis of the Science dataset and top most
probable words
Topic#1
cells
cell
normal
two
growth
development
tissue
body
egg
blood
Topic#11
system
nuclear
new
systems
power
cost
computer
fuel
coal
plant
Topic#2
research
national
government
support
federal
development
new
program
scientific
basic
Topic#12
energy
theory
temperature
radiation
atoms
surface
atomic
mass
atom
time
Topic#3
field
magnetic
solar
energy
spin
state
electron
quantum
temperature
current
Topic#13
association
science
meeting
university
american
society
section
president
committee
secretary
Topic#4
animals
brain
neurons
activity
response
rats
control
fig
effects
days
Topic#14
protein
proteins
cell
membrane
amino
sequence
binding
acid
residues
sequences
Topic#5
energy
oil
percent
production
fuel
total
growth
states
electricity
coal
Topic#15
human
genome
sequence
chromosome
gene
genes
map
data
sequences
genetic
Topic#6
university
professor
college
president
department
research
institute
director
society
school
Topic#16
professor
university
society
department
college
president
director
american
appointed
medical
Topic#7
science
scientific
new
scientists
human
men
sciences
knowledge
meeting
work
Topic#17
virus
rna
viruses
particles
tumor
mice
disease
viral
human
infection
Topic#8
work
research
scientific
laboratory
made
university
results
science
survey
department
Topic#18
energy
electron
state
fig
two
structure
reaction
laser
high
temperature
Topic#9
mice
mouse
type
wild
fig
cells
human
transgenic
animals
mutant
Topic#19
stars
mass
star
temperature
solar
gas
data
density
surface
galaxies
Topic#10
water
surface
temperature
soil
pressure
sea
plants
solution
plant
air
Topic#20
rna
fig
mrna
protein
site
sequence
splicing
synthesis
trna
rnas
Discussion
We have proposed a DRFM framework that could be applied to a broad class of applications such
as dynamic topic model for the analysis of time-stamped document collections joint analysis of multiple time series with ordinal valued observations and iii multivariate ordinal dynamic
factor analysis or dynamic copula analysis for mixed type of data The proposed model is a semiparametric methodology which offers modeling flexibilities and reduces the effect of model misspecification However as the marginal likelihood is distribution-free we could not calculate the
model evidence or other evaluation metrics based on it held-out likelihood As a consequence
we are lack of objective evaluation criteria which allow us to perform formal model comparisons
In our proposed setting we are able to perform either retrospective analysis or multi-step ahead
forecasting using the recursive equations derived in the FFBS algorithm Finally our inference
framework is easily adaptable for using sequential Monte Carlo SMC methods allowing online learning
Acknowledgments
The research reported here was funded in part by ARO DARPA DOE NGA and ONR. The authors
are grateful to Jonas Wallin Lund University Sweden for providing efficient package on simulation
of the GIG distribution

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4618-cprl-an-extension-of-compressive-sensing-to-the-phase-retrieval-problem.pdf

CPRL An Extension of Compressive Sensing to the
Phase Retrieval Problem
Henrik Ohlsson
Division of Automatic Control Department of Electrical Engineering
Link?oping University Sweden
Department of Electrical Engineering and Computer Sciences
University of California at Berkeley CA USA
ohlsson@eecs.berkeley.edu
Allen Y. Yang
Department of Electrical Engineering and Computer Sciences
University of California at Berkeley CA USA
Roy Dong
Department of Electrical Engineering and Computer Sciences
University of California at Berkeley CA USA
S. Shankar Sastry
Department of Electrical Engineering and Computer Sciences
University of California at Berkeley CA USA
Abstract
While compressive sensing has been one of the most vibrant research fields in
the past few years most development only applies to linear models This limits its
application in many areas where CS could make a difference This paper presents
a novel extension of CS to the phase retrieval problem where intensity measurements of a linear system are used to recover a complex sparse signal We propose
a novel solution using a lifting technique CPRL which relaxes the NP-hard
problem to a nonsmooth semidefinite program Our analysis shows that CPRL
inherits many desirable properties from CS such as guarantees for exact recovery
We further provide scalable numerical solvers to accelerate its implementation
Introduction
In the area of X-ray imaging phase retrieval refers to the problem of recovering a complex
multivariate signal from the squared magnitude of its Fourier transform Existing sensor devices for
collecting X-ray images are only sensitive to signal intensities but not the phases However it is
very important to be able to recover the missing phase information as it reveals finer structures of
the subjects than using the intensities alone The PR problem also has broader applications and has
been studied extensively in biology physics chemistry astronomy and more recent nanosciences
18 24
Mathematically PR can be formulated using a linear system Ax CN where the matrix
A may represent the Fourier transform or other more general linear transforms If the complex
measurements are available and the matrix A is assumed given it is well known that the leastsquares solution recovers the model parameter that minimizes the squared estimation error
ky Axk22 In PR we assume that the phase of the coefficients of is omitted and only the squared
magnitude of the output is observed
bi
where AH aN Cn?N yN CN and AH denotes the Hermitian
transpose of A.
Inspired by the emerging theory of compressive sensing and a lifting technique recently
proposed for PR we study the PR problem with a more restricted assumption that the
model parameter is sparse and the number of observations are too few for to have a unique
solution and in some cases even fewer measurements than the number of unknowns The problem
is known as compressive phase retrieval CPR 27 In many X-ray imaging applications
for instance if the complex source signal is indeed sparse under a proper basis CPR provides a
viable solution to exactly recover the signal while collecting much fewer measurements than the
traditional non-compressive solutions
Clearly the PR problem and its CPR extension are much more challenging than the LS problem as
the phase of is lost while only its squared magnitude is available For starters it is important to note
that the setup naturally leads to ambiguous solutions regardless whether the original linear model is
overdetermined or not For example if Cn is a solution to Ax then any multiplication of
and a scalar leads to the same squared output As mentioned in when the
dictionary A represents the unitary discrete Fourier transform the ambiguities may represent
time-reversed or time-shifted solutions of the ground-truth signal Hence these global ambiguities
are considered acceptable in PR applications In this paper when we talk about a unique solution to
PR it is indeed a representative of a family of solutions up to a global phase ambiguity
Contributions
The main contribution of the paper is a convex formulation of the CPR problem Using the lifting technique the NP-hard problem is relaxed as a semidefinite program We will briefly
summarize several theoretical bounds for guaranteed recovery of the complex input signal which
is presented in full detail in our technical report Built on the assurance of the guaranteed
recovery we will focus on the development of a novel scalable implementation of CPR based on
the alternating direction method of multipliers ADMM approach The ADMM implementation
provides a means to apply CS ideas to PR applications high-impact nanoscale X-ray imaging
In the experiment we will present a comprehensive comparison of the new algorithm with the traditional interior-point method other state-of-the-art sparse optimization techniques and a greedy
algorithm proposed in In high-dimensional complex domain the ADMM algorithm demonstrates superior performance in our simulated examples and real images Finally the paper also
provides practical guidelines to practitioners at large working on other similar nonsmooth SDP applications To aid peer evaluation the source code of all the algorithms have been made available at
http://www.rt.isy.liu.se/?ohlsson
Compressive Phase Retrieval via Lifting CPRL
Since is nonlinear in the unknown measurements are in general needed for a unique
solution When the number of measurements are fewer than necessary for such a unique solution
additional assumptions are needed as regularization to select one of the solutions In classical CS the
ability to find the sparsest solution to a linear equation system enables reconstruction of signals from
far fewer measurements than previously thought possible Classical CS is however only applicable
to systems with linear relations between measurements and unknowns To extend classical CS to the
nonlinear PR problem we seek the sparsest solution satisfying
min kxk0
subj to aH
xx
with the square acting element-wise and bN RN As the counting norm k0 is
not a convex function following the norm relaxation in CS can be relaxed as
min kxk1
subj to aH
xx
Note that is still not a convex program as its equality constraint is not a linear equation In the
literature a lifting technique has been extensively used to reframe problems such as to a standard
form in SDP such as in Sparse PCA More specifically given the ground-truth signal Cn
n?n
let X0 xH
be an induced rank-1 semidefinite matrix Then can be reformulated
into
minX0 kXk1
subj to
rank(X bi aH
Xai N.
This is of course still a nonconvex problem due to the rank constraint The lifting approach addresses
this issue by replacing rank(X with For a positive-semidefinite matrix Tr(X is equal to
the sum of the eigenvalues of the norm on a vector containing all eigenvalues of X). This
leads to the nonsmooth SDP
minX0 Tr(X kXk1
subj to bi Tr(?i
n?n
where we further denote aH
and is a design parameter Finally the estimate
of can be found by computing the rank-1 decomposition of via singular value decomposition
We refer to the approach as compressive phase retrieval via lifting CPRL
Consider now the case that the measurements are contaminated by data noise In a linear model
bounded random noise typically affects the output of the system as Ax where CN is a
noise term with bounded norm kek2 However in phase retrieval we follow closely a more
special noise model used in
bi ei
This nonstandard model avoids the need to calculate the squared magnitude output with the
added noise term More importantly in most practical phase retrieval applications measurement
noise is introduced when the squared magnitudes or intensities of the linear system are measured on
the sensing device but not itself Accordingly we denote a linear operator of as
Cn?n RN
which measures the noise-free squared output Then the approximate CPR problem with bounded
norm error model can be solved by the following nonsmooth SDP program
minX0 Tr(X kXk1
subj to kB(X bk2
Due to the machine rounding error in general a nonzero should be always assumed and in its
termination condition during the optimization The estimate of just as in noise free case can
finally be found by computing the rank-1 decomposition of via singular value decomposition
We refer to the method as approximate CPRL
Theoretical Analysis
This section highlights some of the analysis results derived for CPRL The proofs of these results are
available in the technical report The analysis follows that of CS and is inspired by derivations
given in In order to state some theoretical properties for CPRL we need a
generalization of the restricted isometry property
for all
Definition RIP A linear operator as defined in is k)-RIP if kB(X)k
kXk2
kXk0 and
We can now state the following theorem
Theorem Recoverability/Uniqueness Let be a 2kX k0 RIP linear operator with
be the sparsest solution to If satisfies B(X rank{X
and let
then is unique and
We can also give a bound on the sparsity of
be the
k0 from above Let
be the sparsest solution to and let
Theorem Bound on
xx
k0
solution of CPRL If has rank then kXk0
xx
The following result now holds trivially
In this paper kXk1 for a matrix denotes the entry-wise norm and kXk2 denotes the Frobenius norm
be the sparsest solution to The solution
Corollary Guaranteed recovery using RIP Let
RIP with
of CPRL is equal to xx if it has rank and is 2kXk
can not be guaranteed the following bound becomes useful
If
Let and assume to be a 2k)-RIP linear
Theorem Bound on kX Xk
operator Let be any matrix sparse or dense satisfying B(X rank{X
be the CPRL solution and form Xs from by setting all but the largest elements to
let
zero Then
k1
kX
kX Xs k1
with
Given the RIP analysis it may be the case that the linear operator does not well satisfy the RIP
property defined in Definition as pointed out in In these cases RIP-1 maybe considered
for all matrices
Definition A linear operator is if kB(X)k
kXk1
and kXk0
Theorems and Corollary all hold with RIP replaced by RIP-1 and are not restated in detail
here Instead we summarize the most important property in the following theorem
be the sparsest solution to The
Theorem Upper bound recoverability through Let
is equal to
with
if it has rank and is 2kXk
solution of CPRL
The RIP type of argument may be difficult to check for a given matrix and are more useful for
claiming results for classes of matrices/linear operators For instance it has been shown that random Gaussian matrices satisfy the RIP with high probability However given realization of a random Gaussian matrix it is indeed difficult to check if it actually satisfies the RIP. Two alternative
arguments are spark and mutual coherence The spark condition usually gives tighter
bounds but is known to be difficult to compute as well On the other hand mutual coherence may
give less tight bounds but is more tractable We will focus on mutual coherence which is defined as
Definition Mutual coherence For a matrix A define the mutual coherence as
Ha
max1?i,j?n,i6=j ka|a
kaj
By an abuse of notation let be the matrix satisfying BX with being the vectorized
version of We are now ready to state the following theorem
be the sparsest solution to The solution
Theorem Recovery using mutual coherence Let
is equal to
if it has rank and kXk
of CPRL
Numerical Implementation via ADMM
In addition to the above analysis of guaranteed recovery properties a critical issue for practitioners is
the availability of efficient numerical solvers Several numerical solvers used in CS may be applied
to solve nonsmooth SDPs which include interior-point methods used in CVX gradient
projection methods and augmented Lagrangian methods ALM However interior-point
methods are known to scale badly to moderate-sized convex problems in general Gradient projection methods also fail to meaningfully accelerate the CPRL implementation due to the complexity
of the projection operator Alternatively nonsmooth SDPs can be solved by ALM. However the
augmented primal and dual objective functions are still complex SDPs which are equally expensive
to solve in each iteration In summary as we will demonstrate in Section CPRL as a nonsmooth
complex SDP is categorically more expensive to solve compared to the linear programs underlying
CS and the task exceeds the capability of many popular sparse optimization techniques
In this paper we propose a novel solver to the nonsmooth SDP underlying CPRL via the alternating
directions method of multipliers ADMM see for instance and Sec. technique The
motivation to use ADMM are two-fold It scales well to large data sets It is known for its fast
convergence There are also a number of strong convergence results which further motivates the
choice
To set the stage for ADMM rewrite to the equivalent SDP
minX1 f1 f2
subj to X1
X2
where
if
Tr(X if bi
kZk1
f2
f1
otherwise
otherwise
The update rules of ADMM now lead to the following
Xil+1
Yil+1
arg minX fi Tr(Yil kX
P2
arg minZ Tr(Yil kXil+1
Yil
where Yi are constrained to stay in the domain of Hermitian matrices Each of these steps has
a tractable calculation However the Yi and variables are complex-valued and as most of
the optimization literature deals with real-valued vectors and symmetric matrices we will emphasize
differences between the real case and complex case After some simple manipulations we have
argminX kX
I+Y1l
subj to bi Tr(?i N.
Assuming that a feasible solution exists and defining A as the projection onto the convex set given
This optimization problem has a
by the linear constraints the solution is A I+Y
closed-form solution converting the matrix optimization problem in into an equivalent vector
optimization problem yields a problem of the form minx subj to Ax. The answer
is given by the pseudo-inverse of A which can be precomputed This complex-valued problem can
be solved by converting the linear constraint in Hermitian matrices into an equivalent constraint on
real-valued vectors This conversion is done by noting that for Hermitian matrices A
Pn
Pn Pn
Pn Pn
hA Bi
Tr(AB j=1PAij Bij Aii Bii Aij Bij Aij Bij
Aii Bii real(Aij real(Bij imag(Aij imag(Bij
So
if we define the vector A as an vector such that
its elements are Aii for
real(Aij for and imag(Aij for
and similarly define then we can see that hA Bi hAv This turns the constraint
bi Tr(?i into one of the form vN where each vi is in Rn
Thus for this subproblem the memory usage scales linearly with the number of measurements
and quadratically with the dimension of the data Next argminX0 kX
SD where SD denotes the projection onto the positive-semidefinite cone which
can easily be obtained via eigenvalue decomposition This holds for real-valued and complex-valued
P2
Xil+1 and similarly Then the update rule
Hermitian matrices Finally let
can be written
argminZ kZk1
kZ
soft(X
We note that the soft operator in the complex domain must be coded with care One does not simply
check the sign of the difference as in the real case but rather the magnitude of the complex number
if
soft(x
otherwise
where is a positive real number Setting the Hermitian matrices Xil Zil Yil can now be
iteratively computed using the ADMM iterations The stopping criterion of the algorithm is
given by
krl nabs rel max(kX kZ
abs
rel
ksl nabs rel kY
where are algorithm parameters set to and and are the primal and dual residuals
given by rl X2l sl We also update according
to the rule discussed in
if krl ksl
incr
if ksl krl
decr
otherwise
where incr decr and are algorithm parameters Values commonly used are and incr
decr
Experiment
The experiments in this section are chosen to illustrate the computational performance and scalability of CPRL Being one of the first papers addressing the CPR problem existing methods available
for comparison are limited For the CPR problem to the authors best knowledge the only methods
developed are the greedy algorithms presented in 27 and GCPRL The method proposed in handles CPR but is only tailored to random 2D Fourier samples from a 2D array and it
is extremely sensitive to initialization In fact it would fail to converge in our scenarios of interest
formulates the CPR problem as a nonconvex optimization problem that can be solved by solving a series of convex problems proposes to alternate between fit the estimate to measurements
and thresholding GCPRL which stands for greedy CPRL is a new greedy approximate algorithm
tailored to the lifting technique in The algorithm draws inspiration from the matching-pursuit algorithm In each iteration the algorithm adds a new nonzero component of that minimizes
the CPRL objective function the most We have observed that if the number of nonzero elements in
is expected to be low the algorithm can successfully recover the ground-truth sparse signal while
consuming less time compared to interior-point methods for the original SDP.2 In general greedy
algorithms for solving CPR problems work well when a good guess for the true solution is available
are often computationally efficient but lack theoretical recovery guarantees We also want to point
out that CPRL becomes a special case in a more general framework that extends CS to nonlinear
systems In general nonlinear CS can be solved locally by greedy simplex pursuit algorithms Its instantiation in PR is the GCPRL algorithm However the key benefit of developing the
SDP solution for PR in this paper is that the global convergence can be guaranteed
In this section we will compare implementations of CPRL using the interior-point method used by
CVX and ADMM with the design parameter choice recommended in incr decr
will be used in all experiments We will also compare the results to GCPRL and the PR
algorithm PhaseLift The former is a greedy approximate solution while the latter does not
enforce sparsity and is obtained by setting in CPRL
In terms of the scale of the problem the largest problem we have tested is on a image and is
100-sparse in the Fourier domain with measurements Our experiment is conducted on an IBM
M3 server with two Xeon processors cores each at L3 cache and
of RAM. The execution for recovering one instance takes approximately 36 hours to finish in
MATLAB environment comprising of several tens of thousands of iterations The average memory
usage is GB.
A simple simulation
In this example we consider a simple CPR problem to illustrate the differences between CPRL
GCPRL and PhaseLift We also compare computational speed for solving the CPR problem and
illustrate the theoretical bounds derived in Section Let be a 2-sparse complex signal
A RF where is the Fourier transform matrix and a random projection
matrix generated by sampling a unit complex Gaussian and let the measurements satisfy the
PR relation The left plot of Figure gives the recovered signal using CPRL GCPRL and
PhaseLift As seen CPRL and GCPRL correctly identify the two nonzero elements in while
PhaseLift fails to identify the true signal and gives a dense estimate These results are rather typical
the MCMC simulation in For very sparse examples like this one CPRL and GCPRL
often both succeed in finding the ground truth even though we have twice as many unknowns
as measurements PhaseLift on the other side does not favor sparse solutions and would need
considerably more measurements to recover the 2-sparse signal The middle plot of Figure shows
the computational time needed to solve the nonsmooth SDP of CPRL using CVX ADMM and
GCPRL It shows that ADMM is the fastest and that GCPRL outperforms CVX. The right plot of
Figure shows the mutual coherence bound for a number of different and
A RF Cn?n the Fourier transform matrix and CN a random projection
satisfies kXk
matrix This is of interest since Theorem states that when the CRPL solution
where
is the sparsest solution to From
and has rank then
We have also tested an off-the-shelf toolbox that solves convex cone problems called TFOCS Unfortunately TFOCS cannot be applied directly to solving the nonsmooth SDP in CPRL
has rank and only a single nonzero
the plot it can be concluded that if the CPRL solution
We also
component for a choice of Theorem guarantees that
observe that Theorem is conservative since we previously saw that nonzero components could
be recovered correctly for 64 and 32 In fact numerical simulation can be used to show
that suffices to recover the ground truth in 95 out of runs
PhaseLift
CPRL/GCPRL
90
70
xxH
CPRL CVX
GCPRL
CPRL ADMM
time
Figure Left The magnitude of the estimated signal provided by CPRL GCPRL and PhaseLift
plotted against time for ADMM gray line GCPRL solid
Xk
Middle The residual
xx
black line and CVX dashed black line Right A contour plot of the quantity
is taken as the average over realizations of the data
Compressive sampling and PR
One of the motivations of presented work and CPRL is that it enables compressive sensing for PR
problems To illustrate this consider the complex image in Figure Left To measure the
image we could measure each pixel one-by-one This would require us to sample times What
CS proposes is to measure linear combinations of samples rather than individual pixels It has been
shown that the original image can be recovered from far fewer samples than the total number of
pixels in the image The gain using CS is hence that fewer samples are needed However traditional
CS only discuss linear relations between measurements and unknowns
To extend CS to PR applications consider again the complex image in Figure Left and assume that
we only can measure intensities or intensities of linear combinations of pixels Let CN
capture how intensity measurements are formed from linear combinations of pixels in the image
is a vectorized version of the image An essential part in CS is also to find a dictionary
possibly overcomplete in which the image can be represented using only a few basis images For
classical CS applications dictionaries have been derived For applying CS to the PR applications
dictionaries are needed and a topic for future research We will use a 2D inverse Fourier transform
dictionary in our example and arrange the basis vectors as columns in
If we choose and generate by sampling from a unit Gaussian distribution and set
A RF CPRL recovers exactly the true image This is rather remarkable since the PR relation
is nonlinear in the unknown and measurements are in general needed for a unique
solution If we instead sample the intensity of each pixel one-by-one neither CPRL or PhaseLift
recover the true image If we set A and do not care about finding a dictionary we can use
a classical PR algorithm to recover the true image If PhaseLift is used measurements
are sufficient to recover the true image The main reasons for the low number of samples needed in
CPRL is that we managed to find a good dictionary basis images were needed to recover the true
image and CPRL?s ability to recover the sparsest solution In fact setting A RF PhaseLift still
needs measurements to recover the true solution
The Shepp-Logan phantom
In this last example we again consider the recovery of complex valued images from random samples The motivation is twofold Firstly it illustrates the scalability of the ADMM implementation
In fact ADMM has to be used in this experiment as CVX cannot handle the CPRL problem in this
scale Secondly it illustrates that CPRL can provide approximate solutions that are visually close
to the ground-truth images Consider now the image in Figure Middle Left This SheppLogan phantom has a 2D Fourier transform with nonzero coefficients We generate linear
combinations of pixels as in the previous example and square the measurements and then apply
CPRL and PhaseLift with a 2D Fourier dictionary The middel image in Figure shows the recovered result using PhaseLift with the second image from the right shows the recovered
result using CPRL with the same number and the right image is the recovered result using
CPRL with The number of measurements with respect to the sparsity in is too low for
both CPRL and PhaseLift to perfectly recover However CPRL provides a much better approximation and outperforms PhaseLift visually even though it uses considerably fewer measurements
18
18
Figure Left Absolute value of the 2D inverse Fourier transform of used in the experiment in Section Middle Left Ground truth for the experiment in Section Middle
Recovered result using PhaseLift with Middle Right CPRL with Right
CPRL with
Future Directions
The SDP underlying CPRL scales badly with the number of unknowns or basis vectors in the dictionary Therefore learning a suitable dictionary for a specific application becomes even more critical
than that in traditional linear CS setting We also want to point out that when classical CS was first
studied many of today?s accelerated numerical algorithms were not available We are very excited
about the new problem to improve the speed of SDP algorithms in sparse optimization and hope
our paper would foster the community?s interest to address this challenge collaboratively One interesting direction might be to use ADMM to solve the dual of see for instance Another
possible direction is the outer approximation methods
Acknowledgement
Ohlsson is partially supported by the Swedish foundation for strategic research in the center MOVIII
the Swedish Research Council in the Linnaeus center CADICS the European Research Council
under the advanced grant LEARN contract and a postdoctoral grant from the SwedenAmerica Foundation donated by ASEA?s Fellowship Fund and by a postdoctoral grant from the
Swedish Research Council Yang is supported by ARO Dong is supported by the
NSF Graduate Research Fellowship under grant DGE and by the Team for Research in
Ubiquitous Secure Technology TRUST which receives support from NSF award number The authors also want to acknowledge useful input from Stephen Boyd and Yonina Eldar

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2915-variable-kd-tree-algorithms-for-spatial-pattern-search-and-discovery.pdf

Variable KD-Tree Algorithms for Spatial Pattern
Search and Discovery
Jeremy Kubica
Robotics Institute
Carnegie Mellon University
Pittsburgh PA
Joseph Masiero
Institute for Astronomy
University of Hawaii
Honolulu HI
Andrew Moore
Robotics Institute
Carnegie Mellon University
Pittsburgh PA
jkubica@ri.cmu.edu
masiero@ifa.hawaii.edu
awm@cs.cmu.edu
Robert Jedicke
Institute for Astronomy
University of Hawaii
Honolulu HI
Andrew Connolly
Physics Astronomy Department
University of Pittsburgh
Pittsburgh PA
jedicke@ifa.hawaii.edu
ajc@phyast.pitt.edu
Abstract
In this paper we consider the problem of finding sets of points that conform to a given underlying model from within a dense noisy set of observations This problem is motivated by the task of efficiently linking
faint asteroid detections but is applicable to a range of spatial queries
We survey current tree-based approaches showing a trade-off exists between single tree and multiple tree algorithms To this end we present a
new type of multiple tree algorithm that uses a variable number of trees
to exploit the advantages of both approaches We empirically show that
this algorithm performs well using both simulated and astronomical data
Introduction
Consider the problem of detecting faint asteroids from a series of images collected on a
single night Inherently the problem is simply one of connect-the-dots Over a single
night we can treat the asteroid?s motion as linear so we want to find detections that up
to observational errors lie along a line However as we consider very faint objects several difficulties arise First objects near our brightness threshold may oscillate around this
threshold blinking into and out-of our images and providing only a small number of actual
detections Second as we lower our detection threshold we will begin to pick up more spurious noise points As we look for really dim objects the number of noise points increases
greatly and swamps the number of detections of real objects
The above problem is one example of a model based spatial search The goal is to identify
sets of points that fit some given underlying model This general task encompasses a wide
range of real-world problems and spatial models For example we may want to detect
a specific configuration of corner points in an image or search for multi-way structure in
scientific data We focus our discussion on problems that have a high density of both true
and noise points but which may have only a few points actually from the model of interest
Returning to the asteroid linking example this corresponds to finding a handful of points
that lie along a line within a data set of millions of detections
Below we survey several tree-based approaches for efficiently solving this problem We
show that both single tree and conventional multiple tree algorithms can be inefficient and
that a trade-off exists between these approaches To this end we propose a new type of
multiple tree algorithm that uses a variable number of tree nodes We empirically show
that this new algorithm performs well using both simulated and real-world data
Problem Definition
Our problem consists of finding sets of points that fit a given underlying spatial model In
doing so we are effectively looking for known types of structure buried within the data In
general we are interested in finding sets with or more points thus providing a sufficient
amount of support to confirm the discovery Finding this structure within the data may
either be our end goal such as in asteroid linkage or may just be a preprocessor for a more
sophisticated statistical test such as renewal strings We are particularly interested in
high-density low-support domains where there may be many hundreds of thousands of
points but only a handful actually support our model
Formally the data consists of unique D-dimensional points We assume that the underlying model can be estimated from unique points Since the model may overconstrained In these cases we divide the points into two sets Model Points and Support
Points Model points are the points used to fully define the underlying model Support
points are the remaining points used to confirm the model For example if we are searching for sets of linear points we could use a set?s endpoints as model points and treat the
middle as support points Or we could allow any two points to serve as model points
providing an exhaustive variant of the RANSAC algorithm
The prototypical example used in this paper is the linear asteroid linkage problem
For each pair of points find the best support points for the line that
they define such that we use at most one point at each time step
In addition we place restrictions on the validity of the initial pairs by providing velocity
bounds It is important to note that although we use this problem as a running example the
techniques described can be applied to a range of spatial problems
Overview of Previous Approaches
Constructive Algorithms
Constructive algorithms build up valid sets of points by repeatedly finding additional
points that are compatible with the current set Perhaps the simplest approach is to perform
a two-tiered brute force search First we exhaustively test all sets of points to determine
if they define a valid model Then for each valid set we test all of the remaining points for
support For example in the asteroid linkage problem we can initially search over all O(N
pairs of points and for each of the resulting lines test all points to determine if they
support that line A similar approach within the domain of target tracking is sequential
tracking for a good introduction see where points at early time steps are used to
estimate a track that is then projected to later time steps to find additional support points
In large-scale domains these approaches can often be made tractable by using spatial structure in the data Again returning to our asteroid example we can place the points in a
KD-tree We can then limit the number of initial pairs examined by using this tree to
find points compatible with our velocity constraints Further we can use the KD-tree to
only search for support points in localized regions around the line ignoring large numbers
of obviously infeasible points Similarly trees have been used in tracking algorithms to
efficiently find points near predicted track positions We call these adaptations single
tree algorithms because at any given time the algorithm is searching at most one tree
Parameter Space Methods
Another approach is to search for valid sets of points by searching the model?s parameter
space such as in the Hough transform The idea behind these approaches is that we can
test whether each point is compatible with a small set of model parameters allowing us to
search parameter space to find the valid sets However this method can be expensive in
terms of both computation and memory especially for high dimensional parameter spaces
Further if the model?s total support is low the true model occurrences may be effectively
washed out by the noise For these reasons we do not consider parameter space methods
Multiple Tree Algorithms
The primary benefit of tree-based algorithms is that they are able to use spatial structure
within the data to limit the cost of the search However there is a clear potential to push
further and use structure from multiple aspects of the search at the same time In doing
so we can hopefully avoid many of the dead ends and wrong turns that may result from
exploring bad initial associations in the first few points in our model For example in the
domain of asteroid linkage we may be able to limit the number of short initial associations
that we have to consider by using information from later time steps This idea forms the
basis of multiple tree search algorithms
Multiple tree methods explicitly search for the entire set of points at once by searching
over combinations of tree nodes In standard single tree algorithms the search tries to find
individual points satisfying some criteria the next point to add and the search state
is represented by a single node that could contain such a point In contrast multiple tree
algorithms represent the current search state with multiple tree nodes that could contain
points that together conform to the model Initially the algorithm begins with root nodes
from either the same or different tree data structures representing the different points that
must be found At each step in the search it narrows in on a set of mutually compatible
spatial regions and thus a set of individual points that fit the model by picking one of the
model nodes and recursively exploring its children As with a standard single tree search
we constantly check for opportunities to prune the search
There are several important drawbacks to multiple tree algorithms First additional trees
introduce a higher branching factor in the search and increase the potential for taking deep
wrong turns Second care must be taken in order to deal with missing or a variable
number of support points Kubica discuss the use of an additional missing tree
node to handle these cases However this approach can effectively make repeated
searches over subsets of trees making it more expensive both in theory and practice
Variable Tree Algorithms
In general we would like to exploit structural information from all aspects of our search
problem but do so while branching the search on just the parameters of interest To this
end we propose a new type of search that uses a variable number of tree nodes Like a
standard multiple tree algorithm the variable tree algorithm searches combinations of tree
nodes to find valid sets of points However we limit this search to just those points required
Figure The model nodes bounds and define a region of feasible support shaded
for any combination of model points from those nodes As shown in we can classify
entire support tree nodes as feasible node or infeasible nodes a and
to define and thus bound the models currently under consideration Specifically we use
model tree nodes,1 which guide the recursion and thus the search In addition throughout
the search we maintain information about other potential supporting points that can be used
to confirm the final track or prune the search due to a lack of support
For example in the asteroid linking problem each line is defined by only points thus we
can efficiently search through the models using a multiple tree search with model trees
As shown in Figure the spatial bounds of our current model nodes immediately limit
the set of feasible support points for all line segments compatible with these nodes If we
track which support points are feasible we can use this information to prune the search due
to a lack of support for any model defined by the points in those nodes
The key idea behind the variable tree search is that we can use a dynamic representation of
the potential support Specifically we can place the support points in trees and maintain
a dynamic list of currently valid support nodes As shown in Figure by only testing
entire nodes instead of individual points we are using spatial coherence of the support
points to remove the expense of testing each support point at each step in the search And
by maintaining a list of support tree nodes we are no longer branching the search over
these trees Thus we remove the need to make a hard left or right decision Further using
a combination of a list and a tree for our representation allows us to refine our support
representation on the fly If we reach a point in the search where a support node is no
longer valid we can simply drop it off the list And if we reach a point where a support
node provides too coarse a representation of the current support space we can simply
remove it and add both of its children to the list
This leaves the question of when to split support nodes If we split them too soon we may
end up with many support nodes in our list and mitigate the benefits of the nodes spatial
coherence If we wait too long to split them then we may have a few large support nodes
that cannot efficiently be pruned Although we are still investigating splitting strategies the
experiments in this paper use a heuristic that seeks to provide a small number of support
nodes that are a reasonable fit to the feasible region We effectively split a support node
if doing so would allow one of its two children to be pruned For KD-trees this roughly
means checking whether the split value lies outside the feasible region
The full variable tree algorithm is given in Figure A simple example of finding linear
tracks while using the track?s endpoints earliest and latest in time as model points and
Typically although in some cases it may be beneficial to use a different number of model
nodes
13
Variable Tree Model Detection
Input A set of current model tree nodes
A set of current support tree nodes
Output A list of feasible sets of points
and Scurr
IF we cannot prune based on the mutual compatibility of
FOR each Scurr
IF is compatible with
IF is too wide
Add s?s left and right child to the end of Scurr
ELSE
Add to
IF we have enough valid support points
IF all of are leaves
Test all combinations of points owned by the model nodes using
the support nodes points as potential support
Add valid sets to Z.
ELSE
Let be the non-leaf model tree node that owns the most points
Search using left child in place of and instead of S.
Search using right child in place of and instead of S.
Figure A simple variable tree algorithm for spatial structure search This algorithm
shown uses simple heuristics such as searching the model node with the most points and
splitting a support node if it is too wide These heuristics can be replaced by more accurate
problem-specific ones
using all other points for support is illustrated in Figure The first column shows all
the tree nodes that are currently part of the search The second and third columns show
the search?s position on the two model trees and the current set of valid support nodes
respectively Unlike the pure multiple tree search the variable tree search does not branch
off on the support trees allowing us to consider multiple support nodes from the same
time step at any point in the search Again it is important to note that by testing the
support points as we search we are both incorporating support information into the pruning
decisions and pruning the support points for entire sets of models at once
Results on the Asteroid Linking Domain
The goal of the single-night asteroid linkage problem is to find sets of 2-dimensional point
detections that correspond to a roughly linear motion model In the below experiments we
are interested in finding sets of at least detections from a sequence of images The
movements were constrained to have a speed between and degrees per day and
were allowed an observational error threshold of degrees All experiments were run
on a dual GHz Apple G5 with GB of RAM.
The asteroid detection data consists of detections from images of the night sky separated
by half-hour intervals The images were obtained with the MegaCam instrument on the
3.6-meter Canada-France-Hawaii Telescope The detections along with confidence levels
were automatically extracted from the images We can pre-filter the data to pull out only
those observations above a given confidence threshold This allows us to examine how
the algorithms perform as we begin to look for increasingly faint asteroids It should be
noted that only limited preprocessing was done to the data resulting in a very high level
Search Step
Search Step
Search Step
Figure The variable tree algorithm performs a depth first search over the model nodes
At each level of the search the model nodes are checked for mutual compatibility and each
support node on the list is check for compatibility with the set of model nodes Since we
are not branching on the support nodes we can split a support node and add both children
to our list This figure shows the current model and support nodes and their spatial regions
Table The running times seconds for the asteroid linkers with different detection
thresholds and thus different numbers and density of observations
Single Tree
Multiple Tree
Variable Tree
61
of false detections While future data sets will contain significantly reduced noise it is
interesting to examine the performance of the algorithms on this real-world high noise
high density data
The results on the intra-night asteroid tracking domain shown in Table illustrate a clear
advantage to using a variable tree approach As the significance threshold decreases
the number and density of detections increases allowing the support tree nodes to capture
feasibility information for a large number of support points In contrast neither the full
multiple tree algorithm nor the single-tree algorithm performed well For the multiple tree
algorithm this decrease in performance is likely due to a combination of the high number
of time steps the allowance of a missing observation and the high density In particular
the increased density can reduce opportunities for pruning causing the algorithm to explore
deeper before backtracking
Table Average running times seconds for a 2-dimensional rectangle search with
different numbers of points N. The brute force algorithm was only run to
Brute Force
Single Tree
Multi-Tree
Variable-Tree
n/a
n/a
n/a
n/a
Table Average running times seconds for a rectangle search with different numbers
of required corners For this experiment and
Single Tree
Multi-Tree
Variable-Tree
Experiments on the Simulated Rectangle Domain
We can apply the above techniques to a range of other model-based spatial search problems
In this section we consider a toy template matching problem finding axis-aligned hyperrectangles in D-dimensional space by finding or more corners that fit a rectangle We
use this simple albeit artificial problem both to demonstrate potential pattern recognition
applications and to analyze the algorithms as we vary the properties of the data
Formally we restrict the model to use the upper and lower corners as the two model points
Potential support points are those points that fall within some threshold of the other 2D
corners In addition we restrict the allowable bounds of the rectangles by providing a
maximum width
To evaluate the algorithms relative performance we used random data generated from a
uniform distribution on a unit hyper-cube The threshold and maximum width were fixed
for all experiments at and respectively All experiments were run on a dual
GHz Apple G5 with GB of RAM.
The first factor that we examined was how each algorithm scales with the number of points
We generated random data with known rectangles and additional random points and
computed the average wall-clock running time over ten trials for each algorithm The
results shown in Table show a graceful scaling of all of the multiple tree algorithms In
contrast the brute force and single tree algorithms run into trouble as the number of points
becomes moderately large The variable tree algorithm consistently performs the best as it
is able to avoid significant amounts of redundant computation
One potential drawback of the full multiple tree algorithm is that since it branches on all
points it may become inefficient as the allowable number of missing support points grows
To test this we looked at 3-dimensional data and varied the minimum number of required
support points As shown in Table all multiple tree methods become more expensive
as the number of required support points decreases This is especially the case for the
multi-tree algorithm which has to perform several almost identical searches to account for
missing points However the variable-tree algorithm?s performance degrades gracefully
and is the best for all trials
Conclusions
Tree-based spatial algorithms provide the potential for significant computational savings
with multiple tree algorithms providing further opportunities to exploit structure in the
data However a distinct trade-off exists between ignoring structure from all aspects of
the problem and increasing the combinatorics of the search We presented a variable tree
approach that exploits the advantages of both single tree and multiple tree algorithms A
combinatorial search is carried out over just the minimum number of model points while
still tracking the feasibility of the various support points As shown in the above experiments this approach provides significant computational savings over both the traditional
single tree and and multiple tree searches Finally it is interesting to note that the dynamic
support technique described in this paper is general and may be applied to a range of other
algorithms such as the Fast Hough Transform that maintain information on which
points support a given model
Acknowledgments
Jeremy Kubica is supported by a grant from the Fannie and John Hertz Foundation Andrew
Moore and Andrew Connolly are supported by a National Science Foundation ITR grant

<<----------------------------------------------------------------------------------------------------------------------->>

title: 555-extracting-and-learning-an-unknown-grammar-with-recurrent-neural-networks.pdf

Extracting and Learning an Unknown Grammar with
Recurrent Neural Networks
C.L.Gnes C.B. Miller
NEC Research Institute
Independence Way
Princeton NJ.
giles@research.nj.nec.COOl
D. Chen G.Z. Sun B.H. Chen V.C. Lee
Institute for Advanced Computer Studies
Dept of Physics and Astronomy
University of Maryland
College pm Mel
Abstract
Simple secood-order recurrent netwoIts are shown to readily learn sman brown
regular grammars when trained with positive and negative strings examples We
show that similar methods are appropriate for learning unknown grammars from
examples of their strings TIle training algorithm is an incremental real-time recurrent learning RTRL method that computes the complete gradient and updates
the weights at the end of each string After or during training a dynamic clustering
algorithm extracts the production rules that the neural network has learned TIle
methods are illustrated by extracting rules from unknown deterministic regular
grammars For many cases the extracted grammar outperforms the neural net from
which it was extracted in correctly classifying unseen strings
INTRODUCTION
For many reasons there has been a long interest in language models of neural netwoIts
see Elman for an excellent discussion TIle orientation of this work is somewhat different TIle focus here is on what are good measures of the computational capabilities of
recurrent neural networks Since currently there is little theoretical knowledge what problems would be good experimental benchmarks For discrete i.q>uts a natural choice
would be the problem of learning fonnal grammars a hard problem even for regular
grammars Angluin Smith Strings of grammars can be presented one charncter at a
time and strings can be of arbitrary length However the strings themselves would be for
the most part feature independent Thus the learning capabilities would be for the most
part feature independent and therefore insensitive to feature extraction choice
TIle learning of known grammars by recurrent neural networks has sbown promise for ex
ample Qeeresman Giles pollack Sun al
Watrous Kuhn Williams Zipser But what about learning grammars We demonstrate in this paper that not only can unknown grammars be
learned but it is possible to extract the grammar from the neural network both during and
after training Furthennore the extraction process requires no a priori knowledge about the
Giles Miller Chen Sun Chen and Lee
grammar except that the grammar's representation can be regular which is always true for
a grammar of bounded string length which is the grammatical training sample
FORMAL GRAMMARS
We give a brief introduction to grammars for a more detailed explanation see Hopcroft
Ullman We define a grammar as a 4-mple where and are DOOlerminal and tenninal vocabularies is a finite set of production rules and is the start symbol All grammars we discuss are detelUlinistic and regular For every grammar there exists
a language the set of strings the grammar generates and an automaton the machine that
recognizes classifies the grammar's strings For regular grammars the recognizing machine is a deterministic finite automaton There exists a one-ta-one mapping between a DFA and its grammar Once the DFA is known the production rules are the
ordered triples notk arc
Grammatical inference Fu is defined as the problem of finding learning a grammar
from a finite set of strings often called the training sample One can interpret this problem
as devising an inference engine that learm and extracts the grammar see Figure I.
UNKNOWN
LabeBed
striDgs
GRAMMAR
INFERENCE
Extraction
Process
ENGINE
NEURAL
NETWQRKl
INFERRED
GRAMMAR
Figure I Grammatical inference
For a training sample of positive and negative strings and no knowledge of the unknown
regular grammar the problem is NP..complete for a summary see Angluin Smith
It is possible to construct an inference engine that consists of a recurrent neural network and
a rule extraction process that yields an inferred grammar
RECURRENT NEURAL NETWORK
ARCHITEcruRE
Our recmrent neural network is quite simple and can be considered as a simplified version
of the model by Elman For an excellent discussion of recurrent networks full of

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6475-a-scalable-end-to-end-gaussian-process-adapter-for-irregularly-sampled-time-series-classification.pdf

A scalable end-to-end Gaussian process adapter for
irregularly sampled time series classification
Steven Cheng-Xian Li
Benjamin Marlin
College of Information and Computer Sciences
University of Massachusetts Amherst
Amherst MA
cxl,marlin}@cs.umass.edu
Abstract
We present a general framework for classification of sparse and irregularly-sampled
time series The properties of such time series can result in substantial uncertainty
about the values of the underlying temporal processes while making the data
difficult to deal with using standard classification methods that assume fixeddimensional feature spaces To address these challenges we propose an uncertaintyaware classification framework based on a special computational layer we refer to
as the Gaussian process adapter that can connect irregularly sampled time series
data to any black-box classifier learnable using gradient descent We show how
to scale up the required computations based on combining the structured kernel
interpolation framework and the Lanczos approximation method and how to
discriminatively train the Gaussian process adapter in combination with a number
of classifiers end-to-end using backpropagation
Introduction
In this paper we propose a general framework for classification of sparse and irregularly-sampled
time series An irregularly-sampled time series is a sequence of samples with irregular intervals
between their observation times These intervals can be large when the time series are also sparsely
sampled Such time series data are studied in various areas including climate science ecology
biology medicine and astronomy Classification in this setting is challenging both
because the data cases are not naturally defined in a fixed-dimensional feature space due to irregular
sampling and variable numbers of samples and because there can be substantial uncertainty about
the underlying temporal processes due to the sparsity of observations
Recently Li and Marlin introduced the mixture of expected Gaussian kernels MEG framework
an uncertainty-aware kernel for classifying sparse and irregularly sampled time series Classification
with MEG kernels is shown to outperform models that ignore uncertainty due to sparse and irregular
sampling On the other hand various deep learning models including convolutional neural networks
have been successfully applied to fields such as computer vision and natural language processing
and have been shown to achieve state-of-the-art results on various tasks Some of these models
have desirable properties for time series classification but cannot be directly applied to sparse and
irregularly sampled time series
Inspired by the MEG kernel we propose an uncertainty-aware classification framework that enables
learning black-box classification models from sparse and irregularly sampled time series data This
framework is based on the use of a computational layer that we refer to as the Gaussian process
adapter The GP adapter uses Gaussian process regression to transform the irregular time series
data into a uniform representation allowing sparse and irregularly sampled data to be fed into any
black-box classifier learnable using gradient descent while preserving uncertainty However the
Conference on Neural Information Processing Systems NIPS Barcelona Spain
time and space of exact GP regression makes the GP adapter prohibitively expensive
when scaling up to large time series
To address this problem we show how to speed up the key computation of sampling from a GP
posterior based on combining the structured kernel interpolation SKI framework that was recently
proposed by Wilson and Nickisch with Lanczos methods for approximating matrix functions
Using the proposed sampling algorithm the GP adapter can run in linear time and space in terms of
the length of the time series and O(m log time when inducing points are used
We also show that GP adapter can be trained end-to-end together with the parameters of the chosen
classifier by backpropagation through the iterative Lanczos method We present results using logistic
regression fully-connected feedforward networks convolutional neural networks and the MEG kernel
We show that end-to-end discriminative training of the GP adapter outperforms a variety of baselines
in terms of classification performance including models based only on GP mean interpolation or
with GP regression trained separately using marginal likelihood
Gaussian processes for sparse and irregularly-sampled time series
Our focus in this paper is on time series classification in the presence of sparse and irregular sampling
In this problem the data contain independent tuples consisting of a time series Si and a label
Thus y1 SN yN Each time series Si is represented as a list of time points
ti ti|Si and a list of corresponding values vi vi|Si We assume that
each time series is observed over a common time interval However different time series
are not necessarily observed at the same time points ti tj in general This implies that the
number of observations in different time series is not necessary the same Si Sj in general
Furthermore the time intervals between observation within a single time series are not assumed to be
uniform
Learning in this setting is challenging because the data cases are not naturally defined in a fixeddimensional feature space due to the irregular sampling This means that commonly used classifiers
that take fixed-length feature vectors as input are not applicable In addition there can be substantial
uncertainty about the underlying temporal processes due to the sparsity of observations
To address these challenges we build on ideas from the MEG kernel by using GP regression
to provide an uncertainty-aware representation of sparse and irregularly sampled time series We
fix a set of

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2197-learning-to-classify-galaxy-shapes-using-the-em-algorithm.pdf

Learning to Classify Galaxy Shapes Using the
EM Algorithm
Sergey Kirshner
Information and Computer Science
University of California
Irvine CA
skirshne@ics.uci.edu
Igor V. Cadez
Sparta Inc
Mill Creek Drive
Laguna Hills CA
igor cadez@sparta.com
Padhraic Smyth
Information and Computer Science
University of California
Irvine CA
smyth@ics.uci.edu
Chandrika Kamath
Center for Applied Scienti?c Computing
Lawrence Livermore National Laboratory
Livermore CA
kamath2@llnl.gov
Abstract
We describe the application of probabilistic model-based learning to the
problem of automatically identifying classes of galaxies based on both
morphological and pixel intensity characteristics The EM algorithm can
be used to learn how to spatially orient a set of galaxies so that they
are geometrically aligned We augment this ordering-model with a
mixture model on objects and demonstrate how classes of galaxies can
be learned in an unsupervised manner using a two-level EM algorithm
The resulting models provide highly accurate classi?cation of galaxies in
cross-validation experiments
Introduction and Background
The eld of astronomy is increasingly data-driven as new observing instruments permit the
rapid collection of massive archives of sky image data In this paper we investigate the
problem of identifying bent-double radio galaxies in the FIRST Faint Images of the Radio
Sky at Twenty-cm Survey data set FIRST produces large numbers of radio images of
the deep sky using the Very Large Array at the National Radio Astronomy Observatory It
is scheduled to cover more that square degrees of the northern and southern caps
skies Of particular scienti?c interest to astronomers is the identi?cation and cataloging
of sky objects with a bent-double morphology indicating clusters of galaxies see
Figure Due to the very large number of observed deep-sky radio sources on the order
of so far it is infeasible for the astronomers to label all of them manually
The data from the FIRST Survey http://sundog.stsci.edu is available in both raw image
format and in the form of a catalog of features that have been automatically derived from
the raw images by an image analysis program Each entry corresponds to a single
detectable blob of bright intensity relative to the sky background these entries are called
Figure examples of radio-source galaxy images The two on the left are labelled as
bent-doubles and the two on the right are not The con?gurations on the left have more
bend and symmetry than the two non-bent-doubles on the right
components The blob of intensities for each component is tted with an ellipse The
ellipses and intensities for each component are described by a set of estimated features such
as sky position of the centers RA right ascension and Dec declination peak density
ux and integrated root mean square noise in pixel intensities lengths of the major and
minor axes and the position angle of the major axis of the ellipse counterclockwise from
the north The goal is to nd sets of components that are spatially close and that resemble
a bent-double In the results in this paper we focus on candidate sets of components that
have been detected by an existing spatial clustering algorithm where each set consists
of three components from the catalog three ellipses As of the year the catalog
contained over three-component con?gurations and over con?gurations
total The set which we use to build and evaluate our models consists of a total of
examples of bent-double galaxies and 22 examples of non-bent-double con?gurations A
con?guration is labelled as a bent-double if two out of three astronomers agree to label it
as such Note that the visual identi?cation process is the bottleneck in the process since it
requires signi?cant time and effort from the scientists and is subjective and error-prone
motivating the creation of automated methods for identifying bent-doubles
Three-component bent-double con?gurations typically consist of a center or core component and two other side components called lobes Previous work on automated classi?cation of three-component candidate sets has focused on the use of decision-tree classi?ers
using a variety of geometric and image intensity features One of the limitations of the
decision-tree approach is its relative in?exibility in handling uncertainty about the object
being classi?ed the identi?cation of which of the three components should be treated
as the core of a candidate object A bigger limitation is the xed size of the feature vector A primary motivation for the development of a probabilistic approach is to provide a
framework that can handle uncertainties in a exible coherent manner
Learning to Match Orderings using the EM Algorithm
We denote a three-component con?guration by c2 c3 where the ci are the
components blobs described in the previous section Each component is represented as a feature vector where the speci?c features will be de?ned later Our approach
focuses on building a probabilistic model for bent-doubles c2 c3 the likelihood of the observed ci under a bent-double model where we implicitly condition for
now on the class bent-double
By looking at examples of bent-double galaxies and by talking to the scientists studying them we have been able to establish a number of potentially useful characteristics
of the components the primary one being geometric symmetry In bent-doubles two of
the components will look close to being mirror images of one another with respect to a
line through the third component We will call mirror-image components lobe compo
core
core
lobe
lobe
lobe
lobe
component
lobe
component
lobe
lobe
core
lobe
lobe
lobe
core
core
component
core
lobe
lobe
Figure Possible orderings for a hypothetical bent-double A good choice of ordering
would be either or
nents and the other one the core component It also appears that non-bent-doubles either
don?t exhibit such symmetry or the angle formed at the core component is too straight
the con?guration is not bent enough Once the core component is identi?ed we can
calculate symmetry-based features However identifying the most plausible core component requires either an additional algorithm or human expertise In our approach we use a
probabilistic framework that averages over different possible orderings weighted by their
probability given the data
In order to de?ne the features we rst need to determine the mapping of the components to
labels core lobe and lobe l1 and l2 for short We will call such a mapping
an ordering Figure shows an example of possible orderings for a con?guration We can
number the orderings We can then write
cc cl1 cl2
a mixture over all possible orientations Each ordering is assumed a priori to be equally
likely 61 Intuitively for a con?guration that clearly looks like a bentdouble the terms in the mixture corresponding to the correct ordering would dominate
while the other orderings would have much lower probability
We represent each component cx by features we used Note that the features
can only be calculated conditioned on a particular mapping since they rely on properties of
the assumed core and lobe components We denote by fmk the values corresponding
to the mth feature for con?guration under the ordering and by mkj we
denote the feature value of component fmk fmk1 fmkBm our
case Bm is the number of components Conditioned on a particular mapping
where l1 l2 and are de?ned in a cyclical order our features are de?ned as
f1k Log-transformed angle the angle formed at the center of the component
vertex of the con?guration mapped to label
center of to center of next(x
f2k Logarithms of side ratios center of to center of prev(x
peak ux of next(x
f3k Logarithms of intensity ratios peak ux of prev(x
and so f2k f3k for a 9-dimensional feature vector in total
Other features are of course also possible For our purposes in this paper this particular set
appears to capture the more obvious visual properties of bent-double galaxies
For a set dN of con?gurations under an assumption for con?gurations we can write the likelihood as
di fM di
where is the ordering for con?guration While in the general case one can model
di fM di as a full joint distribution for the results reported in this paper
we make a number of simplifying assumptions motivated by the fact that we have relatively little labelled training data available for model building First we assume that the
fmk di are conditionally independent Second we are also able to reduce the number
of components for each fmk di by noting functional dependencies For example given
two angles of a triangle we can uniquely determine the third one We also assume that
the remaining components for each feature are conditionally independent Under these
assumptions the multivariate joint distribution di fM di is factored into
a product of simple distributions which for the purposes of this paper we model using
Gaussians If we know for every training example which component should be mapped to
label we can then unambiguously estimate the parameters for each of these distributions
In practice however the identity of the core component is unknown for each object Thus
we use the EM algorithm to automatically estimate the parameters of the above model
We begin by randomly assigning an ordering to each object For each subsequent iteration
the E-step consists of estimating a probability distribution over possible orderings for each
object and the M-step estimates the parameters of the feature-distributions using the probabilistic ordering information from the E-step In practice we have found that the algorithm
converges relatively quickly to iterations on both simulated and real data It is
somewhat surprising that this algorithm can reliably learn how to align a set of objects
without using any explicit objective function for alignment but instead based on the fact
that feature values for certain orderings exhibit a certain self-consistency relative to the
model Intuitively it is this self-consistency that leads to higher-likelihood solutions and
that allows EM to effectively align the objects by maximizing the likelihood
After the model has been estimated the likelihood of new objects can also be calculated
under the model where the likelihood now averages over all possible orderings weighted
by their probability given the observed features
The problem described above is a speci?c instance of a more general feature unscrambling
problem In our case we assume that con?gurations of three 3-dimensional components
features each are generated by some distribution Once the objects are generated the
orders of their components are permuted or scrambled The task is then to simultaneously
learn the parameters of the original distributions and the scrambling for each object In the
more general form each con?guration consists of dimensional con?gurations Since
there are possible orderings of components the problem becomes computationally
intractable if is large One solution is to restrict the types of possible scrambles to cyclic
shifts for example
Automatic Galaxy Classi?cation
We used the algorithm described in the previous section to estimate the parameters of features and orderings of the bent-double class from labelled training data and then to rank
candidate objects according to their likelihood under the model We used leave-one-out
cross-validation to test the classi?cation ability of this supervised model where for each
of the examples we build a model using the positive examples from the set of
other examples and then score the left-out example with this model The examples are
then sorted in decreasing order by their likelihood score averaging over different possi
True positive rate
False positive rate
Figure ROC plot for a model using angle ratio of sides and ratio of intensities as
features and learned using ordering-EM with labelled data
ble orderings and the results are analyzed using a receiver operating characteristic ROC
methodology We use AROC the area under the curve as a measure of goodness of the
model where a perfect model would have AROC and random performance corresponds to AROC The supervised model using EM for learning ordering models
has a cross-validated AROC score of Figure and appears to be quite useful at
detecting bent-double galaxies
Model-Based Galaxy Clustering
A useful technique in understanding astronomical image data is to cluster image objects
based on their morphological and intensity properties For example consider how one
might cluster the image objects in Figure into clusters where we have features on angles
intensities and so forth Just as with classi?cation clustering of the objects is impeded by
not knowing which of the blobs corresponds to the true core component
From a probabilistic viewpoint clustering can be treated as introducing another level of
hidden variables namely the unknown class cluster identity of each object We can
generalize the EM algorithm for orderings Section to handle this additional hidden
level The model is now a mixture of clusters where each cluster is modelled as a mixture
of orderings This leads to a more complex two-level EM algorithm than that presented
in Section where at the inner-level the algorithm is learning how to orient the objects
and at the outer level the algorithm is learning how to group the objects into classes
Space does not permit a detailed presentation of this algorithm?however the derivation is
straightforward and produces intuitive update rules such as
cmj
cl
cli fmkj di
where cmj is the mean for the cth cluster the mth feature
and the jth component of fmk di and corresponds to ordering for the ith
object
We applied this algorithm to the data set of sky objects where unlike the results in
Section the algorithm now had no access to the class labels We used the Gaussian
conditional-independence model as before and grouped the data into clusters
Figures and show the highest likelihood objects out of total objects under the
Bent?double
Bent?double
Bent?double
Bent?double
Bent?double
Bent?double
Bent?double
Bent?double
Figure The objects with the highest likelihood conditioned on the model for the larger
of the two clusters learned by the unsupervised algorithm
Bent?double
Non?bent?double
Non?bent?double
Non?bent?double
Non?bent?double
Non?bent?double
Bent?double
Non?bent?double
Figure The objects with the highest likelihood conditioned on the model for the
smaller of the two clusters learned by the unsupervised algorithm
Unsupervised Rank
bent?doubles
non?bent?doubles
Supervised Rank
Figure A scatter plot of the ranking from the unsupervised model versus that of the
supervised model
models for the larger cluster and smaller cluster respectively The larger cluster is clearly a
bent-double cluster 89 of the objects are more likely to belong to this cluster under the
model and 88 out of the 89 objects in this cluster have the bent-double label In other words
the unsupervised algorithm has discovered a cluster that corresponds to strong examples
of bent-doubles relative to the particular feature-space and model In fact the non-bentdouble that is assigned to this group may well have been mislabelled image not shown
here The objects in Figure are clearly inconsistent with the general visual pattern of
bent-doubles and this cluster consists of a mixture of non-bent-double and weaker bentdouble galaxies The objects in Figures that are labelled as bent-doubles seem quite
atypical compared to the bent-doubles in Figure
A natural hypothesis is that cluster bent-doubles in the unsupervised model is in fact
very similar to the supervised model learned using the labelled set of bent-doubles in
Section Indeed the parameters of the two Gaussian models agree quite closely and the
similarity of the two models is illustrated clearly in Figure where we plot the likelihoodbased ranks of the unsupervised model versus those of the supervised model Both models
are in close agreement and both are clearly performing well in terms of separating the
objects in terms of their class labels
Related Work and Future Directions
A related earlier paper is Kirshner al where we presented a heuristic algorithm for
solving the orientation problem for galaxies The generalization to an EM framework in
this paper is new as is the two-level EM algorithm for clustering objects in an unsupervised
manner
There is a substantial body of work in computer vision on solving a variety of different
object matching problems using probabilistic techniques?see Mjolsness for early ideas
and Chui for a recent application in medical imaging Our work here differs in
a number of respects One important difference is that we use EM to learn a model for
the simultaneous correspondence of objects using both geometric and intensity-based
features whereas prior work in vision has primarily focused on matching one object to
another essentially the case An exception is the recent work of Frey and Jojic
who used a similar EM-based approach to simultaneously cluster images and estimate
a variety of local spatial deformations The work described in this paper can be viewed as
an extension and application of this general methodology to a real-world problem in galaxy
classi?cation
Earlier work on bent-double galaxy classi?cation used decision tree classi?ers based on a
variety of geometric and intensity-based features In future work we plan to compare
the performance of this decision tree approach with the probabilistic model-based approach
proposed in this paper The model-based approach has some inherent advantages over
a decision-tree model for these types of problems For example it can directly handle
objects in the catalog with only blobs or with or more blobs by integrating over missing
intensities and over missing correspondence information using mixture models that allow
for missing or extra blobs Being able to classify such con?gurations automatically is of
signi?cant interest to the astronomers
Acknowledgments
This work was performed under a sub-contract from the ASCI Scienti?c Data Management Project of the Lawrence Livermore National Laboratory The work of S. Kirshner
and P. Smyth was also supported by research grants from NSF award the
Jet Propulsion Laboratory IBM Research and Microsoft Research I. Cadez was supported
by a Microsoft Graduate Fellowship The work of C. Kamath was performed under the auspices of the U.S. Department of Energy by University of California Lawrence Livermore
National Laboratory under contract No. We gratefully acknowledge our
FIRST collaborators in particular Robert H. Becker for sharing his expertise on the subject

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2271-dynamic-structure-super-resolution.pdf

Dynamic Structure Super-Resolution
Amos Storkey
Institute of Adaptive and Neural Computation
Division of Informatics and Institute of Astronomy
University of Edinburgh
Forrest Hill Edinburgh UK
a.storkey@ed.ac.uk
Abstract
The problem of super-resolution involves generating feasible higher
resolution images which are pleasing to the eye and realistic from
a given low resolution image This might be attempted by using simple filters for smoothing out the high resolution blocks or
through applications where substantial prior information is used
to imply the textures and shapes which will occur in the images
In this paper we describe an approach which lies between the two
extremes It is a generic unsupervised method which is usable in
all domains but goes beyond simple smoothing methods in what it
achieves We use a dynamic tree-like architecture to model the high
resolution data Approximate conditioning on the low resolution
image is achieved through a mean field approach
Introduction
Good techniques for super-resolution are especially useful where physical limitations
exist preventing higher resolution images from being obtained For example in
astronomy where public presentation of images is of significant importance superresolution techniques have been suggested Whenever dynamic image enlargement
is needed such as on some web pages super-resolution techniques can be utilised
This paper focuses on the issue of how to increase the resolution of a single image
using only prior information about images in general and not relying on a specific
training set or the use of multiple images
The methods for achieving super-resolution are as varied as the applications They
range from simple use of Gaussian or preferably median filtering to supervised
learning methods based on learning image patches corresponding to low resolution
regions from training data and effectively sewing these patches together in a consistent manner What method is appropriate depends on how easy it is to get suitable
training data how fast the method needs to be and so on There is a demand for
methods which are reasonably fast which are generic in that they do not rely on
having suitable training data but which do better than standard linear filters or
interpolation methods
This paper describes an approach to resolution doubling which achieves this The
method is structurally related to one layer of the dynamic tree model except
that it uses real valued variables
Related work
Simple approaches to resolution enhancement have been around for some time
Gaussian and Wiener filters and a host of other linear filters have been used for
smoothing the blockiness created by the low resolution image Median filters tend
to fare better producing less blurry images Interpolation methods such as cubicspline interpolation tend to be the most common image enhancement approach
In the super-resolution literature there are many papers which do not deal with the
simple case of reconstruction based on a single image Many authors are interested
in reconstruction based on multiple slightly perturbed subsamples from an image
This is useful for photographic scanners for example In a similar manner other
authors utilise the information from a number of frames in a temporal sequence
In other situations highly substantial prior information is given such as the ground
truth for a part of the image Sometimes restrictions on the type of processing
might be made in order to keep calculations in real time or deal with sequential
transmission
One important paper which deals specifically with the problem tackled here is by
Freeman Jones and Pasztor They follow a supervised approach learning a
low to high resolution patch model rather storing examples of such maps
and utilising a Markov random field for combining them and loopy propagation
for inference Later work simplifies and improves on this approach Earlier
work tackling the same problem includes that of Schultz and Stevenson which
performed an MAP estimation using a Gibbs prior
There are two primary difficulties with smoothing eg Gaussian Wiener Median
filters or interpolation bicubic cubic spline methods First smoothing is indiscriminate It occurs both within the gradual change in colour of the sky say as well
as across the horizon producing blurring problems Second these approaches are
inconsistent subsampling the super-resolution image will not return the original
low-resolution one Hence we need a model which maintains consistency but also
tries to ensure that smoothing does not occur across region boundaries except as
much is as needed for anti-aliasing
The model
Here the high-resolution image is described by a series of very small patches with
varying shapes Pixel values within these patches can vary but will have a common
mean value Pixel values across patches are independent Apriori exactly where
these patches should be is uncertain and so the pixel to patch mapping is allowed
to be a dynamic one
The model is best represented by a belief network It consists of three layers The
lowest layer consists of the visible low-resolution pixels The intermediate layer is a
high-resolution image the size of the low-resolution image The top layer is
a latent layer which is a little more than the size of the low resolution image
The latent variables are positioned at the corners centres and edge centres of
the pixels of the low resolution image The values of the pixel colour of the high
resolution nodes are each a single sample from a Gaussian mixture colour space
where each mixture centre is given by the pixel colour of a particular parent latent
Latent
Hi Res
Low Res
Figure The three layers of the model The small boxes in the left figure of
them give the position of the high resolution pixels relative to the low resolution
pixels the boxes with a thick outline The positions of the latent variable nodes
are given by the black circles The colour of each high resolution pixel is generated
from a mixture of Gaussians right figure each Gaussian centred at its latent
parent pixel value The closer the parent is the higher the prior probability of
being generated by that mixture is
variable node The prior mixing coefficients decay with distance in image space
between the high-resolution node and the corresponding latent node
Another way of viewing this is that a further indicator variable can be introduced
which selects which mixture is responsible for a given high-resolution node We say
a high resolution node chooses to connect to the parent that is responsible for it
with a connection probability given by the corresponding mixing coefficient These
connection probabilities can be specified in terms of positions figure
The motivation for this model comes from the possibility of explaining away In
linear filtering methods each high-resolution node is determined by a fixed relationship to its neighbouring low-resolution nodes Here if one of the latent variables
provides an explanation for a high-resolution node which fits well with it neighbours
to form the low-resolution data then the posterior responsibility of the other latent
nodes for that high-resolution pixel is reduced and they are free to be used to model
other nearby pixels The high-resolution pixels corresponding to a visible node can
be separated into two more independent regions corresponding to pixels on
different sides of an edge edges A different latent variable is responsible for
each region In other words each mixture component effectively corresponds to a
small image patch which can vary in size depending on what pixels it is responsible
for
Let vj denote a latent variable at site in the latent space L. Let
denote the value of pixel in high resolution image space and let yk denote the
value of the visible pixel Each of these is a 3-vector representing colour Let
denote the ordered set of all vj Likewise denotes the ordered set of all and
the set of all In all the work described here a transformed colorspace of gray
red-green blue-yellow is used In other words the data is a linear transformation
on the RGB colour values using the matrix
The remaining component is the connectivity the indicator for the responsibility between the high-resolution nodes and the nodes in the latent layer Let zij
denote this connectivity with zij an indicator variable taking value when vj is a
parent of in the belief network Every high resolution pixel has one and only one
parent in the latent layer Let denote the ordered set of all zij
Distributions
A uniform distribution over the range of pixel values is presumed for the latent
variables The high resolution pixels are given by Gaussian distributions centred
on the pixel values of the parental latent variable This Gaussian is presumed
independent in each pixel component Finally the low resolution pixels are given
by the average of the sixteen high resolution pixels covering the site of the low
resolution pixel This pixel value can also be subject to some additional Gaussian
noise if necessary zero noise is assumed in this paper
It is presumed that each high resolution pixel is allowed to choose its parent from
the set of latent variables in an independent manner A pixel has a higher probability
of choosing a nearby parent than a far away one
For this we use a Gaussian integral form so that
rj
ij
pij where pij
dr exp
Bi
ij
The equations for the other distributions are given here First we have
xm
vj
exp zij
ijm
where is a position in the high resolution picture space rj is the position of the
jth latent variable in the high resolution image space where these are located at
the corners of every second pixel in each direction as described above The integral
is over Bi defined as the region in image space corresponding to pixel gives
the width squared over which the probability decays The larger the more
possible parents with non-negligible probability The connection probabilities can
be illustrated by the picture in figure
where is a variance which determines how much each pixel must be like its
latent parent Here the indicator zij ensures the only contribution for each comes
from the parent of Second
ykm d1 i?P xm
exp
km
Figure An illustration of the connection probabilities from a high resolution pixel
in the position of the smaller checkered square to the latent variables centred at each
of the larger squares The probability is proportional to the intensity of the shading
darker is higher probability
with denoting the set of all the high resolution pixels which go to
make up the low resolution pixel yk In this work we let the variance
determines the additive Gaussian noise which is in the low resolution image
Last is simply uniform over the whole of the possible values of Hence
for the volume of space being considered
Inference
The belief network defined above is not tree structured rather it is a mixture of tree
structures and so we have to resort to approximation methods for inference In this
paper a variational approach is followed The posterior distribution is approximated
using a factorised distribution over the latent space and over the connectivity Only
in the high resolution space do we consider joint distributions we use a joint
Gaussian for all the nodes corresponding to one low resolution pixel The full
distribution can be written as Q(Z)Q(V where
vjm jm
ij
qij
Q(V
exp
and
ij
jm
exp
km
where
is the vector the joint of all high resolution pixel
values corresponding to a given low resolution pixel for a given colour component
Here qij
and are variational parameters to be optimised
As usual a local minima the KL divergence between the approximate distribution
and the true posterior distribution is computed This is equivalent to maximising
the negative variational free energy variational log likelihood
log
where is given by the low resolution image In this case we obtain
hlog log hlog Q(V log p(V iQ(V
hlog Q(X)iQ(X hlog iQ(X,Z,V hlog
Taking expectations and derivatives with respect to each of the parameters in the
approximation gives a set of self-consistent mean field equations which we can solve
by repeated iteration Here for simplicity we only solve for qij and for the means
and jm which turn out to be independent of the variational variance parameters
We obtain
qij
jm
and
qij vim
Dc(i where
ij
where is the child of the low level pixel which is part of Dk is
a Lagrange multiplier and is obtained through constraining the high level pixel
values to average to the low level pixels
Dk Dk
i?P
i?P
In the case where is non-zero this constraint is softened and Dk is given by
Dk The update for the qij is given by
xm
qij pij exp
where the constant of proportionality is given by normalisation
qij
Optimising the KL divergence involves iterating these equations For each
optimisation equations and are iterated a number of times Each
optimisation loop is either done a preset number of times or until a suitable convergence criterion is met The former approach is generally used as the basic criterion
is a limit on the time available for the optimisation to be done
Setting parameters
The prior variance parameters need to be set The variance corresponds to the
additive noise If this is not known to be zero then it will vary from image to image
and needs to be found for each image This can be done using variational maximum
likelihood where is set to maximise the variational log likelihood is presumed
to be independent of the images presented and is set by hand by visualising changes
on a test set The might depend on the intensity levels in the image very dark
images will need a smaller value of for example However for simplicity
is treated as global and set by hand Because the primary criterion for optimal
parameters is subjective this is the most sensible approach and is reasonable when
there are only two parameters to determine To optimise automatically based on
the variational log likelihood is possible but does not produce as good results due to
the complicated nature of a true prior or error-measure for images For example a
highly elaborate texture offset by one pixel will give a large mean square error but
look almost identical whereas a blurred version of the texture would give a smaller
mean square error but look much worse
Implementation
The basic implementation involves setting the parameters running the mean field
optimisation and then looking at the result The final result is a downsampled
version of the image to size the larger image is used to get reasonable
anti-aliasing
To initialise the mean field optimisation is set equal to the bi-cubic interpolated
image with added Gaussian noise The is initialised to Although in
the examples here we used optimisations each of which involves cycles
through the mean field equations for and Q(V it is possible to get reasonable
results with only three optimisation cycles each doing iterations through
the mean field equations In the runs shown here is set to zero the variance
is set to and is set to
Demonstrations and assessment
The method described in this paper is compared with a number of simple filtering and interpolation methods and also with the methods of Freeman
The image from Freeman?s website is used for comparison with that work figure Full colour comparisons for these and other images can be found at
http://www.anc.ed.ac.uk/~amos/superresolution.html First two linear filtering approaches are considered the Wiener filter and a Gaussian filter The third
method is a median filter Bi-cubic interpolation is also given
Quantitative assessment of the quality of super-resolution results is always something of a difficulty because the basic criterion is human subjectivity Even so we
Figure Comparison with approach of Freeman gives the low resolution image the true image a bi-cubic interpolation Freeman al result
taken from website and downsampled dynamic structure super-resolution
median filter
compare the results of this approach with standard filtering methods using a root
mean squared pixel error on a set of by 96 colour images giving
and for the original low resolution image bicubic interpolation the
median filter and dynamic structure super-resolution respectively Unfortunately
the unavailability of code prevents representative calculations for the Freeman al
approach Dynamic structure resolution requires approximately flops per
high resolution pixel per optimisation cycle compared with say flops for
a linear filter so it is more costly Trials have been done working directly with
grids rather than with and then averaging up This is much faster and
the results though not quite as good were still an improvement on the simpler
methods
Qualitatively the results for dynamic structure super-resolution are significantly
better than most standard filtering approaches The texture is better represented
because it maintains consistency and the edges are sharper although there is still
some significant difference from the true image The method of Freeman al
is perhaps comparable at this resolution although it should be noted that their
result has been downsampled here to half the size of their enhanced image Their
method can produce the resolution of the original and so this does not
accurately represent the full power of their technique Furthermore this image is
representative of early results from their work However their approach does require
learning large numbers of patches from a training set Fundamentally the dynamic
structure super-resolution approach does a good job at resolution doubling without
the need for representative training data The edges are not blurred and much of
the blockiness is removed
Dynamic structure super-resolution provides a technique for resolution enhancement and provides an interesting starting model which is different from the Markov
random field approaches Future directions could incorporate hierarchical frequency
information at each node rather than just a single value

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2906-correlated-topic-models.pdf

Correlated Topic Models
David M. Blei
Department of Computer Science
Princeton University
John D. Lafferty
School of Computer Science
Carnegie Mellon University
Abstract
Topic models such as latent Dirichlet allocation can be useful
tools for the statistical analysis of document collections and other discrete data The LDA model assumes that the words of each document
arise from a mixture of topics each of which is a distribution over the vocabulary A limitation of LDA is the inability to model topic correlation
even though for example a document about genetics is more likely to
also be about disease than x-ray astronomy This limitation stems from
the use of the Dirichlet distribution to model the variability among the
topic proportions In this paper we develop the correlated topic model
where the topic proportions exhibit correlation via the logistic
normal distribution We derive a mean-field variational inference algorithm for approximate posterior inference in this model which is complicated by the fact that the logistic normal is not conjugate to the multinomial The CTM gives a better fit than LDA on a collection of OCRed
articles from the journal Science Furthermore the CTM provides a natural way of visualizing and exploring this and other unstructured data
sets
Introduction
The availability and use of unstructured historical collections of documents is rapidly growing As one example JSTOR www.jstor.org is a not-for-profit organization that maintains a large online scholarly journal archive obtained by running an optical character recognition engine over the original printed journals JSTOR indexes the resulting text and provides online access to the scanned images of the original content through keyword search
This provides an extremely useful service to the scholarly community with the collection
comprising nearly three million published articles in a variety of fields
The sheer size of this unstructured and noisy archive naturally suggests opportunities for
the use of statistical modeling For instance a scholar in a narrow subdiscipline searching
for a particular research article would certainly be interested to learn that the topic of
that article is highly correlated with another topic that the researcher may not have known
about and that is not explicitly contained in the article Alerted to the existence of this new
related topic the researcher could browse the collection in a topic-guided manner to begin
to investigate connections to a previously unrecognized body of work Since the archive
comprises millions of articles spanning centuries of scholarly work automated analysis is
essential
Several statistical models have recently been developed for automatically extracting the
topical structure of large document collections In technical terms a topic model is a
generative probabilistic model that uses a small number of distributions over a vocabulary
to describe a document collection When fit from data these distributions often correspond
to intuitive notions of topicality In this work we build upon the latent Dirichlet allocation
LDA model LDA assumes that the words of each document arise from a mixture
of topics The topics are shared by all documents in the collection the topic proportions
are document-specific and randomly drawn from a Dirichlet distribution LDA allows each
document to exhibit multiple topics with different proportions and it can thus capture the
heterogeneity in grouped data that exhibit multiple latent patterns Recent work has used
LDA in more complicated document models and in a variety of settings such
as image processing collaborative filtering and the modeling of sequential data
and user profiles Similar models were independently developed for disability survey
data and population genetics
Our goal in this paper is to address a limitation of the topic models proposed to date they
fail to directly model correlation between topics In many?indeed most?text corpora it
is natural to expect that subsets of the underlying latent topics will be highly correlated In
a corpus of scientific articles for instance an article about genetics may be likely to also
be about health and disease but unlikely to also be about x-ray astronomy For the LDA
model this limitation stems from the independence assumptions implicit in the Dirichlet
distribution on the topic proportions Under a Dirichlet the components of the proportions
vector are nearly independent this leads to the strong and unrealistic modeling assumption
that the presence of one topic is not correlated with the presence of another
In this paper we present the correlated topic model The CTM uses an alternative more flexible distribution for the topic proportions that allows for covariance structure
among the components This gives a more realistic model of latent topic structure where
the presence of one latent topic may be correlated with the presence of another In the
following sections we develop the technical aspects of this model and then demonstrate its
potential for the applications envisioned above We fit the model to a portion of the JSTOR
archive of the journal Science We demonstrate that the model gives a better fit than LDA
as measured by the accuracy of the predictive distributions over held out documents Furthermore we demonstrate qualitatively that the correlated topic model provides a natural
way of visualizing and exploring such an unstructured collection of textual data
The Correlated Topic Model
The key to the correlated topic model we propose is the logistic normal distribution The
logistic normal is a distribution on the simplex that allows for a general pattern of variability
between the components by transforming a multivariate normal random variable Consider
the natural parameterization of a K-dimensional multinomial distribution
p(z exp
The random variable can take on values it can be represented by a K-vector with
exactly one component equal to one denoting a value in K}. The cumulant generating function of the distribution is
log
exp{?i
The mapping between the mean parameterization the simplex and the natural parameterization is given by
log
Notice that this is not the minimal exponential family representation of the multinomial
because multiple values of can yield the same mean parameter
Zd,n
Wd,n
Figure Top Graphical model representation of the correlated topic model The logistic
normal distribution used to model the latent topic proportions of a document can represent
correlations between topics that are impossible to capture using a single Dirichlet Bottom
Example densities of the logistic normal on the 2-simplex From left diagonal covariance
and nonzero-mean negative correlation between components and positive correlation
between components and
The logistic normal distribution assumes that is normally distributed and then mapped
to the simplex
with the inverse of the mapping given in equation that is
exp exp The logistic normal models correlations between components of the
simplicial random variable through the covariance matrix of the normal distribution The
logistic normal was originally studied in the context of analyzing observed compositional
data such as the proportions of minerals in geological samples In this work we extend its
use to a hierarchical model where it describes the latent composition of topics associated
with each document
Let be a K-dimensional mean and covariance matrix and let topics be
multinomials over a fixed word vocabulary The correlated topic model assumes that an
word document arises from the following generative process
Draw
For
Draw topic assignment Zn from Mult(f
Draw word Wn zn from Mult(?zn
This process is identical to the generative process of LDA except that the topic proportions
are drawn from a logistic normal rather than a Dirichlet The model is shown as a directed
graphical model in Figure
The CTM is more expressive than LDA. The strong independence assumption imposed
by the Dirichlet in LDA is not realistic when analyzing document collections where one
may find strong correlations between topics The covariance matrix of the logistic normal
in the CTM is introduced to model such correlations In Section we illustrate how the
higher order structure given by the covariance can be used as an exploratory tool for better
understanding and navigating a large corpus of documents Moreover modeling correlation
can lead to better predictive distributions In some settings such as collaborative filtering
the goal is to predict unseen items conditional on a set of observations An LDA model
will predict words based on the latent topics that the observations suggest but the CTM
has the ability to predict items associated with additional topics that are correlated with the
conditionally probable topics
Posterior inference and parameter estimation
Posterior inference is the central challenge to using the CTM. The posterior distribution of
the latent variables conditional on a document is intractable to compute
once conditioned on some observations the topic assignments and log proportions
are dependent We make use of mean-field variational methods to efficiently obtain an
approximation of this posterior distribution
In brief the strategy employed by mean-field variational methods is to form a factorized
distribution of the latent variables parameterized by free variables which are called the variational parameters These parameters are fit so that the Kullback-Leibler divergence
between the approximate and true posterior is small For many problems this optimization
problem is computationally manageable while standard methods such as Markov Chain
Monte Carlo are impractical The tradeoff is that variational methods do not come with
the same theoretical guarantees as simulation methods See for a modern review of
variational methods for statistical inference
In graphical models composed of conjugate-exponential family pairs and mixtures the
variational inference algorithm can be automatically derived from general principles
In the CTM however the logistic normal is not conjugate to the multinomial We
will therefore derive a variational inference algorithm by taking into account the special
structure and distributions used by our model
We begin by using Jensen?s inequality to bound the log probability of a document
log
Eq log
PN
Eq
log p(zn Eq log p(wn zn
where the expectation is taken with respect to a variational distribution of the latent variables and denotes the entropy of that distribution We use a factorized distribution
QK
QN
q(zn
The variational distributions of the discrete variables are specified by the Kdimensional multinomial parameters The variational distribution of the continuous
variables are independent univariate Gaussians Since the variational parameters are fit using a single observed document there is no advantage in introducing a non-diagonal variational covariance matrix
The nonconjugacy of the logistic normal leads to difficulty in computing the expected log
probability of a topic assignment
PK
Eq log p(zn Eq zn Eq log exp{?i
To preserve the lower bound on the log probability we upper bound the log normalizer
with a Taylor expansion
PK
Eq log
exp
Eq exp{?i
where we have introduced a new variational parameter The expectation Eq exp{?i is
the mean of a log normal distribution with mean and variance obtained from the variational
parameters thus Eq exp{?i exp{?i for K}.
fossil record
birds
fossils
dinosaurs
fossil
evolution
taxa
species
specimens
evolutionary
ancient
found
impact
million years ago
africa
site
bones
years ago
date
rock
mantle
crust
upper mantle
meteorites
ratios
rocks
grains
isotopic
isotopic composition
depth
climate
ocean
ice
changes
climate change
north atlantic
record
warming
temperature
past
earthquake
earthquakes
fault
images
data
observations
features
venus
surface
faults
brain
memory
subjects
left
task
brains
cognitive
language
human brain
learning
co2
carbon
carbon dioxide
methane
water
energy
gas
fuel
production
organic matter
ozone
atmospheric
measurements
stratosphere
concentrations
atmosphere
air
aerosols
troposphere
measured
neurons
stimulus
motor
visual
cortical
axons
stimuli
movement
cortex
eye
ca2
calcium
release
ca2 release
concentration
ip3
intracellular calcium
intracellular
intracellular ca2
ca2
ras
atp
camp
gtp
adenylyl cyclase
cftr
adenosine triphosphate atp
guanosine triphosphate gtp
gap
gdp
synapses
ltp
glutamate
synaptic
neurons
long term potentiation ltp
synaptic transmission
postsynaptic
nmda receptors
hippocampus
males
male
females
female
sperm
sex
offspring
eggs
species
egg
gene
disease
mutations
families
mutation
alzheimers disease
patients
human
breast cancer
normal
genetic
population
populations
differences
variation
evolution
loci
mtdna
data
evolutionary
cell cycle
activity
cyclin
regulation
protein
phosphorylation
kinase
regulated
cell cycle progression
amino acids
cdna
sequence
isolated
protein
amino acid
mrna
amino acid sequence
actin
clone
development
embryos
drosophila
genes
expression
embryo
developmental
embryonic
developmental biology
vertebrate
wild type
mutant
mutations
mutants
mutation
gene
yeast
recombination
phenotype
genes
Figure A portion of the topic graph learned from OCR articles from Science
Each node represents a topic and is labeled with the five most probable phrases from its
distribution phrases are found by the turbo topics method The interested reader can
browse the full model at http://www.cs.cmu.edu/?lemur/science
Given a model and a document the variational inference algorithm optimizes equation with respect to the variational parameters We
use coordinate ascent repeatedly optimizing with respect to each parameter while holding
the others fixed In variational inference for LDA each coordinate can be optimized analytically However iterative methods are required for the CTM when optimizing for and
The details are given in Appendix A.
Given a collection of documents we carry out parameter estimation in the correlated topic
model by attempting to maximize the likelihood of a corpus of documents as a function
of the topics and the multivariate Gaussian parameters We use variational
expectation-maximization where we maximize the bound on the log probability of a
collection given by summing equation over the documents
In the E-step we maximize the bound with respect to the variational parameters by performing variational inference for each document In the M-step we maximize the bound
with respect to the model parameters This is maximum likelihood estimation of the topics and multivariate Gaussian using expected sufficient statistics where the expectation
is taken with respect to the variational distributions computed in the E-step The E-step
and M-step are repeated until the bound on the likelihood converges In the experiments
reported below we run variational inference until the relative change in the probability
bound of equation is less than and run variational EM until the relative change in
the likelihood bound is less than
Examples and Empirical Results Modeling Science
In order to test and illustrate the correlated topic model we estimated a 100-topic CTM
on Science articles spanning to We constructed a graph of the latent topics and the connections among them by examining the most probable words from
each topic and the between-topic correlations Part of this graph is illustrated in Figure In this subgraph there are three densely connected collections of topics material
science geology and cell biology Furthermore an estimated CTM can be used to explore otherwise unstructured observed documents In Figure we list articles that are
assigned to the cognitive science topic and articles that are assigned to both the cog
L(CTM L(LDA
Held?out log likelihood
CTM
LDA
70
Number of topics
90
70
90
Number of topics
Figure The average held-out probability CTM supports more topics than LDA. See
figure at right for the standard error of the difference The log odds ratio of the held-out
probability Positive numbers indicate a better fit by the correlated topic model
nitive science and visual neuroscience topics The interested reader is invited to visit
http://www.cs.cmu.edu/?lemur/science to interactively explore this model including the topics their connections and the articles that exhibit them
We compared the CTM to LDA by fitting a smaller collection of articles to models of varying numbers of topics This collection contains the documents from we used
a vocabulary of words after pruning common function words and terms that occur
once in the collection Using ten-fold cross validation we computed the log probability of
the held-out data given a model estimated from the remaining data A better model of the
document collection will assign higher probability to the held out data To avoid comparing
bounds we used importance sampling to compute the log probability of a document where
the fitted variational distribution is the proposal
Figure illustrates the average held out log probability for each model and the average
difference between them The CTM provides a better fit than LDA and supports more
topics the likelihood for LDA peaks near topics while the likelihood for the CTM peaks
close to 90 topics The means and standard errors of the difference in log-likelihood of the
models is shown at right this indicates that the CTM always gives a better fit
Another quantitative evaluation of the relative strengths of LDA and the CTM is how well
the models predict the remaining words after observing a portion of the document Suppose we observe words from a document and are interested in which model provides
a better predictive distribution p(w of the remaining words To compare these distributions we use perplexity which can be thought of as the effective number of equally
likely words according to the model Mathematically the perplexity of a word distribution is defined as the inverse of the per-word geometric average of the probability of the
observations
PD
QNd
Nd
p(w
Perp
i=P
where denotes the model parameters of an LDA or CTM model Note that lower numbers
denote more predictive power
The plot in Figure compares the predictive perplexity under LDA and the CTM. When a
A Head for Figures
Sources of Mathematical Thinking Behavioral and Brain
Imaging Evidence
Natural Language Processing
A Romance Blossoms Between Gray Matter and Silicon
Computer Vision
Predictive perplexity
Top Articles with
brain memory human visual cognitive and
computer data information problem systems
CTM
LDA
Separate Neural Bases of Two Fundamental Memory
Processes in the Human Medial Temporal Lobe
Inattentional Blindness Versus Inattentional Amnesia for
Fixated but Ignored Words
Making Memories Brain Activity that Predicts How Well
Visual Experience Will be Remembered
The Learning of Categories Parallel Brain Systems for
Item Memory and Category Knowledge
Brain Activation Modulated by Sentence Comprehension
Top Articles with
brain memory human visual cognitive
70
90
observed words
Figure Left Exploring a collection through its topics Right Predictive perplexity for
partially observed held-out documents from the Science corpus
small number of words have been observed there is less uncertainty about the remaining
words under the CTM than under LDA?the perplexity is reduced by nearly words or
roughly The reason is that after seeing a few words in one topic the CTM uses topic
correlation to infer that words in a related topic may also be probable In contrast LDA
cannot predict the remaining words as well until a large portion of the document as been
observed so that all of its topics are represented
Acknowledgments Research supported in part by NSF grants and and by the DARPA CALO project

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2618-seeing-through-water.pdf

Seeing through water
Alexei A. Efros
School of Computer Science
Carnegie Mellon University
Pittsburgh PA
Volkan Isler Jianbo Shi and Mirk?o Visontai
Dept of Computer and Information Science
University of Pennsylvania
Philadelphia PA
efros@cs.cmu.edu
isleri,jshi,mirko}@cis.upenn.edu
Abstract
We consider the problem of recovering an underwater image distorted by
surface waves A large amount of video data of the distorted image is
acquired The problem is posed in terms of finding an undistorted image patch at each spatial location This challenging reconstruction task
can be formulated as a manifold learning problem such that the center
of the manifold is the image of the undistorted patch To compute the
center we present a new technique to estimate global distances on the
manifold Our technique achieves robustness through convex flow computations and solves the leakage problem inherent in recent manifold
embedding techniques
Introduction
Consider the following problem A pool of water is observed by a stationary video camera
mounted above the pool and looking straight down There are waves on the surface of the
water and all the camera sees is a series of distorted images of the bottom of the pool
Figure The aim is to use these images to recover the undistorted image of the pool
floor as if the water was perfectly still Besides obvious applications in ocean optics and
underwater imaging variants of this problem also arise in several other fields including
astronomy overcoming atmospheric distortions and structure-from-motion learning the
appearance of a deforming object Most approaches to solve this problem try to model the
distortions explicitly In order to do this it is critical not only to have a good parametric
model of the distortion process but also to be able to reliably extract features from the data
to fit the parameters As such this approach is only feasible in well understood highly
controlled domains On the opposite side of the spectrum is a very simple method used in
underwater imaging simply average the data temporally Although this method performs
surprisingly well in many situations it fails when the structure of the target image is too
fine with respect to the amplitude of the wave Figure
In this paper we propose to look at this difficult problem from a more statistical angle We
will exploit a very simple observation if we watch a particular spot on the image plane
most of the time the picture projected there will be distorted But once in a while when
the water just happens to be locally flat at that point we will be looking straight down
and seeing exactly the right spot on the ground If we can recognize when this happens
Authors in alphabetical order
Figure Fifteen consecutive frames from the video The experimental setup involved a transparent
bucket of water the cover of a vision textbook Computer Vision/A Modern Approach
Figure Ground truth image and reconstruction results using mean and median
and snap the right picture at each spatial location then recovering the desired ground truth
image would be simply a matter of stitching these correct observations together In other
words the question that we will be exploring in this paper is not where to look but when
Problem setup
Let us first examine the physical setup of our problem There is a ground truth image
on the bottom of the pool Overhead a stationary camera pointing downwards is recording
a video stream In the absence of any distortion at any time
However the water surface refracts in accordance with Snell?s Law. Let us consider what
the camera is seeing at a particular point on the CCD array as shown in Figure
assume 1D for simplicity If the normal to the water surface directly underneath is
pointing straight up there is no refraction and However if the normal is
tilted by angle light will bend by the amount sin?1
sin so the
camera point will see the light projected from G(x dx on the ground plane It
is easy to see that the relationship between the tilt of the normal to the surface and the
displacement dx is approximately linear dx using small angle approximation
where is the height of the water This means that in what the camera will be seeing
over time at point are points on the ground plane sampled from a disk centered at
and with radius related to the height of the water and the overall roughness of the
water surface A similar relationship holds in the inverse direction as well a point
will be imaged on a disk centered around
What about the distribution of these sample points According to Cox-Munk Law the
surface normals of rough water are distributed approximately as a Gaussian centered around
the vertical assuming a large surface area and stationary waves Our own experiments
conducted by hand-tracking Figure confirm that the distribution though not exactly
Gaussian is definitely unimodal and smooth
Up to now we only concerned ourselves with infinitesimally small points on the image
or the ground plane However in practice we must have something that we can compute
with Therefore we will make an assumption that the surface of the water can be locally
approximated by a planar patch This means that everything that was true for points is now
true for local image patches up to a small affine distortion
Tracking via embedding
From the description outlined above one possible solution emerges If the distribution of a
particular ground point on the image plane is unimodal then one could track feature points
in the video sequence over time Computing their mean positions over the entire video will
give an estimate of their true positions on the ground plane Unfortunately tracking over
long periods of time is difficult even under favorable conditions whereas our data is so fast
undersampled and noisy that reliable tracking is out of the question Figure
However since we have a lot of data we can substitute smoothness in time with smoothness
in similarity for a given patch we are more likely to find a patch similar to it somewhere
in time and will have a better chance to track the transition between them An alternative
to tracking the patches directly which amounts to holding the ground patch fixed
and centering the image patches dxt dyt on top of it in each frame is to fix the
image patch in space and observe the patches from G(x dxt dyt appearing
in this window We know that this set of patches comes from a disk on the ground plane
centered around patch our goal If the disk was small enough compared to the
size of the patch we could just cluster the patches together by using translational
EM Unfortunately the disk can be rather large containing patches with no overlap
at all thus making only the local similarity comparisons possible However notice that
our set of patches lies on a low-dimensional manifold in fact we know precisely which
manifold it?s the disk on the ground plane centered at So if we could use the
local patch similarities to find an embedding of the patches in on this manifold
the center of the embedding will hold our desired patch
The problem of embedding the patches based on local similarity is related to the recent
work in manifold learning Basic ingredients of the embedding algorithms are defining a distance measure between points and finding an energy function that optimally places
them in the embedding space The distance can be defined as all-pairs distance matrix or
as distance from a particular

<<----------------------------------------------------------------------------------------------------------------------->>

