query sentence: documentation of contaminants in agricultural activities
---------------------------------------------------------------------
title: 1429-modelling-seasonality-and-trends-in-daily-rainfall-data.pdf

Modelling Seasonality and Trends in Daily
Rainfall Data
Peter Williams
School of Cognitive and Computing Sciences
University of Sussex
Falmer Brighton BN1 UK.
email peterw@cogs.susx.ac.uk
Abstract
This paper presents a new approach to the problem of modelling daily
rainfall using neural networks We first model the conditional distributions of rainfall amounts in such a way that the model itself determines
the order of the process and the time-dependent shape and scale of the
conditional distributions After integrating over particular weather patterns we are able to extract seasonal variations and long-term trends
Introduction
Analysis of rainfall data is important for many agricultural ecological and engineering
activities Design of irrigation and drainage systems for instance needs to take account
not only of mean expected rainfall but also of rainfall volatility In agricultural planning
changes in the annual cycle advances in the onset of winter rain are significant in
determining the optimum time for planting crops Estimates of crop yields also depend
on the distribution of rainfall during the growing season as well as on the overall amount
Such problems require the extrapolation of longer term trends as well as the provision of
short or medium term forecasts
Occurrence and amount processes
Models of daily precipitation commonly distinguish between the occurrence process
whether or not it rains and the amount process how much it rains if it does The
occurrence process is often modelled as a two-state Markov chain of first or higher order
In discussion of Katz traces this approach back to Quetelet in A first order
chain has been considered adequate for some weather stations but second or higher order
models may be required for others or at different times of year Non-stationary Markov
chains have been used by a number of investigators and several approaches have been taken
P. Williams
to the problem of seasonal variation using Fourier series to model daily variation of
parameters
The amount of rain on a given day assuming it rains normally has a roughly exponential
distribution Smaller amounts of rain are generally more likely than larger amounts Several
models have been used for the amount process Katz Parlange for example assume
that IX has a normal distribution where is a positive integer empirically chosen to
minimise the skewness of the resulting historical distribution But use has more commonly
been made of a gamma distribution or a mixture of two exponentials
Stochastic model
The present approach is to deal with the occurrence and amount processes jointly by assuming that the distribution of the amount of rain on a given day is a mixture of a discrete
and continuous component The discrete component relates to rainfall occurrence and the
continuous component relates to rainfall amount on rainy days
We use a gamma distribution for the continuous component This has density proportional
to to within an adjustable scaling of the x-axis The shape parameter
controls the ratio of standard deviation to mean It also determines the location of the
mode which is strictly positive if For certain patterns of past precipitation larger
amounts may be more likely on the following day than smaller amounts Specifically the
distribution of the amount of rain on a given day is modelled by the three parameter
family
if
if
where a
and v,O and
yv-l dy
is the incomplete gamma function For a there is a discontinuity at corresponding to the discrete component Putting it is seen that a P(X is the
probability of rain on the day in question The mean daily rainfall amount is avO and the
variance is aV{l v(l
Modelling time dependency
The parameters a determining the conditional distribution for a given day are understood to depend on the preceding pattern of precipitation the time of year etc To model
this dependency we use a neural network with inputs corresponding to the conditioning
events and three outputs corresponding to the distributional parameters Referring to the
activations of the three output units as ZV and zO we relate these to the distributional
parameters by
expzO
expzv
expzo
in order to ensure an unconstrained parametrization with a and v,O for any
real values of zV zO
1It would be straightforward to use a mixture of gammas or exponentials with time-dependent
mixture components A single gamma was chosen for simplicity to illustrate the approach
A similar approach to modelling conditional distributions by having the network output distributional parameters is used for example by Ghabramani Jordan Nix Weigend Bishop
Legleye Williams Baldi Chauvin
Modelling Seasonality and Trends in Daily Rainfall Data
On the input side we first need to make additional assumptions about the statistical properties of the process Specifically it is assumed that the present is stochastically independent
of the distant past in the sense that
for a sufficiently large number of days T. In fact the stronger assumption will be made that
where is the event of rain on day This assumes that today's rainfall
amount depends stochastically only on the occurrence or non-occurrence of rain in the
recent past and not on the actual amounts Such a simplification is in line with previous
approaches For the present study was taken to be
To assist in modelling seasonal variations cyclic variables sin and cos were also provided as inputs where and is the length of the tropical year
This corresponds to using Fourier series to model seasonality but with the number of harmonics adaptively determined by the model To allow for non-periodic nonstationarity the current value of was also provided as input
Model fitting
Suppose we are given a sequence of daily rainfall data of length N. Equation implies
that the likelihood of the full data sequence
Xo factorises as
I I
I Xo I Xo
p(Xt
I I Tt-T
t=T
where the likelihood IXO of the initial sequence is not modelled and can be
considered as a constant compare Our interest is in the likelihood of the actual
sequence of observations which is understood to depend on the variable weights of the
neural network Note that p(Xt
Tt-T is computed by means of the neural
network outputs zf I zf I zf using weights wand the inputs corresponding to time
I I
The log likelihood of the data can therefore be written to within a constant as
logp(xN-1 I
IXO
logp(xt
I Tt-T
t=T
or more simply
Lt(w
t=T
where from
at
log at lit logxt lit logOt logr(lIt xt/Ot
if Xt
if Xt
where dependence of at lit Ot on and also on the data is implicit
I
To fit the model it is useful to know the gradient This can be computed using
backpropagation if we know the partial derivatives of with respect to network outputs In view of we can concentrate on a single observation and perform a summation
3Note that both sin nr and cos nr can be expressed as non-linear functions of sin and cos
which can be approximated by the network
P. M. Williams
Omitting subscript

----------------------------------------------------------------

title: 2801-rate-distortion-codes-in-sensor-networks-a-system-level-analysis.pdf

Rate Distortion Codes in Sensor Networks
A System-level Analysis
Tatsuto Murayama and Peter Davis
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
Keihanna Science City Kyoto Japan
murayama,davis}@cslab.kecl.ntt.co.jp
Abstract
This paper provides a system-level analysis of a scalable distributed sensing model for networked sensors In our system model a data center acquires data from a bunch of sensors which each independently encode
their noisy observations of an original binary sequence and transmit their
encoded data sequences to the data center at a combined rate which
is limited Supposing that the sensors use independent LDGM rate distortion codes we show that the system performance can be evaluated for
any given finite when the number of sensors goes to infinity The
analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate or the noise level
Introduction
Device and sensor networks are shaping many activities in our society These networks are
being deployed in a growing number of applications as diverse as agricultural management
industrial controls crime watch and military applications Indeed sensor networks can be
considered as a promising technology with a wide range of potential future markets
Still for all the promise it is often difficult to integrate the individual components of a sensor network in a smart way Although we see many breakthroughs in component devices
advanced software and power managements system-level understanding of the emerging
technology is still weak It requires a shift in our notion of what to look for It requires
a study of collective behavior and resulting trade-offs This is the issue that we address in
this article We demonstrate the usefulness of adopting new approaches by considering the
following scenario
Consider that a data center is interested in the data sequence
which cannot be
observed directly Therefore the data center deploys a bunch of sensors which each independently encodes its noisy observation of the sequence
without sharing any
information the sensors are not permitted to communicate and decide what to send to
the data center beforehand The data center collects separate samples from all the sensors
and uses them to recover the original sequence However since
is not the only
pressing matter which the data center must consider the combined data rate at which
the sensors can communicate with it is strictly limited A formulation of decentralized
communication with estimation task the CEO problem was first proposed by Berger
and Zhang providing a new theoretical framework for large scale sensing systems In
this outstanding work some interesting properties of such systems have been revealed If
the sensors were permitted to communicate on the basis of their pooled observations then
they would be able to smooth out their independent observation noises entirely as goes
to infinity Therefore the data center can achieve an arbitrary fidelity where
denotes the distortion rate function of In particular the data center recovers almost
complete information if exceeds the entropy rate of However if the sensors are
not allowed to communicate with each other there does not exist a finite value of for
which even infinitely many sensors can make arbitrarily small
In this paper we introduce a new analytical model for a massive sensing system with a
finite data rate R. More specifically we assume that the sensors use LDGM codes for rate
distortion coding while the data center recovers the original sequence by using optimal
majority vote estimation We consider the distributed sensing problem of deciding
the optimal number of sensors given the combined data rate R. Our asymptotic analysis successfully provides the performance of the whole sensing system when goes to
infinity where the data rate for an individual sensor information vanishes Here we exploit
statistical methods which have recently been developed in the field of disordered statistical
systems in particular the spin glass theory The paper is organized as follows In Section we introduce a system model for the sensor network Section summarizes the
results of our approach where the following section provides the outline of our analysis
Conclusions are given in the last section
System Model
Let be a probability distribution common to and be a stochastic
matrix defined on with denotes the common alphabet of where
and In the general setup we assume that the instantaneous joint probability
distribution in the form
Pr[x y1 yL
Here the random variables Yi
for the temporally memoryless source
are conditionally independent when is given and the conditional probabilities
are identical for all and In this paper we impose the binary assumptions to the problem the data sequence and its noisy observations are
all assumed to be binary sequences Therefore the stochastic matrix can be parameterized
as
if
otherwise
where represents the observation noise Note also that the alphabets have been
selected as Y. Furthermore for simplicity we also assume that always
holds implying that a purely random source is observed
At the encoding stage a sensor encodes a block of length
from the noisy observation
into a block zi zi of length
defined on Z. Hereafter we take the Boolean representation of the binary alphabet
be a reproduction sequence for the
therefore as well Let
block and we have a known integer Then making use of a Boolean matrix Ai of
dimensionality we are to find an bit codeword sequence zi zi
which satisfies
Ai
mod
where the fidelity criterion
dH
holds Here the Hamming distance is used for the distortion measure Note
that we have applied modulo-2 arithmetic for the additive operation in Let A be
characterized by ones per row and per column The finite and usually small numbers
and define a particular LDGM code family The data center then collects the
codeword sequences z1 zL Since all the codewords are of the same length
the combined data rate will be Therefore in our scenario the data center
Lastly the tth
deploys exchangeable sensors with fixed quality reproductions
symbol of the estimate
is to be calculated by majority vote
if y?L
otherwise
Therefore overall performance of the system can be measured by the expected bit error
frequency for decisions by the majority vote Pr[x
In this paper we consider two limit cases of decentralization levels The extreme situation of and the case of R. The former case means that the data rate for
an individual sensor information vanishes while the latter case results in the transmission
without coding techniques In general it is difficult to determine which level is optimal for
the estimation which scenario results in the smaller value of Indeed by using the
rate distortion codes the data center could use as many sensors as possible for a given R.
However the quality of the individual reproduction would be less informative The best
choice seems to depend largely on as well as
Main Results
For simplicity we consider the following two solvable cases for and
the optimal case of Let be a given observation noise level and the finite
real value of a given combined data rate Letting we find the expected bit error
frequency to be
dr
Pe(p
with the constant value
cg
tanh2
and the first step RSB enforcement
where the rescaled variance
tanh2 2x csch sech
holds Here denotes the normal distribution with the mean and the variance
The rescaled variance and the scale invariant parameter is determined numerically
where we use the following notations
dx
exp
tanh?1
exp
Pe
Narrow Band
Figure
Pe
Broadband
for Narrow band Broadband
Therefore it is straightforward to evaluate with for given parameters and R.
For a given finite value of we see what happens to the quality of the estimate when the
noise level varies and shows the typical behavior of the bit error frequency
Pe(p in decibel where the

----------------------------------------------------------------

title: 708-an-object-oriented-framework-for-the-simulation-of-neural-nets.pdf

An Object-Oriented Framework for the
Simulation of Neural Nets
A. Linden
Th. Sudbrak
Ch. Tietz
F. Weber
German National Research Center for Computer Science
Sankt Augustin Germany
Abstract
The field of software simulators for neural networks has been expanding very rapidly in the last years but their importance is still
being underestimated They must provide increasing levels of assistance for the design simulation and analysis of neural networks
With our object-oriented framework SESAME we intend to show
that very high degrees of transparency manageability and flexibility for complex experiments can be obtained SESAME's basic design philosophy is inspired by the natural way in which researchers
explain their computational models Experiments are performed
with networks of building blocks which can be extended very easily Mechanisms have been integrated to facilitate the construction
and analysis of very complex architectures Among these mechanisms are t.he automatic configuration of building blocks for an
experiment and multiple inheritance at run-time
Introduction
In recent years a lot of work has been put into the development of simulation
systems for neural networks Unfortunately their
importance has been largely underestimated In future software environments will
provide increasing It-vels of assistance for the design simulation and analysis of
neural networks as well as for other pattern and signal processing architectures Yet
large improvements are still necessary in order to fulfill the growing demands of the
research community Despite the existence of at least software simulators only
very few of them can deal with multiple learning paradigms and applications
Linden Sudbrak Tietz and Weber
very large experiments
In this paper we describe an object oriented framework for the simulation of neural
networks and try to illustrate its flexibility transparency and extendability The
prototype called SESAME has been implemented using on UNIX workstations
running X-Windows and currently consists of about lines of code implementing over classes for neural network algorithms pattern handling graphical
output and other utilities
Philosophy of Design
The main objective of SESAME is to allow for arbitrary combinations of different
learning and pattern processing paradigms supervised unsupervised selfsupervised or reinforcement learning and different application domains pattern recognition vision speech or control To some degree the design of SESAME
has been based on the observation that many researchers explain their neural information processing systems NIPS with block-diagrams Such a block diagram
consists of a group of primitive elements building blocks Each building block has
inputs and outputs and a functional relationship between them Connections describe the flow of data between the building blocks Scripts related to the building
blocks specify the flow of control Complex NIPS are constructed from a library
of building blocks possibly themselves whole NIPS which are interconnected via
uniform communication links
SESAME Design and Features
All building blocks share a list of common components They all have insites and
outsites that build the endpoints of communication links Datafields contain the
data weight matrices or activation vectors which is sent over the links Action functions process input from the insites update the internal state and compute
appropriate outputs performing weight updates and propagating activation or
error vectors Command functions provide a uniform user interface for all building blocks Scripts control the execution of action or command functions or other
script.s They may contain conditional statements and loops as control structures
Furthermore a symbol table allows run-time access to parameters of the building
hlock as learning rat.E's sizes data ranges etc Many other internal data structures and routines are provided for the administration and maintainance of building
blocks
The description of an experiment
of the building blocks which can
scription language see below the
the cont.rol flow defined by scripts
blocks
may be divided into the functional description
be done either in or in the high-level deconnection topology of the building blocks used
and a set of parameters for each of the building
An Object-Oriented Framework for the Simulation of Neural Nets
Design Highlights
User Interface
The user interface is text oriented and may be used interactively as well as script
driven This implies that any command that the user may choose interactively can
also be used in a command file that is called non-interactively This allows the easy
adaption of arbitrary user interface structures from a primitive batch interface for
large offline simulatiors to a fancy graphical user interface for online experiments
Another consequence is that experiments are specified in the same command language that is used for the user interface The user may thus easily switch from
description files from previously saved experiments to the interactive manipulation
of already loaded ones Since the complete structure of an experiment is accessible
at runtime this not only means manipulation of parameters but also includes any
imaginable modification of the experiment topology The experienced user can for
example include new building blocks for experiment observation or statistical evaluation and connect them to any point of the communication structure Deletion of
building blocks is possible as well as modifying control scripts The complete state
of the experiment the current values of all relevant data can be saved for later
experiments
Hierarchies
In SESAME we distinguish two kinds of building blocks terminal and non-terminal
blocks Non-terminal building blocks are used to structure a complex experiment
into hierarchies of abstract building blocks containing substructures of an experiment that may themselves contain hierarchies of substructures Terminal building
blocks provide the data structures and primitive functions that are used in scripts
of non-terminal blocks to compose the network algorithms A non-terminal building block hides its internal structure and provides abstract sites and scripts as an
interface to its internals Therefore it appears as a terminal building block to the
outside and may be used as such for the construction of an experiment This construction is equivalent to the building of a single non-terminal building block the
Experiment that encloses the complete experiment structure
Construction of New Building Blocks
The functionality of SESAME can be extended using two different approaches New
terminal building blocks can be programmed deriving from existing classes or
new non-terminal building blocks may be assembled by using previously defined
building blocks
Programming New Terminal Building Blocks
Terminal building blocks can be designed by derivation from already existing
classes The complete administration structure and possible predefined properties
are inherited from the parent classes In order to add new properties new
action functions symbols datafields insites or outsites a set of basic operations is being provided by the framework One should note that new algorithms
Linden Sudbrak Tietz and Weber
and structures can be added to a class without any changes to the framework of
SESAME
Composing New Non-Terminal Building Blocks
Non-terminal building blocks can be combined from libraries of already designed
terminal or non-terminal blocks See for an example fig where a set of building
blocks build a multilayer net which can be collapsed into one building block and
reused in other contexts Here insites and outsites define an interface between
building blocks on adjacent levels of the experiment hierarchy The flow of data
inside the new building block is controlled by scripts that call action functions or
scripts of its components Such an abstract building block may be saved in a library
for reuse Even whole experiments can be collapsed to one building block leaving
a lot of possibilities for the experimenter to cope with very large and complicated
experiments
Deriving New Non-Terminal Building Blocks
A powerful mechanism for organizing very complex experiments and allowing high
degrees of flexibility and reuse is offered by the concept of inheritance The basic
mechanism executes the description of the parent building block and thereafter
the description of the refinements for the derived block All this may be done
interactively thus additional refinements can be added at runtime Even the set of
formal parameters of a block may be inherited and/or refined Multiple inheritance
is also possible
For an example consider a general function approximator which may be used at
many points in a more complex architecture It can be implemented as an abstract
base building block only supplying basic structure as input and output and basic
operations as propagate input and train Derivations of it then implement the
algorithm and structure actually used Statistical routines visualization facilities
pattern handling and other utilities can be added as further specializations to a
basic function approximator
Parameters and Generic Building Blocks
A building block may also define formal parameters that allow the user to configure it at the time of its instantiation or inclusion into some other non-terminal
building block Thus non-terminal building blocks can be generic They may be
parameterized with types for interior building blocks names of scripts etc With
this mechanism a multilayer net can be created with an arbitrary type of node or
weight layers
Autoconfiguration
When a user defines an experiment only parameters that are really important
must be specified Redundant parameters that depend on other paremeters of other
building blocks can often be determined automatically In SESAME this is done via
a constraint satisfaction process Not only does this mechanism avoid specification
of redundant information and check experiment parameters for consistency but it
An Object-Oriented Framework for the Simulation of Neural Nets
also enables the construction of generic structures Communication links between
outsites and insites of building blocks check data for matching types Building blocks
impose additional constraints on the data formats of their own sites Constraints are
formed upon information about the base types dimensions sizes and ranges of the
data sent between the sites The primary source of information are the parameters
given to the building blocks at the time of their instantiation After building the
whole experiment a propagation mechanism iteratively tries to complete missing
information in order to satisfy all constraints Thus information which is determined
in one building block of the experiment may spread all over the experiment topology
As an example one can think of a building block which loads patterns from a
file The dimensionality of these patterns may be used automatically to configure
building blocks holding weight layers for a multilayer network
This autoconfiguration can be considered as finding the unique solution of set of
equations where three cases may occur inconsistency contradiction between two
information sources at one site deadlock insufficient information for a site or success unique solution Inconsistencies are a proof of an erroneous design Deadlocks
indicate that the user has missed something
Experiment Observation
Graphical output file I/O or statistical analysis are usually not performed within
the normal building blocks which comprise the network algorithms These features
are built into specialized utility building blocks that can be integrated at any point
of the experiment topology even during experiment runs
Classes of Building Blocks
SESAME supports a rich taxonomy of building blocks for experiment construction
For neural networks one can use building blocks for complete node and weight layers
to construct multilayer networks This granulation was chosen to allow for a more
efficient way of computation than with building blocks that contain single neurons
only This level of abstraction still captures enough flexibility for many paradigms
of NIPS However terminal building blocks for complete classes of neural nets are
also provided if efficiency is first demand
Mathematical building blocks perform arithmetic trigonometric or more general
mathematical transformations as scaling and normalization Building blocks for
coding provide functionality to encode or decode patterns
Utility building blocks provide access to the filesystem where not only input or
output files can be dealt with but also other UNIX processes by means of pipes
Others simply store structured or unstructured patterns to make them randomly
accessible
Graphical building blocks can be used to display any kind of data no matter if
weight matrices activation or error vectors are involved This is a consequence of
the abstract view of combining building blocks with different functionality but a
uniform data interface There are special building blocks for analysis which allow
for clustering averaging error analysis plotting and other statistical evaluations
Linden Sudbrak Tietz and Weber
Finally simulations cart pole robot-arms etc can also be incorporated into building blocks Real-world applications or other software packages can be accessed via
specialized interface blocks
Examples
Some illustrative examples for experiments can be found in and many additional
and more complex examples in the SESAME documentation The full documentation as well as the software are available via ftp below
Here we sketch only briefly how paradigms and applications from different domains
can be easily glued together as a natural consequence of the design of SESAME
Figure shows part of an experiment in which a robot arm is controlled via
a modified Kohonen feature map and a potential field path planner The three
building blocks workspace map and planner form the main part of the experiment
Workspace contains the simulation for the controlled robot arm and its graphical
display and map contains the feature map that is used to transform the map coordinates proposed by planner to robot arm configurations The map has been trained
in another experiment to map the configuration space of the robot arm and the
planner may have stored the positions of obstacles with respect to the map coordinates in still another experiment The configuration and obstacle map have been
saved as the results of the earlier experiments and are reused here The map was
taken from a library that contains different flavors of feature maps in form of nonterminal building blocks and hides the details of its complicated inner structure
The Views help to visualize the experiment and the Buffers are used to provide
start values for the experiment runs A Subtractor is shown that generates control
inputs for the workspace by simply performing vector subtraction on subsequently
proposed state vectors for the robot arm simulation
Epilogue
We designed an object-oriented neural network simulator to cope with the increasing demands imposed by the current lines of research Our implementation offers
a high degree of flexibility for the experimental setup Building blocks may be
combined to build complex experiments in short development cycles The simulator framework provides mechanisms to detect errors in the experiment setup and
to provide parameters for generic subexperiments A prototype was built that is
in use as our main research tool for neural network experiments and is constantly
refined Future developments are still necessary to provide a graphical interface and more elegant mechanisms for the reuse of predefined building blocks
Further research issues are the parallelization of SESAME and the compilation of
experiment parts to optimize their performance
The software and its preliminary documentation can be obtained via ftp at
ftp.gmd.de in the directory gmd/as/sesame Unfortunately we cannot provide
professional support at this moment
Acknowledgments go to the numerous programmers and users of SESAME for all
t.he work valuable discussions and hints
An Object-Oriented Framework for the Simulation of Neural Nets

----------------------------------------------------------------

title: 4056-phoneme-recognition-with-large-hierarchical-reservoirs.pdf

Phoneme Recognition with Large Hierarchical
Reservoirs
Fabian Triefenbach
Azarakhsh Jalalvand
Benjamin Schrauwen
Jean-Pierre Martens
Department of Electronics and Information Systems
Ghent University
Sint-Pietersnieuwstraat 41 Gent Belgium
fabian.triefenbach@elis.ugent.be
Abstract
Automatic speech recognition has gradually improved over the years but the reliable recognition of unconstrained speech is still not within reach In order to
achieve a breakthrough many research groups are now investigating new methodologies that have potential to outperform the Hidden Markov Model technology
that is at the core of all present commercial systems In this paper it is shown
that the recently introduced concept of Reservoir Computing might form the basis
of such a methodology In a limited amount of time a reservoir system that can
recognize the elementary sounds of continuous speech has been built The system already achieves a state-of-the-art performance and there is evidence that the
margin for further improvements is still significant
Introduction
Thanks to a sustained world-wide effort modern automatic speech recognition technology has now
reached a level of performance that makes it suitable as an enabling technology for novel applications such as automated dictation speech based car navigation multimedia information retrieval
etc Basically all state-of-the-art systems utilize Hidden Markov Models HMMs to compose an
acoustic model that captures the relations between the acoustic signal and the phonemes defined
as the basic contrastive units of the sound system of a spoken language The HMM theory has not
changed that much over the years and the performance growth is slow and for a large part owed to
the availability of more training data and computing resources
Many researchers advocate the need for alternative learning methodologies that can supplement or
even totally replace the present HMM methodology In the nineties for instance very promising
results were obtained with Recurrent Neural Networks RNNs and hybrid systems both comprising neural networks and HMMs but these systems were more or less abandoned since then
More recently there was a renewed interest in applying new results originating from the Machine
Learning community Two techniques namely Deep Belief Networks DBNs and Long ShortTerm Memory LSTM recurrent neural networks have already been used with great success for
phoneme recognition In this paper we present the first to our knowledge phoneme recognizer that
employs Reservoir Computing as its core technology
The basic idea of Reservoir Computing is that complex classifications can be performed by
means of a set of simple linear units that read-out the outputs of a pool of fixed not trained
nonlinear interacting neurons The RC concept has already been successfully applied to time series generation robot navigation signal classification audio prediction and isolated
spoken digit recognition In this contribution we envisage a RC system that can recognize the English phonemes in continuous speech In a short period couple of months we have
been able to design a hierarchical system of large reservoirs that can already compete with many
state-of-the-art HMMs that have only emerged after several decades of research
The rest of this paper is organized as follows in Section we describe the speech corpus we are
going to work on in Section we recall the basic principles of Reservoir Computing in Section we
discuss the architecture of the reservoir system which we propose for performing Large Vocabulary
Continuous Speech Recognition LVCSR and in Section we demonstrate the potential of this
architecture for phoneme recognition
The speech corpus
Since the main aim of this paper is to demonstrate that reservoir computing can yield a good acoustic
model we will conduct experiments on TIMIT an internationally renowned corpus that was
specifically designed to support the development and evaluation of such a model
The TIMIT corpus contains English sentences spoken by different speakers representing
eight dialect groups About of the speakers are male the others are female The corpus documentation defines a training set of speakers and a test set of different speakers a main test
set of speakers and a core test set of 24 speakers Each speaker has uttered sentences two
SA sentences which are the same for all speakers SX-sentences from a list of sentences each
one thus appearing times in the corpus and SI-sentences from a set of sentences each one
thus appearing only once in the corpus To avoid a biased result the SA sentences will be excluded
from training and testing
For each utterance there is a manual acoustic-phonetic segmentation It indicates where the phones
defined as the atomic units of the acoustic realizations of the phonemes begin and end There are 61
distinct phones which for evaluation purposes are usually reduced to an inventory of 39 symbols
as proposed by Two types of error rates can be reported for the TIMIT corpus One is the
Classification Error Rate defined as the percentage of the time the top hypothesis of the tested
acoustic model is correct The second one is the Recognition Error Rate defined as the ratio
between the number of edit operations needed to convert the recognized symbol sequence into the

----------------------------------------------------------------

title: 1707-an-meg-study-of-response-latency-and-variability-in-the-human-visual-system-during-a-visual-motor-integration-task.pdf

An MEG Study of Response Latency and
Variability in the Human Visual System
During a Visual-Motor Integration Task
Akaysha C. Tang
Dept of Psychology
University of New Mexico
Albuquerque NM
akaysha@unm.edu
Barak A. Pearlmutter
Dept of Computer Science
University of New Mexico
Albuquerque NM
bap@cs unm edu
Tim A. Hely
Santa Fe Institute
Hyde Park Road
Santa Fe NM
Michael Zibulevsky
Dept of Computer Science
University of New Mexico
Albuquerque NM
Michael P. Weisend
VA Medical Center
San Pedro SE
Albuquerque NM
timhely@santafe edu
michael@cs.unm.edu
mweisend@unm.edu
Abstract
Human reaction times during sensory-motor tasks vary considerably To begin to understand how this variability arises we examined neuronal populational response time variability at early versus
late visual processing stages The conventional view is that precise temporal information is gradually lost as information is passed
through a layered network of mean-rate units We tested in humans whether neuronal populations at different processing stages
behave like mean-rate units A blind source separation algorithm
was applied to MEG signals from sensory-motor integration tasks
Response time latency and variability for multiple visual sources
were estimated by detecting single-trial stimulus-locked events for
each source In two subjects tested on four visual reaction time
tasks we reliably identified sources belonging to early and late visual processing stages The standard deviation of response latency
was smaller for early rather than late processing stages This supports the hypothesis that human populational response time variability increases from early to late visual processing stages
Introduction
In many situations precise timing of a motor output is essential for successful task
completion Somehow the reliability in the output timing is related to the reliability
of the underlying neural systems associated with different stages of processing Recent literature from animal studies suggests that individual neurons from different
brain regions and different species can be surprising reliable 17
A. C. Tang B. A. Pearlmutter T. A. Hely M. Zibulevsky and P. Weisend
on the order of a few milliseconds Due to the low spatial resolution of electroencephalography EEG and the requirement of signal averaging due to noisiness of
magnetoencephalography in vivo measurement of human populational response time variability from different processing stages has not been available
In four visual reaction time tasks we estimated neuronal response time variability at different visual processing stages using MEG. One major obstacle that has
prevented the analysis of response timing variability using MEG before is the relative weakness of the brain's magnetic signals compared to noise in a shielded
environment magnetized lung contaminants abdominal currents lO5f!'j cardiogram and oculogram epileptic and spontaneous activity and in
the sensors Consequently neuronal responses evoked during cognitive
tasks often require signal averaging across many trials making analysis of singletrial response times unfeasible
Recently Bell-Sejnowski Infomax and Fast ICA algorithms have been
used successfully to isolate and remove major artifacts from EEG and MEG data
These methods greatly increase the effective signal-to-noise ratio and
make single-trial analysis of EEG data feasible Here we applied a SecondOrder Blind Identification algorithm SOBI another blind source separation or
BSS algorithm to MEG data to find out whether populational response variability
changes from early to late visual processing stages
Methods
Experimental Design
Two volunteer normal subjects females right handed with normal or correctedto-normal visual acuity and binocular vision participated in four different visual RT
tasks Subjects gave informed consent prior to the experimental procedure During
each task we recorded continuous MEG signals at a sampling rate with a
band-pass filter of using a channel Neuromag-122
In all four tasks the subject was presented with a pair of abstract color patterns
one in the left and the other in the right visual field One of the two patterns was a
target pattern The subject pressed either a left or right mouse button to indicate on
which side the target pattern was presented When a correct response was given a
low or high frequency tone was presented binaurally following respectively a correct
or wrong response The definition of the target pattern varied in the four tasks and
was used to control task difficulty which ranged from easy task to more difficult
task with increasing RTs. The specific differences among the four tasks are not
important for the analysis which follows and are not discussed further
In this study we focus on the one element that all tasks have in common Le. activation of multiple visual areas along the visual pathways Our goal is to identify
visual neuronal sources activated in all four visual RT tasks and to measure and
compare response time variability between neuronal sources associated with early
and later visual processing stages Specifically we test the hypothesis that populational neuronal response times increase from early to later visual processing stages
Source Separation Using SOBI
In MEG magnetic activity from different neuronal populations is observed by many
senSOrs arranged around the subject's head Each sensor responds to a mixture of
the signals emitted by multiple sources We used the Second-Order Blind Identi
MEG Study ofResponse Latency and Variability
fication algorithm SOBI BSS algorithm to simultaneously separate neuromagnetic responses from different neuronal populations associated with different
stages of visual processing Responses from different neuronal populations will be
referred to as source responses and the neuronal populations that give rise to these
responses will be referred to as neuronal sources or simply sources These neuronal sources often but not always consist of a spatially contiguous population of
neurons BSS separates the measured sensor signals into maximally independent
components each having its own spatial map Previously we have shown that some
of these BSS separated components correspond to noise sources and many others
correspond to neuronal sources
To establish the identity of the components we analyzed both temporal and spatial properties of the BSS separated components Their temporal properties are
displayed using MEG images similar to the ERP images described by but
without smoothing across trials These MEG images show stimulus or response
locked responses across many trials in a map from which response latencies across
all displayed trials can be observed with a glance The spatial properties of the separated components are displayed using a field map that shows the sensor projection
of a given component The intensity at each point on the field map indicates how
strongly this component influences the sensor at this location
The correspondence between the separated components and neuronal populational
responses at different visual processing stages were established by considering both
spatial and temporal properties of the separated components For example
a component was identified as an early visual neuronal source if and only if
the field pattern or the sensor projection of the separated component showed
a focal response over the occipital lobe and the ERP image showed visual
stimulus locked responses with latencies shorter than all other visual components
and falling within the range of early visual responses reported in studies using
other methods Only those components consistent both spatially and temporally
with known neurophysiology and neuroanatomy were identified as neuronal sources
Single Event Detection and Response Latency Estimation
For all established visual components we calculated the single-trial response latency
as follows First a detection window was defined using the stimulus-triggered average The beginning of the detection window was defined by the time at
which the STA first exceeded the range of baseline fluctuation Baseline fluctuation
was estimated from the time of stimulus onset for approximately the visual
response occurred no earlier than after stimulus onset The detection window ended when the STA first returned to the same level as when the detection
window began The detection threshold was determined using a control window
with the same width as the detection window but immediately preceding the detection window The threshold was adjusted until no more than five false detections
occurred within the control window for each ninety trials We estimated RTs using
the leading edge of the response rather than the time of the peak as this is more
robust against noise
Results
In both subjects across all four visual RT tasks SOBI generated components that
corresponded to neuronal populational responses associated with early and late
stages of visual processing In both subjects we identified a single component with
a sensor projection at the occipital lobe whose latency was the shortest among all
A. C. Tang B. A. Pearlmutter T. A. Hely Zibulevsky and P. Weisend
late source
a
Figure MEG images and field maps for an early and a late source from each
task for subject top and subject bottom MEG image pixels are brightnesscoded source strength Each row of a bitmap is one trial running from left
to right Vertical bars mark stimulus onset and of pre-stimulus activity is
shown Each panel contains 90 trials Field map brightness indicates the strength
with which a source activates each of the 61 sensor pairs
visual stimulus locked components within task and subject left We identified multiple components that had sensor projections either at occipital-parietal
occipital-temporal or temporal lobes and whose response latencies are longer than
early-stage components within task and subject right
2a shows examples of detected single-trial responses for one early and one late
visual component left early right late from one task To minimize false positives
the detection threshold was set high allowing false detections out of 90 trials
at the expense of a low detection rate When Gaussian filters were
applied to the raw separated data the detection rates were increased to 22
similar results hold but not shown 2b shows such detected response time
histograms superimposed on the stimulus triggered average using raw separated
data One early top row and two late visual components middle and bottom
rows are plotted for each of the four experiments in subject one The histogram
width is smallest for early visual components short mean response latency and
larger for late visual components longer latency
We computed the standard deviation of component response times as a measure of
response variability 2c shows the response variability as a function of mean
response latency for subject one Early components solid boxes shorter mean
latency have smaller variability height of the boxes while late components dashed
boxes longer mean latency have larger variability height of the boxes Multiple
MEG Study of Response Latency and Variability
bd:Qlld
llJ
l~J
l~J
a
a
wo
latency
Figure left Response onset was estimated for each trial via threshold crossing
within a window of eligibility top right The stimulus-locked averages for a
number of sources overlaid on histograms of response onset times bottom
right Scatter plot of visual components from all experiments on subject showing
the standard deviation of the latency axis versus the mean latency axis
with the error bars in each direction indicating one standard error in the respective
measurement Lines connect sources from each task
visual components from each task are connected by a line Four tasks were shown
here There is a general trend of increasing standard deviation of response times
as a function of early-to-late processing stages increasing mean latency from left
to right For the early visual components the standard deviation ranges from
to and for the late visual components from
to
Discussion
By applying SOBl to MEG data from four visual RT tasks we separated components corresponding to neuronal populational responses associated with early and
A. C. Tang B. A. Pearlmutter T. A. Hely Zibulevsky and P. Weisend
later stage visual processing in both subjects across all tasks We performed singletrial RT detection on these early and late-stage components and estimated both
the mean and stdev of their response latency We found that variability of the
populational response latency increased from early to late processing stages
These results contrast with single neuron recordings obtained previously In early
and late visual processing stages the rise time of mean firing rate in single units
remained constant suggesting an invariance in RT variability Characterizing
the precise relationship between single neuron and populational response reliability
is difficult without careful simulations or simultaneous single unit and MEG recording However some major differences exist between the two types of studies While
MEG is more likely to sample a larger neuronal population single unit studies are
more likely to be selective to those neurons that are already highly reliable in their
responses to stimulus presentation It is possible that the most reliable neurons at
both the early and late processing stages are equally reliable while large differences
exist between the early and late stages for the low reliability neurons
Previously ICA algorithms have been used successfully to separate out various noise
and neuronal sources in MEG data Here we show that SOBI can also be
used to separate different neuronal sources particularly those associated with different processing stages The SOBI algorithm assumes that the components are
independent across multiple time scales and attempts to minimize the temporal
correlation at these time scales Although neuronal sources at different stages of
processing are not completely independent as assumed in SOBl's derivation BSS
algorithms of this sort are quite robust even when the underlying assumptions are
not fully met the goodness of the separation is not significantly affected
The ultimate reality check should come from satisfying physiological and anatomical constraints derived from prior knowledge of the neural system under study
This was carried out for our analysis Firstly the average response latencies of the
separated components fell within the range of latencies reported in MEG studies using conventional source modeling methods Secondly the spatial patterns of sensor
responses to these separated components are consistent with the known functional
anatomy of the visual system
We have attempted to rule out many confounding factors Our observed results
cannot be accounted for by a higher signal to noise ratio in the early visual responses The increase in measured onset response time variability from early to
late visual processing stages was actually accompanied by an slightly lower signalto-noise ratio among the early components The number of events detected for the
later components were also slightly greater than the earlier components The higher
signal-to-noise ratio at later components should reduce noise-induced variability in
the later components which would bias against the hypothesis that later visual responses have greater response time variability We also found that response duration
and detection window size cannot account for the observed differential variabilities
Later visual responses also had gentler onset slopes as measured by the stimulustriggered average Sensor noise unavoidably introduces noise into the response
onset detection process We cannot rule out the possibility that the interaction of
the noise with the response onset profiles might give rise to the observed differential variabilities Similarly we cannot rule out the possibility that even greater
control of the experimental situation such as better fixation and more effective
head restraints would differentially reduce the observed variabilities In general all
measured variabilities can only be upper bounds subject to downward revision as
improved instrumentation and experiments become available It is with this caution in mind that we conclude that response time variability of neuronal populations
increases from early to late processing stages in the human visual system
MEG Study of Response Latency and Variability
Acknowledgments
This research was supported by NSF CAREER award and by the National Foundation for Functional Brain Imaging

----------------------------------------------------------------

title: 6602-error-analysis-of-generalized-nystrom-kernel-regression.pdf

Error Analysis of Generalized Nystr?m Kernel
Regression
Hong Chen
Computer Science and Engineering
University of Texas at Arlington
Arlington TX
chenh@mail.hzau.edu.cn
Haifeng Xia
Mathematics and Statistics
Huazhong Agricultural University
Wuhan
haifeng.xia0910@gmail.com
Weidong Cai
School of Information Technologies
University of Sydney
NSW Australia
tom.cai@sydney.edu.au
Heng Huang
Computer Science and Engineering
University of Texas at Arlington
Arlington TX
heng@uta.edu
Abstract
Nystr?m method has been successfully used to improve the computational efficiency of kernel ridge regression Recently theoretical analysis of Nystr?m
KRR including generalization bound and convergence rate has been established
based on reproducing kernel Hilbert space RKHS associated with the symmetric
positive semi-definite kernel However in real world applications RKHS is not
always optimal and kernel function is not necessary to be symmetric or positive
semi-definite In this paper we consider the generalized Nystr?m kernel regression
GNKR with coefficient regularization where the kernel just requires the continuity and boundedness Error analysis is provided to characterize its generalization
performance and the column norm sampling strategy is introduced to construct the
refined hypothesis space In particular the fast learning rate with polynomial decay
is reached for the GNKR Experimental analysis demonstrates the satisfactory
performance of GNKR with the column norm sampling
Introduction
The high computational complexity makes kernel methods unfeasible to deal with large-scale data
Recently the Nystr?m method and its alternatives the random Fourier feature technique
the sketching method have been used to scale up kernel ridge regression KRR 23 The
key step of Nystr?m method is to construct a subsampled matrix which only contains part columns
of the original empirical kernel matrix Therefore the sampling criterion on the matrix column
affects heavily on the learning performance The subsampling strategies of Nystr?m method can be
categorized into two types uniform sampling and non-uniform sampling The uniform sampling is
the simplest strategy which has shown satisfactory performance on some applications 23
From different theoretical aspects several non-uniform sampling approaches have been proposed
such as the square column-norm sampling the leverage score sampling and the
adaptive sampling Besides the sampling strategies there exist learning bounds for Nystr?m
kernel regression from three measurements the matrix approximation the coefficient
approximation and the excess generalization error
Despite rapid progress on theory and applications the following critical issues should be further
addressed for Nystr?m kernel regression
Conference on Neural Information Processing Systems NIPS Barcelona Spain
Nystr?m regression with general kernel The previous algorithms are mainly limited to
KRR with symmetric and positive semi-definite kernels For real-world applications this
restriction may be not necessary Several general kernels have shown the competitive
performance in machine learning the indefinite kernels for regularized algorithms
and PCA Therefore it is important to formulate the learning algorithm for
Generalized Nystr?m Kernel Regression GNKR
Generalization analysis and sampling criterion Previous theoretical results rely on the
symmetric positive semi-definite SPSD matrix associated with a Mercer kernel
However this condition is not satisfied for GNKR which induces the additional difficulty on
error analysis Can we get the generalization error analysis for GNKR It is also interesting
to explore the sampling strategy for GNKR the column-norm sampling in
To address the above issues we propose the GNKR algorithm and investigate its theoretical properties
on generalization bound and learning rate Inspired from the recent studies for data dependent
hypothesis spaces we establish the error analysis for GNKR which implies that the learning
rate with polynomial decay can be reached under proper parameter selection Meanwhile we extend
the column norm subsampling in the linear regression to the GNKR setting
The main contributions of this paper can be summarized as below
GNKR with regularization Due to the lack of Mercer condition associated with general
kernel coefficient regularization becomes a natural choice to replace the kernel norm
regularization in KRR. Note that Nystr?m approximation has the similar role with the
regularization in 18 which addresses the sample sparsity on hypothesis function
Hence we formulate GNKR by combining the Nystr?m method and the least squares
regression with regularization in
Theoretical and empirical evaluations From the view of learning with data dependent
hypothesis spaces theoretical analysis of GNKR is established to illustrate its generalization
bound and learning rate In particular the fast learning rate arbitrarily close to is
obtained under mild conditions where is the size of subsampled set The effectiveness of
GNKR is also supported by experiments on synthetic and real-world data sets
Related Works
Due to the flexibility and adaptivity least squares regression algorithms with general kernel have been
proposed involving various types of regularization the regularizer the regularizer
and the elastic net regularization For the Mercer kernel these algorithms are related
closely with the KRR which has been well understand in learning theory For the general kernel
setting theoretical foundations of regression with coefficient regularization have been studied recently
via the analysis techniques with the operator approximation and the empirical covering numbers
18 Although rich results on theoretical analysis the previous works mainly focus on the
prediction accuracy without considering the computation complexity for large scale data
Nystr?m approximation has been studied extensively for kernel methods recently Almost all existing
studies are relied on the fast approximation of SPSD matrix associated with a Mercer kernel For the
fixed design setting the expectation of the excess generalization error is bounded for least square
regression with the regularizer in RKHS Recently the probabilistic error bounds have been
estimated for Nystr?m KRR in In the fast learning rate with is derived for the
fixed design regression under the conditions on kernel matrix eigenvalues In the convergence
rate is obtained under the capacity assumption and the regularity assumption It is worthy notice
that the learning bound in is based on the estimates of the sample error the computation error
and the approximation error Indeed the computation error is related with the sampling subset and
can be considered as the hypothesis error in which is induced by the variance of hypothesis
spaces Differently from previous works our theoretical analysis of GNKR is dependent on general
continuous kernel and coefficient regularization
Generalized Nystr?m Kernel Regression
Let be a probability distribution on where Rd and are viewed as the
input space and the output space respectively Let be the conditional distribution of for
given and let be a measurable function space on In statistical learning the samples
zi are drawn independently and identically from an unknown distribution
The task of least squares regression is to find a prediction function such that the
expected risk
E(f
as small as possible From the viewpoint of approximation theory this means to search a good
approximation of the regression function
based on the empirical risk
Ez
1X
Let be a continuous and bounded kernel function Without loss of generality we
assume that sup and for all for all throughout this paper
Besides the given samples the hypothesis function space is crucial to reach well learning performance The following data dependent hypothesis space has been used for kernel regression with
coefficient regularization
Hn
K(xi
Rn
Given kernel regression with regularization is formulated as
f?z
z,i K(xi
with
arg min
n1
kKnn
where Knn K(xi yn and is a regularization parameter
Even the positive semi-definiteness is not required for the kernel also can be solved by the
following linear system Theorem in
Knn
Knn In
Knn
where In is the n-order unit matrix
From the viewpoint of learning function in Hn can be rewritten as
f?z arg min Ez nkf
Hn
where
kf inf
nX
i2
K(xi
In a standard implementation of the computational complexity is This computation
requirement becomes the bottleneck of when facing large data sets To reduce the computational
burden we consider to find the predictor in a smaller hypothesis space
Hm
Rm
The generalized Nystr?m kernel regression GNKR can be formulated as
fz arg min Ez mkf
Hm
Denote Knm ij K(xi
Kmm jk
for We can
deduce that
fz
with
Knm
Knm mnIm Knm
Y.
The key problem of is how to select the subset
such that the computational complexity
can be decreased efficiently while satisfactory accuracy can be guaranteed For the KRR there
are several strategies to select the subset with different motivations In this paper we
preliminarily consider the following two strategies with low computational complexity
Uniform Subsampling The subset
is drawn uniformly at random from the input
Column-norm Subsampling The subset
is drawn from independently with
probabilities pi PnkKkK
where Ki K(xn Rn
Some discussions for the column-norm subsampling will be provided in Section
Learning Theory Analysis
In this section we will introduce our theoretical results on generalization bound and learning rate
The detailed proofs can be found in the supplementary materials
Inspired from analysis technique in we introduce the intermediate function for error decomposition firstly Let be the square integrable space on with norm For any bounded
continuous kernel the integral operator LK is defined as
LK
t)f
where is the marginal distribution of Given and LK introduce the function space
LK with kgkH inf kf LK
Since is sample independent the intermediate function can be constructed as LK where
arg min E(LK kf
In learning theory usually is called as the regularized function and
inf kgk2H E(LK
g?H
is called the approximation error
To further bridge the gap between and fz we construct the stepping stone function
The following condition on is used in this paper which has been well studied in learning theory
literature Examples include Gaussian kernel the sigmoid kernel and the fractional
power polynomials
Definition The kernel function is a kernel with if there exists some constant cs
such that
cs kx ks2
The definition of tells us so it is natural to restrict the predictor to The
projection operator
has been extensively used in learning theory analysis
It is a position to present our result on the generalization error bound
Theorem Suppose that is compact subset of Rd and for some For any
with confidence there holds
log2
where constant is independent of and
if
if
if
Theorem is a general result that applies to Lipschitz continuous kernel Although the statement
appears somewhat complicated at first sight it yields fast convergence rate on the error when
specialized to particular kernels Before doing so let us provide a few heuristic arguments for
intuition Theorem guarantees an upper bound of the form
k?(fz inf E(LK kf
Note that a smaller value of reduces the approximation error term but increases the second term
associated with the sample error This inequality demonstrates that the proper should be selected
to balance the two terms This quantitative relationship also can be considered as the oracle
inequality for GNKR where the approximation error only can be obtained by an oracle knowing
the distribution
Theorem tells us that the generalization bound of GNKR depends on the numbers of samples
the continuous degree and the approximation error In essential the subsampling number
has double impact on generalization error one is the complexity of data dependent hypothesis space
Hm and the other is the selection of parameter
Now we introduce the characterization of approximation error which has been studied in
Definition The target function can be approximated with exponent in if there
exists a constant such that for any
If the kernel is not symmetric or positive semi-definite the approximation condition holds true for
2r
is the integral operator associated with
L?X where LK
when LK
x)d?X
Now we state our main results on the convergence rate
Theorem Let be a compact subset of Rd Assume that can be approximated with exponent
in and for some Choose and for some
For any with confidence there holds
log2
where constant is independent of and
min
Theorem states the polynomial convergence rate of GNKR and indicates its dependence on the
subsampling size as Similar observation also can be found in Theorem for
Nystr?m KRR where the fast learning rate also is relied on the grow of under fixed hypothesis
space complexity However even we do not consider the complexity of hypothesis space the increase
of will add the computation complexity Hence a suitable size of is a trade off between the
approximation performance and the computation complexity When means
that can be chosen between and under the conditions in Theorem In particular the fast
convergence rate can be obtained as and
The most related works with Theorems and are presented in where learning bounds are
established for Nystr?m KRR. Compared with the previous results the features of this paper can be
summarized as below
Learning model This paper considered Nystr?m regression with data dependent hypothesis
space and coefficient regularization which can employ general kernel including the indefinite kernel and nonsymmetric kernel However the previous analysis just focuses on the
positive semi-definite kernel and the regularizer in RKHS For a fixed design KRR the fast
convergence in depends on the eigenvalue condition of kernel matrix Differently from our result relies on the Lipschitz continuity of kernel and the approximation
condition for the statistical learning setting
Analysis technique The previous analysis in utilizes the theoretical techniques for
operator approximation and matrix decomposition which depends heavily on the symmetric
positive semi-definite kernel For GNKR the previous analysis is not valid directly since
the kernel is not necessary to satisfy the positive semi-definite or symmetric condition The
flexibility on kernel and the adaptivity on hypothesis space induce the additional difficulty
on error analysis Fortunately the error analysis is obtained by incorporating the error
decomposition ideas in and the concentration estimate techniques in An
interesting future work is to establish the optimal bound of GNKR to extend Theorem in
to the general setting
For the proofs of Theorem and the key idea is using as the stepping stone function to bridge fz
and Additionally the connection between LK and has been well studied in learning
theory Hence the proofs in Appendix follow from the approximation decomposition
In remainder of this section we present a simple analysis for column-norm subsampling
Given the full samples and sampling number the key of subsampling is to
select a subset of with strong inference ability In other words we should select the subset with
small divergence with the full sample estimator Following this idea the optimal subsampling
criterion is studied in for the linear regression
zi and Knn we introduce
Pn Given
1?Lii
the objective function pn
pi kKi by extending in to
the kernel-based setting Here pi are the sampling probabilities with respect to and
Lii Knn Knn
Knn In Knn
ii are basic leverage values obtained from
For the fixed design setting assume that KiT Rn where
are drawn identically and independently from Then for min pn can
be transformed as min Etr((Knn Knn which is related with the A-optimality or
A-criterion for subset selection in
When Lii for any we can get the following sampling probabilities
Theorem When hii for the minimizer of pn can be approximated by
kKi
pi Pn
kKi
Usually the leverage values are computed by fast approximation algorithms since Lii involves
the inverse matrix Different from the leverage values the sampling probabilities in Theorem can
be computed directly which just involves the column-norm of empirical matrix
Table Average RMSE of GNKR with Gaussian(G)/Epanechnikov(E kernel under different
sampling strategies and sampling size US:=Uniform subsampling CS Column-norm subsampling
Function
f1 sin
f2 sin
f3 sign(x
f4 cos(ex sin
Algorithm
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
Experimental Analysis
Since kernel regression with different types of regularization has been well studied in this
section just presents the empirical evaluation of GNKR to illustrate the roles of sampling strategy and
kx?tk2
kernel function Gaussian kernel KG exp is used for simulated data and real
kx?tk2
data Epanechnikov kernel KE is used in the simulated experiment Here
denotes the scale parameter selected form Following the discussion on parameter
selection in we select the regularization parameter of GNKR from The
best results are reported according to the measure of Root Mean Squared Error RMSE
Experiments on synthetic data
Following the empirical studies in we design simulation experiments on f1 sin
f2 sinx f3 sign(x and f4 cos(ex
sin
The function fi is considered as the truly regression function for Note
that f1 f2 are smooth f3 is not continuous and f4 embraces a highly oscillatory part First we select
points randomly from the preset interval and generate the dependent variable according to
the corresponding function Then we divided these data into two parts with equal size we chose one
part as the training samples and the other is regarded as testing samples For the training samples the
output is contaminated by Gaussian noise For each function and each kernel we run the
experiment times The average RMSE is shown in Table The results indicate that the column
norm subsampling can achieve the satisfactory performance In particular GNKR with the indefinite
Epanechnikov kernel has better performance than Gaussian kernel for the noncontinuous function f3
and the non-flat function f4 This observation is consistent with the empirical result in
Experiments on real data
In order to better evaluate the empirical performance four data sets are used in our study including
the Wine Quality CASP Year Prediction datasets http://archive.ics.uci.edu/ml and the census-house
dataset http://www.cs.toronto.edu delve/data/census-house/desc.html The detailed information
about the data sets are showed in Table Firstly each data set is standardized by subtracting its
mean and dividing its standard deviation Then each input vector is unitized For CASP and Year
Prediction samples are drawn randomly from data sets where half is used for training and the
rest is for testing For other datasets we random select part samples to training and use the rest part
as test set Table reports the average RMSE over ten trials
Table shows the performance of two sampling strategies For CASP and Year Prediction we can
see that GNKR with selected samples can achieve the satisfactory performance which reduce
the computation complexity of efficiently Additionally the competitive performance of GNKR
with Epanechnikov kernel is demonstrated via the experimental results on the four data sets These
empirical examples support the effectiveness of the proposed method
Table Statistics of data sets
Dataset
Wine Quality
Year Prediction
Features
90
Instances
Train
Test
Dataset
CASP
census-house
Feature
Instance
Train
Test
Table Average RMSE with Gaussian(G)/Epanechnikov(E kernel under different sampling
levels and strategies US:=Uniform subsampling CS Column-norm subsampling
Function
Wine Quality
CASP
Year Prediction
census-house
Algorithm
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
Conclusion
This paper focuses on the learning theory analysis of Nystr?m kernel regression One key difference
with the previous related work is that GNKR uses general continuous kernel function and coefficient
regularization The stepping-stone functions are constructed to overcome the analysis difficulty
induced by the difference The learning bound with fast convergence is derived under mild conditions
and empirical analysis is provided to verify our theoretical analysis
Acknowledgments
This work was partially supported by U.S. NSF-IIS NSF-IIS NSF-DBI
NSF-IIS NSF-IIS NIH and by National Natural Science Foundation
of China NSFC We thank the anonymous NIPS reviewers for insightful comments

----------------------------------------------------------------

title: 1924-active-support-vector-machine-classification.pdf

Active Support Vector Machine
Classification
L. Mangasarian
Computer Sciences Dept
University of Wisconsin
West Dayton Street
Madison WI
David R. Musicant
Dept of Mathematics and Computer Science
Carleton College
One North College Street
Northfield MN
olvi@cs.wisc.edu
dmusican@carleton.edu
Abstract
An active set strategy is applied to the dual of a simple reformulation of the standard quadratic program of a linear support vector
machine This application generates a fast new dual algorithm
that consists of solving a finite number of linear equations with a
typically large dimensionality equal to the number of points to be
classified However by making novel use of the Sherman-MorrisonWoodbury formula a much smaller matrix of the order of the original input space is inverted at each step Thus a problem with a
32-dimensional input space and million points required inverting
positive definite symmetric matrices of size 33 33 with a total running time of 96 minutes on a MHz Pentium II. The algorithm
requires no specialized quadratic or linear programming code but
merely a linear equation solver which is publicly available
Introduction
Support vector machines SVMs are powerful tools for data classification Classification is achieved by a linear or nonlinear separating surface in the
input space of the dataset In this work we propose a very fast simple algorithm
based on an active set strategy for solving quadratic programs with bounds
The algorithm is capable of accurately solving problems with millions of points and
requires nothing more complicated than a commonly available linear equation solver
for a typically small dimensional input space of the problem
Key to our approach are the following two changes to the standard linear SVM
Maximize the margin distance between the parallel separating planes with
respect to both orientation as well as location relative to the origin
See equation below Such an approach was also successfully utilized in
the successive overrelaxation SOR approach of as well as the smooth
support vector machine SSVM approach of
The error in the soft margin is minimized using the 2-norm squared
instead of the conventional 1-norm See equation Such an approach
has also been used successfully in generating virtual support vectors
These simple but fundamental changes lead to a considerably simpler positive
definite dual problem with nonnegativity constraints only See equation
In Section of the paper we begin with the standard SVM formulation and its
dual and then give our formulation and its simpler dual We corroborate with solid
computational evidence that our simpler formulation does not compromise on generalization ability as evidenced by numerical tests in Section on public datasets
See Table Section gives our active support vector machine ASVM Algorithm
which consists of solving a system of linear equations in dual variables with
a positive definite matrix By invoking the Sherman-Morrison-Woodbury SMW
formula we need only invert an matrix where is the dimensionality of the input space This is a key feature of our approach that allows us to
solve problems with millions of points by merely inverting much smaller matrices of
the order of In concurrent work Ferris and Munson also use the SMW formula
but in conjunction with an interior point approach to solve massive problems based
on our formulation as well as the conventional formulation Burges has
also used an active set method but applied to the standard SVM formulation
instead of as we do here Both this work and Burges appeal in different ways
to the active set computational strategy of More and Toraldo We note that
an active set computational strategy bears no relation to active learning Section
describes our numerical results which indicate that the ASVM formulation has a
tenfold testing correctness that is as good as the ordinary SVM and has the capability of accurately solving massive problems with millions of points that cannot be
attacked by standard methods for ordinary SVMs
We now describe our notation and give some background material All vectors will
be column vectors unless transposed to a row vector by a prime I. For a vector
Rn denotes the vector in Rn with all of its negative components set to
zero The notation A Rm will signify a real matrix For such a matrix
A will denote the transpose of A and Ai will denote the i-th row of A. A vector
of ones or zeroes in a real space of arbitrary dimension will be denoted by or
respectively The identity matrix of arbitrary dimension will be denoted by I.
For two vectors and in Rn denotes orthogonality that is For
Rm xm and UB denotes UiEB QB denotes QiEB
and QBB denotes a principal submatrix of with rows and columns B.
The notation argminxEs denotes the set of minimizers in the set of the
real-valued function defined on S. We use to denote definition The 2-norm
of a matrix will be denoted by IIQI12 A separating plane with respect to two
given point sets A and in is a plane that attempts to separate into two
halfspaces such that each open halfspace contains points mostly of A or B. A special
case of the Sherman-Morrison-Woodbury SMW formula will be utilized
Ilv v(I H(Ilv
where is a positive number and is an arbitrary matrix This formula
enables us to invert a large matrix by merely inverting a smaller matrix
The Linear Support Vector Machine
We consider the problem of classifying points in the n-dimensional real space
represented by the matrix A according to membership of each point Ai
in the class A or A as specified by a given diagonal matrix with
or along its diagonal For this problem the standard SVM with a linear kernel
is given by the following quadratic program with parameter
ve'y D(Aw
w,'Y,y)ERn
mm
x'w
Ox
A
A
x'w
Margln
IIwl12
X'W
Figure The bounding planes with a soft with some errors margin
and the plane approximately separating A from A-.
Here is the normal to the bounding planes
x'w
and'Y determines their location relative to the origin Figure The plane x'w
bounds the A points possibly with error and the plane x'w bounds
the A points also possibly with some error The separating surface is the plane
x'w
midway between the bounding planes The quadratic term in is twice the
reciprocal of the square of the 2-norm distance between the two bounding
planes of Figure This term maximizes this distance which is often called
the margin If the classes are linearly inseparable as depicted in Figure then
the two planes bound the two classes with a soft margin That is they bound each
set approximately with some error determined by the nonnegative error variable
for
for
Dii
Dii
Traditionally the I-norm of the error variable is minimized parametrically with
weight in resulting in an approximate separation as depicted in Figure The
dual to the standard quadratic linear SVM 22 is the following
mill
u'DAA'Du e'u e'Du ve
uER=2
The variables of the primal problem which determine the separating surface
can be obtained from the solution of the dual problem above Eqns and
We note immediately that the matrix DAA'D appearing in the dual objective
function is not positive definite in general because typically Also
there is an equality constraint present in addition to bound constraints which for
large problems necessitates special computational procedures such as SMO
Furthermore a one-dimensional optimization problem must be solved in order
to determine the locator of the separating surface In order to overcome all
these difficulties as well as that of dealing with the necessity of having to essentially
invert a very large matrix of the order of we propose the following simple
but critical modification of the standard SVM formulation We change Il lll to
Ilyll which makes the constraint redundant We also append the term to
w'w This in effect maximizes the margin between the parallel separating planes
with respect to both wand that is with respect to both orientation and
location of the planes rather that just with respect to which merely determines
the orientation of the plane This leads to the following reformulation of the SVM
y'y
min
D(Aw er
the dual of this problem is
I
min D(AA ee')D)u
O~uER
The variables of the primal problem which determine the separating surface
are recovered directly from the solution of the dual above by the relations
w=A'Du
We immediately note that the matrix appearing in the dual objective function is
positive definite and that there is no equality constraint and no upper bound on the
dual variable The only constraint present is a simple nonnegativity one These
facts lead us to our simple finite active set algorithm which requires nothing more
sophisticated than inverting an matrix at each iteration in order
to solve the dual problem
ASVM Active Support Vector Machine Algorithm
The algorithm consists of determining a partition of the dual variable into nonbasic
and basic variables The nonbasic variables are those which are set to zero The
values of the basic variables are determined by finding the gradient of the objective
function of with respect to these variables setting this gradient equal to zero and
solving the resulting linear equations for the basic variables If any basic variable
takes on a negative value after solving the linear equations it is set to zero and
becomes nonbasic This is the essence of the algorithm In order to make the
a lgorithm converge and terminate a few additional safeguards need to be put in
place in order to a llow us to invoke the More Toraldo finite termination result
The other key feature of the algorithm is a computational one and makes use of the
SMW formula This feature allows us to invert an matrix at each
step instead of a much bigger matrix of order
Before stating our a lgorithm we define two matrices to simplifY notation as follows
D[A
I HH'.
With these definitions the dual problem becomes
mm u'Qu eu
It will be understood that within the ASVM Algorithm will always be evaluated using the SMW formula and hence only an matrix is inverted
We state our algorithm now Note that commented parts of the algorithm are
not needed in general and were rarely used in our numerical results presented in
Section The essence of the algorithm is displayed in the two boxes below
O~uER
Algorithm Active SVM ASVM Algorithm for
Start with UO 1e For having compute as
Ifollows
Define Bi I I
Determine
Ui+l
iNi
Bi BiBi
Stop if is the global solution that is if a QUi+1
a
If f(u iH f(u then go to
If Ut.~l QBi+1Bi+l then UH1 is a global solution
on the face of active constraints UNi Set iH and go to
ISet and go to
I
Move in the direction of the global minimum on the face of ac
UNi
UBi
HI Bil Bi eBi an UBi
I we
constrazn
argmino9 df(uki nki I nki Uki O}. If
for some set and go to Otherwise UH1 is a
global minimum on the face UNi and go to
Iterate a gradient projection step and uk Iterate
argmin O<A<l f(uk Quk untilf(u
Set iH ilk Set and go to
Remark All commented parts of the algorithm are optional and are not
usually implemented unless the algorithm gets stuck which it rarely did on our
examples Hence our algorithm is particularly simple and consists of steps
and The commented parts were ins erted in order to comply with the
active set strategy of Morr Toraldo result for which they give finite termination
Remark The iteration in step is a gradient projection step which is guaranteed to converge to the global solution of pp and is placed here to
ensure that the strict inequality f(u eventually holds as required in
Similarly the step in ensures that the function value does not increase when it
remains on the same face in compliance with Algortihm BCQP(b)j
Numerical Implementation and Comparisons
We implemented ASVM in Visual under Windows NT The experiments were run on the UW-Madison Data Mining Institute Locop2 machine which
utilizes a MHz Pentium Xeon Processor and a maximum of Gigabytes of
memory available per process We wrote all the code ourselves except for the linear
equation solver for which we used CLAPACK Our stopping criterion for
ASVM is triggered when the error bound residual Ilu Qu which
is zero at the solution of goes below O.l.
The first set of experiments are designed to show that our reformulation of
the SVM and its associated algorithm ASVM yield similar performance to the
standard SVM referred to here as SVM-QP For six datasets availa ble from the
UCI Machine Learning Repository we performed tenfold cross validation in
order to compare test set accuracies between ASVM and SVM-QP We implemented
SVM-QP using the high-performing CPLEX barrier quadratic programming solver
and utilized a tuning set for both algorithms to find the optimal value of the
parameter using the defa ult stopping criterion of CPLEX Altering the CPLEX
default stopping criterion to match that of ASVM did not result in significant change
in timing relative to ASVM but did reduce test set correctness
In order to obtain additional timing comparison information we also ran the wellknown SVM optimized algorithm SVM1ig ht Joachims the author of SVM1ight
provided us with the newest version of the software Version and advice on
setting the parameters All features for these experiments were normalized to the
range as recommended in the SVM1ig ht documentation We chose to use
Dataset
Training
mxn
1\lqorithm
Liver Disorders CPLEX
VMf~ht
SVM
Cleveland Heart CPLEX
13
VMf~ht
SVM
Testing
Correctness Correctness
Time
CPU sec
70
Pima Diabetes
CPLEX
VMf~ht
SVM
Dataset
mx
Ionosphere
34
ic Tae Toe
Iqorithm
CPLEX
VMf~ht
SVM
CPLEX
Time
CPU sec
Training
Testing
Correctness
Correctness
VMf~ht
SVM
Votes
CPLEX
VMf~ht
SVM
Table ASVM compared with conventional SVM-QP CPLEX and SVM1ig ht
on VCI datasets ASVM test correctness is comparable to SVM-QP with
timing much faster than CPLEX and faster than or comparable to SVM1ig ht
Points
million
million
Iterations
Training
Correctness
Testing
Correctness
Time
CPU min
Table Performance of ASVM on NDC generated datasets in 32
the default termination error criterion in SVM1ig ht of which is act ually a less
stringent criterion than the one we used fo ASVM This is because the criterion we
used for ASVM above is an aggregate over the errors for all points whereas
the SVM1ig ht criterion reflects a minimum error threshold for each point
The second set of experiments show that ASVM performs well on massive datasets
We created synthetic data of Gaussian distribution by using our own NDC Data
Generator as suggested by Usama Fayyad The results of our experiments are
shown in Table We did try to run SVM1ig ht on these datasets as well but we
ran into memory difficulties Note that for these experiments all the data was
brought into memory As such the running time reported consists of the time
used to actually solve the problem to termination excluding I/O time This is
consistent with the measurement techniques used by other popular approaches
Putting all he data in memory is simpler to code and results in faster running
times However it is not a fundamental requirement of our algorithm block
matrix multiplications incremental evaluations of using another application of
the SMW formula and indices on the dataset can be used to create an efficient disk
based version of ASVM
Conclusion
A very fast finite and simple algorithm ASVM capable of classifying massive
datasets has been proposed and implemented ASVM requires nothing more complex than a commonly available linear equation solver for solving small systems
with few variables even for massive datasets Future work includes extensions to
parallel processing of the data handling very large datasets directly from disk as
well as extending our approach to nonlinear kernels
Acknow ledgements
We are indebted to our colleagues Thorsten oachims for helping us to get SVM1ig ht
running significantly faster on the UCI datasets and to Glenn Fung for his efforts
in running the experiments for revisions of this work Research described in this
Data Mining Institute Report April was supported by National Science Foundation Grants and by Air Force Office of
Scientific Research Grant and by Microsoft

----------------------------------------------------------------

title: 5161-binary-to-bushy-bayesian-hierarchical-clustering-with-the-beta-coalescent.pdf

Binary to Bushy Bayesian Hierarchical Clustering
with the Beta Coalescent
Yuening Hu1 Jordan Boyd-Graber2 Hal Daum`e III3 Z. Irene Ying4
Computer Science iSchool and UMIACS Agricultural Research Service
University of Maryland Department of Agriculture
ynhu@cs.umd.edu jbg,hal}@umiacs.umd.edu zhu.ying@ars.usda.gov
Abstract
Discovering hierarchical regularities in data is a key problem in interacting with
large datasets modeling cognition and encoding knowledge A previous Bayesian
solution?Kingman?s coalescent?provides a probabilistic model for data represented as a binary tree Unfortunately this is inappropriate for data better described
by bushier trees We generalize an existing belief propagation framework of
Kingman?s coalescent to the beta coalescent which models a wider range of tree
structures Because of the complex combinatorial search over possible structures
we develop new sampling schemes using sequential Monte Carlo and Dirichlet
process mixture models which render inference efficient and tractable We present
results on synthetic and real data that show the beta coalescent outperforms Kingman?s coalescent and is qualitatively better at capturing data in bushy hierarchies
The Need For Bushy Hierarchical Clustering
Hierarchical clustering is a fundamental data analysis problem given observations what hierarchical
grouping of those observations effectively encodes the similarities between observations This is a
critical task for understanding and describing observations in many domains including natural
language processing computer vision and network analysis In all of these cases natural
and intuitive hierarchies are not binary but are instead bushy with more than two children per parent
node Our goal is to provide efficient algorithms to discover bushy hierarchies
We review existing nonparametric probabilistic clustering algorithms in Section with particular
focus on Kingman?s coalescent and its generalization the beta coalescent While Kingman?s
coalescent has attractive properties?it is probabilistic and has edge lengths that encode how
similar clusters are?it only produces binary trees The beta coalescent Section does not have this
restriction However na??ve inference is impractical because bushy trees are more complex we need
to consider all possible subsets of nodes to construct each internal nodes in the hierarchy
Our first contribution is a generalization of the belief propagation framework for beta coalescent to
compute the joint probability of observations and trees Section After describing sequential Monte
Carlo posterior inference for the beta coalescent we develop efficient inference strategies in Section
where we use proposal distributions that draw on the connection between Dirichlet processes?a
ubiquitous Bayesian nonparametric tool for non-hierarchical clustering?and hierarchical coalescents
to make inference tractable We present results on both synthetic and real data that show the beta
coalescent captures bushy hierarchies and outperforms Kingman?s coalescent Section
Bayesian Clustering Approaches
Recent hierarchical clustering techniques have been incorporated inside statistical models this
requires formulating clustering as a statistical?often Bayesian?problem Heller build
binary trees based on the marginal likelihoods extended by Blundell to trees with arbitrary
branching structure Ryan propose a tree-structured stick-breaking process to generate trees
with unbounded width and depth which supports data observations at leaves and internal nodes.1
However these models do not distinguish edge lengths an important property in distinguishing how
tight the clustering is at particular nodes
Hierarchical models can be divided into complementary fragmentation and coagulation frameworks Both produce hierarchical partitions of a dataset Fragmentation models start with a single
partition and divide it into ever more specific partitions until only singleton partitions remain Coagulation frameworks repeatedly merge singleton partitions until only one partition remains Pitman-Yor
diffusion trees a generalization of Dirichlet diffusion trees are an example of a bushy
fragmentation model and they model edge lengths and build non-binary trees
Instead our focus is on bottom-up coalescent models one of the coagulation models and
complementary to diffusion trees which can also discover hierarchies and edge lengths In this
model nodes are observed we use both observed to emphasize that nodes are known and leaves to
emphasize topology These observed nodes are generated through some unknown tree with latent
edges and unobserved internal nodes Each node both observed and latent has a single parent The
convention in such models is to assume our observed nodes come at time and at time all
nodes share a common ur-parent through some sequence of intermediate parents
Consider a set of individuals observed at the present time All individuals start in one of
singleton sets After time ti a set of these nodes coalesce into a new node Once a set merges their
parent replaces the original nodes This is called a coalescent event This process repeats until there
is only one node left and a complete tree structure Figure is obtained
Different coalescents are defined by different probabilities of merging a set of nodes This is called
the coalescent rate defined by a general family of coalescents the lambda coalescent We
represent the rate via the symbol kn the rate at which out of nodes merge into a parent node
From a collection of nodes can coalesce at some coalescent event can be different for
different coalescent events The rate of a fraction of the nodes coalescing is given by
where is a finite measure on So nodes merge at rate
Choosing different measures yields different coalescents A degenerate Dirac delta measure at
results in Kingman?s coalescent where kn is when and zero otherwise Because this
gives zero probability to non-binary coalescent events this only creates binary trees
Alternatively using a beta distribution BETA(2 as the measure yields the beta coalescent
When is closer to the tree is bushier as approaches it becomes Kingman?s coalescent If we
have nodes at time in a beta coalescent the rate knii?1 for a children set of ki nodes at time
ti and the total rate of any children set merging?summing over all possible mergers?is
knii?1
ki
and
ki
ki
ki
Each coalescent event also has an edge length?duration??i The duration of an event comes from
an exponential distribution exp(?ni?1 and the parent node forms at time ti
Shorter durations mean that the children more closely resemble their parent the mathematical basis
for similarity is specified by a transition kernel Section
Analogous to Kingman?s coalescent the prior probability of a complete tree is the product of all of
its constituent coalescent events merging ki children after duration
p(ki ki
Merge ki nodes After duration
knii?1
This is appropriate where the entirety of a population is known?both ancestors and descendants We focus
on the case where only the descendants are known For a concrete example see Section
Algorithm MCMC inference for generating a tree
for Particle do
Initialize ns ts0 w0s
Initialize the node set
while where ns do
Update
for Particle do
Kingman?s coalescent
if ns then
Continue
Propose a duration is by Equation
Set coalescent time tsi tsi?1 is
Sample partitions psi from DPMM
Propose a set csi according to Equation
Update weight wis by Equation 13
Update ns ns
the beta coalescent
Remove csi from add si to
Compute effective sample size ESS
Figure The beta coalescent can merge four simi17
if ESS then
lar nodes at once while Kingman?s coalescent only
Resample particles
merges two each time
Beta Coalescent Belief Propagation
The beta coalescent prior only depends on the topology of the tree In real clustering applications we
also care about a node?s children and features In this section we define the nodes and their features
and then review how we use message passing to compute the probabilities of trees
An internal node is defined as the merger of other nodes The children set of node coalesces
into a new node This encodes the identity of the nodes that participate in specific
coalescent events Equation in contrast only considers the number of nodes involved in an event
In addition each node is associated with a multidimensional feature vector
Two terms specify the relationship between nodes features an initial distribution p0 and a
transition kernel ti tb yb The initial distribution can be viewed as a prior or regularizer for
feature representations The transition kernel encourages a child?s feature yb at time tb to resemble
feature formed at ti shorter durations tb ti increase the resemblance
Intuitively the transition kernel can be thought as a similarity score the more similar the features
are the more likely nodes are For Brownian diffusion discussed in Section the transition
kernel follows a Gaussian distribution centered at a feature The covariance matrix is decided
by the mutation rate the probability of a mutation in an individual Different kernels
multinomial tree kernels can be applied depending on modeling assumptions of the feature
representations
To compute the probability of the beta coalescent tree and observed data we generalize the
belief propagation framework used by Teh for Kingman?s coalescent this is a more scalable
alternative to other approaches for computing the probability of a Beta coalescent tree We
define a subtree structure thus the tree after the final coalescent event is a
complete tree The message for node marginalizes over the features of the nodes in its children
set.2 The total message for a parent node is
YZ
M?i Z?i
ti tb yb yb dyb
ci
where Z?i is the local normalizer which can be computed as the combination of initial
distribution and messages from a set of children
Z?i p0
ti tb yb yb dyb dyi
ci
When is a leaf the message M?b yb is a delta function centered on the observation
Recursively performing this marginalization through message passing provides the joint probability
of a complete tree and the observations At the root
p0 ym ym dym
where p0 is the initial feature distribution and is the number of coalescent events This gives
the marginal probability of the whole tree
Z?i
The joint probability of a tree combines the prior Equation and likelihood Equation
knii?1 Z?i
Sequential Monte Carlo Inference
Sequential Monte Carlo SMC)?often called particle filters?estimates a structured sequence of
hidden variables based on observations For coalescent models this estimates the posterior
distribution over tree structures given observations Initially each observation is in a
singleton cluster;3 in subsequent particles points coalesce into more complicated tree
structures is where is the particle index and we add superscript to all the related notations to
distinguish between particles We use sequential importance resampling SIR to weight each
particle at time ti denoted as wis
The weights from SIR approximate the posterior Computing the weights requires a conditional distris
bution of data given a latent state p(x|?is a transition distribution between latent states p(?is
and a proposal distribution Together these distributions define weights
wis
p(x is
Then we can approximate the posterior distribution of the hidden structure using the normalized
weights which become more accurate with more particles
To apply SIR inference to belief propagation with the beta coalescent prior we first define the particle
space structure The sth particle represents a subtree
at time tsi?1 and a transition to a new
subtree takes a set of nodes from and merges them at tsi where tsi tsi?1 is and
is
is csi Our proposal distribution must provide the duration is and the children set sci
to merge based on the previous subtree
We propose the duration is from the prior exponential distribution and propose a children set from the
posterior distribution based on the local normalizers This is the priorpost method in Teh
However this approach
is intractable Given
nodes at time ti we must consider all possible
children sets
The computational complexity grows from
Kingman?s coalescent to
beta coalescent
Efficiently Finding Children Sets with DPMM
We need a more efficient way to consider possible children sets Even for Kingman?s coalescent which
only considers pairs of nodes Gorur do not exhaustively consider all pairs Instead they
use data structures from computational geometry to select the closest pairs as their restriction set
reducing inference to O(n log While finding closest pairs is a traditional problem in computational
geometry discovering arbitrary-sized sets is less studied
The relationship between time and particles is non-intuitive Time goes backward with subsequent particles
When we use time-specific adjectives for particles this is with respect to inference
This is a special case of Section algorithm where the restriction set is all possible subsets
In this section we describe how we use a Dirichlet process mixture model DPMM to discover a
restriction set integrating DPMMs into the SMC proposal We first briefly review what DPMMs are
describe why they are attractive and then describe how we incorporate DPMMs in SMC inference
The DPMM is defined by a concentration and a base distribution G0 A distribution over mixtures is
drawn from a Dirichlet process G0 Each observation is assigned to a mixture
component drawn from G. Because the Dirichlet process is a discrete distribution observations
and can have the same mixture component When this happens points are said to be in
the same partition Posterior inference can discover a distribution over partitions A full derivation of
these sampling equations appears in the supplemental material
Attractive Properties of DPMMs
DPMM and Coalescents Berestycki showed that the distribution over partitions in a
Dirichlet process is equivalent to the distribution over coalescents allelic partitions?the set of
members that have the same feature representation?when the mutation rate of the associated
kernel is half of the Dirichlet concentration Section For Brownian diffusion we can connect
DPMM with coalescents by setting the kernel covariance I to
The base distribution G0 is also related with nodes feature The base distribution G0 of a Dirichlet
process generates the probability measure for each block which generates the nodes in a block
As a result we can select a base distribution which fits the distribution of the samples in coalescent
process For example if we use Gaussian distribution for the transition kernel and prior a Gaussian
is also appropriate as the DPMM base distribution
Effectiveness as a Proposal The necessary condition for a valid proposal is that it should have
support on a superset of the true posterior In our case the distribution over partitions provided by
the DPMM considers all possible children sets that could be merged in the coalescent Thus the new
proposal with DPMM satisfies this requirement and it is a valid proposal
In addition Chen gives a set of desirable criteria for a good proposal distribution accounts for
outliers considers the likelihood and lies close to the true posterior The DPMM fulfills these criteria
First the DPMM provides a distribution over all partitions Varying the concentration parameter can
control the length of the tail of the distribution over partitions Second choosing the base distribution
of the DPMM appropriately models the feature likelihood ensuring the DPMM places similar
nodes together in a partition with high probability Third the DPMM qualitatively provides reasonable
children sets when compared with exhaustively considering all children sets Figure
Incorporating DPMM in SMC Proposals
To address the inference intractability in Section we use the DPMM to obtain a distribution over
partitions of nodes Each partition contains clusters of nodes and we take a union over all partitions
to create a restriction set where each is a subset of the nodes A
standard Gibbs sampler provides these partitions supplemental
With this restriction set we propose the duration time is from the exponential distribution and
propose a children set csi based on the local normalizers
Z?i
is sci
I si
fi
fi exp(??sni?1 is
Z0
where si restricts the candidate children sets I is the indicator and we replace Z?i with
Z?i
is sci since they are equivalent here The normalizer is
Z0
Z?i
is I si
Z?i
is
th
Applying the true distribution the multiplicand from Equation and the proposal distribution
Equation and Equation to the SIR weight update Equation
Z?i
is
wis
where is the size of children set csi parameter
is the rate of the children set sci Equas
tion and is the rate of all possible sets given a total number of nodes Equation
We can view this new proposal as a coarse-to-fine process DPMM proposes candidate children
sets SMC selects a children set from DPMM to coalesce Since the coarse step is faster and filters
bad children sets the slower finer step considers fewer children sets saving computation time
Algorithm If has all children sets it recovers exhaustive SMC. We estimate the effective sample
size and resample when needed For smaller sets the DPMM is sometimes impractical and
only provides singleton clusters In such cases it is simpler to enumerate all children sets
Example Transition Kernel Brownian Diffusion
This section uses Brownian diffusion as an example for message passing framework The initial
distribution p0 of each node is the transition kernel ti tb is a Gaussian centered
at with variance ti tb where is the concentration parameter of DPMM
Then the local normalizer Z?i is
Z?i
y?b tb ti dyi
ci
and the node message M?i is normally distributed M?i where
v?i
tb ti
ci
Experiments Finding Bushy Trees
In this section we compare trees built by the beta coalescent beta against those built by Kingman?s
coalescent kingman and hierarchical agglomerative clustering hac on both synthetic and real
data We show beta performs best and can capture data in more interpretable bushier trees
Setup The parameter for the beta coalescent is between and The closer is to bushier the
tree is and we set We set the mutation rate as thus the DPMM parameter is initialized
as and updated using slice sampling All experiments use initial iterations of DPMM
inference with more iterations after each coalescent event forming a new particle
Metrics We use three metrics to evaluate the quality of the trees discovered by our algorithm
purity subtree and path length The dendrogram purity score measures how well the leaves
in a subtree belong to the same class For any two leaf nodes we find the least common subsumer
node and?for the subtree rooted at s?measure the fraction of leaves with same class labels The
subtree score is the ratio between the number of internal nodes with all children in the same
class and the total number of internal nodes The path length score is the average difference?over
all pairs?of the lowest common subsumer distance between the true tree and the generated tree
where the lowest common subsumer distance is the distance between the root and the lowest common
subsumer of two nodes For purity and subtree higher is better while for length lower is better
Scores are in expectation over particles and averaged across chains
Synthetic Hierarchies
To test our inference method we generated synthetic data with edge length full details available
in the supplemental material we also assume each child of the root has a unique label and the
descendants also have the same label as their parent node except the root node
We compared beta against kingman and hac by varying the number of observations Figure
and feature dimensions Figure In both cases beta is comparable to kingman and hac no
edge length While increasing the feature dimension improves both scores more observations do
not for synthetic data a small number of observations suffice to construct a good tree
With DPMM proposals has a negligible effect so we elide further analysis for different values
beta
hac
kingman
beta
hac
kingman
beta
kingman
beta
kingman
length
Number of Observations
Scores
length
Dimension
beta
length
Dimension
enum
beta
enum
Number of Observations
Increasing observations
Scores
Scores
purity
Scores
purity
Scores
Scores
purity
Dimension
Increasing dimension
Dimension
beta enum
Figure Figure and show the effect of changing the underlying data size or number
of dimension Figure shows that our DPMM proposal for children sets is comparable to an
exhaustive enumeration of all possible children sets enum
To evaluate the effectiveness of using our DPMM as a proposal distribution we compare exhaustively
enumerating all children set candidates enum while keeping the SMC otherwise unchanged this
experiment uses ten data points enum is completely intractable on larger data Beta uses the DPMM
and achieved similar accuracy Figure while greatly improving efficiency
Human Tissue Development
Our first real dataset is based on the developmental biology of human tissues As a human develops
tissues specialize starting from three embryonic germ layers the endoderm ectoderm and mesoderm
These eventually form all human tissues For example one developmental pathway is ectoderm
neural crest cranial neural crest optic vesicle cornea Because each germ layer specializes
into many different types of cells at specific times it is inappropriate to model this development as a
binary tree or with clustering models lacking path lengths
Historically uncovering these specialization pathways is a painstaking process requiring inspection
of embryos at many stages of development however massively parallel sequencing data make it
possible to efficiently form developmental hypotheses based on similar patterns of gene expression
To investigate this question we use the transcriptome of 27 tissues with known unambiguous
time-specific lineages We reduce the original dimensions via principle component
analysis PCA We use five chains with five particles per chain
Using

----------------------------------------------------------------

title: 2505-generalised-propagation-for-fast-fourier-transforms-with-partial-or-missing-data.pdf

Generalised Propagation for Fast Fourier
Transforms with Partial or Missing Data
Amos Storkey
School of Informatics University of Edinburgh
Forrest Hill Edinburgh UK
a.storkey@ed.ac.uk
Abstract
Discrete Fourier transforms and other related Fourier methods have
been practically implementable due to the fast Fourier transform
However there are many situations where doing fast Fourier
transforms without complete data would be desirable In this paper it is recognised that formulating the FFT algorithm as a belief
network allows suitable priors to be set for the Fourier coefficients
Furthermore efficient generalised belief propagation methods between clusters of four nodes enable the Fourier coefficients to be
inferred and the missing data to be estimated in near to O(n log
time where is the total of the given and missing data points
This method is compared with a number of common approaches
such as setting missing data to zero or to interpolation It is tested
on generated data and for a Fourier analysis of a damaged audio
signal
Introduction
The fast Fourier transform is a fundamental component in any numerical toolbox
Commonly it is thought of as a deterministic transformation from data to Fourier
space It relies on regularly spaced data ideally of length 2hi for some hi in each
dimension However there are many circumstances where Fourier analysis would
be useful for data which does not take this form The following are a few examples
of such situations
There is temporary/regular instrument failure or interruption
There are scratches on media such as compact disks
Missing packets occur in streamed data
Data is not 2k in length or is from irregularly shaped image patches
There is known significant measurement error in the data
Data is quantised either in Fourier domain jpeg or data domain
integer storage
Setting missing values to zeros or using interpolation will introduce various biases
which will also affect the results these approaches can not help in using Fourier
information to help restore the missing data
Prior information is needed to infer the missing data or the corresponding Fourier
components However to be practically useful inference must be fast Ideally we
want techniques which scale close to O(n log
The FFT algorithm can be described as a belief network with deterministic connections where each intermediate node has two parents and two children form
commonly called the butterfly net The graphical structure of the FFT has been
detailed before in a number of places See for examples Prior distributions for
the Fourier coefficients can be specified By choosing a suitable cluster set for the
network nodes and doing generalised propagation using these clusters reasonable
inference can be achieved In the case that all the data is available this approach is
computationally equivalent to doing the exact FFT.
There have been other uses of belief networks and Bayesian methods to improve
standard transforms In a hierarchical prior model of wavelet coefficients was
used with some success Other authors have recognised the problem of missing data
in hierarchical systems In the authors specify a multiscale stochastic model
which enables a scale recursive description of a random process Inference in their
model is propagation within a tree structured belief network FFT related Toeplitz
methods combined with partition inverse equations are applicable for inference in
grid based Gaussian process systems with missing data
Fast Fourier Transform
The FFT Network
From this point forward the focus will be on the one dimensional fast Fourier
transform The FFT utilises a simple recursive relationship in order to implement the discrete Fourier transform in O(n log time for 2h data points For
the kth Fourier coefficient Fk is given by
def
Fk
kj
2kj x2j
Fke Fko
where Fke denotes the kth component of the length Fourier transform of the
even components of Likewise Fko is the same for the odd components The two
new shorter Fourier transforms can be split in the same way recursively down to the
transforms of length which are just the data points themselves It is also worth
noting that Fke and Fko are in fact used twice as Fke Fko The inverse
FFT uses exactly the same algorithm as the FFT but with conjugate coefficients
This recursive algorithm can be drawn as a network of dependencies using the
inverse FFT as a generative model it takes a set of Fourier components and creates
some data The usual approach to the FFT is to shuffle the data into reverse bit
order for binary is put in position i0 see for more
details This places data which will be combined in adjacent positions Doing this
we get the belief network of Figure 1a as a representation of the dependencies The
top row of this figure gives the Fourier components in order and the bottom row
gives the bit reversed data The intermediate nodes are the even and odd Fourier
coefficients at different levels of recursion
A Prior on Fourier Coefficients
The network of Figure combined with specifies the deterministic conditional distributions for all the nodes below the top layer However no prior distribution is currently set for the top nodes which denote the Fourier coefficients
In general little prior phase information is known but often there might be some
Figure The belief network corresponding to the fast Fourier transform The
top layer are the Fourier components in frequency order The bottom layer is the
data in bit reversed order The intermediate layers denote the partial odd and even
transforms that the algorithm uses The moralised undirected network with
three clusters indicated by the boxes All nodes surrounded by the same box type
form part of the same cluster
expected power spectra For example we might expect a power spectra or in
some circumstances empirical priors may be appropriate For simplicity we choose
independent complex Gaussian1 priors on each of the top nodes Then the variance
of each prior will represent the magnitude of the expected power of that particular
coefficient
Inference in the FFT Network
Suppose that some of the data which would be needed to perform the FFT is
missing Then we would want to infer the Fourier coefficients based on the data that
was available The belief network of Figure 1a is not singly connected and so exact
propagation methods are not appropriate Forming the full Gaussian distribution of
the whole network and calculating using that is too expensive except in the smallest
situations Using exact optimisation eg conjugate gradient in the conditional
Gaussian system is although a smaller number of iterations of conjugate
gradient can provide a good approximation Marrying parents and triangulating
the system will result in a number of large cliques and so junction tree methods will
not work in a reasonable time
Loopy Propagation
Loopy propagation in the FFT network suffers from some serious deficits
Experiments with loopy propagation suggest that often there are convergence problems in the network especially for systems of any significant size Sometimes adding
additional jitter and using damping approaches can help the system to
converge but convergence is then very slow Intuitively the approximation given by
loopy propagation fails to capture the moralisation of the parents which given the
deterministic network provides strong couplings Note that when the system does
converge the mean inferred values are correct but the variances are significantly
underestimated
A complex Gaussian is of the form where is complex and
is positive semi)definite It is a more restrictive distribution than a general Gaussian in
the complex plane
Generalised Belief Propagation for the FFT
In the authors show that stationary points of loopy propagation between nodes
of a Markov network are minimisers of the Bethe free energy of the probabilistic
system They also show that more general propagation procedures such as propagation of information between clusters of a network correspond to the minimisation
of a more general Kikuchi free energy of which The Bethe free energy is a special
case
To overcome the shortfalls of belief propagation methods a generalised belief propagation scheme is used here The basic problem is that there is strong dependence
between parents of a given node and the fact that the values for those two nodes
are fully determined by the two children but undetermined by only one Hence it
would seem sensible to combine these four nodes the two parents and two children
together into one cluster This can be done for all nodes at all levels and we find
that the cluster separator between any two clusters consists of at most one node
At each stage of propagation between clusters only the messages each direction
at single nodes need to be maintained
The procedure can be summarised as follows Start with the belief network of
Figure 1a and convert it to an undirected network by moralisation Figure
Then we identify the clusters of the graph which each consist of four nodes as
illustrated by the boxes in Figure Each cluster consists of two common parents
and their common children Each node not in the extreme layers is also a separator
between two clusters Building a network of clusters involves creating an edge for
each separator From Figure it can be seen that this network will have undirected
loops Hence belief propagation in this system will not be exact However it will be
possible to iteratively propagate messages in this system Hopefully the iteration
will result in an equilibrium being reached which we can use as an approximate
inference for the marginals of the network nodes although such convergence is not
guaranteed
Propagation Equations
This section provides the propagation messages for the approach described above
For simplicity and to maintain symmetry we use an update scheme where messages
are first passed down from what were the root nodes before moralisation to the
leaf nodes and then messages are passed up from the leaf to the root nodes This
process is then iterated The first pass down the network is data independent and
can be precomputed
Messages Propagating Down
The Markov network derived from a belief network has the potentials of each cluster
defined by the conditional probability of all the child nodes in that cluster given
their parents Two adjoining clusters of the network are illustrated in Figure All
the cluster interactions in the network have this form and so the message passing
described below applies to all the nodes
The message
is defined to be that passed down from some cluster
containing nodes y1 y2 originally the parents and y3 y4 originally the children
to the cluster below C2 y5 y6 y7 with y6 and y7 the children
is the
message mean and is the covariance The message is given by the marginal
of the cluster potential multiplied by the incoming messages from the other nodes
The standard message passing scheme can be followed to get the usual form of
results for Gaussian networks
Suppose
is the message passing up the network at node
whereas and
are the messages
passing down the network at nodes and respectively Here we use the notation
of and use to represent variances Defining2
A
B1 A B1
where B1
are the connection coefficients derived from and
A
A
A
allows us to write the downward message as
and
Messages Propagating Up
In the same way we can calculate the messages which are propagated up the network
The message
passed up from cluster C2 to cluster C1 is given by
and where B2
and
diag(0
diag(0
All the other messages follow by symmetry
Calculation of the Final Marginals
The approximate posterior marginal distributions are given by the product of the
and messages Hence the posterior marginal at each node is also a Gaussian
distribution with variance and mean given by
and
respectively
Initialisation
The network is initialised by setting the messages at the leaf nodes to be
for a node known to take value and for the missing data All the other
messages are initialised to The message at a given root node is set
to the prior at that root node No other messages need to be initialised as they
are not needed before they are computed during the first pass Computationally
we usually have to add a small jitter term network noise and represent the infinite
variances by large numbers to avoid numeric problems
Demonstrations and Results
In all the tests in this section the generalised propagation converged in a small
number of iterations without the need to resort to damping First we analyse the
simple case where the variances of the Fourier component priors have a form
where is the component number frequency To test this scenario a set of
The operator is used to denote the complex conjugate transpose adjoint
C1
y1
y2
y3
y5
Proportion of MSE
y6
C2
y7
Power
Figure Two clusters C1 and C2 All the clusters in the network contain
four nodes Each node is also common to one other cluster Hence the interaction
between any two connected clusters is of the form illustrated in this figure
How the weighted mean square error varies for spectra with different power laws
Power The filled line is the belief network approach the dashed line is linear
interpolation the dotted line uses mean-valued data
Mean fill Linear Spline
BN
MSE
WMSE
Table Comparison of methods for estimating the FFT of a function Zero
fill replaces missing values by zero and then does an FFT. Linear interpolates
linearly for the missing values Spline does the same with a cubic spline are
the results using the mean Fourier components produced by the method described
in this paper
complex Fourier components are generated from the prior distribution An inverse FFT is used to generate data from the Fourier components A predetermined
set of elements is then lost?3 The remaining data is then used with the algorithm
of this paper using iterations of the down-up propagation The resulting means
are compared with the components obtained by replacing the missing data with
zeros or with interpolated values and taking an FFT Mean squared errors MSE
in the Fourier components are calculated for each approach over the different
runs Weighted mean squared errors WMSE are also calculated where each frequency component is divided by its prior variance before averaging The results are
presented in Table
The generalised belief propagation produces better results than any of the other approaches Similar results are achieved for a number of different spectral priors The
benefits of interpolation are seen for situations where there are only low frequency
components and the zeroing approach becomes more reasonable in white noise like
situations but across a broad spread of spectral priors the belief network approach
tends to perform better Figure 2b illustrates of how the results vary for an average
of runs as the power spectrum varies from to Note that the approach
is particularly good at the power spectra point which corresponds to the form
of spectra in many real life problems
Data in positions 13 18 22 24 27 28 29 32 33 34 35 36 42 47
51 55 58 61 65 67 71 73 75 77 78 79 81 84 86 94 97
are removed This provides a mix of loss in whole
regions but also at isolated points
Linear
Spline
BN
Table Testing the MSE predictive ability of the belief network approach
Zero fill Linear
Spline
BN
MSE
WMSE
MSEPRED
Table Testing the ability of the belief network approach on real life audio data
The BN approach performs better than all others for both prediction of the correct
spectrum and prediction of the missing data MSE mean squared error WMSE
weighted mean squared error MSEPRED Mean squared error of the data predictor
Next we compare approaches for filling in missing data This time runs are
made on and power spectra Note that ignoring periodic boundary
constraints a power spectra produces a Brownian curve for which the linear
predictor is the optimal mean predictor In this case the mean square error for
the belief network propagation approach Table is close to the linear error On
smooth curves such as that produced by the noise the predictive ability of the
approach for small numbers of iterations does not match interpolation methods
The local smoothness information is not easily used in the belief network propagation because neighbouring points in data space are only connected at the highest
level in the belief network The approximations of loopy propagation methods do
not preserve enough information when propagated over these distances However
for data such as that produced by the common power spectra interpolation
methods are less effective and the belief network propagation performs well In
this situation the belief network approach outperforms interpolation Calculations
using zero values or mean estimates also prove significantly worse
Last tests are made on some real world audio data A point complex audio
signal is built up from a two channel sample from a short stretch of laughter
Fourier power spectra of the mean of other different sections of laughter are used
to estimate the prior power spectral characteristics Randomly selected parts of the
data are removed corresponding to one tenth of the whole A belief network FFT is
then calculated in the usual way and compared with the true FFT calculated on the
whole data The results are given in Table The belief network approach performs
better than all other methods including linear and cubic spline interpolation
Discussion
This paper provides a clear practical example of a situation where generalised propagation overcomes deficits in simpler propagation methods It demonstrates how a
belief network representation of the fast Fourier transform allows Fourier approaches
to be used in situations where data is missing
Kikuchi inference in the FFT belief network proves superior to many naive approaches for dealing with missing data in the calculation of Fourier transforms
It also provides methods for inferring missing data It does this while maintaining O(n log2 nature of the FFT algorithm if we assume that the number of
iterations needed for convergence does not increase with data size In practice additional investigations have shown that this is not the case but that the increase
in the number of iterations does not scale badly Further investigation is needed
to show exactly what the scaling is and further documentation of the benefits of
generalised propagation over loopy propagation and conjugate gradient methods is
needed beyond the space available here It might be possible that variational approximation using clusters could provide another approach to inference in this
system This paper has also not considered the possibility of dependent or sparse
priors over Fourier coefficients or priors over phase information all of which would
be interesting Formalising the extension to dimensions would be straightforward
but valuable as it is likely the convergence properties would be different
In conclusion the tests done indicate that this is a valuable approach for dealing
with missing data in Fourier analysis It is particularly suited to the types of spectra
seen in real world situations In fact loopy propagation methods in FFT networks
are also valuable in many scenarios Very recent work of Yedidia shows that
discrete generalised belief propagation in FFT constructions may enable the benefits
of sparse decoders to be used for Reed-Solomon codes
Acknowledgements
This work was funded by a research fellowship from Microsoft Research Cambridge
The author specifically thanks Erik Sudderth Jonathan Yedidia and the anonymous reviewers for their comments

----------------------------------------------------------------

title: 5156-near-optimal-anomaly-detection-in-graphs-using-lovasz-extended-scan-statistic.pdf

Near-optimal Anomaly Detection in Graphs
using Lov?asz Extended Scan Statistic
Akshay Krishnamurthy
Computer Science Department
Carnegie Mellon University
Pittsburgh PA
akshaykr@cs.cmu.edu
James Sharpnack
Machine Learning Department
Carnegie Mellon University
Pittsburgh PA
jsharpna@gmail.com
Aarti Singh
Machine Learning Department
Carnegie Mellon University
Pittsburgh PA
aarti@cs.cmu.edu
Abstract
The detection of anomalous activity in graphs is a statistical problem that arises in
many applications such as network surveillance disease outbreak detection and
activity monitoring in social networks Beyond its wide applicability graph structured anomaly detection serves as a case study in the difficulty of balancing computational complexity with statistical power In this work we develop from first
principles the generalized likelihood ratio test for determining if there is a well
connected region of activation over the vertices in the graph in Gaussian noise
Because this test is computationally infeasible we provide a relaxation called the
Lov?asz extended scan statistic LESS that uses submodularity to approximate the
intractable generalized likelihood ratio We demonstrate a connection between
LESS and maximum a-posteriori inference in Markov random fields which provides us with a poly-time algorithm for LESS Using electrical network theory
we are able to control type error for LESS and prove conditions under which
LESS is risk consistent Finally we consider specific graph models the torus knearest neighbor graphs and random graphs We show that on these graphs our
results provide near-optimal performance by matching our results to known lower
bounds
Introduction
Detecting anomalous activity refers to determining if we are observing merely noise business as
usual or if there is some signal in the noise anomalous activity Classically anomaly detection
focused on identifying rare behaviors and aberrant bursts in activity over a single data source or
channel With the advent of large surveillance projects social networks and mobile computing
data sources often are high-dimensional and have a network structure With this in mind statistics
needs to comprehensively address the detection of anomalous activity in graphs In this paper we
will study the detection of elevated activity in a graph with Gaussian noise
In reality very little is known about the detection of activity in graphs despite a variety of real-world
applications such as activity detection in social networks network surveillance disease outbreak detection biomedical imaging sensor network detection gene network analysis environmental monitoring and malware detection Sensor networks might be deployed for detecting nuclear substances
water contaminants or activity in video surveillance By exploiting the sensor network structure
based on proximity one can detect activity in networks when the activity is very faint Recent
theoretical contributions in the statistical literature[1 have detailed the inherent difficulty of such
a testing problem but have positive results only under restrictive conditions on the graph topology
By combining knowledge from high-dimensional statistics graph theory and mathematical programming the characterization of detection algorithms over any graph topology by their statistical
properties is possible
Aside from the statistical challenges the computational complexity of any proposed algorithms
must be addressed Due to the combinatorial nature of graph based methods problems can easily
shift from having polynomial-time algorithms to having running times exponential in the size of
the graph The applications of graph structured inference require that any method be scalable to
large graphs As we will see the ideal statistical procedure will be intractable suggesting that
approximation algorithms and relaxations are necessary
Problem Setup
Consider a connected possibly weighted directed graph defined by a set of vertices
and directed edges which are ordered pairs of vertices Furthermore the edges may be
assigned weights We that determine the relative strength of the interactions of the adjacent
vertices For each vertex we assume that there is an observation that has a Normal
distribution with mean and variance This is called the graph-structured normal means problem
and we observe one realization of the random vector
where Rp Ip?p The signal will reflect the assumption that there is an active
cluster in the graph by making if and otherwise Furthermore
the allowable clusters must have a small boundary in the graph Specifically we assume that
there are parameters possibly dependent on such that the class of graph-structured activation
patterns is given as follows
1C out(C
is the total weight of edges leaving the cluster C.
Here out(C Wu,v I{u
In other words the set of activated vertices have a small cut size in the graph G. While we assume
that the noise variance is in this is equivalent to the more general model in which
with known If we wanted to consider known then we would apply all our algorithms to
and replace with in all of our statements For this reason we call the signal-to-noise ratio
and proceed with
In graph-structured activation detection we are concerned with statistically testing the null against
the alternative hypotheses
H0 I
H1
H0 represents business as usual such as sensors returning only noise while H1 encompasses all of
the foreseeable anomalous activity an elevated group of noisy sensor observations Let a test be a
mapping where indicates that we reject the null It is imperative that we control
both the probability of false alarm and the false acceptance of the null To this end we define our
measure of risk to be
R(T E0 sup Ex
x?X
where Ex denote the expectation with respect to I). These terms are also known as the
probability of type and type error respectively This setting should not be confused with the
Bayesian testing setup as considered in where the patterns are drawn at random
We will say that H0 and H1 are asymptotically distinguished by a test if in the setting of large
graphs limp R(T If such a test exists then H0 and H1 are asymptotically distinguishable
otherwise they are asymptotically indistinguishable which occurs whenever the risk does not tend
to We will be characterizing regimes for in which our test asymptotically distinguishes H0
from H1
Throughout the study let the edge-incidence matrix of be Rm?p such that for
We We and is elsewhere For directed graphs vertex degrees refer to dv
Let k.k denote the norm be the norm and be the positive components
of the vector Let and we will be using the notation namely if non-negative
sequences satisfy an bn then an o(bn and bn
Contributions
Section highlights what is known about the hypothesis testing problem particularly we provide
a regime for in which H0 and H1 are asymptotically indistinguishable In section we derive
the graph scan statistic from the generalized likelihood ratio principle which we show to be a computationally intractable procedure In section we provide a relaxation of the graph scan statistic
the Lov?asz extended scan statistic LESS and we show that it can be computed with successive minimum cut programs graph cut that separates a source vertex from a sink vertex
In section we give our main result Theorem that provides a type error control for both test
statistics relating their performance to electrical network theory In section we show that GSS
and LESS can asymptotically distinguish H0 and H1 in signal-to-noise ratios close to the lowest
possible for some important graph models All proofs are in the Appendix
Related Work
Graph structured signal processing There have been several approaches to signal processing over
graphs Markov random fields MRF provide a succinct framework in which the underlying signal
is modeled as a draw from an Ising or Potts model We will return to MRFs in a later section
as it will relate to our scan statistic A similar line of research is the use of kernels over graphs The
study of kernels over graphs began with the development of diffusion kernels and was extended
through Green?s functions on graphs While these methods are used to estimate binary signals
where over graphs little is known about their statistical properties and their use in
signal detection To the best of our knowledge this paper is the first connection made between
anomaly detection and MRFs
Normal means testing Normal means testing in high-dimensions is a well established and fundamental problem in statistics Much is known when H1 derives from a smooth function space such as
Besov spaces or Sobolev spaces[8 Only recently have combinatorial structures such as graphs
been proposed as the underlying structure of H1 A significant portion of the recent work in this area
has focused on incorporating structural assumptions on the signal as a way to mitigate
the effect of high-dimensionality and also because many real-life problems can be represented as
instances of the normal means problem with graph-structured signals see for an example
Graph scan statistics In spatial statistics it is common when searching for anomalous activity
to scan over regions in the spatial domain testing for elevated activity[12 There have been
scan statistics proposed for graphs most notably the work of in which the authors scan over
neighborhoods of the graphs defined by the graph distance Other work has been done on the theory
and algorithms for scan statistics over specific graph models but are not easily generalizable to
arbitrary graphs More recently it has been found that scanning over all well connected
regions of a graph can be computationally intractable and so approximations to the intractable
likelihood-based procedure have been studied We follow in this line of work with a
relaxation to the intractable generalized likelihood ratio test
A Lower Bound and Known Results
In this section we highlight the previously known results about the hypothesis testing problem
This problem was studied in in which the authors demonstrated the following lower bound
which derives from techniques developed in
Theorem Hypotheses H0 and H1 defined in are asymptotically indistinguishable if
pdmax
min
log
dmax
where dmax is the maximum degree of graph G.
Now that a regime of asymptotic indistinguishability has been established it is instructive to consider
test statistics that do not take the graph into account viz the statistics are unaffected by a change
in the graph structure Certainly if we are in a situation where a naive procedure perform nearoptimally then our study is not warranted As it turns out there is a gap between the performance
of the natural unstructured tests and the lower bound in Theorem
Proposition The thresholding test statistic maxv?[p yv asymptotically distinguishes
H0 from H1 if
The sum test statistic yv asymptotically distinguishes H0 from H1 if
As opposed to these naive tests one can scan over all clusters in performing individual likelihood
ratio tests This is called the scan statistic and it is known to be a computationally intractable
combinatorial optimization Previously two alternatives to the scan statistic have been developed
the spectral scan statistic and one based on the uniform spanning tree wavelet basis The
former is indeed a relaxation of the ideal computationally intractable scan statistic but in many
important graph topologies such as the lattice provides sub-optimal statistical performance The
uniform spanning tree wavelets in effect allows one to scan over a subclass of the class but tends
to provide worse performance as we will see in section than that presented in this work The
theoretical results in are similar to ours but they suffer additional log-factors
Method
As we have noted the fundamental difficulty of the hypothesis testing problem is the composite
nature of the alternative hypothesis Because the alternative is indexed by sets with a
low cut size it is reasonable that the test statistic that we will derive results from a combinatorial
optimization program In fact we will show we can express the generalized likelihood ratio GLR
statistic in terms of a modular program with submodular constraints This will turn out to be a
possibly NP-hard program as a special case of such programs is the well known knapsack problem
With this in mind we provide a convex relaxation using the Lov?asz extension to the ideal
GLR statistic This relaxation conveniently has a dual objective that can be evaluated with a binary
Markov random field energy minimization which is a well understood program We will reserve
the theoretical statistical analysis for the following section
Submodularity Before we proceed we will introduce the reader to submodularity and the Lov?asz
extension A very nice introduction to submodularity can be found in For any set which we
may as well take to be the vertex set we say that a function is submodular
if for any A A A B). We will interchangeably use
the bijection between and defined by 1C In this way a submodular function
experiences diminishing returns as additions to large sets tend to be less dramatic than additions to
small sets But while this diminishing returns phenomenon is akin to concave functions for optimization purposes submodularity acts like convexity as it admits efficient minimization procedures
Moreover for every submodular function there is a Lov?asz extension defined in the
following way for let xji denote the ith largest element of then
xj1
ji
Submodular functions as a class is similar to convex functions in that it is closed under addition and
non-negative scalar multiplication The following facts about Lov?asz extensions will be important
Proposition Let be submodular and be its Lov?asz extension Then is convex
if and
min{F min{f
We are now sufficiently prepared to develop the test statistics that will be the focus of this paper
Graph Scan Statistic
It is instructive when faced with a class of probability distributions indexed by subsets
to think about what techniques we would use if we knew the correct set which is often
called oracle information One would in this case be only testing the null hypothesis H0
against the simple alternative H1 1C In this situation we would employ the likelihood
ratio test because by the Neyman-Pearson lemma it is the uniformly most powerful
ptest statistic
The maximum likelihood estimator for is 1C
the
MLE
of
is
and the
likelihood ratio turns out to be
1C
Cy
exp kyk exp
exp
Hence the log-likelihood ratio is proportional to
and thresholding this at gives
us a size test
This reasoning has been subject to the assumption that we had oracle knowledge of C. A
natural statistic when is unknown is the generalized log-likelihood ratio GLR defined by
C. We will work with the graph scan statistic
max pC out(C
which is nearly equivalent to the GLR. We can in fact evaluate for and taking a maximum
and obtain the GLR but statistically this is nearly the same Notice that there is no guarantee that
the program above is computationally feasible In fact it belongs to a class of programs specifically
modular programs with submodular constraints that is known to contain NP-hard instantiations
such as the ratio cut program and the knapsack program Hence we are compelled to form a
relaxation of the above program that will with luck provide a feasible algorithm
Lov?asz Extended Scan Statistic
It is common when faced with combinatorial optimization programs that are computationally infeasible to relax the domain from the discrete to a continuous domain such as
Generally the hope is that optimizing the relaxation will approximate the combinatorial program
well First we require that we can relax the constraint out(C to the hypercube This
will be accomplished by replacing it with its Lov?asz extension k1 We then form the
relaxed program which we will call the Lov?asz extended scan statistic LESS
max max k1
We will find that not only can this be solved with a convex program but the dual objective is a
minimum binary Markov random field energy program To this end we will briefly go over binary
Markov random fields which we will find can be used to solve our relaxation
Binary Markov Random Fields Much of the previous work on graph structured statistical procedures assumes a Markov random field MRF model in which there are discrete labels assigned to
each vertex in and the observed variables yv are conditionally independent given these
labels Furthermore the prior distribution on the labels is drawn according to an Ising model if
the labels are binary or a Potts model otherwise The task is to then compute a Bayes rule from
the posterior of the MRF. The majority of the previous work assumes that we are interested in the
maximum a-posteriori MAP estimator which is the Bayes rule for the This can generally
be written in the form
lv xv yv
min
Wv,u I{xv xu
where lv is a data dependent log-likelihood Such programs are called graph-representable in
and are known to be solvable in the binary case with s-t graph cuts Thus by the min-cut max-flow
theorem the value of the MAP objective can be obtained by computing a maximum flow More
recently a dual-decomposition algorithm has been developed in order to parallelize the computation
of the MAP estimator for binary MRFs
We are now ready to state our result regarding the dual form of the LESS program
Proposition Let and define the dual function of the LESS
max k?xk0
The LESS estimator is equal to the following minimum of convex optimizations
max min
is the objective of a MRF MAP problem which is poly-time solvable with s-t graph cuts
Theoretical Analysis
So far we have developed a lower bound to the hypothesis testing problem shown that some common detectors do not meet this guarantee and developed the Lov?asz extended scan statistic from
first principles We will now provide a thorough statistical analysis of the performance of LESS
Previously electrical network theory specifically the effective resistances of edges in the graph
has been useful in describing the theoretical performance of a detector derived from uniform spanning tree wavelets As it turns out the performance of LESS is also dictated by the effective
resistances of edges in the graph
Effective Resistance Effective resistances have been extensively studied in electrical network theory We define the combinatorial Laplacian of to be Dv,v is the
diagonal degree matrix A potential difference is any such that it satisfies Kirchoff?s potential law the total potential difference around any cycle is Algebraically this means that Rp
such that The Dirichlet principle states that any solution to the following program gives
an absolute potential that satisfies Kirchoff?s law
minx xS vS
for source/sinks and some voltage constraints vS By Lagrangian calculus the
solution to the above program is given by where is over and vS over and
indicates the Moore-Penrose pseudoinverse The effective resistance between a source and
a sink is the potential difference required to create a unit flow between them Hence the
effective resistance between and is rv,w where is the Dirac delta
function There is a close connection between effective resistances and random spanning trees The
uniform spanning tree UST is a random spanning tree chosen uniformly at random from the set of
all distinct spanning trees The foundational Matrix-Tree theorem states that the probability
of an edge being included in the UST is equal to the edge weight times the effective resistance
We re The UST is an essential component of the proof of our main theorem in that it provides a
mechanism for unravelling the graph while still preserving the connectivity of the graph
We are now in a position to state the main theorem which will allow us to control the type error
the probability of false alarm of both the GSS and its relaxation the LESS
Theorem Let rC max Wu,v be the maximum effective resistance of the boundary of a cluster C. The following statements hold under the null hypothesis
H0
The graph scan statistic with probability at least is smaller than
rC
log(p log
log
The Lov?asz extended scan statistic with probability at least is smaller than
log(2p
log log
rC
rC log log
log
The implication of Theorem is that the size of the test may be controlled at level by selecting
thresholds given by and for GSS and LESS respectively Notice that the control provided
for the LESS is not significantly different from that of the GSS. This is highlighted by the following
Corollary which combines Theorem with a type error bound to produce an information theoretic
guarantee for the asymptotic performance of the GSS and LESS
Corollary Both the GSS and the LESS asymptotically distinguish H0 from H1 if
max rC log log
To summarize we have established that the performance of the GSS and the LESS are dictated by
the effective resistances of cuts in the graph While the condition in Cor. may seem mysterious
the guarantee in fact nearly matches the lower bound for many graph models as we now show
Specific Graph Models
Theorem shows that the effective resistance of the boundary plays a critical role in characterizing
the distinguishability region of both the the GSS and LESS On specific graph families we can
compute the effective resistances precisely leading to concrete detection guarantees that we will see
nearly matches the lower bound in many cases Throughout this section we will only be working
with undirected unweighted graphs
Recall that Corollary shows that an SNR of rC log is sufficient while Theorem shows
that
dmax log is necessary for detection Thus if we can show that rC dmax we
would establish the near-optimality of both the GSS and LESS Foster?s theorem lends evidence to
the fact that the effective resistances should be much smaller than the cut size
Theorem Foster?s Theorem
re
e?E
Roughly speaking the effective resistance of an edge selected uniformly at random is
ave so the effective resistance of a cut is dave This intuition can be formalized for specific
models and this improvement by the average degree bring us much closer to the lower bound
Edge Transitive Graphs
An edge transitive graph is one for which there is a graph automorphism mapping e0 to e1 for any
pair of edges e0 e1 Examples include the l-dimensional torus the cycle and the complete graph
Kp The existence of these automorphisms implies that every edge has the same effective resistance
and by Foster?s theorem we know that these resistances are exactly Moreover since
edge transitive graphs must be d-regular we know that so that re Thus as
a corollary to Theorem we have that
both the GSS and LESS are near-optimal optimal modulo
logarithmic factors whenever on edge transitive graphs
Corollary Let be an edge-transitive graph with common degree Then both the GSS and
LESS distinguish H0 from H1 provided that
max log log
Random Geometric Graphs
Another popular family of graphs are those constructed from a set of points in RD drawn according
to some density These graphs have inherent randomness stemming from sampling of the density
and thus earn the name random geometric graphs The two most popular such graphs are symmetric
k-nearest neighbor graphs and graphs We characterize the distinguishability region for both
In both cases a set of points z1 zp are drawn from a density support over RD or a subset
of RD Our results require mild regularity conditions on which roughly speaking require that
supp(f is topologically equivalent to the cube and has density bounded away from zero See
for a precise definition To form a k-nearest neighbor graph Gk we associate each vertex with a
point zi and we connect vertices if zi is amongst the k-nearest neighbors in of zj or vice
versa In the the graph we connect vertices if zj for some metric
The relationship re which we used for edge-transitive graphs was derived in Corollaries
and in The precise concentration arguments which have been done before lead to the
following corollary regarding the performance of the GSS and LESS on random geometric graphs
Figure A comparison of detection procedures spectral scan statistic UST wavelet detector
Wavelet and LESS The graphs used are the square 2D Torus kNN graph and graph
with with respectively and
Corollary Let Gk be a k-NN graph with k/p and suppose the density
meets the regularity conditions in Then both the GSS and LESS distinguish H0 from H1
provided that
max log log
If is an graph with then both distinguish H0 from H1 provided that
max
log
log
p?D
The corollary follows immediately form Corollary and the proofs in Since under the regularity conditions the maximum degree is and in k-NN?and graphs respectively the
corollary establishes the near optimality again provided that of both test statistics
We performed some experiments using the MRF based algorithm outlined in Prop Each experiment is made with graphs with vertices and we report the true positive rate versus the false
positive rate as the threshold varies also known as the ROC For each graph model LESS provides
gains over the spectral scan statistic[16 and the UST wavelet detector[17 each of the gains are
significant except for the graph which is more modest
Conclusions
To summarize while Corollary characterizes the performance of GSS and LESS in terms of effective resistances in many specific graph models this can be translated into near-optimal detection
guarantees for these test statistics We have demonstrated that the LESS provides guarantees similar
to that of the computationally intractable generalized likelihood ratio test Furthermore the
LESS can be solved through successive graph cuts by relating it to MAP estimation in an MRF.
Future work includes using these concepts for localizing the activation making the program robust
to missing data and extending the analysis to non-Gaussian error
Acknowledgments
This research is supported in part by AFOSR under grant and NSF under grant
AK is supported in part by a NSF Graduate Research Fellowship We would like to
thank Sivaraman Balakrishnan for his valuable input in the theoretical development of the paper

----------------------------------------------------------------

