query sentence: machine-learning toolkit 
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 5812-matrix-manifold-optimization-for-gaussian-mixtures.pdf

Matrix Manifold Optimization for Gaussian Mixtures
Reshad Hosseini
School of ECE
College of Engineering
University of Tehran Tehran Iran
reshad.hosseini@ut.ac.ir
Suvrit Sra
Laboratory for Information and Decision Systems
Massachusetts Institute of Technology
Cambridge MA.
suvrit@mit.edu
Abstract
We take a new look at parameter estimation for Gaussian Mixture Model GMMs
Specifically we advance Riemannian manifold optimization on the manifold of
positive definite matrices as a potential replacement for Expectation Maximization which has been the de facto standard for decades An out-of-the-box
invocation of Riemannian optimization however fails spectacularly it obtains
the same solution as EM but vastly slower Building on intuition from geometric
convexity we propose a simple reformulation that has remarkable consequences
it makes Riemannian optimization not only match EM nontrivial result on its
own given the poor record nonlinear programming has had against but also
outperform it in many settings To bring our ideas to fruition we develop a welltuned Riemannian LBFGS method that proves superior to known competing methods Riemannian conjugate gradient We hope that our results encourage a
wider consideration of manifold optimization in machine learning and statistics
Introduction
Gaussian Mixture Models GMMs are a mainstay in a variety of areas including machine learning
and signal processing 19 A quick literature search reveals that for estimating parameters of a GMM the Expectation Maximization algorithm is still the de facto choice
Over the decades other numerical approaches have also been considered but methods such as
conjugate gradients quasi-Newton Newton have been noted to be usually inferior to EM
The key difficulty of applying standard nonlinear programming methods to GMMs is the positive
definiteness constraint on covariances Although an open subset of Euclidean space this constraint can be difficult to impose especially in higher-dimensions When approaching the boundary
of the constraint set convergence speed of iterative methods can also get adversely affected A partial remedy is to remove the PD constraint by using Cholesky decompositions as exploited in
semidefinite programming It is believed that in general the nonconvexity of this decomposition adds more stationary points and possibly spurious local minima.1 Another possibility is
to formulate the PD constraint via a set of smooth convex inequalities and apply interior-point
methods But such sophisticated methods can be extremely slower on several statistical problems
than simpler EM-like iterations especially for higher dimensions
Since the key difficulty arises from the PD constraint an appealing idea is to note that PD matrices
form a Riemannian manifold and to invoke Riemannian manifold optimization
Indeed if we operate on the manifold2 we implicitly satisfy the PD constraint and may have a
better chance at focusing on likelihood maximization While attractive this line of thinking also
fails an out-of-the-box invocation of manifold optimization is also vastly inferior to EM. Thus we
need to think harder before challenging the hegemony of EM. We outline a new approach below
Remarkably using Cholesky with the reformulation in does not add spurious local minima to GMMs
Equivalently on the interior of the constraint set as is done by interior point methods their nonconvex
versions though these turn out to be slow too as they are second order methods
Key idea Intuitively the mismatch is in the geometry For GMMs the M-step of EM is a Euclidean
convex optimization problem whereas the GMM log-likelihood is not manifold convex3 even for a
single Gaussian If we could reformulate the likelihood so that the single component maximization
task which is the analog of the M-step of EM for GMMs becomes manifold convex it might have a
substantial empirical impact This intuition supplies the missing link and finally makes Riemannian
manifold optimization not only match EM but often also greatly outperform it
To summarize the key contributions of our paper are the following
Introduction of Riemannian manifold optimization for GMM parameter estimation for which we
show how a reformulation based on geodesic convexity is crucial to empirical success
Development of a Riemannian LBFGS solver here our main contribution is the implementation
of a powerful line-search procedure which ensures convergence and makes LBFGS outperform
both EM and manifold conjugate gradients This solver may be of independent interest
We provide substantive experimental evidence on both synthetic and real-data We compare manifold optimization EM and unconstrained Euclidean optimization that reformulates the problem
using Cholesky factorization of inverse covariance matrices Our results shows that manifold optimization performs well across a wide range of parameter values and problem sizes It is much less
sensitive to overlapping data than EM and displays much less variability in running times
Our results are quite encouraging and we believe that manifold optimization could open new algorithmic avenues for mixture models and perhaps other statistical estimation problems
Note To aid reproducibility of our results ATLAB implementations of our methods are available
as a part of the IXEST toolbox developed by our group The manifold CG method that we use
is directly based on the excellent toolkit ANOPT
Related work Summarizing published work on EM is clearly impossible So let us briefly mention a few lines of related work Xu and Jordan examine several aspects of EM for GMMs and
counter the claims of Redner and Walker who claimed EM to be inferior to generic secondorder nonlinear programming techniques However it is now well-known that EM can
attain good likelihood values rapidly and scales to much larger problems than amenable to secondorder methods Local convergence analysis of EM is available in with more refined results
in who show that when data have low overlap EM can converge locally superlinearly Our
paper develops Riemannian LBFGS which can also achieve local superlinear convergence
For GMMs some innovative gradient-based methods have also been suggested where the
PD constraint is handled via a Cholesky decomposition of covariance matrices However these
works report results only for low-dimensional problems and near spherical covariances
The idea of using manifold optimization for GMMs is new though manifold optimization by itself is
a well-developed subject A classic

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2276-stochastic-neighbor-embedding.pdf

Stochastic Neighbor Embedding
Geoffrey Hinton and Sam Roweis
Department of Computer Science University of Toronto
King?s College Road Toronto M5S Canada
hinton,roweis @cs.toronto.edu
Abstract
We describe a probabilistic approach to the task of placing objects described by high-dimensional vectors or by pairwise dissimilarities in a
low-dimensional space in a way that preserves neighbor identities A
Gaussian is centered on each object in the high-dimensional space and
the densities under this Gaussian the given dissimilarities are used
to define a probability distribution over all the potential neighbors of
the object The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the
low-dimensional images of the objects A natural cost function is a
sum of Kullback-Leibler divergences one per object which leads to a
simple gradient for adjusting the positions of the low-dimensional images Unlike other dimensionality reduction methods this probabilistic
framework makes it easy to represent each object by a mixture of widely
separated low-dimensional images This allows ambiguous objects like
the document count vector for the word bank to have versions close to
the images of both river and finance without forcing the images of
outdoor concepts to be located close to those of corporate concepts
Introduction
Automatic dimensionality reduction is an important toolkit operation in machine learning both as a preprocessing step for other algorithms to reduce classifier input size
and as a goal in itself for visualization interpolation compression etc There are many
ways to embed objects described by high-dimensional vectors or by pairwise dissimilarities into a lower-dimensional space Multidimensional scaling methods[1 preserve
dissimilarities between items as measured either by Euclidean distance some nonlinear
squashing of distances or shortest graph paths as with Isomap[2 Principal components analysis PCA finds a linear projection of the original data which captures as much
variance as possible Other methods attempt to preserve local geometry or
associate high-dimensional points with a fixed grid of points in the low-dimensional space
self-organizing maps[5 or their probabilistic extension All of these methods however require each high-dimensional object to be associated with only a single
location in the low-dimensional space This makes it difficult to unfold many-to-one
mappings in which a single ambiguous object really belongs in several disparate locations
in the low-dimensional space In this paper we define a new notion of embedding based on
probable neighbors Our algorithm Stochastic Neighbor Embedding SNE tries to place
the objects in a low-dimensional space so as to optimally preserve neighborhood identity
and can be naturally extended to allow multiple different low-d images of each object
The basic SNE algorithm
For each object and each potential neighbor we start by computing the asymmetric
probability that would pick as its neighbor
The dissimilarities may be given as part of the problem definition and need not be
symmetric or they may be computed using the scaled squared Euclidean distance affinity between two high-dimensional points
where is either
set by hand or as in some of our experiments found by a binary search
for the value of that makes the entropy of the distribution over neighbors equal to
Here is the effective number of local neighbors or perplexity and is chosen by hand
In the low-dimensional space we also use Gaussian neighborhoods but with a fixed variance
which we set without loss of generality to be so the induced probability that point
picks point as its neighbor is a function of the low-dimensional images of all the
objects and is given by the expression
The aim of the embedding is to match these two distributions as well as possible This is
achieved by minimizing a cost function which is a sum of Kullback-Leibler divergences
between the original and induced distributions over neighbors for each object
@ A
The dimensionality of the space is chosen by hand much less than the number of objects
Notice that making large when is small wastes some of the probability mass in the
distribution so there is a cost for modeling a big distance in the high-dimensional space with
a small distance in the low-dimensional space though it is less than the cost of modeling
a small distance with a big one In this respect SNE is an improvement over methods
like LLE or SOM in which widely separated data-points can be collapsed as near
neighbors in the low-dimensional space The intuition is that while SNE emphasizes local
distances its cost function cleanly enforces both keeping the images of nearby objects
nearby and keeping the images of widely separated objects relatively far apart
Differentiating is tedious because
the result is simple
affects via the normalization term in but
which has the nice interpretation of a sum of forces pulling toward or pushing it away
depending on whether is observed to be a neighbor more or less often than desired
Given the gradient there are many possible ways to minimize and we have only just begun the search for the best method Steepest descent in which all of the points are adjusted
in parallel is inefficient and can get stuck in poor local optima Adding random jitter that
decreases with time finds much better local optima and is the method we used for the examples in this paper even though it is still quite slow We initialize the embedding by putting
all the low-dimensional images in random locations very close to the origin Several other
minimization methods including annealing the perplexity are discussed in sections
Application of SNE to image and document collections
As a graphic illustration of the ability of SNE to model high-dimensional near-neighbor
relationships using only two dimensions we ran the algorithm on a collection of bitmaps of
handwritten digits and on a set of word-author counts taken from the scanned proceedings
of NIPS conference papers Both of these datasets are likely to have intrinsic structure in
many fewer dimensions than their raw dimensionalities for the handwritten digits and
for the author-word counts
To begin we used a set of digit bitmaps from the UPS database[7 with examples
from each
of the five classes The variance of the Gaussian around each point
in the dimensional raw pixel image space was set to achieve a perplexity of in the
distribution over high-dimensional neighbors SNE was initialized by putting all the
in random locations very close to the origin and then was trained using gradient descent
with annealed noise Although SNE was given no information about class labels it quite
cleanly separates the digit groups as shown in figure Furthermore within each region of
the low-dimensional space SNE has arranged the data so that properties like orientation
skew and stroke-thickness tend to vary smoothly For the embedding shown the SNE
cost function in has a value of
nats
with a uniform
distribution across lowdimensional neighbors the cost is
nats We also applied
principal component analysis to the same data the projection onto the first two
principal components does not separate classes nearly as cleanly as SNE because PCA is
much more interested in getting the large separations right which causes it to jumble up
some of the boundaries between similar classes In this experiment we used digit classes
that do not have very similar pairs like and or and When there are more classes and
only two available dimensions SNE does not as cleanly separate very similar pairs
We have also applied SNE to word-document and word-author matrices calculated from
the OCRed text of NIPS volume papers[9 Figure shows a map locating NIPS authors into two dimensions Each of the authors who published more than one paper
in NIPS vols is shown by a dot at the position found by SNE larger red dots
and corresponding last names are authors who published six or more papers in that period
Distances were computed as the norm of the difference between log aggregate author
word counts summed across all NIPS papers Co-authored papers gave fractional counts
evenly to all authors All words occurring in six or more documents were included except for stopwords giving a vocabulary size of The bow toolkit[10 was used for
part of
the pre-processing of the data The were set to achieve a local perplexity of
neighbors SNE seems to have grouped authors by broad NIPS field generative
models support vector machines neuroscience reinforcement learning and VLSI all have
distinguishable localized regions
A full mixture version of SNE
The clean probabilistic formulation of SNE makes it easy to modify the cost function so
that instead of a single image each high-dimensional object can have several different
versions of its low-dimensional image These alternative versions have mixing proportions
that sum to Image-version of object has location and mixing proportion
The
low-dimensional neighborhood distribution for is a mixture of the distributions induced
by each of its image-versions across all image-versions of a potential neighbor
In this multiple-image model the derivatives with respect to the image locations are
straightforward the derivatives w.r.t the mixing proportions are most easily expressed
Figure The result of running the SNE algorithm on
dimensional grayscale
images of handwritten digits Pictures of the original data vectors scans of handwritten
digit are shown at the location corresponding to their low-dimensional images as found
by SNE. The classes are quite well separated even though SNE had no information about
class labels Furthermore within each class properties like orientation skew and strokethickness tend to vary smoothly across the space Not all points are shown to produce this
display digits are chosen in random order and are only displayed if a region of the
display centered on the location of the digit in the embedding does not overlap any of
the regions for digits that have already been displayed
SNE was initialized by putting all the in random locations very close to the origin and then was
trained using batch gradient descent with annealed noise The learning rate was For
the first iterations each point was jittered by adding Gaussian noise with a standard deviation of after each position update The jitter was then reduced to for a further
iterations
Touretzky
Wiles
Maass
Kailath
Chauvin Munro Shavlik
Sanger
Movellan Baluja Lewicki Schmidhuber
Hertz
Baldi Buhmann Pearlmutter Yang
Tenenbaum
Cottrell
Krogh
Omohundro Abu?Mostafa
Schraudolph
MacKay
Coolen
Lippmann
Robinson Smyth
Cohn
Ahmad Tesauro
Pentland
Goodman
Atkeson
Neuneier
Warmuth
Sollich Moore
Thrun
Pomerleau
Barber
Ruppin
Horn
Meilijson MeadLazzaro
Koch
Obermayer Ruderman
Eeckman HarrisMurray
Bialek Cowan
Baird Andreou
Mel
Cauwenberghs
Brown Li
Jabri
Giles Chen
Spence Principe
Doya Touretzky
Sun
Stork Alspector Mjolsness
Bell
Lee
Maass
Lee
Gold
Pomerleau Kailath Meir
Seung Movellan
Rangarajan
Yang Amari
Tenenbaum
Cottrell Baldi
Abu?Mostafa
MacKay
Nowlan Lippmann
Smyth Cohn Kowalczyk
Waibel
Pouget
Atkeson
Kawato
Viola Bourlard Warmuth
Dayan
Sollich
Morgan Thrun MooreSutton
Barber Barto Singh
Tishby WolpertOpper
Sejnowski
Williamson
Kearns
Singer
Moody
Shawe?Taylor
Saad
Zemel
Saul
Tresp
Bartlett
Platt
Leen
Mozer
Bishop Jaakkola
Solla
Ghahramani
Smola
Williams
Vapnik
Scholkopf
Hinton
Bengio
Jordan
Muller
Graf
LeCun Simard
Denker
Guyon
Bower
Figure Embedding of NIPS authors into two dimensions Each of the authors
who published more than one paper in NIPS vols is show by a dot at the location found by the SNE algorithm Larger red dots and corresponding last names
are authors who published six or more papers in that period The inset in upper left
shows a blowup of the crowded boxed central portion of the space Dissimilarities between authors were computed based on squared Euclidean distance between vectors of
log aggregate author word counts Co-authored papers gave fractional counts evenly
to all authors All words occurring in six or more documents were included except
for stopwords giving a vocabulary size of The NIPS text data is available at
http://www.cs.toronto.edu roweis/data.html
in terms of the probability that version of picks version of
The effect on of changing the mixing proportion for version of object
where
if
is given by
and otherwise The effect of changing on the cost is
Rather than optimizing the mixing proportions directly it is easier
to perform unconstrained
optimization on softmax weights defined by
As a proof-of-concept we recently implemented a simplified mixture version in which
every object is represented in the low-dimensional
space by exactly two components that
are constrained to have mixing proportions of The two components are pulled together
by a force which increases linearly up to a threshold separation Beyond this threshold
the force remains constant.1 We ran two experiments with this simplified mixture version
of SNE. We took a dataset containing pictures of each of the digits and added
hybrid digit-pictures that were each constructed by picking new examples of two of
the classes and taking each pixel at random from one of these two parents After mini of the hybrids and only
of the non-hybrids had significantly different
mization
locations for their two mixture components Moreover the mixture components of each
hybrid always lay in the regions of the space devoted to the classes of its two parents and
never in the region devoted to the third class For this example we used a perplexity of
in defining the local neighborhoods a step size of for each position update of
times the
gradient and used a constant jitter of
Our very simple mixture version of SNE also
makes it possible to map a circle onto a line without losing any near neighbor relationships
or introducing any new ones Points near one cut point on the circle can mapped to a
mixture of two points one near one end of the line and one near the other end Obviously
the location of the cut on the two-dimensional circle gets decided by which pairs of mixture
components split first during the stochastic optimization For certain optimization parameters that control the ease with which two mixture components can be pulled apart only
a single cut in the circle is made For other parameter settings however the circle may
fragment into two or more smaller line-segments each of which is topologically correct
but which may not be linked to each other
The example with hybrid digits demonstrates that even the most primitive mixture version
of SNE can deal with ambiguous high-dimensional objects that need to be mapped to two
widely separated regions of the low-dimensional space More work needs to be done before
SNE is efficient enough to cope with large matrices of document-word counts but it is
the only dimensionality reduction method we know of that promises to treat homonyms
sensibly without going back to the original documents to disambiguate each occurrence of
the homonym
We used a threshold of At threshold the force was
nats per unit length The low-d
space has a natural scale because the variance of the Gaussian used to determine is fixed at
Practical optimization strategies
Our current method of reducing the SNE cost is to use steepest descent with added jitter
that is slowly reduced This produces quite good embeddings which demonstrates that the
SNE cost function is worth minimizing but it takes several hours to find a good embedding
for just datapoints so we clearly need a better search algorithm
The time per iteration could be reduced considerably by ignoring pairs of points for which
all four of are small Since the matrix is fixed during the learning it is
natural to sparsify it by replacing all entries below a certain threshold with zero and renormalizing Then pairs for which both and are zero can be ignored from gradient
calculations if both and are small This can in turn be determined in logarithmic
time in the size of the training set by using sophisticated geometric data structures such
as K-D trees ball-trees and AD-trees since the depend only on 42 Computational physics has attacked exactly this same complexity when performing multibody
gravitational or electrostatic simulations using for example the fast multipole method
In the mixture version of SNE there appears to be an interesting way of avoiding local
optima that does not involve annealing the jitter Consider two components in the mixture
for an object that are far apart in the low-dimensional space By raising the mixing proportion of one and lowering the mixing proportion of the other we can move probability mass
from one part of the space to another without it ever appearing at intermediate locations
This type of probability wormhole seems like a good way to avoid local optima that arise
because a cluster of low-dimensional points must move through a bad region of the space
in order to reach a better one
Yet another search method which we have used with some success on toy problems is
to provide extra dimensions in the low-dimensional space but to penalize non-zero values
on these dimensions During the search SNE will use the extra dimensions to go around
lower-dimensional barriers but as the penalty on using these dimensions is increased they
will cease to be used effectively constraining the embedding to the original dimensionality
Discussion and Conclusions
Preliminary
experiments show that we can find good optima by first annealing the perplex
ities using high jitter and only reducing the jitter after the final perplexity
has been
reached This raises the question of what SNE is doing when the variance of the Gaussian centered on each high-dimensional point is very big so that the distribution across
neighbors is almost uniform It is clear that in the high variance limit the contribution of
to the SNE cost function is just as important for distant neighbors as for
close ones When is very large it can be shown that SNE is equivalent to minimizing the
mismatch between squared distances in the two spaces provided all the squared distances
from an object are first normalized by subtracting off their antigeometric mean
where is the number of objects
This mismatch is very similar to stress functions used in nonmetric versions of MDS
and enables us to understand the large-variance limit of SNE as a particular variant of such
procedures We are still investigating the relationship to metric MDS and to PCA.
SNE can also be seen as an interesting special case of Linear Relational Embedding LRE
In LRE the data consists of triples Colin has-mother Victoria and the task
is to predict the third term from the other two LRE learns an N-dimensional vector for
each object and an NxN-dimensional matrix for each relation To predict the third term in
a triple LRE multiplies the vector representing the first term by the matrix representing
the relationship and uses the resulting vector as the mean of a Gaussian Its predictive
distribution for the third term is then determined by the relative densities of all known
objects under this Gaussian SNE is just a degenerate version of LRE in which the only
relationship is near and the matrix representing this relationship is the identity
In summary we have presented a new criterion Stochastic Neighbor Embedding for mapping high-dimensional points into a low-dimensional space based on stochastic selection
of similar neighbors Unlike self-organizing maps in which the low-dimensional coordinates are fixed to a grid and the high-dimensional ends are free to move in SNE the
high-dimensional coordinates are fixed to the data and the low-dimensional points move
Our method can also be applied to arbitrary pairwise dissimilarities between objects if such
are available instead of in addition to high-dimensional observations The gradient of
the SNE cost function has an appealing push-pull property in which the forces acting on
to bring it closer to points it is under-selecting and further from points it is over-selecting
as its neighbor We have shown results of applying this algorithm to image and document
collections for which it sensibly placed similar objects nearby in a low-dimensional space
while keeping dissimilar objects well separated
Most importantly because of its probabilistic formulation SNE has the ability to be extended to mixtures in which ambiguous high-dimensional objects such as the word bank
can have several widely-separated images in the low-dimensional space
Acknowledgments We thank the anonymous referees and several visitors to our poster for helpful
suggestions Yann LeCun provided digit and NIPS text data This research was funded by NSERC

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf

Scheduled Sampling for Sequence Prediction with
Recurrent Neural Networks
Samy Bengio Oriol Vinyals Navdeep Jaitly Noam Shazeer
Google Research
Mountain View CA USA
bengio,vinyals,ndjaitly,noam}@google.com
Abstract
Recurrent Neural Networks can be trained to produce sequences of tokens given
some input as exemplified by recent results in machine translation and image
captioning The current approach to training them consists of maximizing the
likelihood of each token in the sequence given the current recurrent state and the
previous token At inference the unknown previous token is then replaced by a
token generated by the model itself This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence
We propose a curriculum learning strategy to gently change the training process
from a fully guided scheme using the true previous token towards a less guided
scheme which mostly uses the generated token instead Experiments on several sequence prediction tasks show that this approach yields significant improvements
Moreover it was used succesfully in our winning entry to the MSCOCO image
captioning challenge
Introduction
Recurrent neural networks can be used to process sequences either as input output or both While
they are known to be hard to train when there are long term dependencies in the data some
versions like the Long Short-Term Memory LSTM are better suited for this In fact they have
recently shown impressive performance in several sequence prediction problems including machine
translation contextual parsing image captioning and even video description
In this paper we consider the set of problems that attempt to generate a sequence of tokens of
variable size such as the problem of machine translation where the goal is to translate a given
sentence from a source language to a target language We also consider problems in which the input
is not necessarily a sequence like the image captioning problem where the goal is to generate a
textual description of a given image
In both cases recurrent neural networks their variants like LSTMs are generally trained to
maximize the likelihood of generating the target sequence of tokens given the input In practice this
is done by maximizing the likelihood of each target token given the current state of the model which
summarizes the input and the past output tokens and the previous target token which helps the
model learn a kind of language model over target tokens However during inference true previous
target tokens are unavailable and are thus replaced by tokens generated by the model itself yielding
a discrepancy between how the model is used at training and inference This discrepancy can be
mitigated by the use of a beam search heuristic maintaining several generated target sequences but
for continuous state space models like recurrent neural networks there is no dynamic programming
approach so the effective number of sequences considered remains small even with beam search
The main problem is that mistakes made early in the sequence generation process are fed as input
to the model and can be quickly amplified because the model might be in a part of the state space it
has never seen at training time
Here we propose a curriculum learning approach to gently bridge the gap between training and
inference for sequence prediction tasks using recurrent neural networks We propose to change the
training process in order to gradually force the model to deal with its own mistakes as it would
have to during inference Doing so the model explores more during training and is thus more robust
to correct its own mistakes at inference as it has learned to do so during training We will show
experimentally that this approach yields better performance on several sequence prediction tasks
The paper is organized as follows in Section we present our proposed approach to better train
sequence prediction tasks with recurrent neural networks this is followed by Section which draws
links to some related approaches We then present some experimental results in Section and
conclude in Section
Proposed Approach
We are considering supervised tasks where the training set is given in terms of input/output pairs
where is the input and can be either static like an image or dynamic like a
sequence while the target output is a sequence y1i y2i yTi of a variable number of tokens
that belong to a fixed known dictionary
Model
Given a single input/output pair the log probability can be computed as
log
log
log
where is a sequence of length represented by tokens y1 y2 yT The latter term in the above
equation is estimated by a recurrent neural network with parameters by introducing a state vector
ht that is a function of the previous state and the previous output token
log log ht
where ht is computed by a recurrent neural network as follows
if
ht
otherwise
ht is often implemented as a linear projection1 of the state vector ht into a vector of scores
one for each token of the output dictionary followed by a softmax transformation to ensure the
scores are properly normalized positive and sum to is usually a non-linear function that
combines the previous state and the previous output in order to produce the current state
This means that the model focuses on learning to output the next token given the current state
of the model AND the previous token Thus the model represents the probability distribution of
sequences in the most general form unlike Conditional Random Fields and other models that
assume independence between between outputs at different time steps given latent variable states
The capacity of the model is only limited by the representational capacity of the recurrent and
feedforward layers LSTMs with their ability to learn long range structure are especially well suited
to this task and make it possible to learn rich distributions over sequences
In order to learn variable length sequences a special token that signifies the end of a
sequence is added to the dictionary and the model During training EOS is concatenated to the
end of each sequence During inference the model generates tokens until it generates
Although one could also use a multi-layered non-linear projection
Training
Training recurrent neural networks to solve such tasks is usually accomplished by using mini-batch
stochastic gradient descent to look for a set of parameters that maximizes the log likelihood of
producing the correct target sequence given the input data for all training pairs
arg max
log
Inference
During inference the model can generate the full sequence y1T given by generating one token at a
time and advancing time by one step When an EOS token is generated it signifies the end of
the sequence For this process at time the model needs as input the output token from the
last time step in order to produce Since we do not have access to the true previous token we can
instead either select the most likely one given our model or sample according to it
Searching for the sequence with the highest probability given is too expensive because of the
combinatorial growth in the number of sequences Instead we use a beam searching procedure to
generate best sequences We do this by maintaining a heap of best candidate sequences At
each time step new candidates are generated by extending each candidate by one token and adding
them to the heap At the end of the step the heap is re-pruned to only keep candidates The beam
searching is truncated when no new sequences are added and best sequences are returned
While beam search is often used for discrete state based models like Hidden Markov Models where
dynamic programming can be used it is harder to use efficiently for continuous state based models
like recurrent neural networks since there is no way to factor the followed state paths in a continuous
space and hence the actual number of candidates that can be kept during beam search decoding is
very small
In all these cases if a wrong decision is taken at time the model can be in a part of the
state space that is very different from those visited from the training distribution and for which it
doesn?t know what to do Worse it can easily lead to cumulative bad decisions a classic problem in
sequential Gibbs sampling type approaches to sampling where future samples can have no influence
on the past
Bridging the Gap with Scheduled Sampling
The main difference between training and inference for sequence prediction tasks when predicting
token is whether we use the true previous token or an estimate coming from the model
itself
We propose here a sampling mechanism that will randomly decide during training whether we use
or Assuming we use a mini-batch based stochastic gradient descent approach for every
token to predict of the ith mini-batch of the training algorithm we propose to flip a coin
and use the true previous token with probability or an estimate coming from the model itself with
probability The estimate of the model can be obtained by sampling a token according to
the probability distribution modeled by or can be taken as the arg maxs
This process is illustrated in Figure
When the model is trained exactly as before while when the model is trained in
the same setting as inference We propose here a curriculum learning strategy to go from one to
the other intuitively at the beginning of training sampling from the model would yield a random
token since the model is not well trained which could lead to very slow convergence so selecting
more often the true previous token should help on the other hand at the end of training should
favor sampling from the model more often as this corresponds to the true inference situation and
one expects the model to already be good enough to handle it and sample reasonable tokens
Note that in the experiments we flipped the coin for every token We also tried to flip the coin once per
sequence but the results were much worse most probably because consecutive errors are amplified during the
first rounds of training
Figure Illustration of the Scheduled Sampling approach
where one flips a coin at every time step to decide to use the
true previous token or one sampled from the model itself
Exponential decay
Inverse sigmoid decay
Linear decay
Figure
schedules
Examples of decay
We thus propose to use a schedule to decrease as a function of itself in a similar manner used
to decrease the learning rate in most modern stochastic gradient descent approaches Examples of
such schedules can be seen in Figure as follows
Linear decay ci where is the minimum amount of truth to be
given to the model and and provide the offset and slope of the decay which depend on
the expected speed of convergence
Exponential decay where is a constant that depends on the expected speed
of convergence
Inverse sigmoid decay where depends on the expected speed
of convergence
We call our approach Scheduled Sampling Note that when we sample the previous token from
the model itself while training we could back-propagate the gradient of the losses at times
through that decision This was not done in the experiments described in this paper and is left for
future work
Related Work
The discrepancy between the training and inference distributions has already been noticed in the
literature in particular for control and reinforcement learning tasks
SEARN was proposed to tackle problems where supervised training examples might be different
from actual test examples when each example is made of a sequence of decisions like acting in a
complex environment where a few mistakes of the model early in the sequential decision process
might compound and yield a very poor global performance Their proposed approach involves a
meta-algorithm where at each meta-iteration one trains a new model according to the current policy
essentially the expected decisions for each situation applies it on a test set and modifies the next
iteration policy in order to account for the previous decisions and errors The new policy is thus a
combination of the previous one and the actual behavior of the model
In comparison to SEARN and related ideas our proposed approach is completely online a
single model is trained and the policy slowly evolves during training instead of a batch approach
which makes it much faster to train3 Furthermore SEARN has been proposed in the context of
reinforcement learning while we consider the supervised learning setting trained using stochastic
gradient descent on the overall objective
Other approaches have considered the problem from a ranking perspective in particular for parsing
tasks where the target output is a tree In this case the authors proposed to use a beam search
both during training and inference so that both phases are aligned The training beam is used to find
In fact in the experiments we report in this paper our proposed approach was not meaningfully slower
nor faster to train than the baseline
the best current estimate of the model which is compared to the guided solution the truth using a
ranking loss Unfortunately this is not feasible when using a model like a recurrent neural network
which is now the state-of-the-art technique in many sequential tasks as the state sequence cannot
be factored easily because it is a multi-dimensional continuous state and thus beam search is hard
to use efficiently at training time as well as inference time in fact
Finally proposed an online algorithm for parsing problems that adapts the targets through the
use of a dynamic oracle that takes into account the decisions of the model The trained model
is a perceptron and is thus not state-based like a recurrent neural network and the probability of
choosing the truth is fixed during training
Experiments
We describe in this section experiments on three different tasks in order to show that scheduled
sampling can be helpful in different settings We report results on image captioning constituency
parsing and speech recognition
Image Captioning
Image captioning has attracted a lot of attention in the past year The task can be formulated as a
mapping of an image onto a sequence of words describing its content in some natural language and
most proposed approaches employ some form of recurrent network structure with simple decoding
schemes A notable exception is the system proposed in which does not
directly optimize the log likelihood of the caption given the image and instead proposes a pipelined
approach
Since an image can have many valid captions the evaluation of this task is still an open problem Some attempts have been made to design metrics that positively correlate with human evaluation and a common set of tools have been published by the MSCOCO team
We used the MSCOCO dataset from to train our model We trained on images and report
results on a separate development set of 5k additional images Each image in the corpus has different captions so the training procedure picks one at random creates a mini-batch of examples
and optimizes the objective function defined in The image is preprocessed by a pretrained convolutional neural network without the last classification layer similar to the one described in
and the resulting image embedding is treated as if it was the first word from which the model starts
generating language The recurrent neural network generating words is an LSTM with one layer
of hidden units and the input words are represented by embedding vectors of size The
number of words in the dictionary is We used an inverse sigmoid decay schedule for for the
scheduled sampling approach
Table shows the results on various metrics on the development set Each of these metrics is
a variant of estimating the overlap between the obtained sequence of words and the target one
Since there were target captions per image the best result is always chosen To the best of our
knowledge the baseline results are consistent slightly better with the current state-of-the-art on
that task While dropout helped in terms of log likelihood as expected but not shown it had a
negative impact on the real metrics On the other hand scheduled sampling successfully trained a
model more resilient to failures due to training and inference mismatch which likely yielded higher
quality captions according to all the metrics Ensembling models also yielded better performance
both for the baseline and the schedule sampling approach It is also interesting to note that a model
trained while always sampling from itself hence in a regime similar to inference dubbed Always
Sampling in the table yielded very poor performance as expected because the model has a hard
time learning the task in that case We also trained a model with scheduled sampling but instead
of sampling from the model we sampled from a uniform distribution in order to verify that it was
important to build on the current model and that the performance boost was not just a simple form
of regularization We called this Uniform Scheduled Sampling and the results are better than the
baseline but not as good as our proposed approach We also experimented with flipping the coin
once per sequence instead of once per token but the results were as poor as the Always Sampling
approach
Table Various metrics the higher the better on the MSCOCO development set for the image
captioning task
Approach vs Metric
Baseline
Baseline with Dropout
Always Sampling
Scheduled Sampling
Uniform Scheduled Sampling
Baseline ensemble of
Scheduled Sampling ensemble of
BLEU-4
METEOR
CIDER
It?s worth noting that we used our scheduled sampling approach to participate in the MSCOCO
image captioning challenge and ranked first in the final leaderboard
Constituency Parsing
Another less obvious connection with the any-to-sequence paradigm is constituency parsing Recent
work has proposed an interpretation of a parse tree as a sequence of linear operations that build
up the tree This linearization procedure allowed them to train a model that can map a sentence onto
its parse tree without any modification to the any-to-sequence formulation
The trained model has one layer of LSTM cells and words are represented by embedding vectors
of size We used an attention mechanism similar to the one described in which helps
when considering the next output token to produce to focus on part of the input sequence only
by applying a softmax over the LSTM state vectors corresponding to the input sequence The input
word dictionary contained around words while the target dictionary contained symbols used
to describe the tree We used an inverse sigmoid decay schedule for in the scheduled sampling
approach
Parsing is quite different from image captioning as the function that one has to learn is almost
deterministic In contrast to an image having a large number of valid captions most sentences have
a unique parse tree although some very difficult cases exist Thus the model operates almost
deterministically which can be seen by observing that the train and test perplexities are extremely
low compared to image captioning
This different operating regime makes for an interesting comparison as one would not expect the
baseline algorithm to make many mistakes However and as can be seen in Table scheduled
sampling has a positive effect which is additive to dropout In this table we report the F1 score on the
WSJ 22 development set We should also emphasize that there are only training instances
so overfitting contributes largely to the performance of our system Whether the effect of sampling
during training helps with regard to overfitting or the training/inference mismatch is unclear but the
result is positive and additive with dropout Once again a model trained by always sampling from
itself instead of using the groundtruth previous token as input yielded very bad results in fact so bad
that the resulting trees were often not valid trees hence the in the corresponding F1 metric
Table F1 score the higher the better on the validation set of the parsing task
Approach
Baseline LSTM
Baseline LSTM with Dropout
Always Sampling
Scheduled Sampling
Scheduled Sampling with Dropout
F1
Speech Recognition
For the speech recognition experiments we used a slightly different setting from the rest of the
paper Each training example is an input/output pair where is a sequence of input
vectors xT and is a sequence of tokens y1 y2 yT so each is aligned with the
corresponding Here are the acoustic features represented by log Mel filter bank spectra at
frame and is the corresponding target The targets used were HMM-state labels generated from
a GMM-HMM recipe using the Kaldi toolkit but could very well have been phoneme labels
This setting is different from the other experiments in that the model we used is the following
log
log
log xt1
log ht
where ht is computed by a recurrent neural network as follows
oh
if
ht
otherwise
where oh is a vector of with same dimensionality as ht and is an extra token added to the
dictionary to represent the start of each sequence
We generated data for these experiments using the TIMIT4 corpus and the KALDI toolkit as described in Standard configurations were used for the experiments dimensional log Mel
filter banks and their first and second order temporal derivatives were used as inputs to each frame
dimensional targets were generated for each time frame using forced alignment to transcripts
using a trained GMM-HMM system The training validation and test sets have and
sequences respectively and their average length was frames The validation set was used to
choose the best epoch in training and the model parameters from that epoch were used to evaluate
the test set
The trained models had two layers of LSTM cells and a softmax layer for each of five configurations a baseline configuration where the ground truth was always fed to the model a configuration
Always Sampling where the model was only fed in its own predictions from the last time step
and three scheduled sampling configurations Scheduled Sampling where was ramped linearly from a maximum value to a minimum value over ten epochs and then kept constant at the
final value For each configuration we trained models and report average performance over them
Training of each model was done over frame targets from the GMM. The baseline configurations
typically reached the best validation accuracy after approximately epochs whereas the sampling
models reached the best accuracy after approximately epochs after which the validation accuracy
decreased This is probably because the way we trained our models is not exact it does not account
for the gradient of the sampling probabilities from which we sampled our targets Future effort at
tackling this problem may further improve results
Testing was done by finding the best sequence from beam search decoding using a beam size of
beams and computing the error rate over the sequences We also report the next step error rate
where the model was fed in the ground truth to predict the class of the next frame for each of the
models on the validation set to summarize the performance of the models on the training objective
Table shows a summary of the results
It can be seen that the baseline performs better next step prediction than the models that sample the
tokens for input This is to be expected since the former has access to the groundtruth However it
can be seen that the models that were trained with sampling perform better than the baseline during
decoding It can also be seen that for this problem the Always Sampling model performs quite
https://catalog.ldc.upenn.edu/LDC93S1
well We hypothesize that this has to do with the nature of the dataset The HMM-aligned states
have a lot of correlation the same state appears as the target for several frames and most of the
states are constrained only to go to a subset of other states Next step prediction with groundtruth
labels on this task ends up paying disproportionate attention to the structure of the labels
and not enough to the acoustics input Thus it achieves very good next step prediction error
when the groundtruth sequence is fed in with the acoustic information but is not able to exploit
the acoustic information sufficiently when the groundtruth sequence is not fed in For this model
the testing conditions are too far from the training condition for it to make good predictions The
model that is only fed its own prediction Always Sampling ends up exploiting all the information
it can find in the acoustic signal and effectively ignores its own predictions to influence the next
step prediction Thus at test time it performs just as well as it does during training A model such as
the attention model of which predicts phone sequences directly instead of the highly redundant
HMM state sequences would not suffer from this problem because it would need to exploit both the
acoustic signal and the language model sufficiently to make predictions Nevertheless even in this
setting adding scheduled sampling still helped to improve the decoding frame error rate
Note that typically speech recognition experiments use HMMs to decode predictions from neural
networks in a hybrid model Here we avoid using an HMM altogether and hence we do not have the
advantage of the smoothing that results from the HMM architecture and the language models Thus
the results are not directly comparable to the typical hybrid model results
Table Frame Error Rate FER on the speech recognition experiments In next step prediction
reported on validation set the ground truth is fed in to predict the next target like it is done during
training In decoding experiments reported on test set beam searching is done to find the best
sequence We report results on four different linear schedulings of sampling where was ramped
down linearly from to For the baseline the model was only fed in the ground truth See
Section for an analysis of the results
Approach
Always Sampling
Scheduled Sampling
Scheduled Sampling
Scheduled Sampling
Baseline LSTM
Next Step FER
Decoding FER
Conclusion
Using recurrent neural networks to predict sequences of tokens has many useful applications like
machine translation and image description However the current approach to training them predicting one token at a time conditioned on the state and the previous correct token is different from
how we actually use them and thus is prone to the accumulation of errors along the decision paths
In this paper we proposed a curriculum learning approach to slowly change the training objective
from an easy task where the previous token is known to a realistic one where it is provided by the
model itself Experiments on several sequence prediction tasks yield performance improvements
while not incurring longer training times Future work includes back-propagating the errors through
the sampling decisions as well as exploring better sampling strategies including conditioning on
some confidence measure from the model itself

<<----------------------------------------------------------------------------------------------------------------------->>

title: 182-genesis-a-system-for-simulating-neural-networks.pdf

GENESIS A SYSTEM FOR SIMULATING NEURAL
NETWOfl.KS
Matthew A. Wilson Upinder S. Bhalla John D. Uhley James M. Bower
Division of Biology
California Institute of Technology
Pasadena CA
ABSTRACT
We have developed a graphically oriented general purpose
simulation system to facilitate the modeling of neural networks
The simulator is implemented under UNIX and X-windows and is
designed to support simulations at many levels of detail
Specifically it is intended for use in both applied network
modeling and in the simulation of detailed realistic biologicallybased models Examples of current models developed under this
system include mammalian olfactory bulb and cortex invertebrate
central pattern generators as well as more abstract connectionist
simulations
INTRODUCTION
Recently there has been a dramatic increase in interest in exploring the
computational properties of networks of parallel distributed processing elements
Rumelhart and McClelland often referred to as Itneural networks
Anderson Much of the current research involves numerical simulations of
these types of networks Anderson Touretzky Over the last several
years there has also been a significant increase in interest in using similar computer
simulation techniques to study the structure and function of biological neural
networks This effort can be seen as an attempt to reverse-engineer the brain with
the objective of understanding the functional organization of its very complicated
networks Bower Simulations of these systems range from detailed
reconstructions of single neurons or even components of single neurons to
simulations of large networks of complex neurons Koch and Segev
Modelers associated with each area of research are likely to benefit from exposure to
a large range of neural network simulations A simulation package capable of
implementing these varied types of network models would facilitate this interaction
Wilson Bhalla Uhley and Bower
DESIGN FEATURES OF THE SIMULATOR
We have built GENESIS GEneral NEtwork SImulation System and its graphical
interface XODUS X-based Output and Display Utility for Simulators to provide a
standardized and flexible means of constructing neural network simulations while
making minimal assumptions about the actual structure of the neural components
The system is capable of growing according to the needs of users by incorporating
user-defined code We will now describe the specific features of this system
Device independence
The entire system has been designed to run under UNIX and X-windows version
for maximum portability The code was developed on Sun workstations and has
been ported to Sun3's Sun4's Sun and Masscomp computers It should be
portable to all installations supporting UNIX and X-II In addition we will be
developing a parallel implementation of the simulation system Nelson
Modular design
The design of the simulator and interface is based on a building-block approach
Simulations are constructed of modules which receive inputs perform calculations
on them and generate outputs figs This approach is central to the generality
and flexibility of the system as it allows the user to easily add new features
without modification to the base code
Interactive specification and control
Network specification and control is done at a high level using graphical tools and a
network specification language fig The graphics interface provides the highest
and most user friendly level of interaction It consists of a number of tools which
the user can configure to suit a particular simulation Through the graphical
interface the user can display control and adjust the parameters of simulations The
network specification language we have developed for network modeling represents a
more basic level of interaction This language consists of a set of simulator and
interface functions that can be executed interactively from the keyboard or from
text flies storing command sequences scripts The language also provides for
arithmetic operations and program control functions such as looping conditional
statements and subprograms or macros Figures and demonstrate how some of
these script functions are used
Simulator and interrace toolkits
Extendable toolkits which consist of module libraries graphical tools and the
simulator base code itself fig provide the routines and modules used to
construct specific simulations The base code provides the common control and
support routines for the entire system
GENESIS A System for Simulating Neural Networks
Gra hics Interface
Script Files
DP~~Data
Files
Genesis command
window and ke board
Script Language
Interpreter
Genesis
Figure Levels Of Interaction With The Simulator
CONSTRUCTING SIMULATIONS
The first step in using GENESIS involves selecting and linking together those
modules from the toolkits that will be necessary for a particular simulation fig
Additional commands in the scripting language establish the network and the
graphical interface fig
Module Classes
Modules in GENESIS are divided into computational modules communications
modules and graphical modules All instances of computational modules are called
elements These are the central components of simulations performing all of the
numerical calculations Elements can communicate in two ways via links and via
connections Links allow the passing of data between two elements with no time
delay and with no computation being performed on the data Thus links serve to
unify a large number of elements into a single computational unit they are
used to link elements together to form the neuron in fig Connections on the
other hand interconnect computational units via simulated communication channels
which can incorporate time delays and perform transformations on data being
transmitted axons in fig Graphical modules called widgets are used to
construct the interface These modules can issue script commands as well as respond
to them thus allowing interactive access to simulator structures and functions
Wilson Bhalla Uhley and Bower
Hierarchical organization
In order to keep track of the structure of a simulation elements are organized into a
tree hierarchy similar to the directory structure in UNIX fig The tree
structure does not explicitly represent the pattern of links and connections between
elements it is simply a tool for organizing complex groups of elements in the
simulation
Simulation example
As an example of the types of modules available and the process of structuring them
into a network simulation and graphical interface we will describe the construction
of a simple biological neural simulation fig The I11pdel consists of two
neurons Each neuron contains a passive dendritic compartment an active cell body
an axonal output and a synaptic input onto the dendrite The axon of one neuron
connects to a synaptic input of the other Figure shows the basic structure of the
model as implemented under GENESIS In the model the synapse channels
Simulator and interrace toolkit
Graphics Modules
Communications
modules
Computational
Modules
A
CLinker
oDCO
Earn
Simulation
Simulator
ffi
ca
CQdK
Figure Stages In Constructing A Simulation
GENESIS A System for Simulating Neural Net~orks
network
A
neuron
neuron2
cell-body
Na
A
dendrite
axon
synapse
KEY
Element
Connection
dendrite
Link
Figure Implementation of a two neuron model in GENESIS Schematic diagram of compartmentally modeled neurons Each cell in this simple model has a passive dendritic compartment an active cell-body and an output axon There is a
synaptic input to the dendrite of one cell and two ionic channels on the cell body
Hierarchical representation of the components of the simulation as maintained in
GENESIS The cell-body of neuron is referred to as network/neuronl/cell-body
A representation of the functional links between the basic components of one
neuron Sample interface control and display widgets created using the XODUS
toolkit
Wilson Bhalla Uhley and Bower
dendritic compartments cell body and axon are each treated as separate
computational elements fig Links allow elements to share information
the Na channel needs to have access to the cell-body membrane voltage Figure
shows a portion of the script used to construct this simulation
Create different types or elements and
create
create
active compartment
create
passive_compartment
create
synapse
assign them names
neuronl
cell-body
dendrite
dendrite/synapse
Establish functional links between the elements
link
dendrite
to
cell-body
link
dendrite/synapse
to
dendrite
Set parameters associated with the elements
set
dendrit
capacitance
Make copies or entire element subtrees
copy
neuronl
to
neuron2
Establish connections between two elements
connect neuronl/axon
to
neuron2/dendrite/synapse
Set up a graph to monitor an element variable
graph
neuronl/cell-body
potential
Make a control panel with several control widgets
xform
control
xdialo nstep set-nstep default
xdialog dt
set-dt
default
Xloggle Euler set-euler
Figure Sample script commands for constructing a simulation fig
SIMULATOR SPECIFICATIONS
Memory requirements or GENESIS
Currently GENESIS consists of about lines of simulator code and a similar
amount of graphics code all written in C. The executable binaries take up about
Megabytes A rough estimate of the amount of additional memory necessary for a
particular simulation can be calculated from the sizes and number of modules used
in a simulation Typically elements use around bytes connections and
messages Widgets use Kbytes each
GENESIS A System for Simulating Neural Networks
Performance
The overall efficiency of the GENESIS system is highly simulation specific To
consider briefly a specific case the most sophisticated biologically based simulation
currently implemented under GENESIS is a model of piriform olfactory cortex
Wilson Wilson and Bower Wilson and Bower This
simulation consists of neurons of four different types Each neuron contains from
one to five compartments Each compartment can contain several channels On a
SUN with Mbytes of RAM. this simulation with cells runs at I second
per time step
Other models that have been implemented under GENESIS
The list of projects currently completed under GENESIS includes approximately ten
different simulations These include models of the olfactory bulb Bhalla
the inferior olive Lee and Bower and a motor circuit in the
invertebrate sea slug Tritonia Ryckebusch We have also built several
tutorials to allow students to explore compartmental biological models Hodgkin
and Huxley and Hopfield networks Hopfield
Access/use of GENESIS
GENESIS and XODUS will be made available at the cost of distribution to all
interested users As described above new user-defined modules can be linked into
the simulator to extend the system Users are encouraged to support the continuing
development of this system by sending modules they develop to Caltech These
will be reviewed and compiled into the overall system by GENESIS support staff
We would also hope that users would send completed published simulations to the
GENESIS data base This will provide others with an opportunity to observe the
behavior of a simulation first hand A current listing of modules and full
simulations will be maintained and available through an electronic mail newsgroup
Babel Enquiries about the system should be sent to GENESIS@caltech.edu or
GENESIS@caltech.biblet
Acknowledgments
We would like to thank Mark Nelson for his invaluable assistance in the
development of this system and specifically for his suggestions on the content of
this manuscript We would also like to recognize Dave Bilitch Wojtek Furmanski
Christof Koch innumerable Caltech students and the students of the MBL
summer course on Methods in Computational Neuroscience for their contributions
to the creation and evolution of GENESIS not mutually exclusive This research
was also supported by the NSF the NIH BNS the ONR
Contract the Lockheed Corporation the Caltech Presidents
Fund the JPL Directors Development Fund and the Joseph Drown Foundation
Wilson Bhalla Uhley and Bower

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4022-latent-variable-models-for-predicting-file-dependencies-in-large-scale-software-development.pdf

Latent Variable Models for Predicting File
Dependencies in Large-Scale Software Development
Diane J. Hu1 Laurens van der Maaten1,2 Youngmin Cho1 Lawrence K. Saul1 Sorin Lerner1
Dept of Computer Science Engineering University of California San Diego
Pattern Recognition Bioinformatics Lab Delft University of Technology
dhu,lvdmaaten,yoc002,saul,lerner}@cs.ucsd.edu
Abstract
When software developers modify one or more files in a large code base they
must also identify and update other related files Many file dependencies can be
detected by mining the development history of the code base in essence groups
of related files are revealed by the logs of previous workflows From data of this
form we show how to detect dependent files by solving a problem in binary matrix
completion We explore different latent variable models LVMs for this problem
including Bernoulli mixture models exponential family PCA restricted Boltzmann machines and fully Bayesian approaches We evaluate these models on the
development histories of three large open-source software systems Mozilla Firefox Eclipse Subversive and Gimp In all of these applications we find that LVMs
improve the performance of related file prediction over current leading methods
Introduction
As software systems grow in size and complexity they become more difficult to develop and maintain Nowadays it is not uncommon for a code base to contain source files in multiple programming
languages text documents with meta information XML documents for web interfaces and even
platform-dependent versions of the same application This complexity creates many challenges because no single developer can be an expert in all things
One such challenge arises whenever a developer wishes to update one or more files in the code
base Often seemingly localized changes will require many parts of the code base to be updated
Unfortunately these dependencies can be difficult to detect Let denote a set of starter files that
the developer wishes to modify and let denote the set of relevant files that require updating after
modifying S. In a large system where the developer cannot possibly be familiar with the entire code
base automated tools that can recommend files in given starter files in are extremely useful
A number of automated tools now make recommendations of this sort by mining the development
history of the code base Work in this area has been facilitated by code versioning systems
such as CVS or Subversion which record the development histories of large software projects In
these histories transactions denote sets of files that have been jointly modified?that is whose
changes have been submitted to the code base within a short time interval Statistical analyses of
past transactions can reveal which files depend on each other and need to be modified together
In this paper we explore the use of latent variable models LVMs for modeling the development
history of large code bases We consider a number of different models including Bernoulli mixture
models exponential family PCA restricted Boltzmann machines and fully Bayesian approaches
In these models the problem of recommending relevant files can be viewed as a problem in binary
matrix completion We present experimental results on the development histories of three large
open-source systems Mozilla Firefox Eclipse Subversive and Gimp In all of these applications
we find that LVMs outperform the current leading method for mining development histories
Related work
Two broad classes of methods are used for identifying file dependencies in large code bases one
analyzes the semantic content of the code base while the other analyzes its development history
Impact analysis
The field of impact analysis draws on tools from software engineering in order to identify the
consequences of code modifications Most approaches in this tradition attempt to identify program
dependencies by inspecting and/or running the program itself Such dependence-based techniques
include transitive traversal of the call graph as well as static and dynamic slicing
techniques These methods can identify many dependencies however they have trouble on certain difficult cases such as cross-language dependencies between a data configuration file and
the code that uses it and cross-program dependencies between the front and back ends of a
compiler These difficulties have led researchers to explore the methods we consider next
Mining of development histories
Data-driven methods identify file dependencies in large software projects by analyzing their development histories Two of the most widely recognized works in this area are by Ying and
Zimmerman Both groups use frequent itemset mining FIM a general heuristic for
identifying frequent patterns in large databases The patterns extracted from development histories
are just those sets of files that have been jointly modified at some point in the past the frequent
patterns are the patterns that have occurred at least times The parameter is called the minimum
support threshold In practice it is tuned to yield the best possible balance of precision and recall
Given a database and a minimum support threshold the resulting set of frequent patterns is uniquely
specified Much work has been devoted to making FIM as fast and efficient as possible Ying
uses a FIM algorithm called FP-growth which extracts frequent patterns by using a tree-like
data structure that is cleverly designed to prune the number of possible patterns to be searched FPgrowth is used to find all frequent patterns that contain the set of starter files the joint sets of these
frequent patterns are then returned as recommendations As a baseline in our experiments we use a
variant of FP-growth called FP-Max which outputs only maximal sets for added efficiency
Zimmerman uses the popular Apriori algorithm which uses FIM to solve a subtask to
form association rules from the development history These rules are of the form where
and are disjoint sets they indicate that if is observed then based on experience should
also be observed After identifying all rules in which starter files appear on the left hand side their
tool recommends all files that appear on the right hand side They also work with content on a finer
granularity recommending not only relevant files but also relevant code blocks within files
Both Ying and Zimmerman evaluate the data-driven approach by its f-measure as
measured against ground-truth recommendations For Ying these ground-truth recommendations are the files committed for a completed modification task as recorded in that project?s
Bugzilla For Zimmerman the ground-truth recommendations are the files checked-in
together at some point in the past as revealed by the development history
Other researchers have also used the development history to detect file dependencies but in
markedly different ways Shirabad formulate the problem as one of binary classification they label pairs of source files as relevant or non-relevant based on their joint modification
histories Robillard analyzes the topology of structural dependencies between files at the codeblock level Kagdi al improve on the accuracy of existing file recommendation methods
by considering asymmetric file dependencies this information is also used to return a partial ordering over recommended files Finally Sherriff identify clusters of dependent files by
performing singular value decomposition on the development history
Latent variable modeling of development histories
We examine four latent variable models of file dependence in software systems All these models
represent the development history as an large binary matrix where non-zero elements in
the same row indicate files that were checked-in together or jointly modified at some point in time
To detect dependent files we infer the values of missing elements in this matrix from the values of
known elements The inferences are made from the probability distributions defined by each model
We use the following notation for all models
The file list fD is an ordered collection of all files referenced in a static
version of the development history
A transaction is a set of files that were modified together according to the development history We represent each transaction by a D-dimensional binary vector xD
where if the fi is a member of the transaction and otherwise
A development history is a set of transaction vectors xN We assume
them to be independently and identically sampled from some underlying joint distribution
A starter set is a set of starter files fis that the developer wishes to modify
A recommendation set is a set of recommended files fjr that we label as
relevant to the starter set S.
Bernoulli mixture model
The simplest model that we explore is a Bernoulli mixture model Figure shows
the BMM?s graphical model in plate notation In training the observed variables are the binary elements of each transaction vector The hidden variable is a multinomial label that can be viewed as assigning each transaction vector to one of clusters
The joint distribution of the BMM is given by
p(xi
xizi iz
As implied by the graph in we model the different elements of as conditionally independent given the label Here the parameter denotes the prior probability of the
latent variable while the parameter iz p(xi denotes the conditional mean of the
observed variable
We use the EM algorithm to estimate parameters that maximize the likelihood
p(xn of the transactions in the development history
When a software developer wishes to modify a set of starter files she can query a trained BMM
to identify a set of relevant files Let xis denote the elements of the transaction
vector indicating the files in the starter set S. Let denote the remaining elements of the
transaction vector indicating files that may or may not be relevant In BMMs we infer which
files are relevant by computing the posterior probability p(r|s Using Bayes rule and
conditional independence this posterior probability is given up to a constant factor by
p(r|s
p(s
The most likely set of relevant files according to the model is given by the completed transaction
that maximizes the right hand side of eq Unfortunately while we can efficiently compute the
posterior probability p(r|s for a particular set of recommended files it is not straightforward to
maximize eq over all possible ways to complete the transaction As an approximation we
sort the possibly relevant files by their individual posterior probabilities p(xi for fi
S.
Then we recommend all files whose posterior probabilities p(xi exceed some threshold
we optimize the threshold on a held-out set of training examples
Bayesian Bernoulli mixture model
We also explore a Bayesian treatment of the BMM. In a Bayesian Bernoulli mixture instead
of learning point estimates of the parameters we introduce a prior distribution and
make predictions by averaging over the posterior distribution The generative model for
the BBM is shown graphically in Figure
BMM.
BBM.
RBM.
Logistic PCA.
Figure Graphical model of the Bernoulli mixture model the Bayesian Bernoulli mixture
the restricted Boltzmann machine and logistic PCA.
In our BBMs the mixture weight parameters are drawn from a Dirichlet prior1
Dirichlet
where indicates as before the number of mixture components and is a hyperparameter of the
Dirichlet prior the so-called concentration parameter2 Likewise the parameters of the Bernoulli
distributions are drawn from Beta priors
Beta(?j
where is a D-dimensional vector and and are hyperparameters of the Beta prior
As exact inference in BBMs is intractable we resort to collapsed Gibbs sampling and make predictions by averaging over samples from the posterior In particular we integrate out the Bernoulli
parameters and the cluster distribution parameters and we sample the cluster assignment variables For Gibbs sampling we must compute the conditional probability p(zn j|z?n that
the nth transaction is assigned to cluster given the training data and all other cluster assignments z?n This probability is given by
N?nj N?nij xni N?nj N?nij
p(zn j|z?n
N?nj
where N?nj counts the number of transactions assigned to cluster excluding the nth transaction
and N?nij counts the number of times that the ith file belongs to one of these N?nj transactions
After each full Gibbs sweep we obtain a sample and corresponding counts Nj of the number
of points assigned to cluster which can be used to infer the Bernoulli parameters We use
of these samples to estimate the probability that a file needs to be changed given files in the
starter set S. In particular averaging predictions over the Gibbs samples we estimate
with
p(xi
Nj
n:zn
Restricted Boltzmann Machines
A restricted Boltzmann machine RBM is a Markov random field MRF whose nodes are typically binary random variables The graphical model of an RBM is a fully connected bipartite
In preliminary experiments we also investigated an infinite mixture of Bernoulli distributions that replaces
the Dirichlet prior by a Dirichlet process However we did not find the infinite mixture model to outperform its finite counterpart so we do not discuss it further
For simplicity we assume a symmetric Dirichlet prior we assume
graph with observed variables in one layer and latent variables yj in the other see
Due to the bipartite structure the latent variables are conditionally independent given the observed
variables and vice versa For the RBMs in this paper we model the joint distribution as
exp Wy
where stores the weight matrix between layers and store respectively the biases on observed and hidden nodes and is a normalization factor that depends on the model?s parameters
The product form of RBMs can model much sharper distributions over the observed variables than
mixture models making them an interesting alternative to consider for our application
RBMs are trained by maximum likelihood estimation Exact inference in RBMs is intractable due to
the exponential sum in the normalization factor Z. However the conditional distributions required
for Gibbs sampling have a particularly simple form
p(xi
Wij yj
cj
p(yj
Wij
bi
where e?z is the sigmoid function The obtained Gibbs samples can be used to
approximate the gradient of the likelihood function with respect to the model parameters see
for further discussion of sampling strategies3
To determine whether a file fi is relevant given starter files in we can either clamp the observed variables representing starter files and perform Gibbs sampling on the rest or compute
the posterior over the remaining files using a fast factorized approximation In preliminary
experiments we found the latter to work best Hence we recommend files by computing
p(xi exp(bi
Wj Wi
exp
j:fj
then thresholding these probabilities on some value determined on held-out examples
Logistic PCA
Logistic PCA is a method for dimensionality reduction of binary data see for its graphical model Logistic PCA belongs to a family of algorithms known as exponential family PCA
these algorithms generalize PCA to data modeled by non-Gaussian distributions of the exponential
family To use logistic PCA we stack the transaction vectors of the
development history into a binary matrix Then modeling each element of this matrix as
a Bernoulli random variable we attempt to find a low-rank factorization of the real-valued
matrix whose elements are the log-odds parameters of these random variables
The low-rank factorization in logistic PCA is computed by maximizing the log-likelihood of the
observed data In terms of the log-odds matrix this log-likelihood is given by
LX
Xnd log Xnd log
nd
We obtain a low dimensional representation of the data by factoring the log-odds matrix
as the product of two smaller matrices and Specifically we have
nd
Un V`d
Note that the reduced rank plays a role analogous to the number of clusters in BMMs
After obtaining a low-rank factorization of the log-odds matrix UV we can use it to recommend relevant files from starter files fi2 fis To recommend relevant files we
compute the vector that optimizes the regularized log-loss
LS
log u?vij kuk2
We use the approach in known as contrastive divergence with Gibbs sweeps
Time Period
Support
Mozilla Firefox
March Nov
Eclipse Subversive
Dec May
Train
Train
Test
Files
Test
92
79
59
Files
61
38
Gimp
Nov May
Train
Test
Files
Table Datasets statistics showing the time period from which transactions were extracted and
the number of transactions and unique files in the training and test sets for a single starter file
where in the first term denotes the th column of the matrix and in the second term is a
regularization parameter The vector obtained in this way is the low dimensional representation
of the transaction with starter files in S. To determine whether file fi is relevant we compute the
probability p(xi vi and recommend the file if this probability exceeds some
threshold We tune the threshold on held-out transactions from the development history
Experiments
We evaluated our models on three datasets4 constructed from check-in records of Mozilla Firefox
Eclipse Subversive and Gimp These open-source projects use software configuration management
SCM tools which provide logs that allow us to extract binary vectors indicating which files were
changed during a transaction Our experimental setup and results are described below
Experimental setup
We preprocess the raw data obtained from SCM?s check-in records in two steps First following Ying al we eliminate all transactions consisting of more than files as these usually do
not correspond to meaningful changes Second we simulate the minimum support threshold
Section by removing all files in the code base that occur very infrequently This pruning allows
us to make a fair comparison with latent variable models LVMs
After pre-processing the dataset is chronologically ordered the first two-thirds is used as training
data and the last one-third as testing data For each transaction in the test set we formed a query
and label set by randomly picking a set of changed files as starter files The remaining files that
were changed in the transaction form the label set which is the set of files our models must predict
Following we only include transactions for which the label set is non-empty in the train data
Table shows the number of transactions for training and test set as well as the total number of
unique files that appear in these transactions
We trained the LVMs as follows The Bernoulli mixture models BMMs were trained by
or fewer iterations of the EM algorithm For the Bayesian mixtures BBMs we ran separate
Markov chains and made predictions after full Gibbs sweeps5 The RBMs were trained for
iterations of contrastive divergence starting with and gradually increasing the number
of Gibbs sweeps to The parameters and of logistic PCA were learned using an
alternating least squares procedure that converges to a local maximum of the log-likelihood
We initialized the matrices and from an SVD of the matrix
The parameters of the LVMs number of hidden components in the BMM and RBM as well
as the number of dimensions and the regularization parameter in logistic PCA were selected
based on the performance on a small held-out validation set The hyperparameters of the Bayesian
Bernoulli mixtures were set based on prior knowledge from the domain the Beta-prior parameters
and were set to and respectively to reflect our prior knowledge that most files are not
changed in a transaction The concentration parameter was set to to reflect our prior knowledge
that file dependencies typically form a large number of small clusters
These binary datasets publicly available at http://cseweb.ucsd.edu/?dhu/research/msr
In preliminary experiments we found Gibbs sweeps to be sufficient for the Markov chain to mix
Model Support
FIM
BMM
BBM
RBM
LPCA
Mozilla Firefox
Start
Start
Eclipse Subversive
Start
Start
Gimp
Start
Start
Table Performance of FIM and LVMs on three datasets for queries with or starter files Each
shaded column presents the measure and each white column presents the correct prediction ratio
Results
Our experiments evaluated the performance of each LVM as well as a highly efficient implementation of FIM called FP-Max Several experiments were run on different values of starter files
abbreviated Start and minimum support thresholds abbreviated Support Table shows the
comparison of each model in terms of the measure the harmonic mean of the precision and recall and the correct prediction ratio or CPR the fraction of files we predict correctly assuming
that the number of files to be predicted is given The latter measure reflects how well our models
identify relevant files for a particular starter file without the added complication of thresholding
Experiments that achieve the highest result for each of the two measures are boldfaced
From our results we see that most LVMs outperform the popular FIM approach In particular the
BBMs outperform all other approaches on two of the three datasets with a high of CPR in
Eclipse Subversive This means that an average of of all dependent files are detected as relevant
by the BBM. We also observe that measure generally decreases with the addition of starter files
since the average size of transactions is relatively small around four files for Firefox adding
starter files must make predictions less obvious in the case that the total number of relevant files is
not given to us Increasing support on the other hand seems to effectively remove noise caused by
infrequent files Finally we see that recommendations are most accurate on Eclipse Subversive the
smallest dataset We believe this is because a smaller test set does not require a model to predict as
far into the future as a larger one Thus our results suggest that an online learning algorithm may
further increase accuracy
Discussion
The use of LVMs has significant advantages over traditional approaches to impact analysis
Section namely its ability to find dependent files written in different languages To show this we
present the three clusters with the highest weights as discovered by a BMM in the Firefox data in
Table The table reveals that the clusters correspond to interpretable structure in the code that span
multiple data formats and languages The first cluster deals with the JIT compiler for JavaScript
while the second and third deal with the CSS style sheet manager and web browser properties The
dependencies in the last two clusters would have been missed by conventional impact analysis
Cluster
js/src/jscntxt.h
js/src/jstracer.cpp
js/src/nanojit/Assembler.cpp
js/src/jsregexp.cpp
js/src/jsapi.cpp
js/src/jsarray.cpp
js/src/jsfun.cpp
js/src/jsinterp.cpp
js/src/jsnum.cpp
js/src/jsobj.cpp
Cluster
view/src/nsViewManager.cpp
layout/generic/nsHTMLReflowState.cpp
layout/reftests/bugs/reftest.list
layout/style/nsCSSRuleProcessor.cpp
layout/style/nsCSSStyleSheet.cpp
layout/style/nsCSSParser.cpp
layout/base/crashtests/crashtests.list
layout/base/nsBidiPresUtils.cpp
layout/base/nsPresShell.cpp
content/xbl/src/nsBindingManager.cpp
Cluster
browser/base/content/browser-context.inc
browser/base/content/browser.js
browser/base/content/pageinfo/pageInfo.xul
browser/locales/en-US/chrome/browser/browser.dtd
toolkit/mozapps/update/src/nsUpdateService.js.in
toolkit/mozapps/update/src/updater/updater.cpp
modules/plugin/base/src/nsNPAPIPluginInstance.h
modules/plugin/base/src/nsPluginHost.cpp
browser/locales/en-US/chrome/browser/browser.properties
view/src/nsViewManager.cpp
Table Three of the clusters from Firefox identified by the BMM. We show the clusters with
the largest mixing proportion Within each cluster the files with highest membership probabilities are shown note how these files span multiple data formats and program languages revealing
dependencies that would escape the notice of traditional methods
LVMs also have important advantages over FIM. Given a set of starter files FIM simply looks at
co-occurrence data it recommends a set of files for which the number of transactions that contain
both and is frequent By contrast LVMs can exploit higher-order information by discovering
the underlying structure of the data Our results suggest that the ability to leverage such structure
leads to better predictions Admittedly in terms of computation LVMs have a larger one-time
training cost than the FIM as we must first train the model or generate and store the Gibbs samples
However for a single query the time required to compute recommendations is comparable to that
of the FP-Max algorithm we used for FIM.
The results from the previous section also revealed significant differences between the LVMs we
considered In the majority of our experiments mixture models with many mixture components
appear to outperform RBMs and logistic PCA. This result suggests that our dataset consists of a large
number of transactions with a number of small highly interrelated files Modeling such data with a
product of experts such as an RBM is difficult as each individual expert has the ability to veto a
prediction We tried to resolve this problem by using a sparsity prior on the states of the hidden units
to make the RBMs behave more like a mixture model but in preliminary experiments we
did not find this to improve the performance Another interesting observation is that the Bayesian
treatment of the Bernoulli mixture model generally leads to better predictions than a maximum
likelihood approach as it is less susceptible to overfitting This advantage is particularly useful in
file dependency prediction which requires models with a large number of mixture components to
appropriately model data that consists of many small distinct clusters while having few training
instances transactions
Conclusion
In this paper we have described a new application of binary matrix completion for predicting file
dependencies in software projects For this application we investigated the performance of four
different LVMs and compared our results to that of the widely used of FIM. Our results indicate that
LVMs can significantly outperform FIM by exploiting latent higher-order structure in the data
Admittedly our present study is still limited in scope and it is very likely that our results can be
further improved For instance results from the Netflix competition have shown that blending the
predictions from various models often leads to better performance The raw transactions also
contain additional information that could be harvested to make more accurate predictions Such
information includes the identity of users who committed transactions to the code base as well as
the text of actual changes to the source code It remains a grand challenge to incorporate all the
available information from development histories into a probabilistic model for predicting which
files need to be modified In future work we aim to explore discriminative methods for parameter
estimation as well as online algorithms for tracking non-stationary trends in the code base
Acknowledgments
LvdM acknowledges support by the Netherlands Organisation for Scientific Research grant no
and by EU-FP7 NoE on Social Signal Processing SSPNet

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2131-dynamic-time-alignment-kernel-in-support-vector-machine.pdf

Dynamic Time-Alignment Kernel in
Support Vector Machine
Hiroshi Shimodaira
School of Information Science
Japan Advanced Institute of
Science and Technology
sim@jaist.ac.jp
Mitsuru Nakai
School of Information Science
Japan Advanced Institute of
Science and Technology
mit@jaist.ac.jp
Ken-ichi Noma
School of Information Science
Japan Advanced Institute of
Science and Technology
knoma@jaist.ac.jp
Shigeki Sagayama
Graduate School of Information Science
and Technology
The University of Tokyo
sagayama@hil.t.u-tokyo.ac.jp
Abstract
A new class of Support Vector Machine SVM that is applicable to sequential-pattern recognition such as speech recognition is
developed by incorporating an idea of non-linear time alignment
into the kernel function Since the time-alignment operation of
sequential pattern is embedded in the new kernel function standard SVM training and classification algorithms can be employed
without further modifications The proposed SVM DTAK-SVM
is evaluated in speaker-dependent speech recognition experiments
of hand-segmented phoneme recognition Preliminary experimental results show comparable recognition performance with hidden
Markov models HMMs
Introduction
Support Vector Machine SVM is one of the latest and most successful statistical
pattern classifier that utilizes a kernel technique The basic form of SVM
classifier which classifies an input vector Rn is expressed as
K(xi
where is a non-linear mapping function Rn n0 denotes
the inner product operator and are the i-th training sample its class label
and its Lagrange multiplier respectively is a kernel function and is a bias
Despite the successful applications of SVM in the field of pattern recognition such
as character recognition and text classification SVM has not been applied to speech
recognition that much This is because SVM assumes that each sample is a vector
of fixed dimension and hence it can not deal with the variable length sequences
directly Because of this most of the efforts that have been made so far to apply
SVM to speech recognition employ linear time normalization where input feature
vector sequences with different lengths are aligned to same length A variant
of this approach is a hybrid of SVM and HMM hidden Markov model in which
HMM works as a pre-processor to feed time-aligned fixed-dimensional vectors to
SVM Another approach is to utilize probabilistic generative models as a SVM
kernel function This includes the Fisher kernels and conditional symmetric
independence CSI kernels both of which employ HMMs as the generative models Since HMMs can treat sequential patterns SVM that employs the generative
models based on HMMs can handle sequential patterns as well
In contrast to those approaches our approach is a direct extension of the original
SVM to the case of variable length sequence The idea is to incorporate the operation of dynamic time alignment into the kernel function itself Because of this
the proposed new SVM is called Dynamic Time-Alignment Kernel SVM DTAKSVM Unlike the SVM with Fisher kernel that requires two training stages with
different training criteria one is for training the generative models and the second
is for training the SVM the DTAK-SVM uses one training criterion as well as the
original SVM.
Dynamic Time-Alignment Kernel
We consider a sequence of vectors where Rn is the
length of the sequence and the notation is sometimes used to represent the
length of the sequence instead For simplification we at first assume the so-called
linear SVM that does not employ non-linear mapping function In such case the
kernel operation in is identical to the inner product operation
Formulation for linear kernel
Assume that we have two vector sequences and If these two patterns are
equal in length then the inner product between and can
be obtained easily as a summation of each inner product between xk and for
xk
and therefore an SVM classifier can be defined as given in On the other hand
in case where the two sequences are different in length the inner product can not
be calculated directly Even in such case however some sort of inner product like
operation can be defined if we align the lengths of the patterns To that end let
be the time-warping functions of normalized time frame for the pattern
and respectively and let be the new inner product operator instead of
the original inner product Then the new inner product between the two vector
sequences and can be given by
1X
where is a normalized length that can be either or arbitrary positive
integer
There would be two possible types of time-warping functions One is a linear timewarping function and the other is a non-linear time-warping function The linear
time-warping function takes the form as
where dxe is the ceiling function which gives the smallest integer that is greater than
or equal to As it can be seen from the definition given above the linear warping
function is not suitable for continuous speech recognition frame-synchronous
processing because the sequence lengths and should be known beforehand
On the other hand non-linear time warping or dynamic time warping DTW in
other word enables frame-synchronous processing Furthermore the past research
on speech recognition has shown that the recognition performance by the non-linear
time normalization outperforms the one by the linear time normalization Because
of these reasons we focus on the non-linear time warping based on DTW.
Though the original DTW uses a distance/distortion measure and finds the optimal path that minimizes the accumulated distance/distortion the DTW that is
employed for SVM uses inner product or kernel function instead and finds the optimal path that maximizes the accumulated similarity
subject to
max
where is a nonnegative path weighting coefficient is a path normalizing factor and is a constant constraining the local continuity In the standard
PL
DTW the normalizing factor is given as and the weighting coefficients are chosen so that is independent of the warping functions
The above optimization problem can be solved efficiently by dynamic programming
The recursive formula in the dynamic programming employed in the present study
is as follows
G(i Inp(i
max G(i Inp(i
Inp(i
where Inp(i is the standard inner product between the two vectors corresponding
to point and As a result we have
Formulation for non-linear kernel
In the last subsection a linear kernel the inner product for two vector sequences with different lengths has been formulated in the framework of dynamic
time-warping With a little constraint similar formulation is possible for the case
where SVM?s non-linear mapping function is applied to the vector sequences To
that end is restricted to the one having the following form
where is a non-linear mapping function that is applied to each frame vector
as given in It should be noted that under the above restriction preserves the
original length of sequence at the cost of losing long-term correlations such as the
one between and xL As a result a new class of kernel can be defined by using
the extended inner product introduced in the previous section
Ks
max
max
We call this new kernel dynamic time-alignment kernel
Properties of the dynamic time-alignment kernel
It has not been proven that the proposed function Ks is really an SVM?s admissible kernel which guarantees the existence of a feature space This is because that
the mapping function to a feature space is not independent but dependent on the
given vector sequences Although a class of data-dependent asymmetric kernel for
SVM has been developed in our proposed function is more complicated and
difficult to analyze because the input data is a vector sequence with variable length
and non-linear time normalization is embedded in the function Instead what have
been known about the proposed function so far are Ks is symmetric Ks
satisfies the Cauchy-Schwartz like inequality described bellow
Proposition
Ks Ks X)Ks
Proof For simplification we assume that normalized length is fixed and omit
and in Using the standard Cauchy-Schwartz inequality the following inequality holds
Ks
max
kk
where represent the optimal warping functions that maximize the RHS
of On the other hand
Ks
max
Because here we assume that are the optimal warping functions that
maximize for any warping functions including the following inequality
holds
Ks
In the same manner the following holds
Ks
Therefore
Ks X)Ks Ks
kk
kk kk
DTAK-SVM
Using the dynamic time-alignment kernel DTAK introduced in the previous section the discriminant function of SVM for a sequential pattern is expressed as
Ks
where represents the i-th training pattern As it can be seen from these
expressions the SVM discriminant function for time sequence has the same form
with the original SVM except for the difference in kernels It is straightforward to
deduce the learning problem which is given as
min
subject to
N.
Again since the formulation of learning problem defined above is almost the same
with that for the original SVM same training algorithms for the original SVM can
be used to solve the problem
Experiments
Speech recognition experiments were carried out to evaluated the classification performance of DTAK-SVM As our objective is to evaluate the basic performance
of the proposed method very limited task hand-segmented phoneme recognition
task in which positions of target patterns in the utterance are known was chosen
Continuous speech recognition task that does not require phoneme labeling would
be our next step
Experimental conditions
The details of the experimental conditions are given in Table The training
and evaluation samples were collected from the ATR speech database A-set
Table Experimental conditions
Speaker dependency
Phoneme classes
Speakers
Training samples
Evaluation samples
Signal sampling
Feature values
Kernel type
Experiment-1
Experiment-2
dependent
dependent
voiced consonants
vowels
males
males and females
samples per phoneme
samples per phoneme
samples in all per samples in all per
speaker
speaker
frame-shift
13-MFCCs and 13-?MFCCs
kx
RBF radial basis function K(xi exp
95
90
SVs training samples
Correct classification rate
85
75
70
65
55
RBF-sigma
Recognition performance
RBF-sigma
Number of SVs
Figure Experimental results for Experiment-1 voiced-consonants recognition
showing correct classification rate and the number of SVs as a function of
the parameter of RBF kernel
Japanese words in vocabulary In consonant-recognition task Experiment-1 only
six voiced-consonants were used to save time The classification task
of those phonemes without using contextual information is considered as a relatively difficult task whereas the classification of vowels Experiment-2
is considered as an easier task
To apply SVM that is basically formulated as a two-class classifier to the multiclass problem one against the others type of strategy was chosen The proposed
DTAK-SVM has been implemented with the publicly available toolkit SVMTorch
Experimental results
depicts the experimental results for Experiment-1 where average values over
speakers are shown It can be seen in that the best performance of
was achieved at and Similar results were obtained for Experiment-2
as given in
95
SVs training samples
Correct classification rate
90
85
75
70
65
55
RBF-sigma
RBF-sigma
Recognition performance
Number of SVs
Figure Experimental results for Experiment-2 vowels recognition showing
correct classification rate and the number of SVs as a function of the
parameter of RBF kernel
Table Recognition performance comparison of DTAK-SVM with HMM. Results
of Experiment-1 for male and female speakers are shown numbers represent
correct classification rate
Model
HMM mix
HMM mix
HMM mix
HMM mix
DTAK-SVM
training samples/phoneme
male
female
Next the classification performance of DTAK-SVM was compared with that of the
state-of-the-art HMM. In order to see the effect of generalization performance on
the size of training data set and model complexity experiments were carried out
by varying the number of training samples and mixtures
for each state of HMM. The HMM used in this experiment was a 3-states continuous density Gaussian-distribution mixtures with diagonal covariances contextindependent model HTK was employed for this purpose The parameters of
DTAK-SVM were fixed to The results for Experiment-1 with
respect to male and female speakers are given in Table
It can be said from the experimental results that DTAK-SVM shows better classification performance when the number of training samples is while comparable
performance when the number of samples is One might argue that the number
of training samples used in this experiment is not enough at all for HMM to achieve
best performance But such shortage of training samples occurs often in HMMbased real-world speech recognition especially when context-dependent models are
employed which prevents HMM from improving the generalization performance
Conclusions
A novel approach to extend the SVM framework for the sequential-pattern classification problem has been proposed by embedding a dynamic time-alignment operation
into the kernel Though long-term correlations between the feature vectors are omitted at the cost of achieving frame-synchronous processing for speech recognition the
proposed DTAK-SVMs demonstrated comparable performance in hand-segmented
phoneme recognition with HMMs The DTAK-SVM is potentially applicable to
continuous speech recognition with some extension of One-pass search algorithm

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2434-semi-definite-programming-by-perceptron-learning.pdf

Semidefinite Programming
by Perceptron Learning
Ralf Herbrich
Thore Graepel
Microsoft Research Ltd Cambridge UK
thoreg,rherb}@microsoft.com
Andriy Kharechko
John Shawe-Taylor
Royal Holloway University of London UK
ak03r,jst}@ecs.soton.ac.uk
Abstract
We present a modified version of the perceptron learning algorithm
PLA which solves semidefinite programs SDPs in polynomial
time The algorithm is based on the following three observations
Semidefinite programs are linear programs with infinitely many
linear constraints every linear program can be solved by a
sequence of constraint satisfaction problems with linear constraints
iii in general the perceptron learning algorithm solves a constraint
satisfaction problem with linear constraints in finitely many updates
Combining the PLA with a probabilistic rescaling algorithm which
on average increases the size of the feasable region results in a probabilistic algorithm for solving SDPs that runs in polynomial time
We present preliminary results which demonstrate that the algorithm works but is not competitive with state-of-the-art interior
point methods
Introduction
Semidefinite programming SDP is one of the most active research areas in optimisation Its appeal derives from important applications in combinatorial optimisation
and control theory from the recent development of efficient algorithms for solving
SDP problems and the depth and elegance of the underlying optimisation theory
which covers linear quadratic and second-order cone programming as special cases
Recently semidefinite programming has been discovered as a useful toolkit in machine
learning with applications ranging from pattern separation via ellipsoids to kernel
matrix optimisation and transformation invariant learning
Methods for solving SDPs have mostly been developed in an analogy to linear programming Generalised simplex-like algorithms were developed for SDPs but to
the best of our knowledge are currently merely of theoretical interest The ellipsoid
method works by searching for a feasible point via repeatedly halving an ellipsoid
that encloses the affine space of constraint matrices such that the centre of the ellipsoid is a feasible point However this method shows poor performance in practice
as the running time usually attains its worst-case bound A third set of methods
for solving SDPs are interior point methods These methods minimise a linear
function on convex sets provided the sets are endowed with self-concordant barrier
functions Since such a barrier function is known for SDPs interior point methods
are currently the most efficient method for solving SDPs in practice
Considering the great generality of semidefinite programming and the complexity of
state-of-the-art solution methods it is quite surprising that the forty year old simple
perceptron learning algorithm can be modified so as to solve SDPs In this
paper we present a combination of the perceptron learning algorithm PLA with a
rescaling algorithm originally developed for LPs that is able to solve semidefinite
programs in polynomial time We start with a short introduction into semidefinite
programming and the perceptron learning algorithm in Section In Section we
present our main algorithm together with some performance guarantees whose proofs
we only sketch due to space restrictions While our numerical results presented in
Section are very preliminary they do give insights into the workings of the algorithm
and demonstrate that machine learning may have something to offer to the field of
convex optimisation
For the rest of the paper we denote matrices and vectors by bold face upper and
lower case letters A and We shall use kxk to denote the unit length
vector in the direction of The notation A is used to denote Ax for all
that is A is positive semidefinite
Learning and Convex Optimisation
Semidefinite Programming
In semidefinite programming a linear objective function is minimised over the image
of an affine transformation of the cone of semidefinite matrices expressed by linear
matrix inequalities
minimise
x?R
c0
subject to
F0
Fi
where Rn and Fi Rm?m for all The following proposition shows
that semidefinite programs are a direct generalisation of linear programs
Proposition Every semidefinite program is a linear program with infinitely many
linear constraints
Proof Obviously the objective function in is linear in For any Rm define
the vector au F1 u0 Fn Then the constraints in can be written as
Rm
u0
Rm
This is a linear constraint in for all
au F0
of which there are infinitely many
Since the objective function is linear in we can solve an SDP by a sequence of
semidefinite constraint satisfaction problems CSPs introducing the additional constraint c0 c0 and varying c0 R. Moreover we have the following proposition
Proposition Any SDP can be solved by a sequence of homogenised semidefinite
CSPs of the following form
find
subject to
Gi
Algorithm Perceptron Learning Algorithm
Require A possibly infinite set A of vectors a Rn
Set and
while there exists a A such that x0t a do
a
end while
return
Proof In order to make F0 and c0 dependent on the optimisation variables we
introduce an auxiliary variable the solution to the original problem is given
by
Moreover we can repose the two linear constraints c0 and
as an LMI using the fact that a block-diagonal matrix is positive semi)definite
if and only if every block is positive semi)definite Thus the following matrices are
sufficient
F0
Fi
ci
G0 c0
Gi
Given an upper and a lower bound on the objective function repeated bisection can
be used to determine the solution in O(log steps to accuracy
In order to simplify notation we will assume that and whenever
we speak about a semidefinite CSP for an SDP in variables with Fi Rm?m
Perceptron Learning Algorithm
The perceptron learning algorithm PLA is an online procedure which finds a
linear separation of a set of points from the origin Algorithm In machine
learning this algorithm is usually applied to two sets and of points labelled
and by multiplying every data vector by its class label1 the resulting vector
often referred to as the weight vector in perceptron learning is then read as the
normal of a hyperplane which separates the sets and
A remarkable property of the perceptron learning algorithm is that the total number
of updates is independent of the cardinality of A but can be upper bounded simply
in terms of the following quantity
maxn maxn min a0
x?R
x?R
a?A
This quantity is known as the normalised margin of A in the machine learning
community or as the radius of the feasible region in the optimisation community
It quantifies the radius of the largest ball that can be fitted in the convex region
enclosed by all a A the so-called feasible set Then the perceptron convergence
theorem states that
For the purpose of this paper we observe that Algorithm solves a linear CSP where
the linear constraints are given by the vectors a A. Moreover by the last argument
we have the following proposition
Proposition If the feasible set has a positive radius then the perceptron learning
algorithm solves a linear CSP in finitely many steps
It is worth mentioning that in the last few decades a series of modified PLAs A
have been developed for a good overview which mainly aim at guaranteeing
Note that sometimes the update equation is given using the unnormalised vector a
Algorithm Rescaling algorithm
Require A maximal number of steps and a parameter
Set uniformly at random in kzk
for do
Pun smallest EV of
Find au such that
if does not exists then
Set Gi Gi return
end if
au au
end for
return unsolved
not only feasibility of the solution but also a lower bound on These
guarantees usually come at the price of a slightly larger mistake bound which we
shall denote by that is
Semidefinite Programming by Perceptron Learning
If we combine Propositions and together with Equation we obtain a perceptron algorithm that sequentially solves SDPs However there remain two problems
How do we find a vector a A such that a
How can we make the running time of this algorithm polynomial in the
description length of the data?2
In order to address the first problem we notice that A in Algorithm is not explicitly
given but is defined by virtue of
A Gn au G1 u0 Gn Rm
Hence finding a vector au A such that au is equivalent to identifying a
vector Rm such that
u0 Gi u0
One possible way of finding such a vector and consequently au for the current
solution in Algorithm is to calculate the eigenvector corresponding to the smallest
eigenvalue of if this eigenvalue is positive the algorithm stops and outputs
Note however that computationally easier procedures can be applied to find a
suitable Rm also Section
The second problem requires us to improve the dependency of the runtime from
to To this end we employ a probabilistic rescaling algorithm
Algorithm which was originally developed for LPs The purpose of this algorithm is to enlarge the feasible region terms of A Gn by a constant
factor on average which would imply a decrease in the number of updates of the
perceptron algorithm exponential in the number of calls to this rescaling algorithm
This is achieved by running Algorithm If the algorithm does not return unsolved
the rescaling procedure on the Gi has the effect that au changes into au au
for every Rm In order to be able to reconstruct the solution to the original
problem whenever we rescale the Gi we need to remember the vector used for
rescaling In Figure we have shown the effect of rescaling for three linear con2
Note that polynomial runtime is only guaranteed if A Gn is bounded by
a polynomial function of the description length of the data
Figure Illustration of the rescaling procedure Shown is the feasible region and
one feasible point before left and after left rescaling with the feasible point
straints in R3 The main idea of Algorithm is to find a vector that is close to
the current feasible region and hence leads to an increase in its radius when used for
rescaling The following property holds for Algorithm
Theorem Assume Algorithm did not return unsolved Let
be the
radius of the feasible set before rescaling and be the radius of the feasible set after
rescaling and assume that 4n
Then
with probability at most 34
4n
with probability at least
The probabilistic nature of the theorem stems from the fact that the rescaling can
only be shown to increase the size of the feasible region if the random initial value
already points sufficiently closely to the feasible region A consequence of this theorem is that on average the radius increases by Algorithm
combines rescaling and perceptron learning which results in a probabilistic polynomial runtime algorithm3 which alternates between calls to Algorithm and This
algorithm may return infeasible in two cases either Ti many calls to Algorithm
have returned unsolved or many calls of Algorithm together with rescaling have
not returned a solution Each of these two conditions can either happen because of
an unlucky draw of in Algorithm or because A Gn is too small
Following the argument in one can show that for min the total
probability of returning infeasible despite A Gn min cannot exceed
exp
Experimental Results
The experiments reported in this section fall into two parts Our initial aim was
to demonstrate that the method works in practice and to assess its efficacy on a
Note that we assume that the optimisation problem in line of Algorithm can be
solved in polynomial time with algorithms such as Newton-Raphson
Algorithm Positive Definite Perceptron Algorithm
Require G1 Gn Rm?m and maximal number of iteration
Set In
for do
Call Algorithm for at most A 4n
many updates
if Algorithm converged then return Bx
ln(?i
Set and Ti
34
for Ti do
Call Algorithm with and
if Algorithm returns then In yy goto the outer for-loop
end for
return infeasible
end for
return infeasible
benchmark example from graph bisection
These experiments would also indicate how competitive the baseline method is when
compared to other solvers The algorithm was implemented in MATLAB and all of
the experiments were run on machines The time taken can be compared
with a standard method SDPT3 partially implemented in but running under
MATLAB
We considered benchmark problems arising from semidefinite relaxations to the
MAXCUT problems of weighted graphs which is posed as finding a maximum weight
bisection of a graph The benchmark MAXCUT problems have the following relaxed
SDP form
subject to diag(C1 diag
minimise
x?Rn
F0
Fi
where Rn?n is the adjacency matrix of the graph with vertices
The benchmark used was provided by SDPLIB For this problem
and it is known that the optimal value of the objective function equals
The baseline method used the bisection approach to identify the critical
value of the objective referred to throughout this section as c0
Figure left shows a plot of the time per iteration against the value of c0 for the
first four iterations of the bisection method As can be seen from the plots the time
taken by the algorithm for each iteration is quite long with the time of the fourth
iteration being around seconds The initial value of for c0 was found
without an objective constraint and converged within secs The bisection then
started with the lower infeasible value of and the upper value of Iteration
was run with c0 but the feasible solution had an objective value of This
was found in just secs The second iteration used a value of c0 slightly
above the optimum of The third iteration was infeasible but since it was quite
far from the optimum the algorithm was able to deduce this fact quite quickly The
final iteration was also infeasible but much closer to the optimal value The running
time suffered correspondingly taking hours If we were to continue the next
iteration would also be infeasible but closer to the optimum and so would take even
longer
The first experiment demonstrated several things First that the method does indeed work as predicted secondly that the running times are very far from being
Time sec
Optimal value
Optimal value
Value of objective function
Value of objective function
Iterations
Figure Left Four iterations of the bisection method showing time taken per iteration outer for loop in Algorithm against the value of the objective constraint
Right Decay of the attained objective function value while iterating through Algorithm with a non-zero threshold of
competitive SDPT3 takes under seconds to solve this problem and thirdly that
the running times increase as the value of c0 approaches the optimum with those
iterations that must prove infeasibility being more costly than those that find a solution
The final observation prompted our first adaptation of the base algorithm Rather
than perform the search using the bisection method we implemented a non-zero
threshold on the objective constraint the while-statement in Algorithm The
value of this threshold is denoted following the notation introduced in
Using a value of ensured that when a feasible solution is found its objective
value is significantly below that of the objective constraint c0 Figure right
shows the values of c0 as a function of the outer for-loops iterations the algorithm
eventually approached its estimate of the optimal value at This is within
of the optimum though of course iterations could have been continued Despite
the clear convergence using this approach the running time to an accurate estimate
of the solution is still prohibitive because overall the algorithm took approximately
hours of CPU time to find its solution
A profile of the execution however revealed that up to of the execution time is
spent in the eigenvalue decomposition to identify Observe that we do not need a
minimal eigenvector to perform an update simply a vector satisfying
u0 G(x)u
Cholesky decomposition will either return satisfying or it will converge indicating that is psd and Algorithm has converged
Conclusions
Semidefinite programming has interesting applications in machine learning In turn
we have shown how a simple learning algorithm can be modified to solve higher
order convex optimisation problems such as semidefinite programs Although the
experimental results given here suggest the approach is far from computationally
competitive the insights gained may lead to effective algorithms in concrete applications in the same way that for example SMO is a competitive algorithm for solving
quadratic programming problems arising from support vector machines While the
optimisation setting leads to the somewhat artificial and inefficient bisection method
the positive definite perceptron algorithm excels at solving positive definite CSPs
as found in problems of transformation invariant pattern recognition as solved
by Semidefinite Programming Machines In future work it will be of interest to
consider the combined primal-dual problem at a predefined level of granularity so
as to avoid the necessity of bisection search
Acknowledgments We would like to thank J. Kandola J. Dunagan and A. Ambroladze for interesting discussions This work was supported by EPSRC under grant
number and by Microsoft Research Cambridge

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5767-high-dimensional-neural-spike-train-analysis-with-generalized-count-linear-dynamical-systems.pdf

High-dimensional neural spike train analysis with
generalized count linear dynamical systems
Lars Buesing
Department of Statistics
Columbia University
New York NY
lars@stat.columbia.edu
Yuanjun Gao
Department of Statistics
Columbia University
New York NY
yg2312@columbia.edu
Krishna V. Shenoy
Department of Electrical Engineering
Stanford University
Stanford CA
shenoy@stanford.edu
John P. Cunningham
Department of Statistics
Columbia University
New York NY
jpc2181@columbia.edu
Abstract
Latent factor models have been widely used to analyze simultaneous recordings of
spike trains from large heterogeneous neural populations These models assume
the signal of interest in the population is a low-dimensional latent intensity that
evolves over time which is observed in high dimension via noisy point-process
observations These techniques have been well used to capture neural correlations
across a population and to provide a smooth denoised and concise representation of high-dimensional spiking data One limitation of many current models
is that the observation model is assumed to be Poisson which lacks the flexibility to capture under and over-dispersion that is common in recorded neural data
thereby introducing bias into estimates of covariance Here we develop the generalized count linear dynamical system which relaxes the Poisson assumption by
using a more general exponential family for count data In addition to containing Poisson Bernoulli negative binomial and other common count distributions
as special cases we show that this model can be tractably learned by extending recent advances in variational inference techniques We apply our model to
data from primate motor cortex and demonstrate performance improvements over
state-of-the-art methods both in capturing the variance structure of the data and
in held-out prediction
Introduction
Many studies and theories in neuroscience posit that high-dimensional populations of neural spike
trains are a noisy observation of some underlying low-dimensional and time-varying signal of
interest As such over the last decade researchers have developed and used a number of methods
for jointly analyzing populations of simultaneously recorded spike trains and these techniques have
become a critical part of the neural data analysis toolkit In the supervised setting generalized
linear models GLM have used stimuli and spiking history as covariates driving the spiking of the
neural population In the unsupervised setting latent variable models have been used
to extract low-dimensional hidden structure that captures the variability of the recorded data both
temporally and across the population of neurons
In both these settings however a limitation is that spike trains are typically assumed to be conditionally Poisson given the shared signal The Poisson assumption while offering algorithmic
conveniences in many cases implies the property of equal dispersion the conditional mean and variance are equal This well-known property is particularly troublesome in the analysis of neural spike
trains which are commonly observed to be either over or under-dispersed variance greater
than or less than the mean No doubly stochastic process with a Poisson observation can capture
under-dispersion and while such a model can capture over-dispersion it must do so at the cost of
erroneously attributing variance to the latent signal rather than the observation process
To allow for deviation from the Poisson assumption some previous work has instead modeled the
data as Gaussian or using more general renewal process models the former of
which does not match the count nature of the data and has been found inferior and the latter of
which requires costly inference that has not been extended to the population setting More general
distributions like the negative binomial have been proposed 17 but again these families do
not generalize to cases of under-dispersion Furthermore these more general distributions have not
yet been applied to the important setting of latent variable models
Here we employ a count-valued exponential family distribution that addresses these needs and includes much previous work as special cases We call this distribution the generalized count
distribution and we offer here four main contributions we introduce the GC distribution and
derive a variety of commonly used distributions that are special cases using the GLM as a motivating example we combine this observation likelihood with a latent linear dynamical systems
prior to form a GC linear dynamical system GCLDS iii we develop a variational learning algorithm by extending the current state-of-the-art methods to the GCLDS setting and
we show in data from the primate motor cortex that the GCLDS model provides superior predictive
performance and in particular captures data covariance better than Poisson models
Generalized count distributions
We define the generalized count distribution as the family of count-valued probability distributions
pGC
exp(?k
k?N
k!M
where and the function parameterizes the distribution and
is the normalizing constant The primary virtue of the GC family is that it recovk=0
ers all common count-valued distributions as special cases and naturally parameterizes many common supervised and unsupervised models as will be shown for example the function
implies a Poisson distribution with rate parameter Generalizations of the Poisson
distribution have been of interest since at least and the paper introduced the GC family
and proved two additional properties first that the expectation of any GC distribution is monotonically increasing in for a fixed and second and perhaps most relevant to this study
concave convex functions imply under-dispersed over-dispersed GC distributions Furthermore often desired features like zero truncation or zero inflation can also be naturally incorporated
by modifying the value Thus with controlling the log rate of the distribution
and controlling the shape of the distribution the GC family provides a rich model class for
capturing the spiking statistics of neural data Other discrete distribution families do exist such as
the Conway-Maxwell-Poisson distribution and ordered logistic/probit regression but the
GC family offers a rich exponential family which makes computation somewhat easier and allows
the functions to be interpreted
Figure demonstrates the relevance of modeling dispersion in neural data analysis The left panel
shows a scatterplot where each point is an individual neuron in a recorded population of neurons
from primate motor cortex experimental details will be described in Plotted are the mean and
variance of spiking activity of each neuron activity is considered in bins For

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1992-spectral-relaxation-for-k-means-clustering.pdf

Spectral Relaxation for K-means
Clustering
Hongyuan Zha Xiaofeng He
Dept of Compo Sci. Eng.
The Pennsylvania State University
University Park PA
zha,xhe}@cse.psu.edu
Chris Ding Horst Simon
NERSC Division
Lawrence Berkeley National Lab.
UC Berkeley Berkeley CA
chqding,hdsimon}@lbl.gov
Ming Gu
Dept of Mathematics
UC Berkeley Berkeley CA
mgu@math.berkeley.edu
Abstract
The popular K-means clustering partitions a data set by minimizing a sum-of-squares cost function A coordinate descend method
is then used to find local minima In this paper we show that the
minimization can be reformulated as a trace maximization problem
associated with the Gram matrix of the data vectors Furthermore
we show that a relaxed version of the trace maximization problem
possesses global optimal solutions which can be obtained by computing a partial eigendecomposition of the Gram matrix and the
cluster assignment for each data vectors can be found by computing a pivoted QR decomposition of the eigenvector matrix As a
by-product we also derive a lower bound for the minimum of the
sum-of-squares cost function
Introduction
K-means is a very popular method for general clustering In K-means clusters
are represented by centers of mass of their members and it can be shown that the
K-means algorithm of alternating between assigning cluster membership for each
data vector to the nearest cluster center and computing the center of each cluster
as the centroid of its member data vectors is equivalent to finding the minimum of a
sum-of-squares cost function using coordinate descend Despite the popularity of Kmeans clustering one of its major drawbacks is that the coordinate descend search
method is prone to local minima Much research has been done on computing refined
initial points and adding explicit constraints to the sum-of-squares cost function for
K-means clustering so that the search can converge to better local minimum
In this paper we tackle the problem from a different angle we find an equivalent
formulation of the sum-of-squares minimization as a trace maximization problem
with special constraints relaxing the constraints leads to a maximization problem
that possesses optimal global solutions As a by-product we also have an easily
computable lower bound for the minimum of the sum-of-squares cost function Our
work is inspired by where connection to Gram matrix and extension of Kmeans method to general Mercer kernels were investigated
The rest of the paper is organized as follows in section we derive the equivalent
trace maximization formulation and discuss its spectral relaxation In section we
discuss how to assign cluster membership using pivoted QR decomposition taking
into account the special structure of the partial eigenvector matrix Finally in
section we illustrate the performance of the clustering algorithms using document
clustering as an example
Notation Throughout denotes the Euclidean norm of a vector The trace
of a matrix A the sum of its diagonal elements is denoted as trace(A The
Frobenius norm of a matrix IIAIIF Jtrace(AT A). In denotes identity matrix of
order
Spectral Relaxation
Given a set of m-dimensional data vectors we form the m-by-n data
matrix A A partition of the date vectors can be written in the
following form
where is a permutation matrix and Ai is m-by-si the ith cluster contains
the data vectors in A. For a given partition in the associated sum-of-squares
cost function is defined as
Si
Si
ss(II
Ila~i
s=l
i=l s=l
LL
mi is the mean vector of the data vectors in cluster Let be a vector
of appropriate dimension with all elements equal to one it is easy to see that
mi Aiel Si and
Si
SSi
Ila~i mil1 IIAi mieTII IIAi(Isi ee ISi)II
s=l
Notice that lSi ee I Si is a projection matrix and Isi ee I lSi ee lSi
it follows that
SSi
trace(Ai(Isi
ee I si)Af
trace((Isi
ee I si)AT Ai).
Therefore
ss(II
SSi
trace(AT Ai
AT Ai
Let the n-by-k orthonormal matrix be
lVsl
Sk
elVSi
The sum-of-squares cost function can now be written as
ss(II trace(A A trace(XT AT
and its minimization is equivalent to
max trace(XT AT AX
I
of the form in
REMARK Without loss of generality let I in If we let be the cluster
indicator vector
xT
Si
Then it is easy to see that
trace(XT AT AX
xT AT AXi
Ax il1
i=l
XTXi
i=l il1
Using the partition in the right-hand side of the above can be written as
a weighted sum of the squared Euclidean norms of the mean vector of each clusters
REMARK If we consider the elements of the Gram matrix AT A as measuring
similarity between data vectors then we have shown that Euclidean distance leads
to Euclidean inner-product similarity This inner-product can be replaced by a
general Mercer kernel as is done in
Ignoring the special structure of and let it be an arbitrary orthonormal matrix
we obtain a relaxed maximization problem
max trace(XT AT AX
XTX=h
It turns out the above trace maximization problem has a closed-form solution
Theorem Ky Fan Let be a symmetric matrix with eigenvalues
Al A2 An
and the corresponding eigenvectors Un]. Then
Al
Ak
max trace(XT
XTX=I
Moreover the optimal is given by Uk]Q with an arbitrary
orthogonal matrix
It follows from the above theorem that we need to compute the largest eigenvectors
of the Gram matrix AT A. As a by-product we have
min{m
minss(II trace(A A
max trace(XT AT AX
XT X=h
i=k+l
where oi(A is the largest singular value of A. This gives a lower bound for the
minimum of the sum-of-squares cost function
It is easy to see from the above derivation that we can replace A with
A aeT where a is an arbitrary vector Then we have the following lower bound
REMARK
min{m,n
mJnss(II
aeT
i=k+l
One might also try the following approach notice that
REMARK
IIAi
mi eT2
IIF
Ilaj
aj EAi aj EAi
Let
Ilai ajl12
and and Xij]j=l with
Xij
if aj Ai
otherwise
Then
ss(II
WXi min
XT ZT Z=h
i=l
ZTWZ
i=n-k+l
Unfortunately some of the smallest eigenvalues of can be negative
Let be the n-by-k matrix consisting of the largest eigenvectors of AT A. Each
row of corresponds to a data vector and the above process can be considered as
transforming the original data vectors which live in a m-dimensional space to new
data vectors which now live in a k-dimensional space One might be attempted to
compute the cluster assignment by applying the ordinary K-means method to those
data vectors in the reduced dimension space In the next section we discuss an
alternative that takes into account the structure of the eigenvector matrix
REMARK The similarity of the projection process to principal component analysis
is deceiving the goal here is not to reconstruct the data matrix using a low-rank
approximation but rather to capture its cluster structure
Cluster Assignment Using Pivoted QR Decomposition
Without loss of generality let us assume that the best partition of the data vectors in A that minimizes ss(II is given by A A each submatrix Ai
corresponding to a cluster Now write the Gram matrix of A as
ATA=[A~A
ArA
ArAk
If the overlaps among the clusters represented by the submatrices Ai are small then
the norm of will be small as compare with the block diagonal matrix in the
above equation Let the largest eigenvector of AT Ai be Yi and
AT AiYi fJiYi
then the columns of the matrix
IIYil1
span an invariant subspace of B. Let the eigenvalues and eigenvectors of AT A be
An
AT AXi AiXi
Assume that there is a gap between the two eigenvalue sets flk and
min{lfli Aj
Then Davis-Kahan theorem states that IlynXk+1
Theorem After some manipulation it can be shown that
Xk
IIEII/J
YkV O(IIEII
where is an k-by-k orthogonal matrix Ignoring the O(IIEII term we see that
cluster
cluster
where we have used Yil and VT A key observation is
that all the Vi are orthogonal to each other once we have selected a Vi we can jump
to other clusters by looking at the orthogonal complement of Vi Also notice that
IIYil1 so the elements of Yi can not be all small A robust implementation of
the above idea can be obtained as follows we pick a column of which has the
lar;est norm say it belongs to cluster we orthogonalize the rest of the columns of
against this column For the columns belonging to cluster the residual vector
will have small norm and for the other columns the residual vectors will tend to
be not small We then pick another vector with the largest residual norm and
orthogonalize the other residual vectors against this residual vector The process
can be carried out steps and it turns out to be exactly QR decomposition with
column pivoting applied to we find a permutation matrix such that
QR Q[Rl1,Rd
where is a k-by-k orthogonal matrix and Rl1 is a k-by-k upper triangular matrix
We then compute the matrix
Rj Rd pT Rj
Then the cluster membership of each data vector is determined by the row index of
the largest element in absolute value of the corresponding column of
REMARK Sometimes it may be advantageous to include more than eigenvectors
to form Xs with We can still use QR decomposition with column pivoting
to select columns of Xs to form an s-by-k matrix say Then for each column
of Xs we compute the least squares solution of argmintERk li Xtll Then
the cluster membership of is determined by the row index of the largest element
in absolute value of
Experimental Results
In this section we present our experimental results on clustering a dataset of newsgroup articles submitted to newsgroups.1 This dataset contains about
articles email messages evenly divided among the newsgroups We list the
names of the news groups together with the associated group labels
lThe newsgroup dataset together with the bow toolkit for processing it can be downloadedfrorn http www cs.cmu.edu/afs/cs/project/theo-ll/www/naive-bayes.html
p-Kmeans
Figure Clustering accuracy for five newsgroups
p-QR p-Kmeans left and p-Kmeans Kmeans right
alt.atheism
comp.graphics
comp.os.ms-vindovs.misc
comp.sys.ibm.pc.hardvare
NG5:comp.sys.mac.hardvare
comp.vindovs.x
NG7:misc.forsale
rec.autos
NG9:rec.motorcycles
rec.sport.baseball
NGll:rec.sport.hockey
sci crypt
NG13:sci.electronics
sci.med
NG15:sci.space
soc.religion.christian
NG17:talk.politics.guns
talk.politics.mideast
NG19:talk.politics.misc
talk.religion.misc
We used the bow toolkit to construct the term-document matrix for this dataset
specifically we use the tokenization option so that the UseNet headers are stripped
and we also applied stemming The following three preprocessing steps are done
we apply the usual tf.idf weighting scheme we delete words that appear too
few times we normalized each document vector to have unit Euclidean length
We tested three clustering algorithms p-QR this refers to the algorithm using
the eigenvector matrix followed by pivoted QR decomposition for cluster membership assignment p-Kmeans we compute the eigenvector matrix and then apply
K-means on the rows of the eigenvector matrix K-means this is K-means directly
applied to the original data vectors For both K-means methods we start with a set
of cluster centers chosen randomly from the projected data vectors and we aslo
make sure that the same random set is used for both for comparison To assess the
quality of a clustering algorithm we take advantage of the fact that the news group
data are already labeled and we measure the performance by the accuracy of the
clustering algorithm against the document category labels In particular for a
cluster case we compute a k-by-k confusion matrix Cij with Cij the number
of documents in cluster that belongs to newsgroup category It is actually quite
subtle to compute the accuracy using the confusion matrix because we do not know
which cluster matches which newsgroup category An optimal way is to solve the
following maximization problem
max trace(CP
IP
is a permutation matrix
and divide the maximum by the total number of documents to get the accuracy
This is equivalent to finding perfect matching a complete weighted bipartite graph
one can use Kuhn-Munkres algorithm In all our experiments we used a greedy
algorithm to compute a sub-optimal solution
Table Comparison of p-QR p-Kmeans and K-means for two-way clustering
Newsgroups
NG1/NG2
NG2/NG3
NG8/NG9
NG1/NG
p-QR
p-Kmeans
K-means
62 04
Table Comparison of p-QR p-Kmeans and K-means for multi-way clustering
Newsgroups
NG2/NG3/NG4/NG5/NG6
NG2/NG3/NG4/NG5/NG6 UOO
NG1/NG5/NG7/NG8/NG
NG1/NG5/NG7 NG8/NG
p-QR
p-Kmeans
K-means
18
48 33
In this example we look at binary clustering We choose random
document vectors each from two newsgroups We tested runs for each pair
of newsgroups and list the means and standard deviations in Table The two
clustering algorithms p-QR and p-Kmeans are comparable to each other and both
are better and sometimes substantially better han K-means
EXAMPLE
In this example we consider k-way clustering with and
Three news group sets are chosen with and random samples from each newsgroup as indicated in the parenthesis Again runs are used for each tests and the
means and standard deviations are listed in Table Moreover in Figure we also
plot the accuracy for the runs for the test
Both p-QR and p-Kmeans perform better han Kmeans For news group sets with
small overlaps p-QR performs better han p-Kmeans This might be explained by
he fact hat p-QR explores the special structure of he eigenvector matrix and is
therefore more efficient As a less thorough comparison wit the information bottleneck method used in there for runs of NG2/NG9/NGlO/NG15/NG 18
mean accuracy with maximum accuracy is obtained For runs
of the newsgroup set with samples mean accuracy with maximum
accuracy about is obtained
EXAMPLE
We compare the lower bound given in We only list a typical
sample from NG2/NG9/NGlO/NG15/NG18 The column with NG labels
indicates clustering using the newsgroup labels and by definition has accuracy
It is quite clear that the news group categories are not completely captured by
he sum-of-squares cost function because p-QR and NG labels both have higher
accuracy but also larger sum-of-squares values Interestingly it seems hat p-QR
captures some of this information of the newsgroup categories
EXAMPLE
accuracy
ssm
p-QR
p-Kmeans
K-means
NG labels
lower bound
N/A
Acknowledgments
This work was supported in part by NSF grant and by Department
of Energy through an LBL LDRD fund

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3963-learning-concept-graphs-from-text-with-stick-breaking-priors.pdf

Learning Concept Graphs from Text with
Stick-Breaking Priors
Padhraic Smyth
Department of Computer Science
University of California Irvine
Irvine CA
smyth@ics.uci.edu
America L. Chambers
Department of Computer Science
University of California Irvine
Irvine CA
ahollowa@ics.uci.edu
Mark Steyvers
Department of Cognitive Science
University of California Irvine
Irvine CA
mark.steyvers@uci.edu
Abstract
We present a generative probabilistic model for learning general graph structures
which we term concept graphs from text Concept graphs provide a visual summary of the thematic content of a collection of documents?a task that is difficult
to accomplish using only keyword search The proposed model can learn different
types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents We describe a generative
model that is based on a stick-breaking process for graphs and a Markov Chain
Monte Carlo inference procedure Experiments on simulated data show that the
model can recover known graph structure when learning in both unsupervised and
semi-supervised modes We also show that the proposed model is competitive
in terms of empirical log likelihood with existing structure-based topic models
hPAM and hLDA on real-world text data sets Finally we illustrate the application of the model to the problem of updating Wikipedia category graphs
Introduction
We present a generative probabilistic model for learning concept graphs from text We define a
concept graph as a rooted directed graph where the nodes represent thematic units called concepts
and the edges represent relationships between concepts Concept graphs are useful for summarizing
document collections and providing a visualization of the thematic content and structure of large
document sets a task that is difficult to accomplish using only keyword search An example of
a concept graph is Wikipedia?s category graph1 Figure shows a small portion of the Wikipedia
category graph rooted at the category ACHINE LEARNING2 From the graph we can quickly infer that the collection of machine learning articles in Wikipedia focuses primarily on evolutionary
algorithms and Markov models with less emphasis on other aspects of machine learning such as
Bayesian networks and kernel methods
The problem we address in this paper is that of learning a concept graph given a collection of
documents where optionally we may have concept labels for the documents and an initial graph
structure In the latter scenario the task is to identify additional concepts in the corpus that are
http://en.wikipedia.org/wiki/Category:Main topic classifications
As of May
Applied
Sciences
Software
Engineering
Mathematical
Sciences
Computer
Programming
Applied
Mathematics
Formal
Sciences
Computing
Probability and
Statistics
Philosophy
By field
Thought
Knowledge
Sharing
Algorithms
Society
Cognition
Education
Computational Statistics
Philosophy
Of mind
Artificial
Intelligence
Knowledge
Statistics
Cognitive
Science
Metaphysics
Computer
Science
Learning
Machine learning
Figure A portion of the Wikipedia category supergraph for the node ACHINE LEARNING
Machine
Learning
Bayesian
Networks
Ensemble
Learning
Classification
Algorithms
Genetic
Algorithms
Evolutionary
Algorithms
Kernel
Methods
Genetic
Programming
Interactive
Evolutionary
Computation
Learning in
Computer
Vision
Markov
Models
Markov
Networks
Statistical
Natural
Language
Processing
Figure A portion of the Wikipedia category subgraph rooted at the node ACHINE LEARNING
not reflected in the graph or additional relationships between concepts in the corpus via the cooccurrence of concepts in documents that are not reflected in the graph This is particularly suited
for document collections like Wikipedia where the set of articles is changing at such a fast rate
that an automatic method for updating the concept graph may be preferable to manual editing or
re-learning the hierarchy from scratch The foundation of our approach is latent Dirichlet allocation
LDA LDA is a probabilistic model for automatically identifying topics within a document
collection where a topic is a probability distribution over words The standard LDA model does
not include any notion of relationships or dependence between topics In contrast methods such
as the hierarchical topic model hLDA learn a set of topics in the form of a tree structure The
restriction to tree structures however is not well suited for large document collections like Wikipedia
Figure gives an example of the highly non-tree like nature of the Wikipedia category graph The
hierarchical Pachinko allocation model hPAM is able to learn a set of topics arranged in a fixedsized graph with a nonparametric version introduced in The model we propose in this paper is
a simpler alternative to hPAM and nonparametric hPAM that can achieve the same flexibility
learning arbitrary directed acyclic graphs over a possibly infinite number of nodes within a simpler
probabilistic framework In addition our model provides a formal mechanism for utilizing labeled
data and existing concept graph structures Other methods for creating concept graphs include the
use of techniques such as hierarchical clustering pattern mining and formal concept analysis to
construct ontologies from document collections Our approach differs in that we utilize
a probabilistic framework which enables us for example to make inferences about concepts and
documents Our primary novel contribution is the introduction of a flexible probabilistic framework
for learning general graph structures from text that is capable of utilizing both unlabeled documents
as well as labeled documents and prior knowledge in the form of existing graph structures
In the next section we introduce the stick-breaking distribution and show how it can be used as
a prior for graph structures We then introduce our generative model and explain how it can be
adapted for the case where we have an initial graph structure We derive collapsed Gibbs sampling
equations for our model and present a series of experiments on simulated and real text data We
compare our performance against hLDA and hPAM as baselines We conclude with a discussion of
the merits and limitations of our approach
Stick-breaking Distributions
Stick-breaking distributions are discrete probability distributions of the form
where
and is the delta function centered at the atom The variables are sampled independently
from a base distribution where is assumed to be continuous The stick-breaking weights
have the form
v1
vj
vk for
where the vj are independent Beta(?j random variables Stick-breaking distributions derive
their name from the analogy of repeatedly breaking the remainder of a unit-length stick at a randomly
chosen breakpoint See for more details
Unlike the Chinese restaurant process the stick-breaking process lacks exchangeability The probability of sampling a particular cluster from given the sequences and vj is not equal
to the probability of sampling the same cluster given a permutation of the sequences and
This can be seen in Equation where the probability of sampling depends upon the
value of the proceeding Beta random variables v2 If we fix and permute
every other atom then the probability of sampling changes it is now determined by the Beta
random variables
The stick-breaking distribution can be utilized as a prior distribution on graph structures We construct a prior on graph structures by specifying a distribution at each node denoted as Pt that
governs the probability of transitioning from node to another node in the graph There is some
freedom in choosing Pt however we have two constraints First making a new transition must have
non-zero probability In Figure it is clear that from ACHINE EARNING we should be able to
transition to any of its children However we may discover evidence for passing directly to a leaf
node such as TATISTICAL NATURAL ANGUAGE ROCESSING if we observe new articles
related to statistical natural language processing that do not use Markov models Second making
a transition to a new node must have non-zero probability For example we may observe new articles related to the topic of Bioinformatics In this case we want to add a new node to the graph
IOINFORMATICS and assign some probability of transitioning to it from other nodes
With these two requirements we can now provide a formal definition for Pt We begin with an
initial graph structure G0 with nodes For each node we define a feasible set Ft as the
collection of nodes to which can transition The feasible set may contain the children of node or
possible child nodes of node as discussed above In general Ft is some subset of the nodes in
G0 We add a special node called the exit node to Ft If we sample the exit node then we exit
from the graph instead of transitioning forward We define Pt as a stick-breaking distribution over
the finite set of nodes Ft where the remaining probability mass is assigned to an infinite set of new
nodes nodes that exist but have not yet been observed The exact form of Pt is shown below
Pt
Ft
tj ftj
tj xtj
j=|Ft
The first Ft atoms of the stick-breaking distribution are the feasible nodes ftj Ft The remaining
atoms are unidentifiable nodes that have yet to be observed denoted as xtj for simplicity
This is not yet a working definition unless we explicitly state which nodes are in the set Ft Our
model does not in general assume any specific form for Ft Instead the user is free to define it as
they like In our experiments we first assign each node to a unique depth and then define Ft as any
node at the next lower depth The choice of Ft determines the type of graph structures that can be
learned For the choice of Ft used in this paper edges that traverse multiple depths are not allowed
and edges between nodes at the same depth are not allowed This prevents cycles from forming
and allows inference to be performed in a timely manner More generally one could extend the
definition of Ft to include any node at a lower depth
For node
Sample stick-break weights vtj Beta
ii Sample word distribution Dirichlet
For document
Sample a distribution over levels Beta(a,b
ii Sample path pd Pt
iii For word Nd
Sample level ld,i TruncatedDiscrete(?d
Generate word xd,i ld,i Multinomial(?pd ldi
Figure Generative process for GraphLDA
Due to a lack of exchangeability we must specify the stick-breaking order of the elements in Ft
Note that despite the order the elements of Ft always occur before the infinite set of new nodes in
the stick-breaking permutation We use a Metropolis-Hastings sampler proposed by to learn
the permutation of feasible nodes with the highest likelihood given the data
Generative Process
Figure shows the generative process for our proposed model which we refer to as GraphLDA
We observe a collection of documents where document has Nd words As discussed
earlier each node is associated with a stick-breaking prior Pt In addition we associate with each
node a multinomial distribution over words in the fashion of topic models
A two-stage process is used to generate document First a path through the graph is sampled
from the stick-breaking distributions We denote this path as pd The 1st node in the path
is sampled from Ppdi which is the stick-breaking distribution at the ith node in the path This
process continues until an exit node is sampled Then for each word a level in the path ldi is
sampled from a truncated discrete distribution The word is generated by the topic at level ldi
of the path pd which we denote as pd ldi In the case where we observe labeled documents and an
initial graph structure the paths for document is restricted to end at the concept label of document
One possible option for the length distribution is a multinomial distribution over levels We take
a different approach and instead use a parametric smooth form The motivation is to constrain the
length distribution to have the same general functional form across documents contrast to the relatively unconstrained multinomial but to allow the parameters of the distribution to be documentspecific We considered two simple options Geometric and Poisson both truncated to the number
of possible levels In initial experiments the Geometric performed better than the Poisson so the
Geometric was used in all experiments reported in this paper If word xdi has level ldi then the
word is generated by the topic at the last node on the path and successive levels correspond to earlier
nodes in the path In the case of labeled documents this matches our belief that a majority of words
in the document should be assigned to the concept label itself
Inference
We marginalize over the topic distributions and the stick-breaking weights vtj We use a
collapsed Gibbs sampler to infer the path assignment pd for each document the level distribution
parameter for each document and the level assignment ldi for each word Of the five hyperparameters in the model inference is sensitive to the value of and so we place an Exponential
prior on both and use a Metropolis-Hastings sampler to learn the best setting
Sampling Paths
For each document we must sample a path pd conditioned on all other paths p?d the level variables
and the word tokens We only consider paths whose length is greater than or equal to the maximum
level of the words in the document
p(pd p?d p(xd p(pd
The first term in Equation is the probability of all words in the document given the path pd We
compute this probability by marginalizing over the topic distributions
Np?d
Npd
p(xd
Npd
Np
We use to denote the length of path pd The notation Npd stands for the number of times
word type has been assigned to node pd The superscript means we first decrement the count
Npd for every word in document
The second term is the conditional probability of the path pd given all other paths p?d We present
the sampling equation under the assumption that there is a maximum number of nodes allowed
at each level We first consider the probability of sampling a single edge in the path from a node
to one of its feasible nodes y2 yM where the node y1 has the first position in the stickbreaking permutation y2 has the second position y3 the third and so on
We denote the number of paths that have gone from to as N(x,yi We denote the number of
paths that have gone from to a node with a strictly higher position in the stick-breaking distribution
PM
than as N(x,>yi That is N(x,>yi N(x,yk Extending this notation we denote the
sum N(x,yi N(x,>yi as N(x,?yi The probability of selecting node is given by
p(x p?d
N(x,>yr
N(x,yi
N(x,?yi N(x,?yr
for
If ym is the last node with a nonzero count N(x,ym and it is convenient to compute the
probability of transitioning to for and the probability of transitioning to any node higher
than ym The probability of transitioning to a node higher than ym is given by
p(x yk
Qm
where
A similar derivation can be used to compute the probability of
sampling a node higher than ym when is equal to infinity Now that we have computed the
probability of a single edge we can compute the probability of an entire path pd
p(pd
p(pdj
Sampling Levels
For the ith word in the dth document we must sample a level ldi conditioned on all other levels l?di
the document paths the level parameters and the word tokens
Np?di
ldi
ldi xdi
p(ldi l?di
Np?di
di
The first term is the probability of word type xdi given the topic at node pd ldi The second term is
the probability of the level ldi given the level parameter
Sampling Variables
Finally we must sample the level distribution conditioned on the rest of the level parameters
the level variables and the word tokens
Nd
ldi
a
24
26
Learned Graph labeled documents
Simulated Graph
Learned Graph labeled documents
Learned Graph labeled documents
Figure Learning results with simulated data
Due to the normalization constant Equation is not a recognizable probability
distribution and we must use rejection sampling Since the first term in Equation is always less
than or equal to the sampling distribution is dominated by a Beta(a distribution According
to the rejection sampling algorithm we sample a candidate value for from Beta(a and either
QNd ldi
accept with probability
or reject and sample again
Metropolis Hastings for Stick-Breaking Permutations
In addition to the Gibbs sampling we employ a Metropolis Hastings sampler presented in to
mix over stick-breaking permutations Consider a node with feasible nodes y2 yM We
sample two feasible nodes and yj from a uniform distribution3 Assume comes before yj in
the stick-breaking distribution Then the probability of swapping the position of nodes and yj is
given by
N(x,y
N(x,yj
N(x,>yj
min
N(x,>yj
where
N(x,>yi N(x,yj See for a full derivation After every new path assigni
ment we propose one swap for each node in the graph
Experiments and Results
In this section we present experiments performed on both simulated and real text data We compare
the performance of GraphLDA against hPAM and hLDA
Simulated Text Data
In this section we illustrate how the performance of GraphLDA improves as the fraction of labeled
data increases Figure shows a simulated concept graph with nodes drawn according to the
In feasible nodes are sampled from the prior probability distribution However for small values of
and this results in extremely slow mixing
stick-breaking generative process with parameter values a and
The vocabulary size is words and we generate documents with words each
Each edge in the graph is labeled with the number of paths that traverse it
Figures show the learned graph structures as the fraction of labeled data increases from
labeled and unlabeled documents to all documents being labeled In addition to
labeling the edges we label each node based upon the similarity of the learned topic at the node to
the topics of the original graph structure The Gibbs sampler is initialized to a root node when there
is no labeled data With labeled data the Gibbs sampler is initialized with the correct placement of
nodes to levels The sampler does not observe the edge structure of the graph nor the correct number
of nodes at each level the sampler may add additional nodes With no labeled data the sampler
is unable to recover the relationship between concepts and due to the relatively small number
of documents that contain words from both concepts With labeled documents the sampler is
able to learn the correct placement of both nodes and although the topics contain some noise
Wikipedia Articles
In this section we compare the performance of GraphLDA to hPAM and hLDA on a set of
machine-learning articles taken from Wikipedia The input to each model is only the article text All
models are restricted to learning a three-level hierarchical structure For both GraphLDA and hPAM
the number of nodes at each level was set to For GraphLDA the parameters were fixed at
a and The parameters and were initialized to and respectively and optimized
using a Metropolis Hastings sampler We used the MALLET toolkit implementation of hPAM4 and
hLDA For hPAM we used different settings for the topic hyperparameter 01
For hLDA we set and considered where is the smoothing parameter for the
Chinese restaurant process and where is the smoothing over levels in the graph
All models were run for iterations to ensure burn-in and samples were taken every iterations thereafter for a total of iterations The performance of each model was evaluated
on a hold-out set consisting of of the articles using both empirical likelihood and the left-toright evaluation algorithm Sections and of which are measures of generalization
to unseen data For both GraphLDA and hLDA we use the distribution over paths that was learned
during training to compute the per-word log likelihood For hPAM we compute the MLE estimate of
the Dirichlet hyperparameters for both the distribution over super-topics and the distributions over
sub-topics from the training documents Table shows the per-word log-likelihood for each model
averaged over the ten samples GraphLDA is competitive when computing the empirical log likelihood We speculate that GraphLDA?s lower performance in terms of left-to-right log-likelihood is
due to our choice of the geometric distribution over levels and our choice to position the geometric distribution at the last node of the path and that a more flexible approach could result in better
performance
Table Per-word log likelihood of test documents
Model
Parameters
Empirical LL Left-to-Right LL
GraphLDA MH opt
hPAM
01
hLDA
set
data
learning
concept
model
network
neural
neuron
cnn
function
genetic
fitness
mutation
selection
solution
Markov
time
probability
chain
distribution
graph
Markov
network
random
field
evolution
evolutionary
algorithm
individual
search
word
topic
language
model
document
variables
node
network
parent
Bayesian
model
multitask
inference
Bayesian
Dirichlet
learning
data
model
method
kernel
model
noise
algorithm
hidden
training
learning
policy
decision
graph
function
decision
classification
class
classifier
data
clustering
data
principal
component
kmeans
learning
dimensionality
classification
reduction
method
model
selection
rbm
algorithm
feature
kernel
linear
space
vector
point
learning
algorithm
kernel
convex
constraint
algorithm
svm
vector
problem
multiclass
classifier
boosting
ensemble
hypothesis
margin
Figure Wikipedia graph structure with additional machine learning abstracts The edge widths
correspond to the probability of the edge in the graph
Wikipedia Articles with a Graph Structure
In our final experiment we illustrate how GraphLDA can be used to update an existing category
graph We use the aforementioned machine-learning Wikipedia articles along with their category labels to learn topic distributions for each node in Figure The sampler is initialized with
the correct placement of nodes and each document is initialized to a random path from the root to
its category label After iterations we fix the path assignments for the Wikipedia articles
and introduce a new set of documents We use a collection of machine learning abstracts from
the International Conference on Machine Learning ICML We sample paths for the new collection of documents keeping the paths from the Wikipedia articles fixed The sampler was allowed
to add new nodes to each level to explain any new concepts that occurred in the ICML text set
Figure illustrates a portion of the final graph structure The nodes in bold are the original nodes
from the Wikipedia category graph The results show that the model is capable of augmenting an
existing concept graph with new concepts clustering support vector machines SVMs etc
and learning meaningful relationships boosting/ensembles are on the same path as the concepts
for SVMs and neural networks
Discussion and Conclusion
Motivated by the increasing availability of large-scale structured collections of documents such as
Wikipedia we have presented a flexible non-parametric Bayesian framework for learning concept
graphs from text The proposed approach can combine unlabeled data with prior knowledge in the
form of labeled documents and existing graph structures Extensions such as allowing the model
to handle multiple paths per document are likely to be worth pursuing In this paper we did not
discuss scalability to large graphs which is likely to be an important issue in practice Computing
the probability of every path during sampling where the number of graphs is a product over the
number of nodes at each level is a computational bottleneck in the current inference algorithm and
will not scale Approximate inference methods that can address this issue should be quite useful in
this context
Acknowledgements
This material is based upon work supported in part by the National Science Foundation under Award
Number by a Microsoft Scholarship and by a Google Faculty Research award
The authors would also like to thank Ian Porteous and Alex Ihler for useful discussions
MALLET implements the exit node version of hPAM

<<----------------------------------------------------------------------------------------------------------------------->>

